# # These are the full sweep configurations for the Attention-based RL-MIL models with different subsets and architectures.
# method: bayes
# metric:
#   name: eval/avg_mil_loss
#   goal: minimize
# parameters:
#   hdim:
#     distribution: constant
#     value: 8
#   epochs:
#     distribution: constant
#     value: 200
#   actor_learning_rate:
#     distribution: log_uniform_values
#     min: 1.0e-6
#     max: 1.0e-2
#   critic_learning_rate:
#     distribution: constant
#     value: 0
#   learning_rate:
#     distribution: constant
#     value: 1.0e-6
#   reg_coef:
#     distribution: uniform
#     min: 0.0
#     max: 1.0
#   early_stopping_patience:
#     distribution: constant
#     value: 25
#   batch_size:
#     distribution: constant
#     value: 128
#   temperature:
#     distribution: uniform
#     min: 0.0
#     max: 10
#   attention_size:
#     distribution: categorical
#     values: [16, 32, 64, 128]
#   attention_dropout_p:
#     distribution: uniform
#     min: 0.0
#     max: 0.5
# run_cap: 50


# These are the grid search configurations for the specific models with their respective architectures.
method: grid
metric:
  name: eval/avg_mil_loss
  goal: minimize

parameters:
# # Aggregated_subset MeanMLP 12291866
#   actor_learning_rate:
#     distribution: constant
#     value: 0.00012473486748358433
#   reg_coef:
#     distribution: constant
#     value: 0.585823055105611
#   temperature:
#     distribution: constant
#     value: 8.059741147627053
#   attention_size:
#     distribution: constant
#     value: 128
#   attention_dropout_p:
#     distribution: constant
#     value: 0.31886003932184714

# # Aggregated_subset MaxMLP 12294385
#   actor_learning_rate:
#     distribution: constant
#     value: 8.23952511958619e-05
#   reg_coef:
#     distribution: constant
#     value: 0.9771346942073812
#   temperature:
#     distribution: constant
#     value: 0.6711999536948898
#   attention_size:
#     distribution: constant
#     value: 16
#   attention_dropout_p:
#     distribution: constant
#     value: 0.48287919978974

# # Aggregated_subset AttentionMLP 12294387
#   actor_learning_rate:
#     distribution: constant
#     value: 0.0009424676280443848
#   reg_coef:
#     distribution: constant
#     value: 0.3439211985983467
#   temperature:
#     distribution: constant
#     value: 7.807175854988357
#   attention_size:
#     distribution: constant
#     value: 32
#   attention_dropout_p:
#     distribution: constant
#     value: 0.458900743063866

# # Aggregated_subset repset 12294412
#   actor_learning_rate:
#     distribution: constant
#     value: 7.511369475558717e-06
#   reg_coef:
#     distribution: constant
#     value: 0.7217938900938876
#   temperature:
#     distribution: constant
#     value: 7.071126904745064
#   attention_size:
#     distribution: constant
#     value: 128
#   attention_dropout_p:
#     distribution: constant
#     value: 0.30883837061971753

# # Full_subset MeanMLP 12295170
#   actor_learning_rate:
#     distribution: constant
#     value: 3.935883492451109e-05
#   reg_coef:
#     distribution: constant
#     value: 0.2715301381065949
#   temperature:
#     distribution: constant
#     value: 8.672675370608037
#   attention_size:
#     distribution: constant
#     value: 128
#   attention_dropout_p:
#     distribution: constant
#     value: 0.20754806226949635


# # Full_subset MaxMLP 12295169
#   actor_learning_rate:
#     distribution: constant
#     value: 1.4736440107713691e-05
#   reg_coef:
#     distribution: constant
#     value: 0.15741711125778468
#   temperature:
#     distribution: constant
#     value: 3.847688657882075
#   attention_size:
#     distribution: constant
#     value: 32
#   attention_dropout_p:
#     distribution: constant
#     value: 0.34036963791248664

# # Full_subset Attention 12295168
#   actor_learning_rate:
#     distribution: constant
#     value: 0.0028804333595408827
#   reg_coef:
#     distribution: constant
#     value: 0.5393138034682152
#   temperature:
#     distribution: constant
#     value: 5.849946786310144
#   attention_size:
#     distribution: constant
#     value: 128
#   attention_dropout_p:
#     distribution: constant
#     value: 0.4767835304430022

# Full_subset repset 12295163
  actor_learning_rate:
    distribution: constant
    value: 3.398780970920555e-06
  reg_coef:
    distribution: constant
    value: 0.1274281737346934
  temperature:
    distribution: constant
    value: 0.504322182354342
  attention_size:
    distribution: constant
    value: 16
  attention_dropout_p:
    distribution: constant
    value: 0.2778041031882616

  # --- Other fixed parameters ---
  epochs:
    distribution: constant
    value: 800
  hdim:
    distribution: constant
    value: 8
  critic_learning_rate:
    distribution: constant
    value: 0
  learning_rate:
    distribution: constant
    value: 1.0e-6
  early_stopping_patience:
    distribution: constant
    value: 100
  batch_size:
    distribution: constant
    value: 128