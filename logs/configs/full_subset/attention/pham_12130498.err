wandb: Agent Starting Run: tmghv61g with config:
wandb: 	actor_learning_rate: 8.183053463181886e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 197
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9061640147326836
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041753-tmghv61g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tmghv61g
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json
wandb: uploading history steps 186-197, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▅███
wandb: best/eval_avg_mil_loss ▄▂██▃▁▁
wandb:  best/eval_ensemble_f1 ▁▃▅▅███
wandb:            eval/avg_f1 ▅▄▂▄▃█▃▇▄▄▃▃▄▅▄▃▂▇▄▅▆▇▁█▂▄▁▅▅▆▃▄▇▅▇█▅▇▇▅
wandb:      eval/avg_mil_loss ▄▅▃▃▃▂▄▆▅▅▃█▃▄▅▂▂▄▃▄▃▄▄▅▅▂▃▃▂▃▅▄▅▃▃▃▃▂▁▆
wandb:       eval/ensemble_f1 ▃▅█▇▄▁▄▄▄▅▄█▅▄▄▇▁▄▁▄▅▅▄▆▄▅▄▅▄█▅▇▄▇▇▇▅▅▅▇
wandb:           train/avg_f1 ▇▂▄▄▃▄▄▁▄▂▇▄▂▅▅▁▄▂▆▅▃▅▁▃▅▅▆▄██▄▁▇▃▄▄▅▅▄▇
wandb:      train/ensemble_f1 ▄▂▃▅▂▂▅▃▃▅▂▃▅▂▂▄▇▁▅▇▆▇█▄▅▃▄▅▄▅▄▇▄▄▄▆▅▇▆▁
wandb:         train/mil_loss ▅▆▂▇▅▆▂▃▄▁▄▆▆▄▄▅▃▂▅▃▇▃▅▂▆▄▃▃▆▁▁▂▄█▃▃▃▅▄▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▁▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83697
wandb: best/eval_avg_mil_loss 0.79615
wandb:  best/eval_ensemble_f1 0.83697
wandb:            eval/avg_f1 0.74571
wandb:      eval/avg_mil_loss 0.90628
wandb:       eval/ensemble_f1 0.74571
wandb:           train/avg_f1 0.73874
wandb:      train/ensemble_f1 0.73874
wandb:         train/mil_loss 0.65097
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sparkling-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tmghv61g
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041753-tmghv61g/logs
wandb: ERROR Run tmghv61g errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8sc5dms7 with config:
wandb: 	actor_learning_rate: 3.128973254728881e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 97
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7228414776924362
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042119-8sc5dms7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8sc5dms7
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄▇▇▇▇█
wandb: best/eval_avg_mil_loss ▇▅▅▆██▃▂▁
wandb:  best/eval_ensemble_f1 ▁▃▃▄▇▇▇▇█
wandb:            eval/avg_f1 ▃▄▇▄▇▃▆▄▇▇▇▃▁▆▅▅█▇▆▆▄█▇▆▅▆█▄▃▅▇▄▇▄▄▃▅▁█▂
wandb:      eval/avg_mil_loss ▅▄▆▃▂▆▃▁▅▅▃▄▇▅▅▇▂▁▆▆▁▃▄▂▁▄▄▃▅▅▇▂▃▃▃▁▃█▄▂
wandb:       eval/ensemble_f1 ▁▄▄▇▅▃▇▇▇▇▇██▄▇▆█▇▄█▄▆▆▆▇▆▆▆█▄▇▃▇▃▆▄▄▆▅█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▂▄▃▁▇▅▇▄▅▅▆▆▄▂▃▁▇▆█▆▆▆▇▆▅▄▄▂▂█▇▅▇▆▄▄▆▄
wandb:      train/ensemble_f1 ▅▅▅▅▇▂▅▅▃▃▁▄▆▄▃▅▆▆▆▆▄▅▁▇▆█▇▃▆▆█▇▆▅▅▆▄█▂▃
wandb:         train/mil_loss ▃▆▆▃▄▂▄▄▃▆▄▂█▄▃▂▅▅▅▁▁▁▃▄▅▅▆▃▅▄▅▃▃▄▂▄▅▅▂▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77262
wandb: best/eval_avg_mil_loss 0.76713
wandb:  best/eval_ensemble_f1 0.77262
wandb:            eval/avg_f1 0.43827
wandb:      eval/avg_mil_loss 0.95597
wandb:       eval/ensemble_f1 0.43827
wandb:            test/avg_f1 0.55253
wandb:      test/avg_mil_loss 0.84701
wandb:       test/ensemble_f1 0.55253
wandb:           train/avg_f1 0.61737
wandb:      train/ensemble_f1 0.61737
wandb:         train/mil_loss 0.96808
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lunar-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8sc5dms7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042119-8sc5dms7/logs
wandb: Agent Starting Run: y9yczkmc with config:
wandb: 	actor_learning_rate: 0.0004093592216090446
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 79
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.14269761607415243
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042302-y9yczkmc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y9yczkmc
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▇▇██
wandb: best/eval_avg_mil_loss █▅▃▃▂▁
wandb:  best/eval_ensemble_f1 ▁▃▇▇██
wandb:            eval/avg_f1 ▃▂▅▂▃▃▃▇▂▅▃▁▂▇▃▂▅▇▂▅▅▆▄▄▄▃█▄▅▂▃▄▆▅▅▆█▅▅▇
wandb:      eval/avg_mil_loss ▆▅▄▆█▅▄▃▇▄▆▃▅▁▅▂▄▇▄▅▄▄▃▄▃▇▄▅▄▄▄▄▆▄▄▅▃▄▃▂
wandb:       eval/ensemble_f1 ▃▄▄▁▂▂▇▂▅▄▇▆▃▂▄▂▃▅▆▄▆▄▄▄█▅▄▁▂▆▅▅█▆▄▆▁▄▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▃▃▂▂▄▂▂▁▂▅▅▅▅▄▄▅▆▅▄▄▄▅▁▄▅▆▄▅▄▆▇█▆▆▅▇▅▆▄
wandb:      train/ensemble_f1 ▃▃▂▄▂▃▁▂▅▄▅▄▃▅▄▅▅▃▄▅▅▄▅▄▄▅▁▄▄▅▄█▆▆▆▆▅▇██
wandb:         train/mil_loss ▇▅▅█▅▅▅▇▄▄▆▄▅▃▃▄▆▃▄▅▃▃▂▅▄▂▄▂▅▄▁▁▂▂▂▃▃▂▂▂
wandb:      train/policy_loss ▇▅▅▅▅▅▅▅▅▃▅▅▅▄▆▅▅▄▅▅▃▅▅▅▅▄█▅▅▅▃▅▅▅▁▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▄▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▂▃▁▃▃▃▃▃▃▃█▃▃▃▁▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77923
wandb: best/eval_avg_mil_loss 0.68143
wandb:  best/eval_ensemble_f1 0.77923
wandb:            eval/avg_f1 0.76999
wandb:      eval/avg_mil_loss 0.6585
wandb:       eval/ensemble_f1 0.76999
wandb:            test/avg_f1 0.79078
wandb:      test/avg_mil_loss 0.49598
wandb:       test/ensemble_f1 0.79078
wandb:           train/avg_f1 0.75844
wandb:      train/ensemble_f1 0.75844
wandb:         train/mil_loss 0.58513
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run olive-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y9yczkmc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042302-y9yczkmc/logs
wandb: Agent Starting Run: do0wxj9u with config:
wandb: 	actor_learning_rate: 0.0004436808120775917
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 93
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.14399557390090223
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042425-do0wxj9u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/do0wxj9u
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 91-94, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▇██
wandb: best/eval_avg_mil_loss ▃▁██▆▇
wandb:  best/eval_ensemble_f1 ▁▂▃▇██
wandb:            eval/avg_f1 ▆▆█▅█▄▆▇▅▆▆▅█▆▃▆▆▃█▃▃▆▅▃▅▄▅▄▅▅▆▅▆▆▅▆▇▁▆▅
wandb:      eval/avg_mil_loss ▄▅▅▇▂▃▃▃▁▆▆▄▃▂▁█▃█▄▆▂▂▂▂▅▅▄▂▅▃▂▂▂▂▃▂▂▇▁▃
wandb:       eval/ensemble_f1 ▄▇▁▄▅▄▃▅▇▄█▅▆▆▃▅▇█▁▃▁▄▆█▅▆▂█▅▄▅▄▆▅▆▆▆▅▆▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▆▄▅▆▃▄▆▅▅▅▅▅▅▃▄▄▄▅▂▄█▆▆▆▃▇▆▁▅▇▅▃▆▅▅▅▃▄▅
wandb:      train/ensemble_f1 █▇▅▄▅▆▇▂▆▆▆▅█▆▆▆▆▃▃▂▆▄▁▄▄▄▇█▅▃▅█▆▅▃▆▅▇▄▆
wandb:         train/mil_loss ▃▅▂▄▃▃▄▃▂▂▆▃▃▄█▄▅▁▄▄▁▂▄▂▄▂▅▅▄▄▂▂▃▅▂▄▁▃▃▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▁▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82127
wandb: best/eval_avg_mil_loss 1.0712
wandb:  best/eval_ensemble_f1 0.82127
wandb:            eval/avg_f1 0.73644
wandb:      eval/avg_mil_loss 0.903
wandb:       eval/ensemble_f1 0.73644
wandb:            test/avg_f1 0.86435
wandb:      test/avg_mil_loss 0.67687
wandb:       test/ensemble_f1 0.86435
wandb:           train/avg_f1 0.7395
wandb:      train/ensemble_f1 0.7395
wandb:         train/mil_loss 0.86423
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fiery-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/do0wxj9u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042425-do0wxj9u/logs
wandb: Agent Starting Run: ze1lcut0 with config:
wandb: 	actor_learning_rate: 2.8741824090481746e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 152
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.13324131499936165
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042603-ze1lcut0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ze1lcut0
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▇▇███
wandb: best/eval_avg_mil_loss █▃▂▄▁▆▃▃▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆▇▇███
wandb:            eval/avg_f1 ▄█▇▄▆▆▅▇▄▅▃▆▁▅▇▆▅▅▇▇▆▇▅▂▅▅▁▂▆▆▆▆▄▆▅▇▄▃█▆
wandb:      eval/avg_mil_loss ▄▄▃▃▁▃█▆▄▄▃▂▅▄▄▆▄▆▄▃▂▅▃▅▄▃▂▇▃▅▃▂▃▃▃▁▁▃▂▁
wandb:       eval/ensemble_f1 ▆▅▅▆▆▇▃▆▅▄▅▂█▇▅▁▅▆▅▄▅▇▇▅▃▆▃▃▆▇▇▇▆▆█▇▂▆█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▂▇▆▅▅▆▆▄▅▇▁▃▄▅▆▇▇▃▅▆▄▇▆▆▇▅█▆▃▅▇▄▅▅▇▅▇█▆
wandb:      train/ensemble_f1 ▁▄▃▆▅▅▅▅▅▆▂▇▃█▅▆▆▁▃▆▆▄▄▆▆▅▅▅▁▄▂▆▅▄▄▂▇▅▇▆
wandb:         train/mil_loss ▅▅▄▅▆▅▅▅▇▄▇▁█▄█▄▅▄▆▇▅▃▅▄▄▅▇▂▅▅▃▆▆▅▃▃▄▄▃▃
wandb:      train/policy_loss ▆█▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁███████████████████████▂█████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79697
wandb: best/eval_avg_mil_loss 0.77449
wandb:  best/eval_ensemble_f1 0.79697
wandb:            eval/avg_f1 0.67853
wandb:      eval/avg_mil_loss 0.68924
wandb:       eval/ensemble_f1 0.67853
wandb:            test/avg_f1 0.85203
wandb:      test/avg_mil_loss 0.5477
wandb:       test/ensemble_f1 0.85203
wandb:           train/avg_f1 0.73051
wandb:      train/ensemble_f1 0.73051
wandb:         train/mil_loss 0.80959
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wild-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ze1lcut0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042603-ze1lcut0/logs
wandb: Agent Starting Run: rfl9ql3w with config:
wandb: 	actor_learning_rate: 2.645944739094576e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 83
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1595728951515334
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042837-rfl9ql3w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rfl9ql3w
wandb: uploading wandb-summary.json
wandb: uploading history steps 77-84, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▇▇▇█
wandb: best/eval_avg_mil_loss ▁█▇▇▆▅
wandb:  best/eval_ensemble_f1 ▁▁▇▇▇█
wandb:            eval/avg_f1 ▅▅▅█▄▅▄▅▅▆▅▇▇▅▆▇▇▄▅▆▇▇▅▂▁▇▄█▇▇▆▅▅▆█▅▄▅▂▄
wandb:      eval/avg_mil_loss ▅▄▃▆▆▅▄▇▂▅▂▄▅▅▆▇▇█▅▅▅▅▅▅▂▇▆▄▂▁▃▅▅▄▅▆▄▃▆▃
wandb:       eval/ensemble_f1 ▅▅▇▄▅▅▅▆▄▅▇▆▆▅▅▆▅▂▅▇▅▆▇▇▆▁█▆█▇▆▇▆▅▆▅▇▅▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▆▄▇▂▂▇▅▅▇▁█▄▆█▆█▇▆▃▇▇▅▄▅▅▄▆▅▅▇▅█▇▅█▄▃▇█
wandb:      train/ensemble_f1 ▇▂▃▄▂▆▂▃▁█▄▃▅▄▆▇▆▄▇▇▃▄▇▂▄▅▃▄▁▄▆▅▆▅▆▄▆▅█▅
wandb:         train/mil_loss ▃▁▃▆▆▆▃█▃▁▅▆▅▇▂▁▇▆▁▃▃▃▃█▃▄▄▃▂▂▃▃▅▁▃▂▃▂▃▃
wandb:      train/policy_loss ████▁███████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▁████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83188
wandb: best/eval_avg_mil_loss 0.88732
wandb:  best/eval_ensemble_f1 0.83188
wandb:            eval/avg_f1 0.50776
wandb:      eval/avg_mil_loss 0.85256
wandb:       eval/ensemble_f1 0.50776
wandb:            test/avg_f1 0.73895
wandb:      test/avg_mil_loss 0.37197
wandb:       test/ensemble_f1 0.73895
wandb:           train/avg_f1 0.64199
wandb:      train/ensemble_f1 0.64199
wandb:         train/mil_loss 0.85464
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dutiful-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rfl9ql3w
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042837-rfl9ql3w/logs
wandb: Agent Starting Run: gwdv57c5 with config:
wandb: 	actor_learning_rate: 2.230754742063651e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 160
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.029708684407015573
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043005-gwdv57c5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gwdv57c5
wandb: uploading history steps 156-160, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇█
wandb: best/eval_avg_mil_loss ▁█▆▆
wandb:  best/eval_ensemble_f1 ▁▂▇█
wandb:            eval/avg_f1 ▆▇▇▅▅▅▇▅▄▇▅▃▇▅▃▅▄▆▅▅▅█▇▇▇▆▆▇▆▇▇▁▇▅▄▅▆▅▃█
wandb:      eval/avg_mil_loss ▄▃▆▆▃▆▆▄▃▄▅▂█▄▄▄▄▇▁▃▃▃▇▄▃▃▇▅▃▅▂▇▃▁▂▃▄▅▅▆
wandb:       eval/ensemble_f1 ▇▆▇▆▇█▅▁▆██▆▆▇▆▆▆▇█▆▆▇▆▆▇▇▇▇█▆▇▅▇▆▆▆▇▄█▅
wandb:           train/avg_f1 ▃▅▄▅▄▅▅▁▅▆▃▅▆▃▇▇▄▆▆▇▄█▄▅▇▄▅▅▅▅▅▄▅▇▄▆▇▇▅▇
wandb:      train/ensemble_f1 ▁▂▂▃▃▂▂▂▃▅▄▄▃▆▃▁▅▄▅▄▄▃▅▄▄▃▅▃▄▄▆▃▃▅▄▅█▅▇▅
wandb:         train/mil_loss ▆▇▆▅▆█▆▆▄▅▄▇▆▇▄▆█▅▃▃▁▅▄▇▆▃▅▅▅▆▂▃▃▄▅▆▃▃█▃
wandb:      train/policy_loss ███████████▁████████▄███████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████▅████▁█████████████████████▂███
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78206
wandb: best/eval_avg_mil_loss 0.78425
wandb:  best/eval_ensemble_f1 0.78206
wandb:            eval/avg_f1 0.60906
wandb:      eval/avg_mil_loss 0.91207
wandb:       eval/ensemble_f1 0.60906
wandb:           train/avg_f1 0.69558
wandb:      train/ensemble_f1 0.69558
wandb:         train/mil_loss 0.72526
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run woven-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gwdv57c5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043005-gwdv57c5/logs
wandb: ERROR Run gwdv57c5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: yb4bp7qk with config:
wandb: 	actor_learning_rate: 1.954829334098714e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 117
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.059041003808593895
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043245-yb4bp7qk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yb4bp7qk
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▇▇███
wandb: best/eval_avg_mil_loss █▅▅▂▂▄▃▁▂
wandb:  best/eval_ensemble_f1 ▁▄▅▆▇▇███
wandb:            eval/avg_f1 ▁▄▅▂▃▅▅▂▃▅▃██▆▂▅▅▅▅▇▄▄▆█▄▄▆▆▂▆▂▇▇█▃▆▅▆▇▅
wandb:      eval/avg_mil_loss ▅▃▁▂▂▅▂▅▄▄▂▃▂▂▅▅▄▃▃▅█▃▂▃▁▃▂▂▃▂▆▂▁▄▁▁▄▂▃▂
wandb:       eval/ensemble_f1 ▄▃▇▅▆▆▁▇▂▆█▅▆▅▃▂▅▇▅▅█▆▅▆▇▅█▄▄▃▆▇▃▅▇▇▅▆▅▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▄▃▃▃▃▁▄▄▆▅▄▄▅▃▆▄▄▄▄▇▄█▅▅▅▅▅▅▃▄▆▇█▇▆▅▅▆
wandb:      train/ensemble_f1 ▁▂▄▁▂▅▃▃▅▅▄▅▆▄▂▂▄▆▃▄▅▃▆▅▄▅▆█▅▆▇▆▅▃▄▆▅▅▆▆
wandb:         train/mil_loss ▆▅▆▃▄▃▅▃▄▅▇▄▂▃▆▃▄▅▄▃▃▆▅▄▄▆█▃▄▄▄▂▄▂▄▂▁▃▄▃
wandb:      train/policy_loss ▆▁▅▅▅▅▇▅▅█▅▅▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▂▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▇▇▇▇▅▇▇▇▇▇▇▇▇▁▇▇▇█▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77756
wandb: best/eval_avg_mil_loss 0.63518
wandb:  best/eval_ensemble_f1 0.77756
wandb:            eval/avg_f1 0.69708
wandb:      eval/avg_mil_loss 0.75434
wandb:       eval/ensemble_f1 0.69708
wandb:            test/avg_f1 0.77528
wandb:      test/avg_mil_loss 0.51441
wandb:       test/ensemble_f1 0.77528
wandb:           train/avg_f1 0.70631
wandb:      train/ensemble_f1 0.70631
wandb:         train/mil_loss 0.71362
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run flowing-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yb4bp7qk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043245-yb4bp7qk/logs
wandb: Agent Starting Run: hlb9k6kz with config:
wandb: 	actor_learning_rate: 0.0005146363855436523
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 119
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9034035264605214
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043509-hlb9k6kz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hlb9k6kz
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▄▇██
wandb: best/eval_avg_mil_loss █▄▃▄▅▃▁▂▂
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▄▇██
wandb:            eval/avg_f1 ▄▂▃▂▃▃▄▃▃▂▃▄▅▄▅▄▂█▅▄▃▁▅▆▅▄▇▂▄▆▂▅▃▇▆▆▆▄▆▇
wandb:      eval/avg_mil_loss ▆▃▁▂▃▃▄▆▂▆▃▁▂▂▄▄▄▂█▇▃▃▂▂▄▃▁▁▄▄▃▃▂▂▅▁▄▂▃▁
wandb:       eval/ensemble_f1 ▅▄▄▄▅▆▄▄▇▆▅▆▃▅▄▂▅▇▆▁▄▆▃▆▇▆▇█▅▇▆▄▇▆▆▅▁█▆▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▄▂▃▄▃▃▄▄▆▃▄▄▄▃▄▄▂▅▄▄▆▄▇▅▅▄▅▇▅▆▆▃▇▆█▆▆▅▆
wandb:      train/ensemble_f1 ▂▁▂▂▃▁▃▁▅▃▃▁▄▃▃▄▃▄▃▄▅▃▆▄▃▆▅▅▄▅▅▅▅▄█▆▅▄▄▆
wandb:         train/mil_loss ▅▅▄█▅▂▄▃▄▃▃▂▄▃▄▃▂▃▂▅▂▄▃▃▂▂▂▄▂▃▁▁▆▂▄▄▅▅▃▃
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▅▇▆▆▆▆▆▆▆▁▆▆▆▆█▃▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▇▆▄▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▇▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74953
wandb: best/eval_avg_mil_loss 0.7376
wandb:  best/eval_ensemble_f1 0.74953
wandb:            eval/avg_f1 0.71385
wandb:      eval/avg_mil_loss 0.70272
wandb:       eval/ensemble_f1 0.71385
wandb:            test/avg_f1 0.72944
wandb:      test/avg_mil_loss 0.72512
wandb:       test/ensemble_f1 0.72944
wandb:           train/avg_f1 0.68222
wandb:      train/ensemble_f1 0.68222
wandb:         train/mil_loss 0.78216
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lilac-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hlb9k6kz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043509-hlb9k6kz/logs
wandb: Agent Starting Run: w00anstr with config:
wandb: 	actor_learning_rate: 5.468738679420877e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.160894008708641
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043733-w00anstr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/w00anstr
wandb: uploading history steps 93-94, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▇▇█
wandb: best/eval_avg_mil_loss █▅▂▁▂
wandb:  best/eval_ensemble_f1 ▁▃▇▇█
wandb:            eval/avg_f1 ▄▇▃▃▇▇▆▄▇▆▇▆▇▂▇▅▅▅▆▄▆▇▇█▆▆▆▅▁▄▆█▄▅▇█▄▄▇▆
wandb:      eval/avg_mil_loss ▄▅▃▃▁▃▄█▃▁▄▃▃▄▄▂▂▃▂▃▂▃▃▃▄▃▃▄▄▁▆▂▃▄▅▂▃▃▃▂
wandb:       eval/ensemble_f1 ▅▇▄▃▆▄▃▇▅▄▅▄▇▅▄▅▅▄▅▅▄▇▆▆▃█▅▃▅▄▁▅▆▃▅▆▄▄▃▄
wandb:           train/avg_f1 ▃▇▁▆▆▃▆▃▃▃▁▃▂▂▄▅▆▃▄█▂▃█▅▃▄▂▃▃▅▃▅▆▄▂▄▅█▅▆
wandb:      train/ensemble_f1 ▄▃▄▃▇▅▂▃▄▅▆▄▃▄▃▃▁▄▆▆▃▂▄▄▄█▃▃▅▃▅▄▅▆▄▄▅▃▅█
wandb:         train/mil_loss ▃▆▅▅▅▅▆▅▄▅▅▅▅▅▆▄█▆▇▂▄▆▇▆▅▄▅▆▄▆▄▄▅▅▃▅▁▃▃▃
wandb:      train/policy_loss ███████████████████████████████████████▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆█▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78671
wandb: best/eval_avg_mil_loss 0.82011
wandb:  best/eval_ensemble_f1 0.78671
wandb:            eval/avg_f1 0.67361
wandb:      eval/avg_mil_loss 0.75972
wandb:       eval/ensemble_f1 0.67361
wandb:           train/avg_f1 0.66701
wandb:      train/ensemble_f1 0.66701
wandb:         train/mil_loss 0.74731
wandb:      train/policy_loss -0.26283
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.26283
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run young-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/w00anstr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043733-w00anstr/logs
wandb: ERROR Run w00anstr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: i6ywudqz with config:
wandb: 	actor_learning_rate: 9.82432126016169e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8939964449460334
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043911-i6ywudqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i6ywudqz
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▇██
wandb: best/eval_avg_mil_loss ██▇▅▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▇██
wandb:            eval/avg_f1 ▄▅▅▅▄▃▇▅▄▁▅▅▂▃▅▅▇▅▄▃▆▅▄█▅█▄█▄▄▆█▅▄█▆▄▃▆▆
wandb:      eval/avg_mil_loss ▅▆▄▃▃▃▃█▆▅▂▄▆▄▆▆▇▂▄▄▆▄▃▆▁▄▄▃▃▆▄▄▁▂▂▅▅▇▄▃
wandb:       eval/ensemble_f1 ▃▄▆█▃▆▄▁▃▂▆▂▂▄▂▅▄▇█▅▂▅▆▃▂▅▅▅▅▅▇▅▇▁▇▅▄▆▃▆
wandb:           train/avg_f1 ▂▂▂▄▁▃▄▄▂▆▄▂▇▃▃▇▂▃▅▄▄▄▅▅▆▄██▄▃▆▅▅▅▆▄▆▅▅▇
wandb:      train/ensemble_f1 ▅▅▃▂▅▄▅▄▅▅▄▅▅▃▄▆▃▃▆▅▄▅▄▆▁█▅▅▄▅▆▆▅▆▅▆▅▇▇▇
wandb:         train/mil_loss ▅▅▆▇▆█▅▅▃▄▂▆▂▂▂▂▆▅▅▃▂▂▄█▆▇▃▃▂▆▂▃▃▃▂▄▆▃▁▄
wandb:      train/policy_loss ▆▆▁▆▆▆▆▆▆█▆▆▆▆▃▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▃▅▅▅▅▅▅▅▅▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80699
wandb: best/eval_avg_mil_loss 0.63563
wandb:  best/eval_ensemble_f1 0.80699
wandb:            eval/avg_f1 0.75083
wandb:      eval/avg_mil_loss 0.691
wandb:       eval/ensemble_f1 0.75083
wandb:           train/avg_f1 0.75919
wandb:      train/ensemble_f1 0.75919
wandb:         train/mil_loss 0.73065
wandb:      train/policy_loss -0.08861
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.08861
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run likely-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i6ywudqz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043911-i6ywudqz/logs
wandb: ERROR Run i6ywudqz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1tklhkdj with config:
wandb: 	actor_learning_rate: 0.00013393157987123068
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.09018640064227468
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044145-1tklhkdj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1tklhkdj
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss ▁█▃▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▃▁▂▆▄▂▅▃▃▅▃▅▃▄▅▃▃▄▅▄▄▅▆▆▃▄▅▄▃▆█▄▆█▆▇▅▅▆▅
wandb:      eval/avg_mil_loss ▆▄▅▆▅▃█▂▇▇▁▄▆▃▄▃▇▄▄▆▇▄▄▃▂▇▃▄▃▅▅▃▂▄▂▁▄▅▃▂
wandb:       eval/ensemble_f1 ▅▄▄▃▄▄▂▄▁▄▃▃▅▇▅▄▆▄▅▄▃▄▅▆▄▅█▆█▃▅▇▆▁▅▅▃▆▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▂▁▂▄▃▃▄▃▄▄▃▅▄▃▄▄▂▅▃▆▃▃▅▇▇▃▅▄▄▅▆▅▅▆█▄▆▆
wandb:      train/ensemble_f1 ▅▃▃▁▂▂▄▃▄▆▃▂▃▆▄▅▃▄▄▃▅▆▆▃▄▅▅█▄▃▆▅▄▅▄▄▆▇▅▆
wandb:         train/mil_loss ▆▇█▆█▄█▅▇▇▅▅▅▅▆▅▆▆▆▆▄▃▄▄▆▄▅▆▃▅▃▂▆▄▄▄▄▄▃▁
wandb:      train/policy_loss ▅▁▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▃▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▄▄▅▆▅▅▅▅▅▅▅▅▃▅▅▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▁▂▅▅▅█▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81014
wandb: best/eval_avg_mil_loss 0.62704
wandb:  best/eval_ensemble_f1 0.81014
wandb:            eval/avg_f1 0.73952
wandb:      eval/avg_mil_loss 0.65791
wandb:       eval/ensemble_f1 0.73952
wandb:            test/avg_f1 0.77095
wandb:      test/avg_mil_loss 0.55612
wandb:       test/ensemble_f1 0.77095
wandb:           train/avg_f1 0.74782
wandb:      train/ensemble_f1 0.74782
wandb:         train/mil_loss 0.67299
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run polished-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1tklhkdj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044145-1tklhkdj/logs
wandb: Agent Starting Run: jhmcbiaw with config:
wandb: 	actor_learning_rate: 1.8547287549979115e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 122
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.33325189789388143
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044405-jhmcbiaw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jhmcbiaw
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▅▅▆██
wandb: best/eval_avg_mil_loss ▅▄█▄▄▆▅▁
wandb:  best/eval_ensemble_f1 ▁▃▃▅▅▆██
wandb:            eval/avg_f1 ▃▃▄▁▃▄▅▄▄▃▃▅▄▄▄▅▅▆▅▄▅▅▆▆▆▆▃▄▇▆▄▇▇▆▇▇▆█▆▆
wandb:      eval/avg_mil_loss ▄▅▄▆▄▄▅█▆▃▃▇▃▂▂▆▃▂▇▂▂▂▃▂▂▆▁▂▆▂▅▂▁▆▂▃▃▄▁▄
wandb:       eval/ensemble_f1 ▁▁▁▃▇▄▃█▁▅▄▄▆▅▅▅▄▇▅▅▃▅▅▅▆▅▇▅▆▇▆▇▄▆█▆▇▇▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▁▃▄▃▃▄▃▃▃▅▄▄▄▄▅▅▆▄▅▆▅▆▆▅▆▆▆▇▆█▇▇▇▆▆▆▇▆
wandb:      train/ensemble_f1 ▁▂▃▃▄▃▄▃▄▄▅▅▅▅▆▅▄▆▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇█▇█▇█▇▇
wandb:         train/mil_loss ▅█▄▆▅▅▆▆▄▇▄▇▃▄▂▆▅▄▆▃▄▂▂▂▆▃▄▂▄▂▃▃▇▃▅▁▂▁▁▃
wandb:      train/policy_loss ▅█▅▅▃█▅▄▅▅▅▅▅▁▅▅▅▅▅▅▅▃▄▅▅▅▅▅▅▅▅▄▅▂▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▅▆▆▆▆█▆▆▆▆▆▆▁▆▆▆▆▆▆▅▅▆▆▆▆▆▆▅▆▆▆▆▆▆▃▆▆█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77977
wandb: best/eval_avg_mil_loss 0.62758
wandb:  best/eval_ensemble_f1 0.77977
wandb:            eval/avg_f1 0.77467
wandb:      eval/avg_mil_loss 0.72823
wandb:       eval/ensemble_f1 0.77467
wandb:            test/avg_f1 0.77528
wandb:      test/avg_mil_loss 0.51071
wandb:       test/ensemble_f1 0.77528
wandb:           train/avg_f1 0.74675
wandb:      train/ensemble_f1 0.74675
wandb:         train/mil_loss 0.62475
wandb:      train/policy_loss -0.22548
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.22548
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run zesty-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jhmcbiaw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044405-jhmcbiaw/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8l81pkmo with config:
wandb: 	actor_learning_rate: 0.0002485897967293602
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 168
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.85812232598203
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044701-8l81pkmo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8l81pkmo
wandb: uploading history steps 142-151, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▆█
wandb: best/eval_avg_mil_loss ▇█▃▃▂▁
wandb:  best/eval_ensemble_f1 ▁▄▄▅▆█
wandb:            eval/avg_f1 ▃▇▂▂▁▄▃▄▃▂▅▄▄▅▅▅▃▆▅▇▆▂▃▃█▅▇▇▅▄▇▄▆▆▃▅▇▆▅█
wandb:      eval/avg_mil_loss ▄▄▃█▄▅▄▂▆▃▅▄▁▃▄▃▄▂▄▇▂▃▅▄▆▄▃▂▃▃▅▂▅▅▃▂▂▃▁▃
wandb:       eval/ensemble_f1 ▄▆▄▁▄▃▄▅▃▆▅▆▅▅▆▆██▆▆▆▆▄▅▄▄▄▆▄▃▅▆▅▆▅▄▅▂▆▄
wandb:           train/avg_f1 ▆▁▆▃▃▄▅▆▄▄▄▅▅▇▅▇▆▄▆▆▄▅▇▄▄▇▆▅▅▅▆▅▇▆▅▄█▅▆▆
wandb:      train/ensemble_f1 ▃▄▅▁▂▄▅▄▅▃▄▆▂▆▆▄▆▄▁▇▇▄▄▆▇▃▆▂▄█▇▁▇▇▆▇▅▆█▇
wandb:         train/mil_loss ▆▃▃▅▅▆▄▆▃▆▁▅▆█▅▄▄█▆▃▇▅▃▆▇▃▂▅▄▄▄▄▂▄▆▅▃▂▅▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▄▆▆▆▆▆▆▆▂▆▆▆▆▅▆▆▆▆▆▆▆█▆▄▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▄██████████▁█▇█▇██████████▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8248
wandb: best/eval_avg_mil_loss 0.71473
wandb:  best/eval_ensemble_f1 0.8248
wandb:            eval/avg_f1 0.71723
wandb:      eval/avg_mil_loss 0.58041
wandb:       eval/ensemble_f1 0.71723
wandb:           train/avg_f1 0.71732
wandb:      train/ensemble_f1 0.71732
wandb:         train/mil_loss 0.69937
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glowing-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8l81pkmo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044701-8l81pkmo/logs
wandb: ERROR Run 8l81pkmo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: hsl17ki3 with config:
wandb: 	actor_learning_rate: 0.0009091823336460736
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 101
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5941656474377738
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045005-hsl17ki3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hsl17ki3
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 90-101, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄▆▄▃▅▄▄▄▄▄▄▄▄▄▆▄▄▄▅▃▃▅▄▃▄▅█▄▄▆▃▃▁▆▇▅▆▅▃▄
wandb:      eval/avg_mil_loss ▅▃▂▅▅▄▅▄█▃▂▅▃▃▄▂▆▄▃▄▁▂▅▄▃▄▅▄▂▆▃▁▇▁▃▃▄▄▃▇
wandb:       eval/ensemble_f1 █▃▁▅▅▅▃▄▅▂▃▄▃▆▃▃▃▄▄▂▄▇▅▅▃▅▅▃▆▃▄▄▄▁▆▂▂▃▄▄
wandb:           train/avg_f1 ▃▄▄▅▆▅▅▄▃▄▃▅▆▆▆▃▅█▅▂█▇▆▄▃▂▆█▅▁▆▅▄▇▃▆▃▄▅▆
wandb:      train/ensemble_f1 ▇▆▅▇▄▁▅▃▆▅▄▄▃█▆▄▇▄▃▃█▃▇▂▄▇▅▇▆▄▂▄▆█▆▃▅▆▃▇
wandb:         train/mil_loss ▅▇▅▇▃▃▃▆█▆▄▂▃▄▇▄▅▆▄▁▅▃▅▆▅▇▆▅▃▃▅▆▄▂▇▆▅▆▅▃
wandb:      train/policy_loss ▂▂▂▂▂▁▂▂▂▆▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77002
wandb: best/eval_avg_mil_loss 0.63507
wandb:  best/eval_ensemble_f1 0.77002
wandb:            eval/avg_f1 0.60584
wandb:      eval/avg_mil_loss 1.14229
wandb:       eval/ensemble_f1 0.60584
wandb:           train/avg_f1 0.66072
wandb:      train/ensemble_f1 0.66072
wandb:         train/mil_loss 0.81434
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run celestial-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hsl17ki3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045005-hsl17ki3/logs
wandb: ERROR Run hsl17ki3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: om36sjiq with config:
wandb: 	actor_learning_rate: 3.390921205307796e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 198
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4913184286297601
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045212-om36sjiq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/om36sjiq
wandb: uploading wandb-summary.json
wandb: uploading history steps 103-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▅▆▆█
wandb: best/eval_avg_mil_loss █▃▃▇▃▁▃
wandb:  best/eval_ensemble_f1 ▁▂▃▅▆▆█
wandb:            eval/avg_f1 ▃▅▂▄▃▄▃▄▄▃▄▄▄▆▅▅▅█▃▄▆▆▆▄▂▇▁▅▄▆▃▅▂▇▂▃▆▄▄▂
wandb:      eval/avg_mil_loss ▅▄▄▄▇▄▅▅▇▂▃▃▄▅▅▆▄▇▄▂▂▃▅▅▄▅█▁▃█▃▂▆▄▃▃▄▅▄▆
wandb:       eval/ensemble_f1 ▁▃▅▇▃▄▄▂▄▃▂▂▃▄▆▄▄▂▃▂▄▄▅▅█▄▃▄▃▅▃▅▂▇▃▄▄▄▃▁
wandb:           train/avg_f1 ▄▂▅▄▁▅▄▂▄▄▃▅▄▄▅▆▅▇▇▅▅▄▆▅█▄▆▆▅▇▆▇▆▆▇▆▆▅▅▆
wandb:      train/ensemble_f1 ▅▃▆▄▁▄▂▅▄▃▅▂▅▃▄▅▇▅▄▅▂▅▇▅▅▆▇▄▅▅▇█▇▇█▇▃▆▆▆
wandb:         train/mil_loss ▆█▆▇▅▇▇▄▅▆▄▇▄▄▅▆▇▄▄▄▅▆▆▃▄▅▆▆▅▄▄▇▆█▄▅▅▃▅▁
wandb:      train/policy_loss ▅▅▅▁▅▅█▅▅▇▅▃▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▅▆▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆██▆▆▆▆▆▄▆▆▆▆▁▆▆▆▄▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80956
wandb: best/eval_avg_mil_loss 0.70575
wandb:  best/eval_ensemble_f1 0.80956
wandb:            eval/avg_f1 0.64203
wandb:      eval/avg_mil_loss 0.85623
wandb:       eval/ensemble_f1 0.64203
wandb:           train/avg_f1 0.73469
wandb:      train/ensemble_f1 0.73469
wandb:         train/mil_loss 0.68736
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pretty-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/om36sjiq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045212-om36sjiq/logs
wandb: ERROR Run om36sjiq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	size mismatch for task_model.mlp.0.weight: copying a param with shape torch.Size([512, 20]) from checkpoint, the shape in current model is torch.Size([128, 20]).
wandb: ERROR 	size mismatch for task_model.mlp.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
wandb: ERROR 	size mismatch for task_model.mlp.3.weight: copying a param with shape torch.Size([2, 512]) from checkpoint, the shape in current model is torch.Size([2, 128]).
wandb: ERROR 
wandb: Agent Starting Run: ykejev61 with config:
wandb: 	actor_learning_rate: 5.568415889295736e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 154
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.032299506110540666
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045429-ykejev61
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ykejev61
wandb: uploading history steps 102-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅█
wandb: best/eval_avg_mil_loss █▂▅▁
wandb:  best/eval_ensemble_f1 ▁▄▅█
wandb:            eval/avg_f1 ▄▆▁▄█▂▃█▆▃▅▄▄▂▂▅▆▆▅▇▄▆▇▄▃▆▂▃▄▄▅█▄▄▆▅▅▇▃▄
wandb:      eval/avg_mil_loss ▅▄▇▆▇▅▂█▄█▅▃█▃▃▅▇▇▃▂▇▇▃▃▄▅▇▅▁▇▄█▄▅▁▅▄▆▂▅
wandb:       eval/ensemble_f1 ▆▁▆▃█▄▂▃▄▅█▃▅▄▆▅▃▆▄▇▇▇▃▄▂▅▆▆▅▃▄█▄▅▅▄▅▄▆▅
wandb:           train/avg_f1 ▅▄▂█▆▂▆▄▇▆▄▆▅▃▅▄▁▅▆▄▄▄▅▅▅▅▄▅▅▁█▅▆▄▇▄▆▇▂▃
wandb:      train/ensemble_f1 █▅▅▆▂▇▆▆▆▇▅▄▅▁▂▇█▅▇▅▅▄▅▃▅▇▅▃▁▆▄▇▄█▇█▅▆▃▄
wandb:         train/mil_loss ▃▇█▂▄▆█▄▅▇▃▇▂▄▃▃▄▅▃▂▄▃▄▃▆▇▄▆▃▃▃▃▅▂▆▂█▁▄▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75928
wandb: best/eval_avg_mil_loss 0.85686
wandb:  best/eval_ensemble_f1 0.75928
wandb:            eval/avg_f1 0.73302
wandb:      eval/avg_mil_loss 0.80629
wandb:       eval/ensemble_f1 0.73302
wandb:           train/avg_f1 0.66014
wandb:      train/ensemble_f1 0.66014
wandb:         train/mil_loss 0.86623
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rose-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ykejev61
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045429-ykejev61/logs
wandb: ERROR Run ykejev61 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xt8xp9rl with config:
wandb: 	actor_learning_rate: 8.957260818029275e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.734625713413434
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045645-xt8xp9rl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xt8xp9rl
wandb: uploading history steps 102-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃██
wandb: best/eval_avg_mil_loss █▁▃▃▂
wandb:  best/eval_ensemble_f1 ▁▃▃██
wandb:            eval/avg_f1 ▄▅▄█▅▅▅▁█▆▄▂▂▂▄▅▄▅▆▅▅▂▃▅▂▄▄▃▇▄▅▄▅▅▅▅▃▆▅▅
wandb:      eval/avg_mil_loss ▅▂▅▄▅▆▂▇▅▅▄▃▃▃▄▅▆▃▅█▄▂▃▅▁▁▄▄▂▄▅▄▃▃▂▃▂▃▃▂
wandb:       eval/ensemble_f1 ▄▃▅▄▄▅▃▄▃▁▄▃▄▆▂▅▃▃▅▇▃█▄▇▁▅▁▃▄▂▅▄▅▄▅▅▅▄▄▄
wandb:           train/avg_f1 ▃▅▆▇▅▄▂▄▅▃▅█▅▆▄▄▄▃▆▄▁▃▅▃▆▅▄▃▄▅▂▅▅▆▅▅▇▅█▅
wandb:      train/ensemble_f1 ▅█▆▃▂▃▅▃▆█▁▃▅▂▄▆▇▅▄▆▄▇▃█▁▄▄▃▃▂▃▇█▅▅▅▇▆▆▃
wandb:         train/mil_loss ▂▄▆▆▆▆▅▆▇▅█▇▆▅▂▆▆▅▅▇▅▄▄▃▃█▄▄▄▁▂▆▆▁▅▄▆▃▂▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇▅▅▅▁█▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.76458
wandb: best/eval_avg_mil_loss 0.7559
wandb:  best/eval_ensemble_f1 0.76458
wandb:            eval/avg_f1 0.6736
wandb:      eval/avg_mil_loss 0.91935
wandb:       eval/ensemble_f1 0.6736
wandb:           train/avg_f1 0.64423
wandb:      train/ensemble_f1 0.64423
wandb:         train/mil_loss 0.8653
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run classic-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xt8xp9rl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045645-xt8xp9rl/logs
wandb: ERROR Run xt8xp9rl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: bqrcclxg with config:
wandb: 	actor_learning_rate: 1.3571721763649243e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 177
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4377874631203982
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045909-bqrcclxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bqrcclxg
wandb: uploading wandb-summary.json
wandb: uploading history steps 165-177, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▆▇█
wandb: best/eval_avg_mil_loss ▂█▃▁▂
wandb:  best/eval_ensemble_f1 ▁▂▆▇█
wandb:            eval/avg_f1 █▆▄▆█▅▃▂▆▄█▆▆▆▇▃▄▅▄▄▆▄▆▅▆▄▆█▅▄▄▄▄▁▁▆▄▆▅▇
wandb:      eval/avg_mil_loss ▃█▂▅▄▅▃▅▄▅▃▄▄▂▅▃▂▃▇▅▄▇▄▁▇▇▂▃▄▄▇▄▁▄▄▁▅▃█▅
wandb:       eval/ensemble_f1 ▂▇▆▇█▅▂▅▄▅▇▆▆▅█▆▅▄▂▄▆▃▆▂▄▄▄▆▄▅▄▄▇▆▁▇█▄▄▄
wandb:           train/avg_f1 ▃▆▇▆▆▄▆▅▆▅█▇▇▅▇▅▄▆▆▇▄█▄▄▅▇▅▃▇▃▁▅▃▅▆▇▇▇▅▆
wandb:      train/ensemble_f1 ▇▃▆▆▂▄▅▁▅▅▁▅▅▆▇▅▅▆▅▆█▃▄▅█▅▅▅▇▅▄▅▇▆▅▄▅▄▅▁
wandb:         train/mil_loss ▆▂▁▄▂█▄▇▄▆▆▅▁▅▆▅▇▅▃▁██▇▃▂▄▃▃▃▄▁▂▂▇▂▃▄▁▃▇
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.85169
wandb: best/eval_avg_mil_loss 0.66833
wandb:  best/eval_ensemble_f1 0.85169
wandb:            eval/avg_f1 0.71888
wandb:      eval/avg_mil_loss 0.77085
wandb:       eval/ensemble_f1 0.71888
wandb:           train/avg_f1 0.63839
wandb:      train/ensemble_f1 0.63839
wandb:         train/mil_loss 0.92355
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run misunderstood-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bqrcclxg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045909-bqrcclxg/logs
wandb: ERROR Run bqrcclxg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 8sv7w6q7 with config:
wandb: 	actor_learning_rate: 0.00021169327581865093
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 189
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.059005898538945334
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050214-8sv7w6q7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8sv7w6q7
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 178-189, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄▅▆▇▇█
wandb: best/eval_avg_mil_loss █▄▁▆▄▄▄▃▄
wandb:  best/eval_ensemble_f1 ▁▃▃▄▅▆▇▇█
wandb:            eval/avg_f1 ▆▁▇▆▆▇▅▆▇▇▆█▄▆█▆▅▄▇█▅▇▇▇██▆▆▅▆▇▁▅▇▅██▄▅▄
wandb:      eval/avg_mil_loss ▂▇▃▄▅▄█▃▅▄▃▅▇▆▄▂▄█▄▃▄▄▅▃▄▆▄▅▃▁▅▅▄▆▆▃▃▃▃▃
wandb:       eval/ensemble_f1 ▇▇▆▅▇▆▇█▇▆▇▅▆▅▄▆▄▅▇▇▄█▅▆▆▆▇▁▄█▇▄█▇▇▄▄▆▆█
wandb:           train/avg_f1 ▆▅▃▄▄▆▆▆▃▃▆▃▂▄▅▄▄▇▆▅▄▁▆▆▆▃▅▆▂▅▄▃▃█▆▅▆▄▆▅
wandb:      train/ensemble_f1 ▃▆▄▃▅▃▃▄▂▆▁▇▃▅▃▃▆▃▆▅▆▆▇▃▇▄▆▃▂▅▄▆▆█▅▆▆▆▆▅
wandb:         train/mil_loss ▂▄▄▅▆▆▇▃▅▂▃▄▄▅▄▄▃▃▅▅▃▆▃▁▁▄▅▅█▃▅▂▄▃▆▃▄▄▅▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▂██████████████████▁██████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84717
wandb: best/eval_avg_mil_loss 0.78361
wandb:  best/eval_ensemble_f1 0.84717
wandb:            eval/avg_f1 0.67579
wandb:      eval/avg_mil_loss 0.84211
wandb:       eval/ensemble_f1 0.67579
wandb:           train/avg_f1 0.72101
wandb:      train/ensemble_f1 0.72101
wandb:         train/mil_loss 0.76816
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run quiet-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8sv7w6q7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050214-8sv7w6q7/logs
wandb: ERROR Run 8sv7w6q7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9cd6j8pg with config:
wandb: 	actor_learning_rate: 1.933001487146608e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 72
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8884126267856782
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050541-9cd6j8pg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9cd6j8pg
wandb: uploading wandb-summary.json
wandb: uploading history steps 59-73, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇▇▇██
wandb: best/eval_avg_mil_loss ▄█▅▂▁▁▄▃
wandb:  best/eval_ensemble_f1 ▁▅▆▇▇▇██
wandb:            eval/avg_f1 ▂▆▅▆▂▇▁▇▃▄▄▂▄▇▆▄▂▇▁▂▅▁▄▃▁▆▃▇▇▄▂▁▇█▇▇▇▃▇▂
wandb:      eval/avg_mil_loss ▆▇▄▁▁▇▂▃▃▃▄▄▃▃▇▄▃▄▃▅▄▇▅▄▅▄▅▇█▂▂▆▁▁▁▆▄▃▅▅
wandb:       eval/ensemble_f1 ▂▆▂▅▂▂▁▅▇▃▁▇▄▂▇▇▇▄▂▇▇▁▂▃▄▇▄▂▁▁█▇▂▇▁▆▇▇▄▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▇▇▄▃▆▆▄▃▁▄▇▄▅▅▃▂▅▆▄▄▃▅▅▅▃▂▃▇▅▅█▅▆▃▅▁▄▇▄
wandb:      train/ensemble_f1 █▃▇▄▅▆▄▄▃▃▇▄▆▇▅▃▅▆▄▃▅▅▅▂▆▄▃▇▅▄▆▅█▅▆▅▃▂▁▅
wandb:         train/mil_loss ▅▄▄▆▄▃▁█▃▄▃▅▄▅▃▆▄▆▆▄▃▄▆▄▆▃▆▃▅▅▄▃▅▄▂▄▄▇▄▅
wandb:      train/policy_loss ██████████████████████████████▁█████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.86804
wandb: best/eval_avg_mil_loss 0.71843
wandb:  best/eval_ensemble_f1 0.86804
wandb:            eval/avg_f1 0.40771
wandb:      eval/avg_mil_loss 1.01495
wandb:       eval/ensemble_f1 0.40771
wandb:            test/avg_f1 0.33691
wandb:      test/avg_mil_loss 1.12488
wandb:       test/ensemble_f1 0.33691
wandb:           train/avg_f1 0.63176
wandb:      train/ensemble_f1 0.63176
wandb:         train/mil_loss 0.88947
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dulcet-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9cd6j8pg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050541-9cd6j8pg/logs
wandb: Agent Starting Run: i5s3mbl7 with config:
wandb: 	actor_learning_rate: 5.3141706496978616e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 172
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.375873028689643
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050702-i5s3mbl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i5s3mbl7
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 104-115, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅█
wandb: best/eval_avg_mil_loss █▅▁▅▄▆
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅█
wandb:            eval/avg_f1 ▁▆▁▆▆▆▇█▁▅▁▆▄▅▇▇▁▇▆▁▆▇▇▇▆▃▇▅▄▆▇▅▆▇▅▇▇▂▂▆
wandb:      eval/avg_mil_loss █▆▄▃▆▂▆▆▆█▅▄▇▇▄▁▅▄▄▆▅██▇▇▂▇▃▆▂▄▃▅▃▄▇▅▅▇▆
wandb:       eval/ensemble_f1 ▁▆▆▇▂▁▁▄▄▆▇▇▇█▆▇▇▃▇▇▇▅▅▇▇▄█▆▇▇▅▇▇▇▇▅▇█▅▇
wandb:           train/avg_f1 ▃█▄▁▇▇▇▆▄█▄▇▇▆▄▄▅▆▇▂▆▇▅▇▅█▇█▆▇▄▇▆▇▆▆▅█▇▇
wandb:      train/ensemble_f1 ▄▂▇▆▂▅▆▃▅▄▇▄▆▄▄▄▅▇▇▅▇▅▁▂▇▆▆▅▇▃▆▅▆▆▇█▄▆▅▆
wandb:         train/mil_loss ▂▂▄█▄▄▃▃▃▆▆▄▆▄▄▄▄▄▂▃▃▂▃▅▃▃▄▃▄▃▄▄█▅▅▅▄▃▄▁
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84046
wandb: best/eval_avg_mil_loss 0.90662
wandb:  best/eval_ensemble_f1 0.84046
wandb:            eval/avg_f1 0.72011
wandb:      eval/avg_mil_loss 0.90961
wandb:       eval/ensemble_f1 0.72011
wandb:           train/avg_f1 0.71917
wandb:      train/ensemble_f1 0.71917
wandb:         train/mil_loss 0.79821
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run happy-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i5s3mbl7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050702-i5s3mbl7/logs
wandb: ERROR Run i5s3mbl7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: cko2s2bx with config:
wandb: 	actor_learning_rate: 2.454837458158047e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1707678497764703
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050907-cko2s2bx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cko2s2bx
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅▆▆▇█
wandb: best/eval_avg_mil_loss ▇▇█▅▆▁▄▂▂
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅▆▆▇█
wandb:            eval/avg_f1 ▆▇▇▇▆▇▆█▇▇▇▅▆▅▁▆█▆▇█▇█▅█▆▇▁▆█▇█▆▆▆███▆▇█
wandb:      eval/avg_mil_loss ▃▅▅▄▄▄▄▆▃▇▄▄▇▄▇▇▄▇▂▅▂▅▄█▃▄▆▄▁▁▂▂▇▄▄▄▄▄▄▅
wandb:       eval/ensemble_f1 ▆█▆▆▆█▇▆▇▇▆▇▇█▂█▆▇▁▇▇▃▆▆▃▇▆▆█▆█▇██▇██▆█▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▁▅▅▇▄▄▅▃▄▃▄▃▄█▂▆▇▅▅▅▃▅▄▅▆▅▄▇▆▁▅▇▂▃▅▂▆▃
wandb:      train/ensemble_f1 ▄▅▁▅▄▅▃▅▂▄▃▄▁▂▅▄▅█▂▄▄▁▇▆▂▅▃▄▆▇▅▆▅▇▃▃▃▆▇▄
wandb:         train/mil_loss ▆▆▅▆▇▆▆▅▅▄▂▄▆▆▃▄▄▃▇▆█▄▇▆▇▄▅▅▅▁▃▄▇▄▃▄▅▅▄▄
wandb:      train/policy_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃█▃▃▃▃▃▆▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83952
wandb: best/eval_avg_mil_loss 0.65345
wandb:  best/eval_ensemble_f1 0.83952
wandb:            eval/avg_f1 0.66767
wandb:      eval/avg_mil_loss 1.08751
wandb:       eval/ensemble_f1 0.66767
wandb:            test/avg_f1 0.84536
wandb:      test/avg_mil_loss 0.35273
wandb:       test/ensemble_f1 0.84536
wandb:           train/avg_f1 0.74515
wandb:      train/ensemble_f1 0.74515
wandb:         train/mil_loss 0.58857
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run misty-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cko2s2bx
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050907-cko2s2bx/logs
wandb: Agent Starting Run: mnc1177z with config:
wandb: 	actor_learning_rate: 2.365342338914513e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 150
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.202536471864727
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051227-mnc1177z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mnc1177z
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 147-151, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇█
wandb: best/eval_avg_mil_loss █▄▁▅▅
wandb:  best/eval_ensemble_f1 ▁▄▇▇█
wandb:            eval/avg_f1 █▄▇▇▆▇█▄▇▆▇▇█▇▅▇▆▇▅▇█▆▆█▆▆▆▆▇▇▇▅▆▇█▇▁▇▆█
wandb:      eval/avg_mil_loss ▅▃▄▂▆▅▂▅▃▃▁▄▄▄▁▁▃▃▃▃▄▄▁▄▁▁▅▁▁▄▄▁▄█▃▃▆▄▃▄
wandb:       eval/ensemble_f1 ▇▆▇▇▆▆▇▇▇▇▆█▅▆▇▅▇▇▆█▇▆▇█▆▇▇▅▁▇██▅▁▅▅▇▅██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▆▄▄▆█▆▄▄▆▄▃▃▃▄▁▅▃▄▃▃▃▅▇▃▅▅▃▆▆█▃▃▅▅▅▆▃▆█
wandb:      train/ensemble_f1 ▁▆▄▇▆█▅▆▇▄▆▄▂▂▅▄▆▅▃▂▃▄▃▄▃▁▄▇▇▆▅▃▅▅▅▆▂▅▆█
wandb:         train/mil_loss ▂▃▆▃▆▅▂▄▄▅▄▅▃▄▂▃▄▅▇▃▃▂▂▃▁▃▂▇▆▂▄▃▄▂▄▃▄▃█▂
wandb:      train/policy_loss ███████████████████████████████████▁████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████▁████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82533
wandb: best/eval_avg_mil_loss 0.762
wandb:  best/eval_ensemble_f1 0.82533
wandb:            eval/avg_f1 0.69284
wandb:      eval/avg_mil_loss 0.85596
wandb:       eval/ensemble_f1 0.69284
wandb:            test/avg_f1 0.85426
wandb:      test/avg_mil_loss 0.3818
wandb:       test/ensemble_f1 0.85426
wandb:           train/avg_f1 0.76911
wandb:      train/ensemble_f1 0.76911
wandb:         train/mil_loss 0.68483
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run classic-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mnc1177z
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051227-mnc1177z/logs
wandb: Agent Starting Run: mex2wdmk with config:
wandb: 	actor_learning_rate: 2.2048437328332342e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 123
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6470653121121944
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051507-mex2wdmk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mex2wdmk
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 104-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅█
wandb: best/eval_avg_mil_loss █▇▄▆▁
wandb:  best/eval_ensemble_f1 ▁▂▄▅█
wandb:            eval/avg_f1 ▆▅▆▆▆▇▅█▇█▁▇▆▆▂▆▆▄▃▇▅▇▃▇▇▇▇▃▆▄▅▆▄▁▆█▆▄▇▆
wandb:      eval/avg_mil_loss ▅▃▄▃▃▆▅▇█▂▆▃▅▃▂▄▅▄▃▇▂▅▃▄▅▃▃▃▂▃▇▁▄▂█▄▂▆▁█
wandb:       eval/ensemble_f1 ▆▇▅▇▇▇▅▇▄▅▇▆▄▆▆▇▇▇▇▇▆██▃▇▃▆▇▆▅█▆██▁█▆▅▄▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▄▄▆▆▃▃▅▇▇▆▃▃▇▅▆▃█▁█▅▃▇▆▅▆▃▄▆▂█▃▃▅▃▅▇▄▃▄
wandb:      train/ensemble_f1 ▄▃▆▃▃█▃▃▅▇▆▁▆▄▆▂▇▄▇▅█▃▃▅▄▆▆▆▅▃▅▇▇▃▇█▇▄▆▆
wandb:         train/mil_loss ▆▄▅▅▄▅▆▄▄▄▃▂▄▄▃▁▇▃▅█▄▄▆▂▅▃▄▃▃▃▅▄▄▄▆▆▁▃▂▃
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▁▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83188
wandb: best/eval_avg_mil_loss 0.53317
wandb:  best/eval_ensemble_f1 0.83188
wandb:            eval/avg_f1 0.77611
wandb:      eval/avg_mil_loss 0.80659
wandb:       eval/ensemble_f1 0.77611
wandb:            test/avg_f1 0.80389
wandb:      test/avg_mil_loss 0.45162
wandb:       test/ensemble_f1 0.80389
wandb:           train/avg_f1 0.74036
wandb:      train/ensemble_f1 0.74036
wandb:         train/mil_loss 0.78229
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run polar-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mex2wdmk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051507-mex2wdmk/logs
wandb: Agent Starting Run: 2u7tj572 with config:
wandb: 	actor_learning_rate: 2.0905395925457973e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 93
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4215402590536058
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051706-2u7tj572
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2u7tj572
wandb: uploading history steps 88-94, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▁▃
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▆▄▆▆▅▁▅▅▅▄▆▄▇▅▅▅▅▇▄█▅▆▄▄▁▇▇█▅▆▇▄▅▇▅▆▄▆▅▅
wandb:      eval/avg_mil_loss ▃▄▃▄█▅▄▄▁▃▄▄▂▃▁▃▃▄▃▄▄▃▄▁▅▅▃▅▃▂▄▁▇▄▁▅▁▅▃▃
wandb:       eval/ensemble_f1 ▄▄▅▄▁▅▆▃▄▇▇▄█▅▇▆▇▄▄█▆▆▄▇▄▂▇▆▅█▆▅▅▄▅▅▆▆▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▃▂▅▂▅▄▄▃▃▃▅▂▃▆▄▁▃▃▅▄▃▅▂▆▆▅▂▅█▄▅▅▄▃▅▃▄▂▃
wandb:      train/ensemble_f1 ▂▄▃▃▇▆▅▃▄▁▄▆▃▃▅▂▄█▁▆▄▆▅▅▄▇▆▇▂▆▂▅▆▅▅▂▅▆▄▅
wandb:         train/mil_loss ▆█▆▆▆▅▄▅▂▄▆▇▃▃▅▅▅▅▅▂▇▄▅▄▅▆▄▄▃▄▄▃▃▂▆▁▁▂▄▆
wandb:      train/policy_loss ▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▁▃▃▃▃▁▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80961
wandb: best/eval_avg_mil_loss 0.68622
wandb:  best/eval_ensemble_f1 0.80961
wandb:            eval/avg_f1 0.65834
wandb:      eval/avg_mil_loss 0.79052
wandb:       eval/ensemble_f1 0.65834
wandb:            test/avg_f1 0.65674
wandb:      test/avg_mil_loss 0.84185
wandb:       test/ensemble_f1 0.65674
wandb:           train/avg_f1 0.65294
wandb:      train/ensemble_f1 0.65294
wandb:         train/mil_loss 0.92745
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run logical-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2u7tj572
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051706-2u7tj572/logs
wandb: Agent Starting Run: iamx3gek with config:
wandb: 	actor_learning_rate: 7.350027642319752e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 51
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.19925413554880733
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051905-iamx3gek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iamx3gek
wandb: uploading history steps 50-51, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▇██
wandb: best/eval_avg_mil_loss ▇▄█▁▅▁
wandb:  best/eval_ensemble_f1 ▁▃▄▇██
wandb:            eval/avg_f1 ▅▆▅▄▅▁▄▄▃▅▅▇▅▅▃▇▄▄▆▅▅▄▅▅▆▅▄█▆▄▆▆▅▇▅▄▅▄█▄
wandb:      eval/avg_mil_loss ▃▂▄▃▄▇▄▄▃▂▁▃▃▄▂▄█▂▂▂▂▄▂▃▃▃▃▂▃▃▂▂▁▂▂▃▄▃▁▃
wandb:       eval/ensemble_f1 ▄▄▂▄▆▂▃▂▄▅▇▄▄▁▆▃▃▅▅▆▄▃▃▄▆▄▃█▆▃▇▅▅▄▆▆▂▄▃▃
wandb:           train/avg_f1 ▃▃▃▆▄▅▃▃▄▂▅▅▁▆▃▇▄▄▇█▅▁▂▇▅▆▄▅▄▅▅▄▃▅▂▃▄▅▅▄
wandb:      train/ensemble_f1 ▃█▃▃▆▅▃▃▄▂▅▁▆▃▇▇▄▄▇█▅▁▂▇▅▆▄▅▄▅▄▃▄▅▂▃▄▅▅▄
wandb:         train/mil_loss ▆▅██▇▇▅▄▆▆▅▆▅▆▁▆▅▇▃▃▅▇▆▆▄▆▄▅▄█▄▂▃▅▄▃▅▃▆▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅█▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▅▅▅▅▅▅▅█▅▅▅▁▅▅▅▅▅▅▅▅▅▅▂▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75837
wandb: best/eval_avg_mil_loss 0.76187
wandb:  best/eval_ensemble_f1 0.75837
wandb:            eval/avg_f1 0.71435
wandb:      eval/avg_mil_loss 0.82283
wandb:       eval/ensemble_f1 0.71435
wandb:           train/avg_f1 0.74438
wandb:      train/ensemble_f1 0.74438
wandb:         train/mil_loss 0.63691
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run clean-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iamx3gek
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051905-iamx3gek/logs
wandb: ERROR Run iamx3gek errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: sm9jgudl with config:
wandb: 	actor_learning_rate: 2.859631691486875e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 165
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.16174277753886557
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052012-sm9jgudl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sm9jgudl
wandb: uploading history steps 163-166, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇▇█
wandb: best/eval_avg_mil_loss █▅▅▂▁▃
wandb:  best/eval_ensemble_f1 ▁▄▅▇▇█
wandb:            eval/avg_f1 ▁▂▄▅▂▄▆▂▄▁▅▄▆▃▂▃▅▂▃▄▅▂▃▆▄█▅▄▆▅▁▄▃▄▅▆▄▇▅▇
wandb:      eval/avg_mil_loss ▅▃▅▅▅▅▃▃▂█▃▆▃▃▅▁▄▆▂▃▃▂▄▁▆▄▂▂▁▄▅▅▃▅█▂▄▃▂▃
wandb:       eval/ensemble_f1 ▁▅▁▄▄▄▄▁▄▃▂▃▃▃▄▅▆▂▃▅▅▅▇▅▃▄▄▃▃▁▅▅▂█▄▅▅▄▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▄▄▁▄▂▄▆▆▇▄▅▆▄▆▃▅▅▄▅▄▇▃█▄▂▆▅▅▄▅▆▅▄▅▅▄▅▅
wandb:      train/ensemble_f1 ▅▂▆▁▆▄▆▆▇▆▃▂▆▄▄▆▆▅▇▄▅▄▅█▅▃▇▅▆▆▆▅▅▅▇▆▆▁█▇
wandb:         train/mil_loss ▆▇█▅▅▆▅▆▄▄▄▃▅█▃▆▂▃▇▆▃▃▄▃▇▅▄▆▅▂▁▅▄▄▃▇▃▃▂▆
wandb:      train/policy_loss ▆█▅██████████████▁██████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83388
wandb: best/eval_avg_mil_loss 0.70051
wandb:  best/eval_ensemble_f1 0.83388
wandb:            eval/avg_f1 0.73933
wandb:      eval/avg_mil_loss 0.69798
wandb:       eval/ensemble_f1 0.73933
wandb:            test/avg_f1 0.61015
wandb:      test/avg_mil_loss 0.775
wandb:       test/ensemble_f1 0.61015
wandb:           train/avg_f1 0.69157
wandb:      train/ensemble_f1 0.69157
wandb:         train/mil_loss 0.85738
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sunny-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sm9jgudl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052012-sm9jgudl/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: g8wtzlas with config:
wandb: 	actor_learning_rate: 0.0007838290581396433
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 159
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2984103558697444
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052343-g8wtzlas
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g8wtzlas
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml; uploading history steps 146-159, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅███
wandb: best/eval_avg_mil_loss █▁▇▆▆▅
wandb:  best/eval_ensemble_f1 ▁▂▅███
wandb:            eval/avg_f1 ▅▇▄█▇▇▇▁▆▃▇▇▅█▆▆▆▆███▆▇▇▇█▁▅████▇█▆██▇▇█
wandb:      eval/avg_mil_loss ▆▄▇▇▆▅▇▅▆▁▃▂▅▆▆▆▆▇█▄▅▆▃▇▆▅█▆▄▁▂▇▁▂▆▄▆▆▇▅
wandb:       eval/ensemble_f1 ▆▆▆▆▆▇██▇▆▇▆█▆▅▇▇▆█▆▇▇▆▇▇█▆▇▇█▇█▅▆▁▆▅▆▇▇
wandb:           train/avg_f1 ▂▂▅▃▂▂▆▄▄▁▄▄█▄▅▅▆▅▄▆▅▂▅▇▇▇▆▆▆▅▆▅▇▃▅▇▅▅█▆
wandb:      train/ensemble_f1 ▆▆▆▅▅▂▄▁▅▂▄▄▄▆▂█▃▄▅▆▅▅▄▆▄▅▂▄▇█▆▅▅▆▅▄▆▇▄▆
wandb:         train/mil_loss ▆▄▂▇▄▅▄▂▆▆▄▆▇▅▂█▇▇▁▅▃▅▃▅▄▂▃▃▃▄▃▆▄▃▇▅▂▄▅▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8348
wandb: best/eval_avg_mil_loss 0.75616
wandb:  best/eval_ensemble_f1 0.8348
wandb:            eval/avg_f1 0.80277
wandb:      eval/avg_mil_loss 0.50106
wandb:       eval/ensemble_f1 0.80277
wandb:           train/avg_f1 0.74942
wandb:      train/ensemble_f1 0.74942
wandb:         train/mil_loss 0.71444
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run tough-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g8wtzlas
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052343-g8wtzlas/logs
wandb: ERROR Run g8wtzlas errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: cerjg4zi with config:
wandb: 	actor_learning_rate: 1.7923206852559134e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4698250376187164
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052633-cerjg4zi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cerjg4zi
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆█
wandb: best/eval_avg_mil_loss █▅▄▃▁
wandb:  best/eval_ensemble_f1 ▁▄▆▆█
wandb:            eval/avg_f1 ▅▄▄▆▆▆▅▄▆▃▄▆▄▄▃▆▄▆▇▆▇▄▄▆▆▇▄▆▄▄▆▆█▇▄▇▁▅▂▁
wandb:      eval/avg_mil_loss ▅▆▅▆▅▄▅▅▇▁▆█▅▃█▅▆▅▃▃▆▄▆▅▇▆▄▆▆▄▁▂▃▆▆▅▆▄▅▄
wandb:       eval/ensemble_f1 ▄▄▂▇▅▆▆▆▆▅▃▆▄▄▁▆▆▆▅▆▆▃▄▆▇▃▃▆▆█▄▄▄▇▅▆▆▅▁▄
wandb:           train/avg_f1 ▃▅▇▅▄▄▃▄▆▆▇▇▆▆▇▅▅▅█▇▆▇▁▅▃▂▅▃▇▃█▅▇▇▅▆▁▅▆▇
wandb:      train/ensemble_f1 ▄▃▆▄█▁▄▇▄▄▆▂▆▄▇▄▇▄▅▆▆▁▆▇▃▄▃▄▇▅▂▅▆▆▄▅▇▃█▇
wandb:         train/mil_loss ▆▅▃▅▆▆▅█▆▅▇▅▂▃▅▅▂▂█▆▅▄▁▄▆▅▂▅▃▂▁▅▃▁▃▅▆▂▅▁
wandb:      train/policy_loss ▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▅▇▇▇▇▇▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80257
wandb: best/eval_avg_mil_loss 0.59303
wandb:  best/eval_ensemble_f1 0.80257
wandb:            eval/avg_f1 0.59777
wandb:      eval/avg_mil_loss 0.94372
wandb:       eval/ensemble_f1 0.59777
wandb:           train/avg_f1 0.70294
wandb:      train/ensemble_f1 0.70294
wandb:         train/mil_loss 0.7483
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run jumping-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cerjg4zi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052633-cerjg4zi/logs
wandb: ERROR Run cerjg4zi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1kpo4t7c with config:
wandb: 	actor_learning_rate: 0.0004958388609648109
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 122
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3987538008602366
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052822-1kpo4t7c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1kpo4t7c
wandb: uploading history steps 117-122, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▄▄▅▅▆█
wandb: best/eval_avg_mil_loss ▇█▅▅▁▅▁▄▆▃▁
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▄▄▅▅▆█
wandb:            eval/avg_f1 ▆▆▆▇▄▆▇▆▃▅▇▇▆█▇▆▇▆▇▆▃▆▇▆▅▆▆▅▆▆▆▆▆▇▁▆▇▆▅▆
wandb:      eval/avg_mil_loss ▄▆▄▅▃▃▃▃▁▃▂▂▃▃▂▁▃▆▃▃▂▄▃▇▃▃▄▃▂█▃▃▄▂▃█▄▃▃▃
wandb:       eval/ensemble_f1 ▂▃▆▅▇▆█▇▆█▇█▆▁▆▁▅▆▇▆█▆█▅▆▇▅█▄▇▇▆▅▆▃▅▅▆▆▆
wandb:           train/avg_f1 ▂▂▆▂▂▄▄▃▄▅▃▅▅▄▅▆▃█▂▇▂▆▃▅▆▆▄▂▄▄▅▄▄▅▆▁▄▄▄▃
wandb:      train/ensemble_f1 ▁▆▄▂▆▆▆▃▃▆▃▂▅▄▆▃▆▁▃▂▁▆▆▅▆▆▄█▅▅▄▅▆▄▄▅▄▂▂▇
wandb:         train/mil_loss ▆▅▅▇▃▇▄▄▆█▆▅▃▅▅▄▃▅▁▂▃▅▂▃▄▆▃▅▄▂▃▄▃▅▅▁▃▂▄▄
wandb:      train/policy_loss ▂▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▁▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79404
wandb: best/eval_avg_mil_loss 0.70132
wandb:  best/eval_ensemble_f1 0.79404
wandb:            eval/avg_f1 0.69797
wandb:      eval/avg_mil_loss 0.87629
wandb:       eval/ensemble_f1 0.69797
wandb:           train/avg_f1 0.68894
wandb:      train/ensemble_f1 0.68894
wandb:         train/mil_loss 0.7186
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sandy-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1kpo4t7c
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052822-1kpo4t7c/logs
wandb: ERROR Run 1kpo4t7c errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: pn9zx4bg with config:
wandb: 	actor_learning_rate: 1.1057726374027227e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 187
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.13376297241031676
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053041-pn9zx4bg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pn9zx4bg
wandb: uploading wandb-summary.json
wandb: uploading history steps 174-188, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▆▆▇▇███
wandb: best/eval_avg_mil_loss ▇▇▇███▄▅▃▁
wandb:  best/eval_ensemble_f1 ▁▄▄▆▆▇▇███
wandb:            eval/avg_f1 ▆▆▆▇▅▆▆▇▆▅▆▂▆▁▄▃▅▇▆▅▁▅▆▅▇▇▁▇▇▆▇▆▅▅▇▇▆██▅
wandb:      eval/avg_mil_loss ▅▆▃▂▃▁▁▅▄▄▄▅▅▁▃▃▄▃█▃▃▅▄▃▄▄▅▁▃▄▅▅▄▅▂▂▃▃▃▅
wandb:       eval/ensemble_f1 ▆▆▇▆▃▇▆▆▂▅▁▇▆█▅▅▇▇▇▇▆▁▅▆█▇▆▇▆█▂▅▅▆▅▇▅▇▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▆▅▄▅▄▅▆▄▁▄▂▃▄▆▄▅▅▄▅▇▄▅▇▅█▅▅▃▅▇▄▅▄▆▆█▄▆
wandb:      train/ensemble_f1 ▄▄▅▅▃▇▅▁▅▄▇▃▆▃▂▃▄▃▆▅▄▄▂▅▅▅▄█▁█▅▂▅▅▇▃▂▆▆▃
wandb:         train/mil_loss ▆▄▅▄▃▅▂▆▆▄▅▅▆▁▅▄▂▅▇▅▂▄▂▄▃▃▃█▂▇▆▃▂▂▃▃▁▄▅▅
wandb:      train/policy_loss ████████████████████████████████▁███████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████████████████████▁██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.86409
wandb: best/eval_avg_mil_loss 0.50825
wandb:  best/eval_ensemble_f1 0.86409
wandb:            eval/avg_f1 0.70059
wandb:      eval/avg_mil_loss 0.85464
wandb:       eval/ensemble_f1 0.70059
wandb:            test/avg_f1 0.76232
wandb:      test/avg_mil_loss 0.49622
wandb:       test/ensemble_f1 0.76232
wandb:           train/avg_f1 0.71929
wandb:      train/ensemble_f1 0.71929
wandb:         train/mil_loss 0.66128
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fragrant-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pn9zx4bg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053041-pn9zx4bg/logs
wandb: Agent Starting Run: 5iprh2by with config:
wandb: 	actor_learning_rate: 5.668882692177237e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 177
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8404017855091248
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053402-5iprh2by
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5iprh2by
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇▇█████
wandb: best/eval_avg_mil_loss █▆▆▆▂▇▁▂▁
wandb:  best/eval_ensemble_f1 ▁▅▇▇█████
wandb:            eval/avg_f1 ▅▅█▄▄█▅▅▅▇▆▇█▆▅▅▇▇▁▅▇█▆▄▅▄▂▅▁▆▆▇▆▅▇▅▅▆▅▅
wandb:      eval/avg_mil_loss ▇▇▄▅▂▅▄▂█▅▅▃▅▄▁▂▄▂▄▃▇▃▁▅▄▂▅▂▁▂▇▄▃▂▄▄▂▄▄▁
wandb:       eval/ensemble_f1 █▆▆▅▅▁▆▇▃▆█▆▇▆▇▆▇▆▇▅▆▆▆▅▆█▄▇▆▆▆▆▇▆▇▅▆█▆█
wandb:           train/avg_f1 ▇▅▅▆▇▆▄▅▄▆▅▆▆▅▅▅▅▇▆▆▆▃▇▄▇▇▅▆▅▁▅▇▅▅▅█▆▆▆▅
wandb:      train/ensemble_f1 ▄▄▄▆▄▅▃▄▃▆▇▁▅▄▃▄█▇▄▄▂▅▅▅▇▆▄▃▂▃▇▇▇▅▅▄▅▆▄▆
wandb:         train/mil_loss ▅▇█▃▄█▅▆▄▆▂▅▄▆▃▆▇▅▅▅▅▃▅▄▄▃▅▇▃▁▂▂▆▂▇▃▅▇▆▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83342
wandb: best/eval_avg_mil_loss 0.58749
wandb:  best/eval_ensemble_f1 0.83342
wandb:            eval/avg_f1 0.80833
wandb:      eval/avg_mil_loss 0.62477
wandb:       eval/ensemble_f1 0.80833
wandb:           train/avg_f1 0.70291
wandb:      train/ensemble_f1 0.70291
wandb:         train/mil_loss 0.68731
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pretty-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5iprh2by
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053402-5iprh2by/logs
wandb: ERROR Run 5iprh2by errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: eoljvasj with config:
wandb: 	actor_learning_rate: 1.7570527127581364e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 143
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10137626068073124
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053714-eoljvasj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eoljvasj
wandb: uploading wandb-summary.json
wandb: uploading history steps 102-105, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅█
wandb: best/eval_avg_mil_loss █▇▁▁
wandb:  best/eval_ensemble_f1 ▁▃▅█
wandb:            eval/avg_f1 ▆█▄▅▄▄▆▆▇▇▆▇▆▆▇▆▇▇▇▆▇▆▅█▅▁▇▇▇▆█▆▇▅▇▅▆▆▇▆
wandb:      eval/avg_mil_loss ▅▄▃▁▃▅▄▃▃▄▂▃▅▄▅▄▄▄▃▄▄▂▃▃▄█▄▄▄▂▄▂▁▄▃▄▄▃▃▃
wandb:       eval/ensemble_f1 ▁▆█▁▁▅▁▅▅▁█▆▆▅▄▅▅▅▆▅▅▁▅▇▃▇▇▅▆▂▇▅▆▇▅▅▅▆▇▆
wandb:           train/avg_f1 ▆█▄▃▄▆▆▅▇▅▆▄▃▅▅▅▃▃▄▄▃█▃▄▅▅▅▁▆▃▇▅▃▂▆▂▄▄▄▂
wandb:      train/ensemble_f1 ▄▆▄▂▃▃▃▅▄▃▅▅▃▄▄▃▄▄▇▆▄▇▄▄▅▂▂▅▇▃▅▅▅▃▂▁▅▃█▁
wandb:         train/mil_loss █▅▆▅▅▄▅▆▃▃▅▅▃▂▃▄▁▃▃▅▃▆▇▅▂▅▆▇▄▄▅▃▂█▁▅▃▅▅▄
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▂▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄█▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████▁█████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81204
wandb: best/eval_avg_mil_loss 0.85397
wandb:  best/eval_ensemble_f1 0.81204
wandb:            eval/avg_f1 0.73788
wandb:      eval/avg_mil_loss 0.96261
wandb:       eval/ensemble_f1 0.73788
wandb:           train/avg_f1 0.66444
wandb:      train/ensemble_f1 0.66444
wandb:         train/mil_loss 0.73137
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lucky-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eoljvasj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053714-eoljvasj/logs
wandb: ERROR Run eoljvasj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: whignmrq with config:
wandb: 	actor_learning_rate: 0.0001229725791576035
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 163
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3637741605266264
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053907-whignmrq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/whignmrq
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▄▅▆▆█
wandb: best/eval_avg_mil_loss █▇▄▅▅▄▄▁
wandb:  best/eval_ensemble_f1 ▁▄▄▄▅▆▆█
wandb:            eval/avg_f1 ▅▄▆▅▆▅▅▆▃▇▇▅▅▆▅▇▇▇▆▃▇▆█▇▄▆▆▄▅▁▆▇▆▂▆▅▅▃█▃
wandb:      eval/avg_mil_loss ▅▆▄▃▆█▅▃█▁▁▄▃▄▂▄▃▂▅▃▂▁▆▄▃▃▄▆▃▃▃▂▃▄▅▇▄▂▄▄
wandb:       eval/ensemble_f1 ▆▆▄▁▃▄▅▄▄▂▄▅▄▄▄▃▇▅▄▄▆▂▅▃▆▅▃▅█▅▇▅▆▅▆▅▂▁▅▅
wandb:           train/avg_f1 ▁▄▆▆▄▁▅▄▅▄▃█▅▃▄▆▇▆▅█▅▆▅▅▇▇▅▂▄▇▄▅▂▄▅▅▇▃▇▅
wandb:      train/ensemble_f1 ▅▄▆▂▄▆▄▄▃▇▇▂▅▇▅▇▆▇▆▆▄▃▃▆█▃▅▂▄▅▁▆▄▄▇▃▇▅▆▇
wandb:         train/mil_loss ▂▇█▅▇▄▅▁▂█▆▅▅▂▄▃▁▇▃▂▄▄▆▃▆▄▅▂▃▆▅▄▄▄▄▃▆▄▃▃
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82414
wandb: best/eval_avg_mil_loss 0.54109
wandb:  best/eval_ensemble_f1 0.82414
wandb:            eval/avg_f1 0.50243
wandb:      eval/avg_mil_loss 1.18969
wandb:       eval/ensemble_f1 0.50243
wandb:           train/avg_f1 0.71257
wandb:      train/ensemble_f1 0.71257
wandb:         train/mil_loss 0.7533
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run magic-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/whignmrq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053907-whignmrq/logs
wandb: ERROR Run whignmrq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: gyft0fk1 with config:
wandb: 	actor_learning_rate: 1.5173766160684695e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 170
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.708623118656289
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054232-gyft0fk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gyft0fk1
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▆▇▇██
wandb: best/eval_avg_mil_loss █▄▅▃▅▄▂▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▆▇▇██
wandb:            eval/avg_f1 ▆▇▅▄▅▄▇█▆▆▇▄▇█▄▅▁▅▃▁▇▆▄▆▃▅▇▆▅█▇▅▄▂▆█▄▄▃▄
wandb:      eval/avg_mil_loss ▆▃▅▄▄▃▇▄▆▅▃█▅▅▄▄█▅▅▄▅▄▅▁▇▂▅▅▃▁▁▄▅▃▅▂▄▅▃▃
wandb:       eval/ensemble_f1 ▅▅▆▇▅█▅▂▅▄▄▁▄▄▇▄▂▅▅▄▅▃▁▃▃▅█▆▅█▆▅▅▄▃▅▃▃▃▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▅▄▅▆▂▆▅▃▅▃▆▅▃▇▅▆▆▅▆▄▄▆▅▆▆▄▃▃▆█▇▅▆▃▅▆▄▆▆
wandb:      train/ensemble_f1 ▆▁▄▅▆▅▅▄▃▃▅▃▅▃▃▄▆▅▄▆▅▆▅▇▄▃▆▆▄▆▅▅▅█▆▃▆▅▇▅
wandb:         train/mil_loss ▄█▄▅▅▄▅▄▃▁▆▆█▇▅▆▃▅▃▅▄▄▆▂▃▅▄▆▅▇█▇▃▄▅▄▃▂▅▃
wandb:      train/policy_loss █████████████▁██████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████████████████▁███▁███████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79818
wandb: best/eval_avg_mil_loss 0.63801
wandb:  best/eval_ensemble_f1 0.79818
wandb:            eval/avg_f1 0.62596
wandb:      eval/avg_mil_loss 0.86166
wandb:       eval/ensemble_f1 0.62596
wandb:            test/avg_f1 0.70963
wandb:      test/avg_mil_loss 0.64201
wandb:       test/ensemble_f1 0.70963
wandb:           train/avg_f1 0.67777
wandb:      train/ensemble_f1 0.67777
wandb:         train/mil_loss 0.79925
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gyft0fk1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054232-gyft0fk1/logs
wandb: Agent Starting Run: zj2ekkbk with config:
wandb: 	actor_learning_rate: 5.2230421464239945e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 167
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9705725431029408
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054605-zj2ekkbk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zj2ekkbk
wandb: uploading history steps 163-167, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄▅▅█
wandb: best/eval_avg_mil_loss ▇▆▄▂█▄▁
wandb:  best/eval_ensemble_f1 ▁▃▃▄▅▅█
wandb:            eval/avg_f1 ▄▃▃▃▄▄▄▄▃▃▃▅▅▇▆▅▃▃▄▄▆▂█▄▄▄▅▃▆▇▂▇▄▁▆▄█▆▄▄
wandb:      eval/avg_mil_loss ▄▇▃▅▄█▄▁▆▆▃▇▇▅▃▃▅▄▂▅█▄▃▅▅▅▃▄▅▄▄▇▅█▃▃▅▂▂▁
wandb:       eval/ensemble_f1 ▃▃▂▃▂▄▅▂▆▁▄▃▃▆▂▃▃▄▄▄▆▃▂▄▂▅▄▄▄█▄▃▁▆▅▃▆▇▁▃
wandb:           train/avg_f1 ▄▆▄▂▁▃▄▁▄▄▄▅▅█▃▄▂▄▃▅▆▂▄▇▅▇▃▇▅▁█▂▅▅▅▅▃▅▃▅
wandb:      train/ensemble_f1 ▃▃▂▂▂▁▃▃▃▄▃▃▄▂▂▂▃▃▄▇▄▃▆▃▅▃▁▅▃█▆▅▅▇▅▂▂▅▅▃
wandb:         train/mil_loss ▇▇▇▇▅▆▅▅▄▇▅▆█▅▄▇▄▅▆▆▂▅▇▅▃▂▃▃▁▄▄▃▄▂▃▄▅▂▃▄
wandb:      train/policy_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▁▂▂▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂█▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.85123
wandb: best/eval_avg_mil_loss 0.50381
wandb:  best/eval_ensemble_f1 0.85123
wandb:            eval/avg_f1 0.80053
wandb:      eval/avg_mil_loss 0.58351
wandb:       eval/ensemble_f1 0.80053
wandb:           train/avg_f1 0.73741
wandb:      train/ensemble_f1 0.73741
wandb:         train/mil_loss 0.67321
wandb:      train/policy_loss -0.14957
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.14957
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run colorful-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zj2ekkbk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054605-zj2ekkbk/logs
wandb: ERROR Run zj2ekkbk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: nyv7nzma with config:
wandb: 	actor_learning_rate: 2.771907346480686e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 172
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.18276892180104776
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054936-nyv7nzma
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nyv7nzma
wandb: uploading history steps 159-173, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▆▆▆▇▇▇▇███
wandb: best/eval_avg_mil_loss ▅▆█▂▄▃▃▃▂▂▂▁▁
wandb:  best/eval_ensemble_f1 ▁▆▆▆▆▆▇▇▇▇███
wandb:            eval/avg_f1 ▃▆▆▅▄▁▆▆▅▃▃▇▃▆▇▇▃▆▃▆▇▇▇▇▆▇▃▃▃▃▄▇▇▇▇▇██▃▄
wandb:      eval/avg_mil_loss ▂▄▄▆▅█▅▂▄▃▃▃▅▂▃▄▂▃▂▂▅▂▃▄▅▂▆▄▂▁▆▃▂▄▅▆▄▁▃▁
wandb:       eval/ensemble_f1 ▄▅▆▆▄▆▁▇▆▄▇▇▇▇▇█▇▇▇▇▇▇▇▅█▆▄▇▇▄▇█▁▇▅▆█▆▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▄▄▆▄▄▂▇▁▆▅▅▅▂▆▃▆▆▄▄▅▅▆▃▆▇▅▇▇▆▅▅▆▆▃▇█▃▂
wandb:      train/ensemble_f1 ▃▅▆▃▄▄▇▅▅▆▅▂▇▇▄▃▅▅█▄▂▇▅▆▆▆▃▄▇▅▄▄▅▆▃▃▅▁█▁
wandb:         train/mil_loss ▆█▇▆▇▆▆▆▄█▇▆█▅▆▄▆▅▆▄▆▄▅▄▄█▄▂▇▄█▃▄▅▂▄▁▃▄▃
wandb:      train/policy_loss ▁▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▆▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.87231
wandb: best/eval_avg_mil_loss 0.437
wandb:  best/eval_ensemble_f1 0.87231
wandb:            eval/avg_f1 0.72463
wandb:      eval/avg_mil_loss 0.68755
wandb:       eval/ensemble_f1 0.72463
wandb:            test/avg_f1 0.66752
wandb:      test/avg_mil_loss 0.59745
wandb:       test/ensemble_f1 0.66752
wandb:           train/avg_f1 0.73265
wandb:      train/ensemble_f1 0.73265
wandb:         train/mil_loss 0.58916
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run olive-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nyv7nzma
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054936-nyv7nzma/logs
wandb: Agent Starting Run: nq2bpbq1 with config:
wandb: 	actor_learning_rate: 3.355028320910748e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 119
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8776498605205327
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055241-nq2bpbq1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nq2bpbq1
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁█
wandb: best/eval_avg_mil_loss ██▁
wandb:  best/eval_ensemble_f1 ▁▁█
wandb:            eval/avg_f1 ▆██▇█▃▅▃▇▇▃▆▇▃▃▆▇▆▁▇▄▇█▆▆▆█▇█▇█▆▆█▆▆█▆▄▄
wandb:      eval/avg_mil_loss ▃▄▂▄▄▆▁▆▂▃▅▆▅▄▅▄▂▃▂▂█▅▄▂▆▄▄▂▃▃▄▃▆▄▄▄▄▆▂▆
wandb:       eval/ensemble_f1 ██▆▆▃▄▇▇▇▆▅█▆▃▇▆▃▃▅▆▆█▆▇▅▄▆█▇▅▁█▆▃▆█▃▆▆▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▅▅▆▆█▃█▄▅▂▄▄▆▆▆▅▇▃▁▆▆▂▇▃▃▅▅▅▆▃▅▅▅▄▃▃▇▆▇
wandb:      train/ensemble_f1 ▂▄▆▆▆▁▇▆▃▄▆▃█▄▅▄▅▇▄▁▆▅▆▇▇▆▅▄█▃▆▅▆▄▅▂▃▆▅▇
wandb:         train/mil_loss ▅▃▆▂▇▁▆▃▆▆▄▃█▁▂▂▇▄▃▃▆▃▃▅▄▃▄▄▃▃▁▃▁▄▄▄█▆█▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8648
wandb: best/eval_avg_mil_loss 0.54424
wandb:  best/eval_ensemble_f1 0.8648
wandb:            eval/avg_f1 0.56934
wandb:      eval/avg_mil_loss 0.79651
wandb:       eval/ensemble_f1 0.56934
wandb:            test/avg_f1 0.77859
wandb:      test/avg_mil_loss 0.34471
wandb:       test/ensemble_f1 0.77859
wandb:           train/avg_f1 0.74688
wandb:      train/ensemble_f1 0.74688
wandb:         train/mil_loss 0.87728
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run good-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nq2bpbq1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055241-nq2bpbq1/logs
wandb: Agent Starting Run: 54d593r8 with config:
wandb: 	actor_learning_rate: 0.0006437901564506275
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 168
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.64513733898379
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055456-54d593r8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/54d593r8
wandb: uploading history steps 159-168, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅▆██
wandb: best/eval_avg_mil_loss ▄▃█▃▂▄▁▂
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅▆██
wandb:            eval/avg_f1 ▁▇▇▅▅▅▅█▆▄▅▆▅▄▆▄▅▇▄▅▅▅▄▅▅▄▅▅▆▄▇▆▃▇▅▄▅█▆▅
wandb:      eval/avg_mil_loss ▇▅▃▄▃▄▄▇▅▃▄▅▇▃▇▅▇▂▅▅▂▅▇▆▁▄▂▅▂▂▄▂▄▃███▇▅▃
wandb:       eval/ensemble_f1 ▄▆▅▂▅▆▅▇▅▆▅▄▆▅▃▃▆▆▃▆▃▃█▆▅▅▄▅▇▄▅▆▃▅▆▅▁▄▃▆
wandb:           train/avg_f1 ▆▆▅▃▄▇▄▄▅▆▅▄▃▂▆▃▅▄▃▃▆▃▄█▁▆▃▇▄▄▃▅▇▇▂▆▄▆▅▄
wandb:      train/ensemble_f1 ▇▇▄▅▇▇▅▅▅▆▃▅▆▅▅▄█▄▇▁▇▅▆▅█▅█▅▆▇▆▆▃▅▆▅▅▄▆▅
wandb:         train/mil_loss ▄▆▇▃▆█▇▅▃▂▅▇█▅▅▄▆▇▁▆▇▅▅▂▃▄▅▄▅▅▇▅▅▆▄▃▆▂▃▆
wandb:      train/policy_loss ▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄███▁▁▄█▁▄██▆▁▄▁▄▄▄█▃▄▁▁█▄▄▆█▁▄█▁▁▄█▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77493
wandb: best/eval_avg_mil_loss 0.6979
wandb:  best/eval_ensemble_f1 0.77493
wandb:            eval/avg_f1 0.58759
wandb:      eval/avg_mil_loss 0.89921
wandb:       eval/ensemble_f1 0.58759
wandb:           train/avg_f1 0.61951
wandb:      train/ensemble_f1 0.61951
wandb:         train/mil_loss 0.84259
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run serene-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/54d593r8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055456-54d593r8/logs
wandb: ERROR Run 54d593r8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: pgkp1gnf with config:
wandb: 	actor_learning_rate: 3.2474349011586263e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 129
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04078006427032632
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055828-pgkp1gnf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pgkp1gnf
wandb: uploading wandb-summary.json; uploading config.yaml; uploading history steps 116-129, summary
wandb: uploading history steps 116-129, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆█
wandb: best/eval_avg_mil_loss ▁█▃█
wandb:  best/eval_ensemble_f1 ▁▄▆█
wandb:            eval/avg_f1 █▇▇▅▂▃▃▇▆▆▄▇▃▆▄█▄▂▆▇▇▄▇██▇▇▅██▅▁█▇▇▆▅▅▆▇
wandb:      eval/avg_mil_loss ▃██▃▄▅▄▇▄▁▅▄▇▆▅▅▅▇▅▄▅▃▃▅▅▂▆▃▄▇▅▇▄█▇▄▅▅▇▅
wandb:       eval/ensemble_f1 █▆▇▇▇▆█▄▆▇▆▅▆▆▃▆▇▄▄▄▆▇▆▃█▄▄▇▆▅█▅▁▆▆▅█▆▃▅
wandb:           train/avg_f1 ▂▂▂▄▄▇▁▁▄▆▆▄▄▅▅▇▃▅▇▅▂█▄▅▃▄▄▅▂▆▆▂▄▂▅▆▂▅█▂
wandb:      train/ensemble_f1 ▁▁▂▅▆▂█▂▆▆▇▄▆▄▅▂▂▅▆▇▂▄▆▅▆▆▅▄▆▄▆▃▃▄▇▂▄▂▅▂
wandb:         train/mil_loss █▆▄▆▆▆▅▆▅▅▅▄▆▄▆▅▆▇▆█▃▄▅▇▅▄▇▅▇▇▁▇▆▃▄▃▂█▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8228
wandb: best/eval_avg_mil_loss 0.92696
wandb:  best/eval_ensemble_f1 0.8228
wandb:            eval/avg_f1 0.65535
wandb:      eval/avg_mil_loss 0.92794
wandb:       eval/ensemble_f1 0.65535
wandb:           train/avg_f1 0.70501
wandb:      train/ensemble_f1 0.70501
wandb:         train/mil_loss 0.60172
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fast-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pgkp1gnf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055828-pgkp1gnf/logs
wandb: ERROR Run pgkp1gnf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: v2zvhyve with config:
wandb: 	actor_learning_rate: 0.00025109168519178016
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 188
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5709450969100892
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060057-v2zvhyve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v2zvhyve
wandb: uploading history steps 186-189, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▅▆▆▆▆▇█
wandb: best/eval_avg_mil_loss █▇▆▅▅▄▄▄▅▁
wandb:  best/eval_ensemble_f1 ▁▂▅▅▆▆▆▆▇█
wandb:            eval/avg_f1 ▃▄▃▇▅▆▆▆▅▅█▁▁▄▅▅▇▅▃▆▄▇▆▇▆▇▄▅▇▄▅▇▆▅▁▇▂▅▇█
wandb:      eval/avg_mil_loss █▇▆▇▅▇█▆▄▅▆▆▄▅▅▆▆▆█▆▇▅▆▅▆▄█▆▇▁▆▃▅▇▅▄▅▅▂▆
wandb:       eval/ensemble_f1 ▃▆▃▇▅▆▅▇▆▆▅▅▅█▅▇▄▄▄▄▆▁▆▆▁▆▄▃▅▇▆▂▂▅▄█▅▄▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆▃▃█▄▄▆▇▄▄▇▆▄▅▂▃▅▇▆▃▄▅██▅▇▄▆▁▅▅▆▅█▃▆█▆▆
wandb:      train/ensemble_f1 ▆▄▂▂▅▃▇▂▆▃▅▄▅▄▃▄▇▄▆▆▂▅▁▅▅▆▅█▆▆▅█▆▅▅█▃█▅▅
wandb:         train/mil_loss ▂▅▃▆▃▅▁█▂▅▆▅▃▆▆▅▆▃▇▄▅▄▂▃▃▄▂▃▅▅▄▆▅▄▃▅▆▃▂▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅█▅▁▁▅▅▅▅▅▅▅▁▅▅▅▅█▅▅▅▁██▅▁▅▅▅▁▅▅█▅▅▅▅█▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8436
wandb: best/eval_avg_mil_loss 0.53068
wandb:  best/eval_ensemble_f1 0.8436
wandb:            eval/avg_f1 0.78927
wandb:      eval/avg_mil_loss 0.6665
wandb:       eval/ensemble_f1 0.78927
wandb:            test/avg_f1 0.78736
wandb:      test/avg_mil_loss 0.66511
wandb:       test/ensemble_f1 0.78736
wandb:           train/avg_f1 0.73962
wandb:      train/ensemble_f1 0.73962
wandb:         train/mil_loss 0.65236
wandb:      train/policy_loss 0.525
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.525
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run polar-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v2zvhyve
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060057-v2zvhyve/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qxgwqbts with config:
wandb: 	actor_learning_rate: 0.0006301622196264024
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1588695049739861
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060428-qxgwqbts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qxgwqbts
wandb: uploading wandb-summary.json
wandb: uploading history steps 116-117, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss █▃▁
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 ▄█▁▆▆▃▂█▄▂▄▇▇▃▂▆▃▇▃▇▇▇▆▇▄▄▇▇▃▆▅▁▄▃▅▄▆▂▇█
wandb:      eval/avg_mil_loss ▃▆▃▅▅▃▅▄▄▃▄▄▃▄▁▅▂▄▆▃▃▃▂▃▂▂▆▃▂▄█▆▃▃▃▆▃▄▂▅
wandb:       eval/ensemble_f1 ▆█▁▇▃▆▇██▄▇▃▆▆▃▇▇▃▇▇▇▇█▂▁▅█▄▃▁▇▃▇▇▇▆▂▇▂▅
wandb:           train/avg_f1 ▆▅▃▁▃▆▆▄▅▄▄▃▆▄▅▁▆▄▃▅▆▃▄▁█▂▆▆▄▂▄▅▃▅▄▄▅▅▅▄
wandb:      train/ensemble_f1 ▄▇▅▃▅▂▇▇▆▄▂▃▆▄▆█▅▆▆▅▇▄▅█▇▅█▇▇▅▇▆▁▄▆▆▅▆▆▃
wandb:         train/mil_loss ▄▄▁▅▃▆▅▆▇▄▂▁▆▂▃▅▃▃▅▅▄▅▃▆▃█▄▄▇▃▅▄▁▅▄▄▄▄▃▄
wandb:      train/policy_loss ███████████████████████▁████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████▁████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81526
wandb: best/eval_avg_mil_loss 0.79337
wandb:  best/eval_ensemble_f1 0.81526
wandb:            eval/avg_f1 0.5827
wandb:      eval/avg_mil_loss 1.03543
wandb:       eval/ensemble_f1 0.5827
wandb:           train/avg_f1 0.65032
wandb:      train/ensemble_f1 0.65032
wandb:         train/mil_loss 0.94432
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run usual-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qxgwqbts
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060428-qxgwqbts/logs
wandb: ERROR Run qxgwqbts errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5w2fvwxy with config:
wandb: 	actor_learning_rate: 2.168567723674717e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 141
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4228941782035466
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060638-5w2fvwxy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5w2fvwxy
wandb: uploading wandb-summary.json
wandb: uploading history steps 129-142, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▁▁██▆▆▆▆█▁▆▅▆█▆▆▆▅▂▇▅▆▆▆▆▇▆▁▇▅▇█▆▂█▃▆▄▃
wandb:      eval/avg_mil_loss █▇▄▄▄▄▂▅▆▆▃▁▅▃█▂▄▃▂▆▄▃▄▄▃▅▅▅▅▅▃▆▃▆▃▆▅▅▃▂
wandb:       eval/ensemble_f1 ▁▆▆▇▇█▁▁█▇▂█▄▇█▆█▃▂▇▂▇▄▇▆▂█▅▃▆▆▃▃▇▃▂▆▁█▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅█▇▇▆█▇▅▆▄▁▅▇▆▅▆▆▅▂█▅▇▃▃▅▅▃▆▇▃▅▆▇▆▇▇▆▄▂
wandb:      train/ensemble_f1 █▂▅▄█▁▄▅▇▇▆▄▆▇▁▆▆▆█▅▆▅▆▇▇▅▅▅▇▄▅▇▂▃▅▇▇▅▂▆
wandb:         train/mil_loss █▅▂▅▃▄▁▄▄▄▅▄▅▃▆▁▆▁▇▄▃▅▄▂▅▄▃▂▃▃▂▁▁▂▄▇▂▄▂▁
wandb:      train/policy_loss ████████████▁████████████▁██████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃█▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.85627
wandb: best/eval_avg_mil_loss 1.18599
wandb:  best/eval_ensemble_f1 0.85627
wandb:            eval/avg_f1 0.655
wandb:      eval/avg_mil_loss 0.75015
wandb:       eval/ensemble_f1 0.655
wandb:            test/avg_f1 0.85465
wandb:      test/avg_mil_loss 0.66335
wandb:       test/ensemble_f1 0.85465
wandb:           train/avg_f1 0.68434
wandb:      train/ensemble_f1 0.68434
wandb:         train/mil_loss 0.70451
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run prime-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5w2fvwxy
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060638-5w2fvwxy/logs
wandb: Agent Starting Run: tgar004y with config:
wandb: 	actor_learning_rate: 0.00036420837374157326
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 103
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2936177956175936
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060913-tgar004y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tgar004y
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▅█
wandb: best/eval_avg_mil_loss ▅▄█▄▁
wandb:  best/eval_ensemble_f1 ▁▂▃▅█
wandb:            eval/avg_f1 ▁▅▇▇▇▆▃▅██▅▃▄▇▄▇█▇██▇▇▄▇▆▇▅▇▆▇▄▇▆█▂█▆█▂▆
wandb:      eval/avg_mil_loss ▄▆▅▃▅▇▆▃▅▃▇▃▁▃▄▃▅▁▅▃▆▃▃▃▂▅▃▂▄▆▃▃▅▆▃▃▃█▅▅
wandb:       eval/ensemble_f1 █▁▇▇▆█▇▆████▅▄▄▄██▇▇█▇▄▇██▆██▇▁▂▆█▆▂██▆█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▄▆▇▃▆▅▆▂▃▅▂▃█▅▄▄▄▅▃▅▆▄▅▂▄▆▆▅▄▅█▅▁▅▄▆▅▄
wandb:      train/ensemble_f1 ▅▄▇▇▃▅▆▆▇▆█▆▃▆▄▃▅▇▄▅▅▅▅▆▆▄▄▆▅▇▆▅▄▆▁▄▆▄▅▆
wandb:         train/mil_loss ▄▃▅▄▅▅▂▃▄▄▃▇▅▂▅▄▆▃▄▃▃▄▅▂▆▅▅▅▃▃▅▅█▅▁▅▁▃▆▃
wandb:      train/policy_loss █████▃████████████████▁████████████████▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████▁██████████████████████████████████▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8266
wandb: best/eval_avg_mil_loss 0.56621
wandb:  best/eval_ensemble_f1 0.8266
wandb:            eval/avg_f1 0.68227
wandb:      eval/avg_mil_loss 1.02639
wandb:       eval/ensemble_f1 0.68227
wandb:            test/avg_f1 0.84982
wandb:      test/avg_mil_loss 0.29431
wandb:       test/ensemble_f1 0.84982
wandb:           train/avg_f1 0.74545
wandb:      train/ensemble_f1 0.74545
wandb:         train/mil_loss 0.7416
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worldly-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tgar004y
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060913-tgar004y/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 64o3p8sn with config:
wandb: 	actor_learning_rate: 6.797231477134681e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 131
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9017662075411088
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061135-64o3p8sn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/64o3p8sn
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇█
wandb: best/eval_avg_mil_loss █▄▄▁▃
wandb:  best/eval_ensemble_f1 ▁▅▆▇█
wandb:            eval/avg_f1 ▅▆▄▄▆▄▄▆▅▅▄▄▅▄▆▄▃▂▅▄▅▅▃▁▅▅▄▅▆▂▅▆▅▄▆▄▆▆█▅
wandb:      eval/avg_mil_loss ▇▄▃▃▆▄▄▄▅▂▄▃▄▃▅▇▄▄▅█▆▂▄▃▄▆▂▃▅▇▅▄▅▃▄▂▁▂▄▂
wandb:       eval/ensemble_f1 ▆▅▅▅▆▆▅▅▆▅▅█▇▂▇▃▄▆▄▄▃▁▅▅▄▄▂▅▂▇▆▅▄▇▅▆▅▅▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▂▆▃▂▂▅▅▆▆▇▇▄█▁▄▆▅▁▁▆▅▃▅▄▃▄▄▃▃▆▅▅▅▄█▅▄▄
wandb:      train/ensemble_f1 ▇▇▃█▅▃▅▅▇▆▅▅▅▄▇▃▃▆▇▃▅▅█▇▃▅█▆▄▆▄▇▇█▅▄▁▅▄▆
wandb:         train/mil_loss ▄▆▅▇▄▆▄▅▅█▅▃▄▅▄▅▃▄▅▄▅▅▄▃▅▄▅▄▃▄▄▁▆▄▆▄▃▅▄▄
wandb:      train/policy_loss ▅▅▅▅▅▅▅▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▁▅▅▅▅▅▅▅▅▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▇▅▅▅▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84015
wandb: best/eval_avg_mil_loss 0.76518
wandb:  best/eval_ensemble_f1 0.84015
wandb:            eval/avg_f1 0.72957
wandb:      eval/avg_mil_loss 0.70635
wandb:       eval/ensemble_f1 0.72957
wandb:            test/avg_f1 0.78736
wandb:      test/avg_mil_loss 0.60064
wandb:       test/ensemble_f1 0.78736
wandb:           train/avg_f1 0.70722
wandb:      train/ensemble_f1 0.70722
wandb:         train/mil_loss 0.62847
wandb:      train/policy_loss -0.46516
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.46516
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run graceful-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/64o3p8sn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061135-64o3p8sn/logs
wandb: Agent Starting Run: 2glpvtou with config:
wandb: 	actor_learning_rate: 1.8603741224438782e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 85
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6489150423635117
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061400-2glpvtou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2glpvtou
wandb: uploading history steps 74-86, summary; uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▇███
wandb: best/eval_avg_mil_loss █▅▅▃▁▃▁
wandb:  best/eval_ensemble_f1 ▁▆▆▇███
wandb:            eval/avg_f1 ▅▄▅▆▆▆▄▃▅▅▆▅▆▅▁▅▇▆▆▅▅▁▆█▅▄▆▅▆▅█▇▃█▇▆▅▆▆▇
wandb:      eval/avg_mil_loss ▇▄▅▄▆▇▅█▆▅▃▅▅▄▅▅▇▄▄▄▃▇▁▃▅▅▄▄▆▆▄▅▃▅▅▄▅▇▆▇
wandb:       eval/ensemble_f1 ▁▆▇▄▆▅▃▅▅▆▆▂▅▅▆▄▆▆▅█▂▆█▆▅▆▆▇▆▄▃▅▅▇▆▅▃▅▅▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▅▃▆▆▁▄▂▄▅▆▅▂▅▂▄▆▄▇▄▂▃▇▂▆▇▅▃█▃▂▅▄▇█▄▅█▆
wandb:      train/ensemble_f1 ▃▂▄▂▄▄▃▁▄▃▅▂▄▄▄▅▄▆▅▃▄▂▅▄▅▂▆▁▄▄▃▄▆▅▄█▅▄▆▃
wandb:         train/mil_loss ▂▆▆▂▆▅▆▃▅▅▅█▆▅▄▂▃▅▇▂▆▃▄▆▆▂▃▃▃▆▁▂▅▃▄▄▂▄▂▃
wandb:      train/policy_loss █████▁███████▇██████████████████████▆███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████▁████▅██████████████████████▆███████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.85571
wandb: best/eval_avg_mil_loss 0.459
wandb:  best/eval_ensemble_f1 0.85571
wandb:            eval/avg_f1 0.61674
wandb:      eval/avg_mil_loss 1.0268
wandb:       eval/ensemble_f1 0.61674
wandb:            test/avg_f1 0.63655
wandb:      test/avg_mil_loss 0.9413
wandb:       test/ensemble_f1 0.63655
wandb:           train/avg_f1 0.72008
wandb:      train/ensemble_f1 0.72008
wandb:         train/mil_loss 0.69751
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run treasured-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2glpvtou
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061400-2glpvtou/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9bhdrdak with config:
wandb: 	actor_learning_rate: 1.7340925134925288e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7540514603693792
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061558-9bhdrdak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9bhdrdak
wandb: uploading history steps 183-185, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆██
wandb: best/eval_avg_mil_loss █▃▁▁
wandb:  best/eval_ensemble_f1 ▁▆██
wandb:            eval/avg_f1 ▄▆▄▅▅▅▅▆▄▄▇▅▇▅▄▃▇▁▆▇▅▅▆▆▅▄▄▄▅▄▃▆▄▆▆█▁▅▇▅
wandb:      eval/avg_mil_loss █▆▅▃▅▇▃▃▃▄▆▄▃▃▃▅▂▄▄▅▅▆▁▁▄▃▂▆▃▂▂▃▅▁▄▃▂▂▁▃
wandb:       eval/ensemble_f1 ▄▄▄▃▃▅▆▇▆▆▅▅▃▇▇█▁▇▇▄▃▇▅▆▅▆▇▄▅▆▇▇▆▃▃█▅█▇▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▆▁▄▃▄▂▆▃▅▅▅▄▂▆▅▆▇▃█▄▅▁▅▄▅▆▅▃▇▅▆▇▆█▆▄▅▇
wandb:      train/ensemble_f1 ▄▃▃▅▅▄▅▁▃▅▄▃▃▁▃▇▄▃▃▂▂▆▃▁▅▄▄▄▄▅▆█▅▆▅▅▄▅▄█
wandb:         train/mil_loss ▇▇▇▅▅█▆▇▆▅▃▅▅▇▄▅▄▄▅▂▄▄▄▃▂▂▃▁▄▄▂▃▂▃▄▁▂▃▃▃
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▂▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▆▇▇▇▇▇▇▃▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83388
wandb: best/eval_avg_mil_loss 0.58828
wandb:  best/eval_ensemble_f1 0.83388
wandb:            eval/avg_f1 0.74747
wandb:      eval/avg_mil_loss 0.65606
wandb:       eval/ensemble_f1 0.74747
wandb:            test/avg_f1 0.82039
wandb:      test/avg_mil_loss 0.53141
wandb:       test/ensemble_f1 0.82039
wandb:           train/avg_f1 0.79881
wandb:      train/ensemble_f1 0.79881
wandb:         train/mil_loss 0.59651
wandb:      train/policy_loss 0.15822
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.15822
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sandy-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9bhdrdak
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061558-9bhdrdak/logs
wandb: Agent Starting Run: ms8e2lix with config:
wandb: 	actor_learning_rate: 1.2118555336415223e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 119
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8197175603176269
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061950-ms8e2lix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ms8e2lix
wandb: uploading history steps 110-120, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▅▇█
wandb: best/eval_avg_mil_loss █▅▃▅▃▁
wandb:  best/eval_ensemble_f1 ▁▅▅▅▇█
wandb:            eval/avg_f1 ▄▁▂▃▄▃▅▆▂▅▅▄▄▂▁▄▅▃▄▆▃▂▆▃▄▁▅▃▄▃▆▆▃█▃▆▅▇▅▄
wandb:      eval/avg_mil_loss █▄▄▅▆▄▂▅▃▇▄▄▇▅▆▄▅▃▃▂▇▄▂▅▆▂▁▄▅▅▅▂▄▄▃▄▅▄▄▂
wandb:       eval/ensemble_f1 ▂▃▁▂▃▄▇▃▅▄▅▂▃▅▇▆▇▂▄▅▇▄▅▆█▅▅▇▃▄▄▄▆█▄▅▅█▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▅▁▁▄▆▄▅▄▄▃▄▃▄▆▄▅▆▃▂▆▇▆▇▄▄▅▇▆▇▅▇▅▆▇▅▆█▅▆
wandb:      train/ensemble_f1 ▄▅▁▄▄█▅▆▇▃▄▇▅▇▅▄▆▆▄▆▆▅▅▆▇▅▅▆▃▅▆▇█▆▇█▇▇█▆
wandb:         train/mil_loss ▅▇█▅▆▅▅▃▇▅▆▆▅▄▄▅▅▃▆▂▃▅▄▅▅▄▂▃▃▁▃▂▃▃▁▁▃▃▃▂
wandb:      train/policy_loss █████████▆███▇███▇██████▆███▁█████▇█████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▁▇█▆▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84499
wandb: best/eval_avg_mil_loss 0.4676
wandb:  best/eval_ensemble_f1 0.84499
wandb:            eval/avg_f1 0.78022
wandb:      eval/avg_mil_loss 0.56589
wandb:       eval/ensemble_f1 0.78022
wandb:            test/avg_f1 0.78301
wandb:      test/avg_mil_loss 0.48267
wandb:       test/ensemble_f1 0.78301
wandb:           train/avg_f1 0.78071
wandb:      train/ensemble_f1 0.78071
wandb:         train/mil_loss 0.58092
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run divine-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ms8e2lix
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061950-ms8e2lix/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 7x0tglla with config:
wandb: 	actor_learning_rate: 8.292866705110153e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2550818839156864
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062230-7x0tglla
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cfz59uci
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7x0tglla
wandb: uploading wandb-summary.json; uploading config.yaml; uploading history steps 43-57, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▆▆█
wandb: best/eval_avg_mil_loss ▇█▅▅▁▂
wandb:  best/eval_ensemble_f1 ▁▃▄▆▆█
wandb:            eval/avg_f1 ▇▃▇▇▅▇▆▆▇█▇▇▇▇▇▃▇▁▇▇▆▇▅█▆▇▅▇▇▇▆█▅▆▆▆▇▄▄█
wandb:      eval/avg_mil_loss ▂▂▅▂▂▄▃▃▁▆▂▂▂▃▃▆▂█▂▂▂▃▂▃▃▂▅▄▁▁▂▂▂▄▂▃▂▃▅▄
wandb:       eval/ensemble_f1 ▇▃▇▅▁▇▇▇█▄▇▇█▇▇▃▇▁█▇▆▇▅█▆▆▇▇▄▅▇▇█▆▅▆▆▇▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▃▅▆▄▅█▅▂▄▄▆▃▄▅▅▄▇▇▆▄▅▁▆▇▅▄▆▅▁▅▅▅▆▅▇▆▇▆
wandb:      train/ensemble_f1 ▅▅▅▆▄█▂▇▄▆▄▆▄▆▅▄▇▇▄▆▅▁▆▄▇▄▆▆▅▅▅▅▆▇▅▇▅█▇▆
wandb:         train/mil_loss ▂▁▄▃▃▂▆▃▃▇▇▃▂▇▄▁▃▄▅▇█▅▅▄▅▅▅▄▅▆▆▁▄▄▃▅▂▃▂▂
wandb:      train/policy_loss ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄█▄▄▄▄▄▄▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81889
wandb: best/eval_avg_mil_loss 0.66779
wandb:  best/eval_ensemble_f1 0.81889
wandb:            eval/avg_f1 0.81889
wandb:      eval/avg_mil_loss 0.66779
wandb:       eval/ensemble_f1 0.81889
wandb:            test/avg_f1 0.69704
wandb:      test/avg_mil_loss 0.55596
wandb:       test/ensemble_f1 0.69704
wandb:           train/avg_f1 0.74748
wandb:      train/ensemble_f1 0.74748
wandb:         train/mil_loss 0.65656
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run frosty-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7x0tglla
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062230-7x0tglla/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 7qgnjsf1 with config:
wandb: 	actor_learning_rate: 7.734920312042465e-05
wandb: 	attention_dropout_p: 0.4757306587706696
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9059335219025488
wandb: 	temperature: 8.449042611113123
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062434-7qgnjsf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7qgnjsf1
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading history steps 119-127, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▆█
wandb: best/eval_avg_mil_loss ▃▃▆█▁
wandb:  best/eval_ensemble_f1 ▁▂▅▆█
wandb:            eval/avg_f1 ██▂▅██▅▅▂▁▅█▁▂▇██▂█▇█▂▂█▂█▅▅███▆▅▇▁▄██▅▄
wandb:      eval/avg_mil_loss ▇▄▂▅▄▇▅▃▁▅▂▂▅▄▁▄▃▂▅▅▄▄▄▂▁▂▃▅▂▁▅▆▄▃▄▃▃▆▃█
wandb:       eval/ensemble_f1 ▅█▅▆▄▅▂▁▅█▂▇█▇▅▅██▅▂▅▂▆▆██▅▅▅▆▅▆▇▇▄▅█▅█▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▆▅▇▄▅▆▆▇▆▆▆▆█▅▆▇▄▇▅▆▆▄▁▅▅▆▆▆▆▅▄▇▆▇▄█▇▃
wandb:      train/ensemble_f1 ▅▄▆▇▃▄▅▇▆▅▇▆██▃▇▅▅▄▃▇▄▁▃▃▅▄▄▄▆▂▅▇▆▆▃▅▇▇▂
wandb:         train/mil_loss ▆▇▇█▆▇█▅▄▇▄▅▄▆▇▁▂▆█▅▄▆▃▇█▇▄█▇▇▅▄▇▃▃▆▄▃▄▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁██████████████▅███████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77957
wandb: best/eval_avg_mil_loss 0.57326
wandb:  best/eval_ensemble_f1 0.77957
wandb:            eval/avg_f1 0.72747
wandb:      eval/avg_mil_loss 0.68229
wandb:       eval/ensemble_f1 0.72747
wandb:            test/avg_f1 0.34754
wandb:      test/avg_mil_loss 0.84945
wandb:       test/ensemble_f1 0.34754
wandb:           train/avg_f1 0.57917
wandb:      train/ensemble_f1 0.57917
wandb:         train/mil_loss 0.69802
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glamorous-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7qgnjsf1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062434-7qgnjsf1/logs
wandb: Agent Starting Run: 29rg9709 with config:
wandb: 	actor_learning_rate: 0.0004925631227808748
wandb: 	attention_dropout_p: 0.1445032993586461
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 121
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8298206110144439
wandb: 	temperature: 4.947292121122989
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062647-29rg9709
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/29rg9709
wandb: uploading history steps 107-121, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▇▇▇█
wandb: best/eval_avg_mil_loss █▃▆▆▃▄▁▂
wandb:  best/eval_ensemble_f1 ▁▅▆▆▇▇▇█
wandb:            eval/avg_f1 ▇▆▅▇▇▇▃▆▁▅█▆▇▅▅▅▅▆▆▇█▇▇▇▇▅▇▅▇▅▅▇█▅▃▇▆▇▅▅
wandb:      eval/avg_mil_loss ▅▂▄▃▃▃▄▄▃▄▃▄█▅▂▅▄▃▅▅▃▄▅▇▅▄▄▆▅▄▃▄▄▃▃▁▂▃▂▅
wandb:       eval/ensemble_f1 ▅▇█▇▅▆█▁▅▅▆█▇▅▅▇▅▇▅▅▃▅██▇▇▅▅▇▅▆▂█▇▇█▅▆█▆
wandb:           train/avg_f1 ▄▅█▃▃▄▅▆▄▅█▅▄▃▁▆▃▃▇▇▇▆▄▃▅▆▅▄▄▁▇▄▄▅▆▅▅▄▇▅
wandb:      train/ensemble_f1 ▄▃▆▄▄▄▄▂▅▃▅▃▃▂▅▅▅█▄▅▇▅▆▆▅▄▅▁▂▄▄▆▃▃▂▄▇▃▆▅
wandb:         train/mil_loss ▂▃▄▂▄▃▆▂▂▇▃▁▆▂▃▆▅▆▄▄▂▂▃▃▃▃▂▄▆▃▄▄▄█▅▅▄▃▃▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80767
wandb: best/eval_avg_mil_loss 0.54715
wandb:  best/eval_ensemble_f1 0.80767
wandb:            eval/avg_f1 0.62527
wandb:      eval/avg_mil_loss 0.61652
wandb:       eval/ensemble_f1 0.62527
wandb:           train/avg_f1 0.63754
wandb:      train/ensemble_f1 0.63754
wandb:         train/mil_loss 0.70904
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run ruby-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/29rg9709
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062647-29rg9709/logs
wandb: ERROR Run 29rg9709 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 4u70klne with config:
wandb: 	actor_learning_rate: 0.00036023269091007355
wandb: 	attention_dropout_p: 0.4879364792230948
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 141
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4031303028107487
wandb: 	temperature: 6.9516811879332865
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062852-4u70klne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4u70klne
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇██
wandb: best/eval_avg_mil_loss █▅▅▅▁
wandb:  best/eval_ensemble_f1 ▁▄▇██
wandb:            eval/avg_f1 ▃▅▇▄▄▄▆▇█▇█▅▇▃▂▅▅▇▅▄▇▅▇▄▂▅▄▅▁▃▅▅▁▂▂▇▇▆▇▅
wandb:      eval/avg_mil_loss ▄▄▂▃▁▄▄▄▆▆▄▄▄▃▆█▄▅▂▃▄▆▂▆▆█▆▇▂▄▇▇▄▆▁▅▃▇▁▅
wandb:       eval/ensemble_f1 ▇▇▆▄▆▄▁▁▇█▅██▅▇▄▂▆▇█▅▅▅▅█▆▁▃▅▅▇▇▄█▁▂▇▃▇█
wandb:           train/avg_f1 ▆▆▃▃▅▅▄▅▃█▆▅▄▅▆▅▅▄▁▆▅▂▇▄▅▃▅▅▅▅▅▄▅▄▅▆▄▅▇▇
wandb:      train/ensemble_f1 ▄▆▃▃▅▅▄▄▆█▃▅▄▅▃▄▄▄▃▆▅▇▇▆▂▄▅▁▃▄▄▆▃▅▁▅▅▁▃▅
wandb:         train/mil_loss ▂▄▃▆▅▁▁█▅▄▅▆▁▁▅▄▄▅▆▃▂▄▅█▅▄▅▇██▃▆▃▁▇▃▆▃▅▅
wandb:      train/policy_loss ▄▄▆▄▄▄██▄▄▄▄█▁█▄▄▆█▄██▄▃█▄▄▄▄▄▄█▁▄▄▄▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▃▅█▅▁▃▅█▅▅█▅▅▅▅▅▅▅█▁▅▃▅▁█▅▅▅█▅▁█▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83476
wandb: best/eval_avg_mil_loss 0.46716
wandb:  best/eval_ensemble_f1 0.83476
wandb:            eval/avg_f1 0.61983
wandb:      eval/avg_mil_loss 0.79493
wandb:       eval/ensemble_f1 0.61983
wandb:           train/avg_f1 0.66943
wandb:      train/ensemble_f1 0.66943
wandb:         train/mil_loss 0.73672
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run celestial-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4u70klne
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062852-4u70klne/logs
wandb: ERROR Run 4u70klne errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9pz8j84k with config:
wandb: 	actor_learning_rate: 1.0157782153568212e-06
wandb: 	attention_dropout_p: 0.4483930450327599
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 98
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7755767673333852
wandb: 	temperature: 7.037798175552007
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063116-9pz8j84k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9pz8j84k
wandb: uploading wandb-summary.json
wandb: uploading history steps 93-99, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇█
wandb: best/eval_avg_mil_loss ▁█▆▁▆
wandb:  best/eval_ensemble_f1 ▁▅▆▇█
wandb:            eval/avg_f1 ▆▂▇▄▃▇▂▄▅▇▄▄▆█▅▃▇▁█▂▅▄▇▅▂▃▄▅▇▁▇▇▃▃▃▄▄▆▃▄
wandb:      eval/avg_mil_loss ▂▇▆▅▅▂▁▅▃▆▇▆▂▅█▂▅▆█▅▅▅▆▇▅▆▂▆▄▅▆▁▇▄▅▅▅▂▆▂
wandb:       eval/ensemble_f1 ▇▄▇▅▄▇▆▄▆▅▇▇▆▃▅▆▄▃▆▃▆▁▇▆█▄▇▇▇▅▄▅▄▄▇▆▇▅▇▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▃▃▃▄▄▃█▄▅▄▂▄▆▄▅▅▅▄▅▅▄▂▃▃▃▇▄▄▁▆▄▄▅▆▃▅▂▂▃
wandb:      train/ensemble_f1 ▄▃▃▁▃▃▄▇▃▅▅▅█▃▂▆▆▅▅▃▄▂▄▃▄▆▅▇▄▁▆▄▇▅▆▆▄▆▂▂
wandb:         train/mil_loss ▃▄▇▆▆▄▅▇▅▅▆▅▅▆▃█▄▄▆▆▅▇▃▆▄▅▁▅▅█▅▂▅▃▆▄▄▄▇▆
wandb:      train/policy_loss ████████████▁███████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8272
wandb: best/eval_avg_mil_loss 0.64749
wandb:  best/eval_ensemble_f1 0.8272
wandb:            eval/avg_f1 0.76379
wandb:      eval/avg_mil_loss 0.63818
wandb:       eval/ensemble_f1 0.76379
wandb:            test/avg_f1 0.77871
wandb:      test/avg_mil_loss 0.44304
wandb:       test/ensemble_f1 0.77871
wandb:           train/avg_f1 0.76137
wandb:      train/ensemble_f1 0.76137
wandb:         train/mil_loss 0.56138
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run polished-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9pz8j84k
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063116-9pz8j84k/logs
wandb: Agent Starting Run: mmtju8py with config:
wandb: 	actor_learning_rate: 9.376460044976456e-06
wandb: 	attention_dropout_p: 0.4195649386610502
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 154
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22556544349083263
wandb: 	temperature: 1.101464212334884
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063301-mmtju8py
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mmtju8py
wandb: uploading history steps 152-154, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆██
wandb: best/eval_avg_mil_loss ▃▁▅█▅▂
wandb:  best/eval_ensemble_f1 ▁▄▆▆██
wandb:            eval/avg_f1 ▆▁▁▅▂▁▇▂▂▇▇█▁▇▁▂█▂▇▁▂▂▂▇▁▇▇▆▇▅▆▇▂▄▅▆▂▇▃▅
wandb:      eval/avg_mil_loss ▇▆▅▄▅▃▅▆▂▃▅▃▇▆▂▄▅▅▆▁▃▃▇▆▆▃▃▄▆█▅▁▅▆▃▂▅▅▅▃
wandb:       eval/ensemble_f1 ▇▁▇▃▇▁▁▇▂▇█▂▇▁▇▂▁▇▇▂▇▇▇▇▇▂▂▇█▇█▃▇▇▁▆▇▇▄▇
wandb:           train/avg_f1 ▅▄▅▇▅█▇▃▇▆▅▇▄▇▁▅▂▆▆▆▆▇▅▃▃▇▁▇▄▇▄▆▆▆▁█▆▄▆▆
wandb:      train/ensemble_f1 █▇▅▆▆▅▆▄▇▆▆▆▆▇▃▇▅▅▁▇▅▇▆▆▇▅▃▄▅▆▇▄▆▅▇▅▆▇▆▆
wandb:         train/mil_loss ▃▅▃▃▃▃▃▃▃▄▄▄▅▅▁▂▆█▄▃▄▆▅▆▇▆▆▄▆▃▃▄▄▅▂▅▃▂▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████▁████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83388
wandb: best/eval_avg_mil_loss 0.52107
wandb:  best/eval_ensemble_f1 0.83388
wandb:            eval/avg_f1 0.74526
wandb:      eval/avg_mil_loss 0.45607
wandb:       eval/ensemble_f1 0.74526
wandb:           train/avg_f1 0.66423
wandb:      train/ensemble_f1 0.66423
wandb:         train/mil_loss 0.66714
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run twilight-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mmtju8py
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063301-mmtju8py/logs
wandb: ERROR Run mmtju8py errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: y5dl3ioa with config:
wandb: 	actor_learning_rate: 5.51288179165023e-06
wandb: 	attention_dropout_p: 0.13617941457721394
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 122
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.329884823237916
wandb: 	temperature: 7.687060359870042
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063540-y5dl3ioa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y5dl3ioa
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▅▆▆█
wandb: best/eval_avg_mil_loss ▅█▂▄▆▆▁
wandb:  best/eval_ensemble_f1 ▁▄▅▅▆▆█
wandb:            eval/avg_f1 ▃▃▄▂▄▂▄▄▄▄▃▃▆▆▁▄▂▂▃▆▂▅▃▇▆▃▄▃▄▃▄▃▁█▅▁▂▇▅▂
wandb:      eval/avg_mil_loss ▅▄▇▅▁▇▆▆▄▅▅▇▄▅▅▇▇▄▇▃▄▇▇▁▃▇▆▅▅▁▃█▆▂▄▃▃▆▂▃
wandb:       eval/ensemble_f1 ▅▂▄▇▄▄▃▆▅▄▅▅▂▄▄█▅▃▃▁▄▆▄▅▁▇▂▆▄█▇▆▁▃▅▄▂▃▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆▇▄▅▆▆▅▄▇▆▅▅▄▁▇▇▆▄▅▆▃█▅▇▅▆▇▇███▇▇▅▄█▆▆▅
wandb:      train/ensemble_f1 ▃▄▅▄▃▃▃▄▁▂▃▆▂▅▅▂▄▁▃▅▆▇▆▂▄▅▄▆▅▆▅▄▂▃▅▆▂▆█▇
wandb:         train/mil_loss ▆█▅▃▆▅▃▃▆▄▇▄▆▃█▇▅▅▅▄▄▅▅▂▆▆▂▁▂▃▂▄▃▅▄▆▃▂▆▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█████████████▅████████████████▄██████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84939
wandb: best/eval_avg_mil_loss 0.42439
wandb:  best/eval_ensemble_f1 0.84939
wandb:            eval/avg_f1 0.74696
wandb:      eval/avg_mil_loss 0.51874
wandb:       eval/ensemble_f1 0.74696
wandb:            test/avg_f1 0.79697
wandb:      test/avg_mil_loss 0.44948
wandb:       test/ensemble_f1 0.79697
wandb:           train/avg_f1 0.80208
wandb:      train/ensemble_f1 0.80208
wandb:         train/mil_loss 0.52685
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run charmed-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y5dl3ioa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063540-y5dl3ioa/logs
wandb: Agent Starting Run: 5i80cr8l with config:
wandb: 	actor_learning_rate: 4.94471393663813e-06
wandb: 	attention_dropout_p: 0.0828724264091803
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 108
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.18400777438137517
wandb: 	temperature: 6.55759740922524
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063809-5i80cr8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5i80cr8l
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▆▇▇██
wandb: best/eval_avg_mil_loss ██▆▃▄▆▂▆▁
wandb:  best/eval_ensemble_f1 ▁▄▄▅▆▇▇██
wandb:            eval/avg_f1 ▆▇▂▆▃▆▃▄▇▇▆█▄▄▇▆▄▆▇▁▆▆▇▇▆▆▆▄▅▇█▇▆█▆▅█▇▄█
wandb:      eval/avg_mil_loss ▃▄▃▂▂▅█▂▅▃▆▂▃▆▁▃▂▅█▃▆▃▃▄▄▅▂▁▆▂▅▂▂▄▃▄▄▃▄▃
wandb:       eval/ensemble_f1 ▃▆▆▄▅▃▇▃▃▅▄▄▄▆▇▅▅▄▁▄▆▃▆▆▆▇▄▅▆▆▅▇▇▇█▃▇▄█▆
wandb:           train/avg_f1 ▃▂▁▄▁▄▅▃▁▂▅▂▃▄▅▃▄▅▃▄▄▄▄▄▅▄▆▇█▆▅▃▄▆▅▆▄▂▂▆
wandb:      train/ensemble_f1 ▁▄▄▂▄▄▅▅▂▄▅▅▄▃▂▆▅▄▃▃▄▅▇▅█▃▆▅▆▄▆▇▃▇▂▇▅▂▅▄
wandb:         train/mil_loss ▂▄▃▄▄▄▂▆▁▄▄▃▃▂▁█▂▅▅▂▃▆▂▅▂▁▂▁▃▃▁▃▄▄▂▃▁▁▂▂
wandb:      train/policy_loss ██████▇██▇█████████████▁███████▃████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████▅██████████▂███▆████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8098
wandb: best/eval_avg_mil_loss 0.47452
wandb:  best/eval_ensemble_f1 0.8098
wandb:            eval/avg_f1 0.78951
wandb:      eval/avg_mil_loss 0.50517
wandb:       eval/ensemble_f1 0.78951
wandb:           train/avg_f1 0.7632
wandb:      train/ensemble_f1 0.7632
wandb:         train/mil_loss 0.58308
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pious-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5i80cr8l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063809-5i80cr8l/logs
wandb: ERROR Run 5i80cr8l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: p1emm307 with config:
wandb: 	actor_learning_rate: 0.00010582770243307648
wandb: 	attention_dropout_p: 0.2312670422389535
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6609256262399915
wandb: 	temperature: 7.637611133179508
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064024-p1emm307
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p1emm307
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆██
wandb: best/eval_avg_mil_loss ▃▁█▅▁█
wandb:  best/eval_ensemble_f1 ▁▄▅▆██
wandb:            eval/avg_f1 ▂▆▂▇▇▁▃█▇█▂▅▇▆▇▆▇▃▇▇███▇▃▇▂▂▇▇█▁▇██▇▇▇▇▅
wandb:      eval/avg_mil_loss ▃▃▃▄█▂▆▃▂▃▂▇▂▄▄▂▃▅▁▄▄▇▆▄▄▃▃▃▅▃▇▃▃▃▃▄▃▆▂▅
wandb:       eval/ensemble_f1 ▇▁█▇▇▅█▇█▇███▁██▅▆█▇▇▁▆▇▇▂▇▇▇▇█▇▁▇▇▇▇█▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ██▅▄▁▆▄▃▆▆▄▃▂▇▅▃▃▇▆▆▆▄▅█▄▅▆▇▇▆█▆▆▇▇▄█▇▇▆
wandb:      train/ensemble_f1 ▇▄▅▅▇█▅▂▄▄▄▁▃▄▅▅▅▂▇▆▄▄▅▇▅▄▆▅▅▅▅▃▇▅▇▂▆▄▆▅
wandb:         train/mil_loss ▁▄▆▃▇▄▂▂▄▂▅▄▄▆▆▄▃▆▃▅▄█▆▆▃▃▄▂▂▆▄▅▆▃▆▆▇▃▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79264
wandb: best/eval_avg_mil_loss 0.7005
wandb:  best/eval_ensemble_f1 0.79264
wandb:            eval/avg_f1 0.73606
wandb:      eval/avg_mil_loss 0.6898
wandb:       eval/ensemble_f1 0.73606
wandb:            test/avg_f1 0.80277
wandb:      test/avg_mil_loss 0.41963
wandb:       test/ensemble_f1 0.80277
wandb:           train/avg_f1 0.69728
wandb:      train/ensemble_f1 0.69728
wandb:         train/mil_loss 0.60644
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run exalted-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p1emm307
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064024-p1emm307/logs
wandb: Agent Starting Run: 41ersb1b with config:
wandb: 	actor_learning_rate: 1.780075639033794e-05
wandb: 	attention_dropout_p: 0.3435034154185942
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9216990729173048
wandb: 	temperature: 2.098379268132912
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064239-41ersb1b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/41ersb1b
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇███
wandb: best/eval_avg_mil_loss █▂▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▄▇▇███
wandb:            eval/avg_f1 ▆▂▁▇▆▆█▃▆▆▇▇▇█▃▁▆▅▆▇▇▄▆▆▆▇█▇▆▅▂▆▇▇▇▇▅█▃▆
wandb:      eval/avg_mil_loss ▅▁▄█▄▃▁▆▂▅▃▁▃▃▄▁▄▄▄▁▄▃▄▂▃▁▁▁▁▁▄▁▄▄▃▁▂▃▁▂
wandb:       eval/ensemble_f1 ▇▆▂▆▆▇▂▆▁▆▆▇█▆█▇▇▇▆▇▆▇▆▅▇▄▇▆▆▆▆▇▇▇▆▆█▄▄▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇█▇▅▃▇▆▇█▄▇▆▇▅▁▇▄▅▄▃▄▅▄█▂▂▆▅▅▃▆▆▆▇▆▇▆▅▄▅
wandb:      train/ensemble_f1 ▅▇▅▆▃▆▆▆▃▇▅▃▇▇▄▃▅▄▅▅▅▅▇▂▆▆▆▇▆▇▆▁▇▆▆▄▆█▃▆
wandb:         train/mil_loss ▃▅▄▃▄▃▆▃▅▂█▄▅▄▂▂▃▂▅▁▅▃▃▂▅▄▂▆▄▆▂▄▆▃▂▄▃▄▃▂
wandb:      train/policy_loss ▅▁▁▁▅▅▅▅▅▅▅█▅▅▅▅▅▁█▅█▅▁▁█▅▅█▅▅▁██▁▅▅█▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▁▅▅▅▅█▁█▅▅▅▅▅▅▁█▅▅▅▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84455
wandb: best/eval_avg_mil_loss 0.47852
wandb:  best/eval_ensemble_f1 0.84455
wandb:            eval/avg_f1 0.79049
wandb:      eval/avg_mil_loss 0.58829
wandb:       eval/ensemble_f1 0.79049
wandb:            test/avg_f1 0.77467
wandb:      test/avg_mil_loss 0.46347
wandb:       test/ensemble_f1 0.77467
wandb:           train/avg_f1 0.76108
wandb:      train/ensemble_f1 0.76108
wandb:         train/mil_loss 0.55808
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run exalted-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/41ersb1b
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064239-41ersb1b/logs
wandb: Agent Starting Run: 54rmck8j with config:
wandb: 	actor_learning_rate: 0.0006560220984037228
wandb: 	attention_dropout_p: 0.4830601364446454
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 60
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22500385418476865
wandb: 	temperature: 1.8272184040156505
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064432-54rmck8j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/54rmck8j
wandb: uploading history steps 51-60, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▄▂▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▄▇█▇█▇▅▅▅▇▆▇▇▇▆▆█▆█▇▇▇▆▇▇█▅▅▇▆█▇▅▅▇▆▆▆▁▆
wandb:      eval/avg_mil_loss ▇▄▂▃▃▂█▃▆▆▄▃▄▃▃▃▅▂▄▅▃▃▅▆▄▁▆▅▄▃▃▂▄▅▅▃▃▄▃▄
wandb:       eval/ensemble_f1 ▄▇██▇▂▇▆▅█▇▇▆▆▇▆█▆▆█▇▇▇▆▄▇▆▆▅▇▇▂▄▇▇▇▇▆▁▆
wandb:           train/avg_f1 ▅▄▄▄▄▄▇▅▅▅██▇█▁▆▃▇█▅▄██▆▄▅▇▆▆▆▅▅█▆▃▇▆▄▂▄
wandb:      train/ensemble_f1 ▄▃▄▄▇▅█▅▄▇▇█▁▄▆▇▇▄▄█▇▇▆▄▅▆▆▅▅▅▇▆▃██▄▅▄▂▄
wandb:         train/mil_loss ▆▇▅▆▆▄▅▄▁▆▅▅▇▄▄▅▄▂▃▃▃▃▅█▅▅▅▂▂▆█▅▆▅▇▄▇▄▅▂
wandb:      train/policy_loss ▄▄▄▄▄▁▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8065
wandb: best/eval_avg_mil_loss 0.51881
wandb:  best/eval_ensemble_f1 0.8065
wandb:            eval/avg_f1 0.64598
wandb:      eval/avg_mil_loss 0.70783
wandb:       eval/ensemble_f1 0.64598
wandb:           train/avg_f1 0.66448
wandb:      train/ensemble_f1 0.66448
wandb:         train/mil_loss 0.58808
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run logical-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/54rmck8j
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064432-54rmck8j/logs
wandb: ERROR Run 54rmck8j errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: upqnz9yb with config:
wandb: 	actor_learning_rate: 1.5674787090107516e-05
wandb: 	attention_dropout_p: 0.0014564722218897153
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 173
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8799129956389368
wandb: 	temperature: 9.699835473816208
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064549-upqnz9yb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/upqnz9yb
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 118-131, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss ▅▁█▆
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▇▅▅▇▇▆▂█▆██▃▇▅█▄▆▇▇▅▇▇▄▇▆▁█▇▅▇█▂▅█▇▇▇▆█▅
wandb:      eval/avg_mil_loss ▁▄▆▆▃▃▂█▂▇▆▂▇▂▅▄▄▃▆▃▆▇▂▅▃▂▄▃▃▆▄▂▄▃▆▂▇▄▄▅
wandb:       eval/ensemble_f1 ██▅▇█▆▇▅███▆█▆▇▆▆██▆▅▃▇██▃▆▇█▇▆▆▆▆▇▁▅▇▇▆
wandb:           train/avg_f1 ▆▅█▁▃▃▂▄▅▃▄▅▂▆▇▃▇▂▇▃▁▇▄▇▃▅▃▄▇█▅▅▃▂▂▆▇▇▆▆
wandb:      train/ensemble_f1 ▂▄▅▄▂█▄▆▃▅▅▆▃▂▂▆▃▆▆▁▆▃▄▃▆▄▃▅▅▆▆▇▅▅▅▅▃▅▆▃
wandb:         train/mil_loss ▇▄▃▇▅▄▄▇▆▅▂▂▆▂▄▆▆▁▄▇▆▆▁▄▆▄▅▂█▂▅█▆▅▇▃▂▄▄▃
wandb:      train/policy_loss ██████████████████████▁█████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79084
wandb: best/eval_avg_mil_loss 0.5609
wandb:  best/eval_ensemble_f1 0.79084
wandb:            eval/avg_f1 0.64519
wandb:      eval/avg_mil_loss 0.76689
wandb:       eval/ensemble_f1 0.64519
wandb:           train/avg_f1 0.6649
wandb:      train/ensemble_f1 0.6649
wandb:         train/mil_loss 0.71462
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run feasible-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/upqnz9yb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064549-upqnz9yb/logs
wandb: ERROR Run upqnz9yb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: zkk1z8sm with config:
wandb: 	actor_learning_rate: 0.00040680999429698074
wandb: 	attention_dropout_p: 0.19531463545185537
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 166
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2382617840377379
wandb: 	temperature: 8.444916521151868
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064809-zkk1z8sm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zkk1z8sm
wandb: uploading history steps 105-110, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▁▂▃▄█
wandb: best/eval_avg_mil_loss ▅▅█▄▂▃▁
wandb:  best/eval_ensemble_f1 ▁▁▁▂▃▄█
wandb:            eval/avg_f1 ▇▇▇▇▇▃▆▅█▇▇▇▇▂▆█▇▂▄▇▇▇▇▁▆▇▂▇▇▆▇▇▆▃▂▆▇▆█▇
wandb:      eval/avg_mil_loss ▃▂▂▂▂▃▃▃▂▃▃▂▂▃▆▃▃▄▃▃▃▂▁▃▄▃▃▃▃▃▄▄█▁▃▃▁▃▃▄
wandb:       eval/ensemble_f1 ▆▇▇▇█▇▅▆▇▆▇▇▇▇▇▆▇▅▇▅▃▆▇▇▇▆▆▅▇▅▇▇▄▁▇▆▇█▆▅
wandb:           train/avg_f1 ▅▅▂▅▆▄▆▆▅▆▅▆▄▆▅▄▆▇▆▅▆▅▁▆▄▅▄▅▄▅▆▅▃▆▆▅▆█▄▃
wandb:      train/ensemble_f1 ▇▄▄▅▆▆▆▁▆▄▆▄▆▂▅▆▆▅▄▆▆▇▄▅▅▃▄▆▄▇▅▆▆▇▄▆█▄▅▆
wandb:         train/mil_loss ▆█▅▃▄▆▃▅▅▂▇▆▄▅▆▅▃▄▆▄▂▅▃▄▄▄▃█▄▆▆▄█▁▆▅▃▆▃▆
wandb:      train/policy_loss ▁███████████▃█▃█████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁███████████▃██████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7977
wandb: best/eval_avg_mil_loss 0.57783
wandb:  best/eval_ensemble_f1 0.7977
wandb:            eval/avg_f1 0.74182
wandb:      eval/avg_mil_loss 0.67288
wandb:       eval/ensemble_f1 0.74182
wandb:           train/avg_f1 0.70228
wandb:      train/ensemble_f1 0.70228
wandb:         train/mil_loss 0.59188
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run breezy-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zkk1z8sm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064809-zkk1z8sm/logs
wandb: ERROR Run zkk1z8sm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 3ru90nik with config:
wandb: 	actor_learning_rate: 0.00034219678150341885
wandb: 	attention_dropout_p: 0.4828326607977986
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 83
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17081744159311096
wandb: 	temperature: 8.611305377909815
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065008-3ru90nik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3ru90nik
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 75-83, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁████
wandb: best/eval_avg_mil_loss █▆▁▄▁
wandb:  best/eval_ensemble_f1 ▁████
wandb:            eval/avg_f1 █▇▆▆▁▇▇▆▆███▆▇▅█▇▇▆▇▅██▆▆▆▆▇█▆▇█▆▆▆▆██▆█
wandb:      eval/avg_mil_loss ▇▅▅▁▅█▂▇▁▅▆▃▅▅▅▅▅▄▂▆▅▁▆▁▂▂▃▄▄▁▂▄▂▅▅▃▄▆▂▅
wandb:       eval/ensemble_f1 ▁█▇▇▇█▄▇▆█▆▆▆█▇▆▆▇▇▅▅█▇█▆▆▇▁█▇▇█▇█▆█▆██▇
wandb:           train/avg_f1 ▄▅▆▅▇▅▆▅▆▇▅▅▂▇▄▇▅▆▅▆▃▄▆▅█▆▃█▄▁▅▅▆▄▇▃▆▅▄▆
wandb:      train/ensemble_f1 ▃▄▅▄▃▃▄▄▅▆▅▄▃▅▂▅▆▅▄▃▅▄▃▆▃▅▃▁▅▄▃▂▇▅█▄▃▂▇▄
wandb:         train/mil_loss ▇▅▇▃▃▆▆▃▇▅▇▄▅▇▁▅▄▆▇▆▄▆▅▇▅▆█▅▆▇▄▆▅▃▆▅▅▄▃▅
wandb:      train/policy_loss ██████████████████████████████████████▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████████▁██▆█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84408
wandb: best/eval_avg_mil_loss 0.43898
wandb:  best/eval_ensemble_f1 0.84408
wandb:            eval/avg_f1 0.78649
wandb:      eval/avg_mil_loss 0.39469
wandb:       eval/ensemble_f1 0.78649
wandb:           train/avg_f1 0.78232
wandb:      train/ensemble_f1 0.78232
wandb:         train/mil_loss 0.54992
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run gallant-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3ru90nik
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065008-3ru90nik/logs
wandb: ERROR Run 3ru90nik errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: x3xny669 with config:
wandb: 	actor_learning_rate: 1.938765369794086e-06
wandb: 	attention_dropout_p: 0.3484092521426673
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 154
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6437303010057296
wandb: 	temperature: 2.9576102410821346
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065141-x3xny669
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x3xny669
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 147-155, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▇▇██
wandb: best/eval_avg_mil_loss █▇▃▁▁▃▁▂
wandb:  best/eval_ensemble_f1 ▁▄▅▆▇▇██
wandb:            eval/avg_f1 ▇▇▇█▇████▁██▇██▇▇████▇█▃█▇████▇▇▁▇██▇█▁▁
wandb:      eval/avg_mil_loss ▅▇▆▂▁▅▃▃▇▁▃▅▂▆▃▂▂▁▁▂▃▂▂▃▄█▇▅▃▅▂▂▅▆▃▂▁▇▂▂
wandb:       eval/ensemble_f1 ▇▇▇▇▇▇▁███▄▇▇███▇▇▇▇▂▇▇█▇▇███▇█▇▇████▂█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▄▇▄▆▅▅▅▆▄▆▂▂▅▇▇▅▂▅▅▄▁▇▄▆▄▇▅▆▆▆▃▇▆▂▄█▆▅▅
wandb:      train/ensemble_f1 ▃▁▃▅▃▁▃▆▅▇▅▄▆▆▇▆█▄▅▃▆▃▆▅▆▆▆▃▄▁▂▆▂▃█▇▄▅▇▃
wandb:         train/mil_loss ▃▂▁▂▄▅▂▂▃▃▂▅▃▁█▃▃▁▂▁▅▅▃▄▅▅▂▃▃█▅▄▂▃▂▂▅▂▄▅
wandb:      train/policy_loss ▁█████████████████████████▂█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79697
wandb: best/eval_avg_mil_loss 0.57769
wandb:  best/eval_ensemble_f1 0.79697
wandb:            eval/avg_f1 0.74297
wandb:      eval/avg_mil_loss 0.60809
wandb:       eval/ensemble_f1 0.74297
wandb:            test/avg_f1 0.79957
wandb:      test/avg_mil_loss 0.65234
wandb:       test/ensemble_f1 0.79957
wandb:           train/avg_f1 0.73161
wandb:      train/ensemble_f1 0.73161
wandb:         train/mil_loss 0.56943
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run morning-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x3xny669
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065141-x3xny669/logs
wandb: Agent Starting Run: mz0ksglt with config:
wandb: 	actor_learning_rate: 0.0005787563282084093
wandb: 	attention_dropout_p: 0.38503915748385453
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4234758934827466
wandb: 	temperature: 0.15003527596298305
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065426-mz0ksglt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mz0ksglt
wandb: uploading history steps 119-126, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▆▇█
wandb: best/eval_avg_mil_loss █▁▁▃▂▂
wandb:  best/eval_ensemble_f1 ▁▅▅▆▇█
wandb:            eval/avg_f1 ▂▆▅▂▄▃▅▅▃▃▁▆▆▄▄▇▆▅▄▄▆▄▃█▄▅▆▅▃▄▅▆▂▅▆▆▆▆▆▃
wandb:      eval/avg_mil_loss ▂▅▆▃▆▆▆▂▃▅▅▇▅▃▆█▂▄▁▂▁▅▇▆▆▇▅▃▂▅▁▇▅▇▁▆▃▅▃▅
wandb:       eval/ensemble_f1 ▂▂▄▅▆▅▅▄▃▆▁▅▃▃▄▅▆▄▄▃▆▆▆▃█▇▄▆▆▇▆▆▆▂▄▆▄▆▆▂
wandb:           train/avg_f1 ▄▁▂▅▃▁▄▅▃▂▄▄█▆▂▂▆▅▄▅▃▅▇▅▆▂▆▆▇▆▅▃▃▆▄█▅▂▃▅
wandb:      train/ensemble_f1 ▄▄▆▅█▇▄▅▂▃█▄▂▂▆▄█▅▅▂▂▅▆▃▄▇▅▃▃▄▄▃▆▂▆▄▁▅▂▄
wandb:         train/mil_loss ▆▃▄▅▄▅▇▇▄▇▃▂▄█▁▄▅▃▃▄▆█▃▆▅▄▂▂▃▆▂▃▃▅▂▄▇▄▃▃
wandb:      train/policy_loss █▅▅▅▅▅▅▅██▅▅▅▅▅▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁█▅▅▁██▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▅▁▅▅▅▅▅▅▅██▅█▁▅▁▅▅▅▅▁▁▅▅█▅▅▅▅▅█▅▅▁█▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83597
wandb: best/eval_avg_mil_loss 0.48783
wandb:  best/eval_ensemble_f1 0.83597
wandb:            eval/avg_f1 0.74297
wandb:      eval/avg_mil_loss 0.48298
wandb:       eval/ensemble_f1 0.74297
wandb:           train/avg_f1 0.79333
wandb:      train/ensemble_f1 0.79333
wandb:         train/mil_loss 0.54485
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rare-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mz0ksglt
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065426-mz0ksglt/logs
wandb: ERROR Run mz0ksglt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: r33solf6 with config:
wandb: 	actor_learning_rate: 0.0001681351589395993
wandb: 	attention_dropout_p: 0.21205200985557188
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.387808270674543
wandb: 	temperature: 8.693535756597907
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065640-r33solf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r33solf6
wandb: uploading history steps 88-92, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇██
wandb: best/eval_avg_mil_loss █▂▁▂▂
wandb:  best/eval_ensemble_f1 ▁▇▇██
wandb:            eval/avg_f1 ▂▅▄▅▅▄▅▄▅▅▄▆▇▄▁▅▆▆▇▅▅▄▄▆▅▃▄▅▅▇▃▄█▃▆▁▅▆▇▃
wandb:      eval/avg_mil_loss █▄▄▅▆▄▅▄▆█▆▅▂▆▅▆▅▆▃▁▂▄▃▄▇▆▂▃▇▇▇▇▇▅▅▇▅▂▄▃
wandb:       eval/ensemble_f1 ▂▄▅▅▅▆▄▄▇█▅▅▅▄▅▄▃▅▅▁▇▆▇▄▅▄▅▂▃▅▇▅▅▅█▃▃▄▇▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▆▃▄▃▄▅▄▃▂▅▅▄▄▁▆▅▅▄▃▇▅▆▆▆▅▇▅▄▅▇▇▅▆▆▄▅▄▇▇
wandb:      train/ensemble_f1 ▃▄▇▅▃▄▄▄▁▃▄▃▃▃▅▆▅▄▅▅▁▅▆▄▅▁▄▂▆▅▆▅▅█▆▄▄▆▆▆
wandb:         train/mil_loss ▂▄▄▃▃▆▆▄▄▂▅▄█▇▄▃▃▂▅▁▄▅▆▇▄▄▁▁▃▃▅▄▅▁▇▃▃▅▃▇
wandb:      train/policy_loss ▃▅▅▁▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▁▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▇▄▄▄▄▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79512
wandb: best/eval_avg_mil_loss 0.55656
wandb:  best/eval_ensemble_f1 0.79512
wandb:            eval/avg_f1 0.78451
wandb:      eval/avg_mil_loss 0.53868
wandb:       eval/ensemble_f1 0.78451
wandb:            test/avg_f1 0.77265
wandb:      test/avg_mil_loss 0.49848
wandb:       test/ensemble_f1 0.77265
wandb:           train/avg_f1 0.74595
wandb:      train/ensemble_f1 0.74595
wandb:         train/mil_loss 0.59425
wandb:      train/policy_loss 0.29876
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.29876
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run celestial-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r33solf6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065640-r33solf6/logs
wandb: Agent Starting Run: goghz60o with config:
wandb: 	actor_learning_rate: 1.9990466073557043e-06
wandb: 	attention_dropout_p: 0.00042338105105210744
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 153
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.29843886482993154
wandb: 	temperature: 5.274036817816543
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065834-goghz60o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/goghz60o
wandb: uploading history steps 100-104, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃█
wandb: best/eval_avg_mil_loss ▇█▁
wandb:  best/eval_ensemble_f1 ▁▃█
wandb:            eval/avg_f1 ▆▇▄▃▆█▇▂▇▄▆▇▅▂▁▅▇█▇▆▃▆▃▅▃▃▂▆▄▆▃▃▂▄▆▇▂▆▄▅
wandb:      eval/avg_mil_loss ▃▅▆▂▁▅▄▃▅▅▂▃▆▁▇▃▃▂▂▃▅▇▅▆▅▅▃▄▄▄█▃▄▆▃▄▄▄▃▅
wandb:       eval/ensemble_f1 ▄█▄▄▃▆▄▆▇▁▆▅▄▃▇▆▄▂▄▆▃▆▄▇▅▆▆▃▄▄▇▄█▆▄▄▇▅▆▅
wandb:           train/avg_f1 ▃▆▄▆▆▃▅▅▄▅▃▅▅▆▇▃▅▆▄▄▆▅▅▄▁▂▄▄▅▅▅▄▅▃█▃▄▆▄▅
wandb:      train/ensemble_f1 ▂▅▄▄▁▅▄▅▁▄▆▆▅▃▅▃▅▄▅▅▄▅▆█▄▅▄▅▄▅▄▄▄▃▃█▄▂▆▃
wandb:         train/mil_loss █▆▅▄▅▃▅▆▆▄▅▆▅▅▅▁▄▅▂▅▅▆▅▄▃▄▄▄▄▅▆▅▅▄▄▅▃▂▆▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81002
wandb: best/eval_avg_mil_loss 0.53832
wandb:  best/eval_ensemble_f1 0.81002
wandb:            eval/avg_f1 0.6352
wandb:      eval/avg_mil_loss 0.70954
wandb:       eval/ensemble_f1 0.6352
wandb:           train/avg_f1 0.67065
wandb:      train/ensemble_f1 0.67065
wandb:         train/mil_loss 0.75487
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sage-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/goghz60o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065834-goghz60o/logs
wandb: ERROR Run goghz60o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: nf1kd8fu with config:
wandb: 	actor_learning_rate: 0.0007100302548671122
wandb: 	attention_dropout_p: 0.460760156834563
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 167
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8317695851339422
wandb: 	temperature: 2.7491033231645137
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070048-nf1kd8fu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nf1kd8fu
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 102-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▆▂▆█▁▆▁▅▇▂▇█▆▂▆▃▆▆▆▇▆▇▇▇▆▂▄▇▆▇▆▆▇▇▁▆▃▅▇█
wandb:      eval/avg_mil_loss ▁▄▃▂▄▆▆▃▄▄▇▃▅▆▁▆▆█▃▁▃▄▅▅▄▂▄▃▅▃▃▅▄▆▆▁▂▄▃▄
wandb:       eval/ensemble_f1 █▇▇▄▄▇▇▆▄▆▃▇▇█▄▇▇▇▁▆▄▇▆█▅▇▁▆▇▇█▇█▅▄▇▆▅▅█
wandb:           train/avg_f1 ▅▆▆▅▅▇▇▆▄▇▄▃▄▆▆▆▃▅▅▆▇█▆▁▆▇▃▇▃▄▅▆▅▃▆▃▇▅▅▅
wandb:      train/ensemble_f1 ▅▅▇▇▇▁▆▆▇█▄▃▅▇▄█▅▅▂▃▆▅█▂▃▄▆▂▄▂▃▃▁▄▄█▄▅▄▆
wandb:         train/mil_loss ▃▄▇█▆▃▃▄▅▁▂▄▅▇▃▄▄█▇█▂▂▅▇▄▅█▅▂▂▃▄▄▃▅▄▆▂▃▂
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████▁█████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84186
wandb: best/eval_avg_mil_loss 0.4449
wandb:  best/eval_ensemble_f1 0.84186
wandb:            eval/avg_f1 0.82882
wandb:      eval/avg_mil_loss 0.39278
wandb:       eval/ensemble_f1 0.82882
wandb:           train/avg_f1 0.75822
wandb:      train/ensemble_f1 0.75822
wandb:         train/mil_loss 0.569
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run balmy-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nf1kd8fu
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070048-nf1kd8fu/logs
wandb: ERROR Run nf1kd8fu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qm3r2z1l with config:
wandb: 	actor_learning_rate: 0.0004096284901416452
wandb: 	attention_dropout_p: 0.4008160068316042
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 68
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.39202872326902993
wandb: 	temperature: 6.277033192135039
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070246-qm3r2z1l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qm3r2z1l
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄██
wandb: best/eval_avg_mil_loss █▆▄▁
wandb:  best/eval_ensemble_f1 ▁▄██
wandb:            eval/avg_f1 ▇▅▇█▅▄▆▆▅█▅▆█▄▆▇▆▆▅▁▅▆▆▄▃▅█▆▆▆▆▄▄▆▆█▄█▃▁
wandb:      eval/avg_mil_loss ▄▆▅▂▆▄▄▆▃▅▆▄▅▃▃▆▅█▅▄▆▇▅▃▅▄▄▅▃▇▇▅▃▆▂▁▄▅▅▄
wandb:       eval/ensemble_f1 ▆▇▅█▅▆▆█▆▅█▅▆▄▇▇▆▅▅▁▆▄▇▅█▆▅▅▆▇▄▄▆▆▅██▅▃▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▇▅▃▅█▇▆▃▅▆▄▆▂▅▄▄▅▇▄▃▅▄▅▆▅▄▇▆▃▁▆▅▃▄▅█▃▂
wandb:      train/ensemble_f1 ▅▃▆▅▇▅█▃▇▃▆▅▆▄▃▄▅▆▂▄▄▄▅▇▄▆▆▅▄▆▁▆▆▅▃▄▅▂█▅
wandb:         train/mil_loss ▁▆▄▆▆▄▆▅▄▄▃▃▆▃▇▂▂▆▃▃█▃▃▆▆▂▄▄▅▅▆▅▅▃▃▇▃▆▂▃
wandb:      train/policy_loss █▅█▅█▁▅▅▅▁▅▁▁▅▅▆▅▅▆▁▃▁▅▅▅█▁██▁▁██▅▅██▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████████▁███████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7769
wandb: best/eval_avg_mil_loss 0.47092
wandb:  best/eval_ensemble_f1 0.7769
wandb:            eval/avg_f1 0.65021
wandb:      eval/avg_mil_loss 0.72257
wandb:       eval/ensemble_f1 0.65021
wandb:            test/avg_f1 0.59067
wandb:      test/avg_mil_loss 0.67695
wandb:       test/ensemble_f1 0.59067
wandb:           train/avg_f1 0.63106
wandb:      train/ensemble_f1 0.63106
wandb:         train/mil_loss 0.66581
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run brisk-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qm3r2z1l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070246-qm3r2z1l/logs
wandb: Agent Starting Run: g8rvy27d with config:
wandb: 	actor_learning_rate: 0.00016514314532696162
wandb: 	attention_dropout_p: 0.13852740806248626
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 54
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6137103798584225
wandb: 	temperature: 9.218290499506107
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070415-g8rvy27d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g8rvy27d
wandb: uploading wandb-summary.json
wandb: uploading history steps 44-54, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅█
wandb: best/eval_avg_mil_loss ▆▆▁▁█
wandb:  best/eval_ensemble_f1 ▁▂▄▅█
wandb:            eval/avg_f1 ▇█▇▆▇██▇█▅▇▇█▇▇█▂▇▂██▅▁▇█▇▇▇▇▇▇▇▇█▇▇▇█▆▇
wandb:      eval/avg_mil_loss ▄▄▁▅▆▅▅▁▅▄▅▁▅▆▁▅▄█▄▄▇█▅▅▅▆▅▅▃▅▅▅▄▄▅▅▄▄▅▂
wandb:       eval/ensemble_f1 ███▇▇██▇█▅▇██▇▅▇▇█▂█▅▁███▇██▇▇██▇██▇██▇█
wandb:           train/avg_f1 ▄▅▄▅▄▄▅▃▇▅▅▄▆▆▆▆▄▅▅▆▆▄▆▃▅▅▂▄▄▆▆▁▃▄▅▅█▄▅▇
wandb:      train/ensemble_f1 ▄▆▅▅▄▄▅▃▇▅▄▆▆▅▆▅▅▆▆▅▆▃▄▆▃▂▅▂▄▄▆▆▁▃▅▅█▄▅▇
wandb:         train/mil_loss █▂▄▂▃▆▃▆▁▄▃▅▄▄▅▆▂▅▁▄▄▂▁▇▃▃▂▄▃▅▄▄▁▄▄▄▄▄▂▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79228
wandb: best/eval_avg_mil_loss 0.71857
wandb:  best/eval_ensemble_f1 0.79228
wandb:            eval/avg_f1 0.75042
wandb:      eval/avg_mil_loss 0.50956
wandb:       eval/ensemble_f1 0.75042
wandb:           train/avg_f1 0.78074
wandb:      train/ensemble_f1 0.78074
wandb:         train/mil_loss 0.56105
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sage-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g8rvy27d
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070415-g8rvy27d/logs
wandb: ERROR Run g8rvy27d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: nqrxklt2 with config:
wandb: 	actor_learning_rate: 0.00011500086568542453
wandb: 	attention_dropout_p: 0.2806761236503586
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 124
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.16490581635811363
wandb: 	temperature: 0.7736841335454736
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070517-nqrxklt2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nqrxklt2
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▁▄▆█
wandb: best/eval_avg_mil_loss ▅▄▅█▆▁
wandb:  best/eval_ensemble_f1 ▁▁▁▄▆█
wandb:            eval/avg_f1 █▅▆█▅▅▅▆▁███▅▅▅█▅█▅▁█▇▅█▇██▁▅▅▁▅█▇██▅█▇▇
wandb:      eval/avg_mil_loss ▅▆▂▆▄▄▂▄▂▂▅▄▄▂▅▂▄▂▃▁▅▃▄▆▇▃▂▅▂█▃▂▂▂▅▃▂▃▅▄
wandb:       eval/ensemble_f1 ▆▆▆█▆█▆███▅▆▂███▆█▂▆▁▆▆▂█▅█▅██▂▆▆▅███▆▂█
wandb:           train/avg_f1 ▁▇▆▅▆▇▄▇▇▇▄▃▆▅▇▄▇▇▅▅▁▂▂▄▆█▇▄▆▆▇▃▆▆▅▅▅▅▆▇
wandb:      train/ensemble_f1 ▁▆▅▆▇▃▂█▅▆█▁▆▄▄▅▇▅▇█▄█▆▃▇▃▇▇▄▆▂███▆▅▅█▆▅
wandb:         train/mil_loss ▃▄▄▂▄▂▆▄▄▅▇▃▁▂▅▂▆▅█▄▅▂▂▂▆▃▄▂▆▂▁▅▃▂▂▃▃▂▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8228
wandb: best/eval_avg_mil_loss 0.57328
wandb:  best/eval_ensemble_f1 0.8228
wandb:            eval/avg_f1 0.72708
wandb:      eval/avg_mil_loss 0.68161
wandb:       eval/ensemble_f1 0.72708
wandb:           train/avg_f1 0.68603
wandb:      train/ensemble_f1 0.68603
wandb:         train/mil_loss 0.67943
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cerulean-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nqrxklt2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070517-nqrxklt2/logs
wandb: ERROR Run nqrxklt2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 05dszkmj with config:
wandb: 	actor_learning_rate: 1.9442586613124617e-05
wandb: 	attention_dropout_p: 0.17838405621003883
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 95
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.932650180398216
wandb: 	temperature: 1.247182380671913
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070731-05dszkmj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/05dszkmj
wandb: uploading history steps 88-95, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇▇███
wandb: best/eval_avg_mil_loss █▁▂▂▂▂▂
wandb:  best/eval_ensemble_f1 ▁▆▇▇███
wandb:            eval/avg_f1 ▅▇▆▆█▅▇█▅██▇▅▅▅█▇▅▇▅████▅███▆▆█▆▇▇▇▅▅▇█▁
wandb:      eval/avg_mil_loss ▅▁▅▇▄▂▅▄▄▂▃▃▄▆█▃▁▁█▂▁▅▁▅▂▄▁██▄▁▄▄▁▁▅▄▂▁▅
wandb:       eval/ensemble_f1 ▆▇▇▅▇█▆█▇▅█▅█▇▅▇▅▅▇▅▇█▅█▆▇▅██▅█▆▆██▆▆▇█▁
wandb:           train/avg_f1 ▇▂▄▆▆▃▃▇▆▅▆▁▂▇█▄▅▆▅▇▅▆▇▅▆▆█▇▅▅▄▅▄▂▃▅▆▃▇▆
wandb:      train/ensemble_f1 ▂▆▅▅▃▃▅▇▅▂▁▁▂▆▇▄▅▆▅▇▅▅▆▄▅▇▅▅█▅▅▄▄▅▅▇▂▆▂▆
wandb:         train/mil_loss ▁▄▄▂▄▂▂▅▅▅▃█▁▄▄▇▂▄▄▆▄▃▅▃▇▂▂▅▄▅▁▅▂▄▇▇▄▅▃▄
wandb:      train/policy_loss ▁██▄████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▁▇▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7873
wandb: best/eval_avg_mil_loss 0.56026
wandb:  best/eval_ensemble_f1 0.7873
wandb:            eval/avg_f1 0.74642
wandb:      eval/avg_mil_loss 0.58173
wandb:       eval/ensemble_f1 0.74642
wandb:           train/avg_f1 0.71495
wandb:      train/ensemble_f1 0.71495
wandb:         train/mil_loss 0.60963
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run splendid-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/05dszkmj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070731-05dszkmj/logs
wandb: ERROR Run 05dszkmj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: pj4ja90u with config:
wandb: 	actor_learning_rate: 3.838632101143836e-05
wandb: 	attention_dropout_p: 0.379005373238484
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 125
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3561125224866354
wandb: 	temperature: 6.122591021646896
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070914-pj4ja90u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pj4ja90u
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄▅▆▆▃▅▄▅▅██▅██▅▇▇█▅▇█▁▆▅▅▆█▆▅▆▁▂▇▂█▄▆█▅█
wandb:      eval/avg_mil_loss ▄▃▇▃▄█▄▅▄▄▆▃▂▄▅▅▂▇▃▅▃▇▆▅▆▅▆▅▇█▅▇▅▁▅▃▃▅▄▆
wandb:       eval/ensemble_f1 ▁▁▇▄▆▂█▁▇██▅██▅██▅▇█▅▅█▅▅▅▆▅▃▁▇█▄▅▅▅█▂▅▂
wandb:           train/avg_f1 ▄▆▄▄▆▃▃▅█▅▄▂▅▂▄▄▆▃▄▃▃▂▁█▅▁▆▅▆▅▅▄▆▅▃▆▃▆▄▂
wandb:      train/ensemble_f1 ▃▆▇▅▄▅▇▃▃▄▆▄▅▂▅▆▄▃▁▇▄▅▁▆▁▆▇▄▆▆▇▂▇█▆▅▄█▄▆
wandb:         train/mil_loss ▇▅▂▆▃▂▇▁▁▅▇▅▆▅▃▇▇▃▅▄▅▄▄▂▁▅█▃▅▄▅▅▆▄▃▅▅█▅▆
wandb:      train/policy_loss ▅▅█▁▅▃▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅██▅▁▅▅▅▅▅▁▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▃▄▁█▄▄▄▄▄▄▄▁▄█▄▄▄▄▄▄▄▄██▄▄▄█▄▄▄▄▁▄▄▁▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82767
wandb: best/eval_avg_mil_loss 0.45293
wandb:  best/eval_ensemble_f1 0.82767
wandb:            eval/avg_f1 0.78798
wandb:      eval/avg_mil_loss 0.69836
wandb:       eval/ensemble_f1 0.78798
wandb:           train/avg_f1 0.59394
wandb:      train/ensemble_f1 0.59394
wandb:         train/mil_loss 0.77331
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run laced-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pj4ja90u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070914-pj4ja90u/logs
wandb: ERROR Run pj4ja90u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: swhw8gil with config:
wandb: 	actor_learning_rate: 1.2311530511101655e-06
wandb: 	attention_dropout_p: 0.26078423291400943
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 130
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2239997172526853
wandb: 	temperature: 9.438084172704082
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071108-swhw8gil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/swhw8gil
wandb: uploading history steps 130-131, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇▇█
wandb: best/eval_avg_mil_loss █▅▁▄▁▆
wandb:  best/eval_ensemble_f1 ▁▄▇▇▇█
wandb:            eval/avg_f1 ▆▆█▇██▅▆▅▁▅▃▇▇▅▆█▆▇▇▅▆▁▁▄▅▅█▅▃██▆▇▇▅██▅▆
wandb:      eval/avg_mil_loss ▅▃▆▂▅▄▃▆█▃▅▅▆▇▅▄▃▆█▃▃▃▄▇▇▅▅▆▅▆▅▆▆▃▁▂▆▂▆▅
wandb:       eval/ensemble_f1 ▅██▇▅▇▅▇▇▁▅▃▄▁▅▇▇▇▁█▄▇▇▆▅▇▄▃▅██▃█▅██▅▁▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▇▃▄▅█▅▄█▄▆▇▆▆▇▇▃▇▆▄▇▇▄▅▄▇▅█▆▁▅▆▄▆▃▇█▇▇█
wandb:      train/ensemble_f1 ▆▄▆▄▇█▅▆▅▆▄▂▄▅▅▄█▇█▃▄▇▆▆▆▄▅▆▁▇▅▄▇▄▅▆▇▇▇▄
wandb:         train/mil_loss ▄▃▇▅▃▃█▅▂▆▆▄▆▅▅▄▆▇▃▃▁▆▃▃▄▃▄▄▅▃▅▃▆▄▃▆▄▇▄▆
wandb:      train/policy_loss ████▁███████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▁▆▄▄▆▆▄█▄▄▄▄▄▆▄▄█▄▁▄██▁▄▄▄▁█▄▁█▄▄▄▃▄▄▆▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80471
wandb: best/eval_avg_mil_loss 0.74611
wandb:  best/eval_ensemble_f1 0.80471
wandb:            eval/avg_f1 0.62532
wandb:      eval/avg_mil_loss 0.7703
wandb:       eval/ensemble_f1 0.62532
wandb:            test/avg_f1 0.78256
wandb:      test/avg_mil_loss 0.43754
wandb:       test/ensemble_f1 0.78256
wandb:           train/avg_f1 0.69081
wandb:      train/ensemble_f1 0.69081
wandb:         train/mil_loss 0.62586
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run stellar-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/swhw8gil
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071108-swhw8gil/logs
wandb: Agent Starting Run: nxvs7fyq with config:
wandb: 	actor_learning_rate: 1.093090180834644e-06
wandb: 	attention_dropout_p: 0.4762463162516009
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8409338288163539
wandb: 	temperature: 2.459719361033712
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071327-nxvs7fyq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nxvs7fyq
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 102-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss ▆█▃▁
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 ▃▇▆▇▇▅▆▇▇▆▇▇▇▆▁▆▇█▇▇▇▇▇▆▇▇▇▇█▃▇▇▇▇▇▇▇▅▆█
wandb:      eval/avg_mil_loss █▇▅▄▂▁▄▅▄▅▆▁▃▅█▄▄▃▆▄▄▄▅▄▂▃▄▇▂▅█▃▄▄▄▄▅▂▃▃
wandb:       eval/ensemble_f1 ▃▆▇▆█▇▅▇▆▇▆▅▆▇▆▅▃▇▁▇▆▅▇▇▇▆▆▆▄▇▇▆▆▄▆▆▆▆▆▆
wandb:           train/avg_f1 ▃▁▆▅▆▅▇▂▇▆▃▄▃▅▆▇▄▁▅▂▅▇▅█▅▇▄▂▅▅█▆▅▇▇▅▄▅▆█
wandb:      train/ensemble_f1 ▃▄▅▆█▄▆▃▅▄▄▃▅▆▅▄▁▅▇▇▃▃▄▅▅▇▅▄▅▅▅█▁▇▅▄▅▇▅█
wandb:         train/mil_loss █▃▄▄▂▅▁▄▃▃▄▅▂▄▁▃▄▆▄▃▂▁▃▅▇▄▃▃▅▂▂▆▃▃▂▄▃▅▄▄
wandb:      train/policy_loss ▄▄████▄▄▄██▄███▁███▁██▄▁▄▄▁▄█▄▄▄█▄▄▄▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄██▄▄█▄█▄▁█▄██▁▄▁█▄▄▄▁▄▄▄▄▄▄██▄▄▄▄▄▁▄▄▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78482
wandb: best/eval_avg_mil_loss 0.5947
wandb:  best/eval_ensemble_f1 0.78482
wandb:            eval/avg_f1 0.77957
wandb:      eval/avg_mil_loss 0.53964
wandb:       eval/ensemble_f1 0.77957
wandb:           train/avg_f1 0.76435
wandb:      train/ensemble_f1 0.76435
wandb:         train/mil_loss 0.54908
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run graceful-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nxvs7fyq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071327-nxvs7fyq/logs
wandb: ERROR Run nxvs7fyq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ui2tfjsa with config:
wandb: 	actor_learning_rate: 0.00011245490332365838
wandb: 	attention_dropout_p: 0.2574137071041007
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 163
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8105350801726853
wandb: 	temperature: 0.3486164056894514
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071527-ui2tfjsa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ui2tfjsa
wandb: uploading history steps 159-163, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▆▇█
wandb: best/eval_avg_mil_loss █▆▄▃▁
wandb:  best/eval_ensemble_f1 ▁▂▆▇█
wandb:            eval/avg_f1 ▄▅▂▅▇▂▂▇▄▅▄▆▆▇▅▇▆▅▅▄▄▅█▆▄▄▆▃▆▆▇▄▆▅▁▆▇▇▁▇
wandb:      eval/avg_mil_loss ▆▂▃▁▆▂▅▆▂▅▄▇▅▄▂▅▅▄▃▄▂▄▅▂▅▃▅█▁▄▃▂▆█▂▅▅▄▃▅
wandb:       eval/ensemble_f1 ▃▆▂▆█▄▆▆▁▃▄▆█▅▇▅▄▄▅▄▆█▇▆▇▅▅▆▅▆▆▄▂▃▁▁▆▄▆▄
wandb:           train/avg_f1 ▁▅▅▆▇▅▅▄▇▄▅▄▆▆▄▅▇▅▄▃▅▅▄▆▃▃█▃▆▅▃▆▃▁▆▇▅▅▆▂
wandb:      train/ensemble_f1 ▄▅▆▅▄▇▃█▇▄▄▅▅▂▁▁▆▃▅█▅▃▆▄▂▂▃▄▃▇▇▃▆▇▅▃▇▅█▃
wandb:         train/mil_loss ▆▃▆█▄▅▆▅▁▅▄▆▆▄▃▆▅▂▅▆▆▆▄▄▄▇▂▅▅▅▄▄▂▃▆▅▄▄▃▅
wandb:      train/policy_loss █▆▄▁▁▃▁▄███▁█▁▁█▄▆▄▄██▆▄██▄█▄▄█▆▄▄██▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████████████████▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82127
wandb: best/eval_avg_mil_loss 0.49887
wandb:  best/eval_ensemble_f1 0.82127
wandb:            eval/avg_f1 0.55831
wandb:      eval/avg_mil_loss 0.80726
wandb:       eval/ensemble_f1 0.55831
wandb:           train/avg_f1 0.64118
wandb:      train/ensemble_f1 0.64118
wandb:         train/mil_loss 0.70321
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ui2tfjsa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071527-ui2tfjsa/logs
wandb: ERROR Run ui2tfjsa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: hj0mnpjy with config:
wandb: 	actor_learning_rate: 5.34455000470148e-06
wandb: 	attention_dropout_p: 0.23294220977461896
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.016219082460368073
wandb: 	temperature: 1.1502681213456
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071852-hj0mnpjy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hj0mnpjy
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 100-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▇▇██
wandb: best/eval_avg_mil_loss █▅▇▆▄▇▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃▆▇▇██
wandb:            eval/avg_f1 ▆▇▆▆▆▆▁▆▆▆▅▆▇▇█▆█▆█▇▇▆▃▇▆█▆▇██▅▅▆█▇▇█▁▆▇
wandb:      eval/avg_mil_loss ▅▅▅▅▇▂▅█▅▅▃▅▂▄▄▄▆▅▄▅▅▅▄▅▄▄▇▅▅▅▁▅▅▅▇▇▆▅▂▄
wandb:       eval/ensemble_f1 ▆▇▆▇▆▁▇▇▂▇▇▇█▇▃▁██▆▇▆█▆▇▇▂█▇█▇▆█▇█▆▂▇▇█▇
wandb:           train/avg_f1 ▆▂▅▅▇▆▄▆▇▄▆▅▅▅▁▅▇▂▅▄▆▅▆▅▆▇▃▄▆▄▅▆▆▅▆█▅▇▁▄
wandb:      train/ensemble_f1 ▆▄▅▂▇▂▇▆▆▇▅▁▅▇▅▆▅▆▆▅▅▄▅▂▆▆▇▅▆▂▅▄▃█▃▆▄▁▆▇
wandb:         train/mil_loss ▅▆█▂▄▄█▁▄▅▅▆▆▄▅▂▅▃▁▃▆▄▂▃▄▁▃▅▂▄▄▁▁▅▄▃▃▄▂▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84542
wandb: best/eval_avg_mil_loss 0.39685
wandb:  best/eval_ensemble_f1 0.84542
wandb:            eval/avg_f1 0.76573
wandb:      eval/avg_mil_loss 0.61273
wandb:       eval/ensemble_f1 0.76573
wandb:           train/avg_f1 0.78987
wandb:      train/ensemble_f1 0.78987
wandb:         train/mil_loss 0.59311
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run radiant-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hj0mnpjy
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071852-hj0mnpjy/logs
wandb: ERROR Run hj0mnpjy errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: myiz0nl9 with config:
wandb: 	actor_learning_rate: 3.820541008580758e-06
wandb: 	attention_dropout_p: 0.134615384634593
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 146
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.010968355657717256
wandb: 	temperature: 7.175204806542583
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072056-myiz0nl9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/myiz0nl9
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇██
wandb: best/eval_avg_mil_loss █▁▇▇▇
wandb:  best/eval_ensemble_f1 ▁▅▇██
wandb:            eval/avg_f1 ▇▇▆▇█▂▇▂▇▇██▇█▇▇█▆▇▇▆▇▂█▇▂▁▁█▁▃▇▁▇▂▇▇▇▇▄
wandb:      eval/avg_mil_loss ▃▄▅▇▇▄▅▄▂▆▆▃▃▇▄▄▂█▆▃▆▂▂▁█▄▃▅▂▆▅▄▅▃▃▅▃▄▄▃
wandb:       eval/ensemble_f1 ▇▇▂▆█▂▇▇█▇▂▆▇██▄▂▆▇▇█▇▇▂█▁▅█▁▇▇▂▁▇▇▇▁▇▇█
wandb:           train/avg_f1 ▃▆▆▆▂▃▆▁▆▅▄▆▄▃▅▅▅▅▅▃▇▇▆▃▆▆▂▅▄▃█▂▇▂▆▄▅▆▄▄
wandb:      train/ensemble_f1 ▁▇▇▄▆▄▄▅▆▅▄▄▆▃▆▃▇▅▇▇▄▇▄▅█▇▄▅▄▇▅█▄▅▃▄▇▅▅▅
wandb:         train/mil_loss ▅▃▅▂▆▇▄▆▄▄▅▃▃▃▂▅█▅▃▃▂▂▆▄▅▄▁▃▁▆▂▅▆▅█▅▁▂▅▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82987
wandb: best/eval_avg_mil_loss 0.61249
wandb:  best/eval_ensemble_f1 0.82987
wandb:            eval/avg_f1 0.80719
wandb:      eval/avg_mil_loss 0.58276
wandb:       eval/ensemble_f1 0.80719
wandb:           train/avg_f1 0.65762
wandb:      train/ensemble_f1 0.65762
wandb:         train/mil_loss 0.80013
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fancy-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/myiz0nl9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072056-myiz0nl9/logs
wandb: ERROR Run myiz0nl9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: be6zfd8d with config:
wandb: 	actor_learning_rate: 2.563673110691598e-05
wandb: 	attention_dropout_p: 0.1509929192704721
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4141038925737759
wandb: 	temperature: 6.477575574044211
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072337-be6zfd8d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/be6zfd8d
wandb: uploading wandb-summary.json
wandb: uploading history steps 184-192, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂▃▃▇█
wandb: best/eval_avg_mil_loss █▄▁▇▃▁▃
wandb:  best/eval_ensemble_f1 ▁▁▂▃▃▇█
wandb:            eval/avg_f1 ▇█▆█▄▇▂▆█▁██▇▇█▂▅▁█▁▃▇█▁█▂▂▁▂▁██▇▁▇▂█▂▃█
wandb:      eval/avg_mil_loss ▃▄█▃▄▂▆▅▇▇▇▅▇▇▄▂▁▇▂▅▇▃▄▄▆▃▄▆▄▅▃▂▄▅▃▃▄▅▅▃
wandb:       eval/ensemble_f1 ██▄█▂▂▂▂███▂▂█▂█▁███▁███▁▁█▇▁▁▇▁▂▇▂▂▃█▂▂
wandb:           train/avg_f1 ▇▄▄▇▇▆▇▅▄▆▇▆▅▅▃▃▆▆▆▆▁▇▆▆▅▂█▄▇▅▆▄▄▆▄▆▄▇▅▆
wandb:      train/ensemble_f1 █▇█▄█▃▇▆▄█▅▆▇▇▁▇▆▆▅▆▆▅▅▃▆█▇▆▄▆▅▃▄██▆▆▂▇▇
wandb:         train/mil_loss ▁▃▁▅▃▃▃▄▄▄▃▂▂▄▂▅▂▂▄▁▄▃▃▃█▄▃▂▁▅▅▃▄▄▄▂▄▄▁▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁███████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81184
wandb: best/eval_avg_mil_loss 0.54977
wandb:  best/eval_ensemble_f1 0.81184
wandb:            eval/avg_f1 0.777
wandb:      eval/avg_mil_loss 0.59351
wandb:       eval/ensemble_f1 0.777
wandb:           train/avg_f1 0.6819
wandb:      train/ensemble_f1 0.6819
wandb:         train/mil_loss 0.63831
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sleek-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/be6zfd8d
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072337-be6zfd8d/logs
wandb: ERROR Run be6zfd8d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: bzcgw4lo with config:
wandb: 	actor_learning_rate: 4.873476873416405e-05
wandb: 	attention_dropout_p: 0.4558845199749065
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.601132278696145
wandb: 	temperature: 1.1640103662092272
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072709-bzcgw4lo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bzcgw4lo
wandb: uploading output.log; uploading config.yaml; uploading history steps 100-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇███
wandb: best/eval_avg_mil_loss █▅▅▁▂▁
wandb:  best/eval_ensemble_f1 ▁▅▇███
wandb:            eval/avg_f1 ▅▅▇█▄▇▆▇▁▇▅▇▇▇█▇▆▇▇▇▃▇▅▆▅▇▇▇▇▇▇▇▇█▇▆█▆▇▇
wandb:      eval/avg_mil_loss ▆▆▄▅▅▄▄▅▅▅▁▁▃▃▃▂▂▂▅▄▄▇▆▆▂▆▆▇▇▄▅▃▄▅▃▄█▅▄▆
wandb:       eval/ensemble_f1 ▅▇▇▆▇▇▆▇▇▅▇▇▄█▁▇▇▇█▇▅▆▄▇▅▇▇▇▇█▇▇█▇▇▇▆▆▇▇
wandb:           train/avg_f1 ▃▃▆▆▄▆▆▂█▄▄▅▅▅▃▅▄▂▇▅▆▅▄▅▃▂▁▆▆▄▅▄▆▄▄▄▃▂▆▁
wandb:      train/ensemble_f1 ▃▃▆▅▆█▃▄▆▅▂▅▅▃▂▁▅▄▅▅▄▅▆▂▃▄▅▅▄▆▆▄▃▄▃▁▂▅▂▁
wandb:         train/mil_loss ▄▁▄▇▄▁▅▆▆▃▅▅▅▃▄▅▆▇▄▅▃▄█▄▅▄▇▂▄▃▂▅▇▆▇▁█▇▆▂
wandb:      train/policy_loss ▄▄▁█▄▄▄▄▄▄▄███▄▄▄▄▄▄▄▄▄▄▄▄██▄▁▁▄▄▄▄▄▄▁▄▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81471
wandb: best/eval_avg_mil_loss 0.49612
wandb:  best/eval_ensemble_f1 0.81471
wandb:            eval/avg_f1 0.75967
wandb:      eval/avg_mil_loss 0.62487
wandb:       eval/ensemble_f1 0.75967
wandb:           train/avg_f1 0.69207
wandb:      train/ensemble_f1 0.69207
wandb:         train/mil_loss 0.60767
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bzcgw4lo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072709-bzcgw4lo/logs
wandb: ERROR Run bzcgw4lo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: llreoqp3 with config:
wandb: 	actor_learning_rate: 0.00014690452808316902
wandb: 	attention_dropout_p: 0.2122923659009431
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 104
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.010501310132672192
wandb: 	temperature: 0.9535464241022706
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072912-llreoqp3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/llreoqp3
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆██
wandb: best/eval_avg_mil_loss █▁▅▁▃
wandb:  best/eval_ensemble_f1 ▁▃▆██
wandb:            eval/avg_f1 ▇▄▄▄▆█▇█▅█▂▇▆▄▅▅▆▆▂▃▅▁▅▃▆▄▇▅▆▇▂▂▅▆▂▃▃▇▄▆
wandb:      eval/avg_mil_loss ▁▄▆▇▄▆▄▃▇▁▄▄▂▅▂▁▅▃▄█▃▃▇▁▂▆▆▃▂▃▆▅▅▆▂▂▆▅▄▄
wandb:       eval/ensemble_f1 ▆▄▄▆▄▄▂▅█▇▆▇▁▂▇▄▁▅▄▅▄▄▅▄▆▄▆▆▇▆▂▂▁▄▃▇▇▄▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▄▆▆▄▄▃▅▆▄▃▇▅▄▆▂▅▅▄▂▄▄▆▄▄▅▅▄▆▆█▄▄▅▄▄▁▅▄▇
wandb:      train/ensemble_f1 ▁▃▂▃▇▄▆▄▂▄▆▅▁▇▆▆▇▄▇▄▂▃▅▂▃▅▃▁▇▆█▇▅▃▄▅▄▄▃▄
wandb:         train/mil_loss ▅▅▄▃▇▃▇▅▆▄▅▅▇▆▄▆▆▄▂▂▁▆▅▄██▄▅█▅▃▅▅▇▄▁▆▄▅▄
wandb:      train/policy_loss ▆▅▁▁▃▁▆▁▅▅▅▁▅█▅▅▁▁▅▆▅█▅▅▅▆▅█▁▅█▆▆█▁▁▁▃▅▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▁▁▁▃▅▅▃▆▅▁▅▁▁▁▅▅▆▅▅▅▁█▅▅▅▅█▅█▆▁▅▆▃▅▁▅▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.72991
wandb: best/eval_avg_mil_loss 0.74857
wandb:  best/eval_ensemble_f1 0.72991
wandb:            eval/avg_f1 0.57997
wandb:      eval/avg_mil_loss 0.85496
wandb:       eval/ensemble_f1 0.57997
wandb:            test/avg_f1 0.57188
wandb:      test/avg_mil_loss 0.75003
wandb:       test/ensemble_f1 0.57188
wandb:           train/avg_f1 0.56026
wandb:      train/ensemble_f1 0.56026
wandb:         train/mil_loss 0.78924
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run balmy-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/llreoqp3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072912-llreoqp3/logs
wandb: Agent Starting Run: d6vczghi with config:
wandb: 	actor_learning_rate: 1.4467800852177037e-05
wandb: 	attention_dropout_p: 0.1772539619190441
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 169
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8744207138876056
wandb: 	temperature: 9.11388431841903
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073127-d6vczghi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d6vczghi
wandb: uploading wandb-summary.json; uploading history steps 156-169, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆█
wandb: best/eval_avg_mil_loss ▅█▃▃▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆█
wandb:            eval/avg_f1 ▇▇▁▇▇▇▇▆▆▇▁▇▇▇██▆▆██▆█▇█▅█▇▆▇▁▆▇█▇▁██▇▆▃
wandb:      eval/avg_mil_loss ▂▅▇▁▂▇▂▄▄▇▆▅▂▆▃▃▆█▇▂▆▄▃▁▃▁▄▄▄▄▂▁▄▃▄▅▂▄▃▅
wandb:       eval/ensemble_f1 █▁▇▇▇▇▃▇▆▆▆█▇▇█▇▇▇█▇▅█▇▇▅▇▇▇▇█▁█▇▆▅▇▅▇▃▇
wandb:           train/avg_f1 ▅▇▆▅▅▅▅▅▇▆▅▇▇▂▅▅▅▇█▇▇▇▇▇▆█▇▅▇▆▅▇▄▅▇▇▇▆▁▃
wandb:      train/ensemble_f1 ▄▄▅▅▃▅▁▄▃▂▅▆▇▇▇▆▅▂▄▆▅▆▅▃▃██▄▇▆▇▃▅▅▃▆▂▆▆▃
wandb:         train/mil_loss ▄▃▇▆▅▇▄▅▇▆▃▄▃▁▂▃▂▇▇▅▂▃▅█▃▅▃▁▂▅▆▃▄▅▇▇▁▅▅▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83952
wandb: best/eval_avg_mil_loss 0.44726
wandb:  best/eval_ensemble_f1 0.83952
wandb:            eval/avg_f1 0.7776
wandb:      eval/avg_mil_loss 0.63806
wandb:       eval/ensemble_f1 0.7776
wandb:           train/avg_f1 0.70916
wandb:      train/ensemble_f1 0.70916
wandb:         train/mil_loss 0.5771
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run volcanic-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d6vczghi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073127-d6vczghi/logs
wandb: ERROR Run d6vczghi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5pr7e2no with config:
wandb: 	actor_learning_rate: 0.0005225572435810106
wandb: 	attention_dropout_p: 0.4223470767123675
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 168
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8026752628639867
wandb: 	temperature: 1.2638371758680975
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073434-5pr7e2no
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5pr7e2no
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▆▇▇█
wandb: best/eval_avg_mil_loss ▇█▄▂▄▆▁
wandb:  best/eval_ensemble_f1 ▁▃▄▆▇▇█
wandb:            eval/avg_f1 ▂▇▁▆▇▄▄▆▄▅▅▇█▆▂▇▆▅█▄█▇▅▅▁▆█▁▇▂▇▁▇▂▇▆▆▅█▄
wandb:      eval/avg_mil_loss ▂▆▆▃▂▁▂▆▅▄▂▅▅▂▁▅▄▂▆▄▄▄▄▄▃▆▃▃▅▇▄▄▃█▇▆▅▂▃▃
wandb:       eval/ensemble_f1 ▂█▇█▇▄▇▄███▃▄▄▇█▁▅▇█▅▆▅▇▆█▆▂▆▁▇▅▇▇▆▇▄▆▇▅
wandb:           train/avg_f1 ▆▅▅▆▇▅▃▃█▇▄▆▃▂▅▄▃▄▅▄▆▅▂▃██▆▄▆▅▂▆▅▁▄▄▅█▆▆
wandb:      train/ensemble_f1 ▂▅▄▇▇▃▆▇▇▃▁█▂▆▃▅▄▇▃▅▅▂▃▄▄█▆▁▂▃▄▆▄▅▅▃▅▆▆▆
wandb:         train/mil_loss ▆▆▅▅▂▄▃▄▂▅▅▆▃▇▄▄▅▅▅▆▆▃▆█▄▄▄▆▆▆▄▄▆▆▄▁▂▄▂▆
wandb:      train/policy_loss █████████████████████▁██████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄█▄▄▄▄▁▄▄▄▄▃▄▄▃▄▃█▁▆▄▄▄▄▄▄▁▄▄▄▁█▄▄▁█▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84338
wandb: best/eval_avg_mil_loss 0.39711
wandb:  best/eval_ensemble_f1 0.84338
wandb:            eval/avg_f1 0.52849
wandb:      eval/avg_mil_loss 0.83327
wandb:       eval/ensemble_f1 0.52849
wandb:           train/avg_f1 0.68544
wandb:      train/ensemble_f1 0.68544
wandb:         train/mil_loss 0.77137
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pleasant-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5pr7e2no
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073434-5pr7e2no/logs
wandb: ERROR Run 5pr7e2no errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5zu3902u with config:
wandb: 	actor_learning_rate: 1.1468841254602786e-06
wandb: 	attention_dropout_p: 0.05277174157754488
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 105
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2830979735095871
wandb: 	temperature: 6.078186213554161
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073713-5zu3902u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5zu3902u
wandb: uploading history steps 97-105, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇▇█
wandb: best/eval_avg_mil_loss ██▃▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇▇█
wandb:            eval/avg_f1 ▅▅█▃▅▄▃▆▄▂▅▇▅▇█▇▄▆█▆▂▂▅▆▆▆█▆▄▆▆▆▄▅▃▁▆▂▅▇
wandb:      eval/avg_mil_loss ▂▃▃▂▆▆▁▄▅▁▃▅▂▅▆▃█▂▇▇▅▄▁▅▁█▅▆▃▇▇▃▂▅▅▆▆▄▃▆
wandb:       eval/ensemble_f1 ▅█▇▂▂▆▅▅▄▇█▂▆▄▄▅▃▃█▁▂▅▇█▆▆▆▇▇▆▅▆▇▆▄▃▇▅▅▄
wandb:           train/avg_f1 █▅▁▄▁▂▁▃▅▅▆▅▆▇▅▃▆▄▆▃▆▃▅▃▆▄▄▅▂▅▅▁▄▇▅▆▂▂▆▇
wandb:      train/ensemble_f1 █▄▄▆▆▂▃▅▃▆▅▃▁▆▆▇▆▇▅▅▆▅▄▆▅▆▆▅▅▆▅▅▃█▆▃▇▇▃▅
wandb:         train/mil_loss ▃▇▄▃▅▁▅▅▅▃▂▂▅▆▆▄▃▃▄▃▅█▄▄▅▅▂▇▄▅▁▄▂▃▃▄▄▅▃▅
wandb:      train/policy_loss ██████▁█████████████████████████████▅███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82767
wandb: best/eval_avg_mil_loss 0.46843
wandb:  best/eval_ensemble_f1 0.82767
wandb:            eval/avg_f1 0.74532
wandb:      eval/avg_mil_loss 0.64829
wandb:       eval/ensemble_f1 0.74532
wandb:           train/avg_f1 0.78422
wandb:      train/ensemble_f1 0.78422
wandb:         train/mil_loss 0.57737
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5zu3902u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073713-5zu3902u/logs
wandb: ERROR Run 5zu3902u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5jltwxb5 with config:
wandb: 	actor_learning_rate: 6.371743771833903e-05
wandb: 	attention_dropout_p: 0.09830512591210684
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 172
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10490284541322448
wandb: 	temperature: 1.6045342453007427
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073938-5jltwxb5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5jltwxb5
wandb: uploading history steps 168-172, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▆▇▇▇█
wandb: best/eval_avg_mil_loss ██▆▄▇▄▁▂▃
wandb:  best/eval_ensemble_f1 ▁▄▄▅▆▇▇▇█
wandb:            eval/avg_f1 ▆▇▇▇▆▇▇▂▅▅▇▃▇▇▃▆▇▁▃█▆▇█▆▄▂█▃█▇▇█▂▁▆▇▇▄▇▇
wandb:      eval/avg_mil_loss ▃▅▃▄▅▄▃▅█▄▄▆▃▆▇▃▄▄▄▆▃▃▅▇▅▄▆▃▄▃█▄▃▆▁▄▅▅▃▄
wandb:       eval/ensemble_f1 █▅▇█▇▁▇█▅▅█▇▅▅▅▇▇▆██▇██▇▆██▇▇▅█▁▅▇▇█▆▅▇▆
wandb:           train/avg_f1 ▄▅▇▆▅▃▆▁▅▂▆▆▄▃▄▄▁▇█▆▅▆█▆▆▄▇▄█▇▄▅▆▃▇▇▇▅▂▇
wandb:      train/ensemble_f1 ▅▆▅▃▂▆▆▅▆▁▅▅█▅▇▄▂█▅▆▄▅▅▄▄▇▄▆▅▇▄▆▅▆▆▆▅█▂▇
wandb:         train/mil_loss ▆▅▄▄▆▄█▃▃▅▄▄▇▄▆▂▄▄▄▂▆▆▆▄█▅▆▄▁▂▁▄▅▅▃▁▅▅▄▇
wandb:      train/policy_loss ██▁█████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄█▄▄▄▁▄▄▄▄▁▁▄▁▄▄▄▄▄█▄█▄▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80018
wandb: best/eval_avg_mil_loss 0.52122
wandb:  best/eval_ensemble_f1 0.80018
wandb:            eval/avg_f1 0.73721
wandb:      eval/avg_mil_loss 0.62197
wandb:       eval/ensemble_f1 0.73721
wandb:           train/avg_f1 0.75169
wandb:      train/ensemble_f1 0.75169
wandb:         train/mil_loss 0.63672
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run stellar-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5jltwxb5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073938-5jltwxb5/logs
wandb: ERROR Run 5jltwxb5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: tpbghbe3 with config:
wandb: 	actor_learning_rate: 0.0003649760905713604
wandb: 	attention_dropout_p: 0.1356976755621493
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 140
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.42678469820441745
wandb: 	temperature: 1.4696201415353594
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074249-tpbghbe3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tpbghbe3
wandb: uploading history steps 97-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▅▅▆▄▁▄▆▆█▇▆▅▅▅▇▆▅▆▄▆▄▄▆▅▇▆▃▃▇▇▆▆▂▄▄▅▇▆▄
wandb:      eval/avg_mil_loss ▅▄▃▅▁▄▁▃▄▅▂▂▅▅▄▅▃▆▅▂▇▂▅▃▅▇▁▄▃▂▆▆▆▄▇▅▄▆█▄
wandb:       eval/ensemble_f1 ██▅▅▆▂▅▄▆▆▅▆▆▄▂▅▆▆▆▇▆▅▆▆▆▇▄▆█▄▃▄▄▇▇▇▇▁▇▅
wandb:           train/avg_f1 ▃▅▄▇▆▂▇▅▆▃▅▄▆▄▅▂▄▄▆▁▄▇▇▁▅▆▆▅▇▆▆▃▅▄▅██▃▇▂
wandb:      train/ensemble_f1 ▃▅▇▆▅▂▂▇▅▆▃▇▄▅▃▄▅▂▅▄▃▄▄▃▄▇▇▁▆▃█▅▆▃▃▆██▄▆
wandb:         train/mil_loss ▄▃▇▅▂▃▇▅▄▄▃▃▄▅▁▄▄▄▆▄▃▆▅▁▃▂▆█▃▆▅▃▅▅▂▅▆▄▃▅
wandb:      train/policy_loss ▁▅█▅▅▁▅▅██▅▁█▅█▁▅▅▁▁██▅▁▅▅▅██▅▅▁▁▁▅█▅▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81366
wandb: best/eval_avg_mil_loss 0.52081
wandb:  best/eval_ensemble_f1 0.81366
wandb:            eval/avg_f1 0.78188
wandb:      eval/avg_mil_loss 0.54283
wandb:       eval/ensemble_f1 0.78188
wandb:           train/avg_f1 0.7461
wandb:      train/ensemble_f1 0.7461
wandb:         train/mil_loss 0.58714
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run generous-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tpbghbe3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074249-tpbghbe3/logs
wandb: ERROR Run tpbghbe3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: st7fif95 with config:
wandb: 	actor_learning_rate: 0.00018222203071640788
wandb: 	attention_dropout_p: 0.19381093596877536
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 116
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6613768689902628
wandb: 	temperature: 1.164452617592272
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074503-st7fif95
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/st7fif95
wandb: uploading history steps 111-117, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▆▆█
wandb: best/eval_avg_mil_loss █▄█▃▁▂
wandb:  best/eval_ensemble_f1 ▁▅▅▆▆█
wandb:            eval/avg_f1 ▇▇▇▆▆▆▇▇▇▇▆█▆▆▇▁▇█▇▂▇▇█▁▆▇▆█▆▆▇▁▇█▇▄▇▅▆▆
wandb:      eval/avg_mil_loss ▄▂▄▄▃▄▄▄▄▄▄▆▄▄▂█▅▄▁▃▂▆▃▅▂▃▅▃▂▅▃▆▅▃▄▁▄▄▂▄
wandb:       eval/ensemble_f1 ▆▆▇▆▆▇▆▆▇▆▇▆▆▂█▇▇▅▇▆▅█▆▁▆▇▅▇▆█▁▆▆▆▇▃▇▆▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▁▁▃▅▃▃▅▃▃▃▆▃▄▄▆▄▆▃▂▃▅▃▃▃▅▄▄▇▅▄▄▃▄▂▄▂▃▅█
wandb:      train/ensemble_f1 ▆▄▆▅▇▄▅▇▄▃▆█▆▂▇▇▆▄▄▂▅▄▇▄▆▇▄▄▃▆▆▄▅▆▁▆▇▄▆▁
wandb:         train/mil_loss ▃▂▃▆▂▆▃▅▄▃▄▆▅▅▆▅▁▅▆▃▅▃▄█▂▄▇▆▅▅▄▅▅▅▃▂▄▂▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78482
wandb: best/eval_avg_mil_loss 0.51943
wandb:  best/eval_ensemble_f1 0.78482
wandb:            eval/avg_f1 0.72619
wandb:      eval/avg_mil_loss 0.62744
wandb:       eval/ensemble_f1 0.72619
wandb:            test/avg_f1 0.78951
wandb:      test/avg_mil_loss 0.50701
wandb:       test/ensemble_f1 0.78951
wandb:           train/avg_f1 0.76588
wandb:      train/ensemble_f1 0.76588
wandb:         train/mil_loss 0.55217
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run eager-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/st7fif95
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074503-st7fif95/logs
wandb: Agent Starting Run: q9aqveg0 with config:
wandb: 	actor_learning_rate: 0.0001221306881483886
wandb: 	attention_dropout_p: 0.18192782193148663
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 193
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6632107030950932
wandb: 	temperature: 0.5694239695905412
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074718-q9aqveg0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/q9aqveg0
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇███
wandb: best/eval_avg_mil_loss ▆▅█▁▁▁▂
wandb:  best/eval_ensemble_f1 ▁▇▇▇███
wandb:            eval/avg_f1 ▂▇▂▇▂▅▁▁▇█▇▂▂▁▁▄▇▂▁▂▁█▂▇▂▇▅▂▂██▅▂▅▂█▇██▇
wandb:      eval/avg_mil_loss ▃▇▃▃▁▅▆▇▅▅▃▇▁▅▅▇▆▁▂▅██▅▅▃▂▃▆▄▆▃▅▅▇█▅▅▂▅▆
wandb:       eval/ensemble_f1 ▇▂▇▁▁▂▂▇▂▅▅▂▇█▂▄▇▁▅▂▅▂██▇▇▁▂▂▇▇█▂▆▇▇▆▇█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▅▅▅▃▄▆▆▇▄▃▂▄▄▆▃▄▃▄▇▃▅▅▅▄▅▄▂▃▁▃█▅▂▅▄▂▁▄
wandb:      train/ensemble_f1 ▄▄▄▇▅▄▆▄▅▅▅▃▆▅▃▂▅▃▆▆▆▆▆▅▁▆▄▅▆▅█▆▁▄▆▂▆▅▄▅
wandb:         train/mil_loss ▃▅▁▄▄▄▄▄▄▃▄▄▄▄▄▅▅▃▄▆▄▇▅▆▆▃▄▄▅▄▄▃▄▆▃▇█▃▃▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████▁████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84103
wandb: best/eval_avg_mil_loss 0.51103
wandb:  best/eval_ensemble_f1 0.84103
wandb:            eval/avg_f1 0.46262
wandb:      eval/avg_mil_loss 0.94413
wandb:       eval/ensemble_f1 0.46262
wandb:            test/avg_f1 0.45212
wandb:      test/avg_mil_loss 0.6995
wandb:       test/ensemble_f1 0.45212
wandb:           train/avg_f1 0.6446
wandb:      train/ensemble_f1 0.6446
wandb:         train/mil_loss 0.83028
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wild-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/q9aqveg0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074718-q9aqveg0/logs
wandb: Agent Starting Run: i18gnz44 with config:
wandb: 	actor_learning_rate: 1.4130755557441716e-06
wandb: 	attention_dropout_p: 0.002716148263062379
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 110
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3205933860532669
wandb: 	temperature: 2.791888560752509
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075029-i18gnz44
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i18gnz44
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅▆▆▆▇██
wandb: best/eval_avg_mil_loss █▅█▇▅▁▃▄▄▇▃
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅▆▆▆▇██
wandb:            eval/avg_f1 ▅▆▆▆▆▆▄▇▆▆▅▄▄▅▇▅▇▆▅▁▆▅▂▄▆▄▆▆▇▇▄█▇▆▇▇▆█▆▅
wandb:      eval/avg_mil_loss ▅▃▅▄▃▁▂▆▁▂▂▄▇▄▂▂▃▅▂▁▁▁▃▇█▅▄▅▃▃▁▁▂▃▁▇▃▁▄▇
wandb:       eval/ensemble_f1 ▅▅▆▃▆▅▅▄▆▁▄▇▆▂▄▄▆▆▇▆▆▆▆▆▆▄▇▇▆▅▆▆▇▃█▄█▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▃▅▅▄▆▅▄▆▆▂█▄▅▄▆▂▇▅▁▄▅▇▇▆▅█▅▇▆▅▆▅▆▇▅▅▇▆
wandb:      train/ensemble_f1 ▄▁▇▇▄▂▂▅▃▅▅▇▄▆▄▄▃▆▇▃▆▆▆▆▆█▆▆▅▅▄▅▄▅▅▄█▅▇█
wandb:         train/mil_loss ▄▆▅▄▄▃▂▆▄▄█▂██▃▂▃▄▃▇▅▄▅▅▄▄▄▅▃▁▅▃▄▁▅▅▄▃▅▄
wandb:      train/policy_loss ███████▂██████████████████████████████▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████▁████████▃████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83121
wandb: best/eval_avg_mil_loss 0.50006
wandb:  best/eval_ensemble_f1 0.83121
wandb:            eval/avg_f1 0.72011
wandb:      eval/avg_mil_loss 0.70441
wandb:       eval/ensemble_f1 0.72011
wandb:            test/avg_f1 0.81952
wandb:      test/avg_mil_loss 0.54286
wandb:       test/ensemble_f1 0.81952
wandb:           train/avg_f1 0.7862
wandb:      train/ensemble_f1 0.7862
wandb:         train/mil_loss 0.54571
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run upbeat-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i18gnz44
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075029-i18gnz44/logs
wandb: Agent Starting Run: eegg99rr with config:
wandb: 	actor_learning_rate: 9.007574832286728e-05
wandb: 	attention_dropout_p: 0.018989275002083472
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 64
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9499104736290996
wandb: 	temperature: 7.456935009180309
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075253-eegg99rr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eegg99rr
wandb: uploading history steps 56-64, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇██
wandb: best/eval_avg_mil_loss ▇█▃▁▂
wandb:  best/eval_ensemble_f1 ▁▆▇██
wandb:            eval/avg_f1 ▇▇▅▇█▁█▇▇▆▇▁▇█▇▇▇▇██▇▇▇▆▆▆▇▂▇▇▆▇▁▆▇▇▂▇▂█
wandb:      eval/avg_mil_loss ▃▃▅▃▃▇▄▆▆▇▅▄▅▅▄▅▅▃▇▃▆▃▄▁▁▇▁█▃▄▄▇▄▇▅▅▅▅▃▄
wandb:       eval/ensemble_f1 ▇▇▅██▇▆▇█▇▁█▅▇▇▇█▂█▆▇▇█▆▆▇▇▇▇▇▁▇▆▇▁▇▇▇▇▇
wandb:           train/avg_f1 ▄▆▇▄▇▂▅▃▄▅▄▅█▅▆█▂▆▆▂▇▆▇▅▄▆▆▄▇▆▁▆█▆▆▇▃▅▄▂
wandb:      train/ensemble_f1 ▅▆▄▆█▄▅▃▁▄▅▇▄▅▇▅▆▅▄▂▇▅▆▅▅▅▄▅▄▆▄▁▅▅▇▅▆▄▅▂
wandb:         train/mil_loss ▃▆▃▇▄▆▄▅▅▄▄▄▇▅▃▇▇█▃▂▂▄▂▆▃▃▅▄▅▆▇▆▃▁▆▄▆█▃▇
wandb:      train/policy_loss ███████████████████▁████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80896
wandb: best/eval_avg_mil_loss 0.58621
wandb:  best/eval_ensemble_f1 0.80896
wandb:            eval/avg_f1 0.80203
wandb:      eval/avg_mil_loss 0.65698
wandb:       eval/ensemble_f1 0.80203
wandb:           train/avg_f1 0.66939
wandb:      train/ensemble_f1 0.66939
wandb:         train/mil_loss 0.67715
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lyric-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eegg99rr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075253-eegg99rr/logs
wandb: ERROR Run eegg99rr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mxb4ay8o with config:
wandb: 	actor_learning_rate: 1.0529291897233456e-05
wandb: 	attention_dropout_p: 0.3105960714357317
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.024237435341690428
wandb: 	temperature: 3.792045385041136
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075412-mxb4ay8o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mxb4ay8o
wandb: uploading history steps 180-184, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▇██
wandb: best/eval_avg_mil_loss ▅▆▂█▂▂▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆▇██
wandb:            eval/avg_f1 ▆▆▇▆█▆▇▆▇█▁▄▅▅▆▅█▅▅▅█▁▇▆▃▅▂▇▄▄█▇▇▇▆▆▅▆▅▆
wandb:      eval/avg_mil_loss ▃▂▅▄▃▂▄▁▆▃▃▃▂▂▂▂▂▅█▄▅▅▄▃▃▁▅▂▄▃▂▃▄▄▇▃▄▁▆▄
wandb:       eval/ensemble_f1 ▇▄▅▆▇▅▇██▇▇▇█▆▇█▇▃▁▇▅▅▇▇█▇▆▄▅█▆▆▇█▆▆▆▇▇▇
wandb:           train/avg_f1 ▆▇▆▅▅▆▃▅▆▇▇▇▆▅▇▃▆▇▆▆▅▇▆▂█▅▆▄█▇▆▅█▅▆▆▇▁▆▆
wandb:      train/ensemble_f1 ▄▅▄▅▇▂▅▆▆▆▅▄▇▅█▆▅▆▄▅▅▅▅▆▆▄▃▁▅▅█▅▃▅▆▆▇█▆▆
wandb:         train/mil_loss ▇▃▅▃▇█▆▃▂▃▃▅▂▅▅▄▅▃▅▇▅▁▆▅▂▇▄▅▂▅▄▃▅▁▃▃█▄▃▃
wandb:      train/policy_loss █████████████████████████████████▁██████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄█▄▄▄▁█▄▄▁▄▁▄▄▄▄▁▁▁█▁▄▄█▁▁▄▄▄▄█▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83488
wandb: best/eval_avg_mil_loss 0.451
wandb:  best/eval_ensemble_f1 0.83488
wandb:            eval/avg_f1 0.75492
wandb:      eval/avg_mil_loss 0.69876
wandb:       eval/ensemble_f1 0.75492
wandb:           train/avg_f1 0.72302
wandb:      train/ensemble_f1 0.72302
wandb:         train/mil_loss 0.72507
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run drawn-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mxb4ay8o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075412-mxb4ay8o/logs
wandb: ERROR Run mxb4ay8o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: kyzc4ajb with config:
wandb: 	actor_learning_rate: 0.0005881591396435628
wandb: 	attention_dropout_p: 0.4910794318775117
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 77
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.45580123821496
wandb: 	temperature: 0.5729732335318971
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075739-kyzc4ajb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kyzc4ajb
wandb: uploading wandb-summary.json
wandb: uploading history steps 69-77, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▇██
wandb: best/eval_avg_mil_loss █▇▂▅▁▁
wandb:  best/eval_ensemble_f1 ▁▁▃▇██
wandb:            eval/avg_f1 ▆▆▆▆▆▇▇▆▆██▆▆▆▆▆▇▆▇█▆█▅██▇▇▆▆▆█▆▇▁▆▆▆██▅
wandb:      eval/avg_mil_loss ▅▅▄▂▄▄▁▂▁▃▁▄▅▂▅▆▄▅▁▁▅▄▅▁▂▆▇▃▅▁▁▂▆▅█▅▅▁▅▅
wandb:       eval/ensemble_f1 ▆▆▆▆█▆▆▆▁▆█▆█▇▇▆▆▆▆▆▆█▇▇▇█▅█▆▆▇▆▆▇▆▆▇██▆
wandb:           train/avg_f1 ▅▇▆▇▅▅▄▅▅▄▂▄▇▁▄▆▅▁▆▄▇▂▄▄▆▄▇▄▇▆▄▅▅▅▆▄▇▄▄█
wandb:      train/ensemble_f1 ▅▇▆▅█▅▅▅▃▅▅▅▄▆▁▅▇▄▄▅▄▄▄▇▄▄▆▅▆▅▄▅▅▅▆▅▄▆▄█
wandb:         train/mil_loss ▂▆▆▅▇▂▅▃▆█▄▃▃▆▃▄▅▄▄▂▄▆▅▃▆▅▄▄▃▃▂▂▄▂▁▃▆▇▂▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84765
wandb: best/eval_avg_mil_loss 0.4367
wandb:  best/eval_ensemble_f1 0.84765
wandb:            eval/avg_f1 0.73896
wandb:      eval/avg_mil_loss 0.6658
wandb:       eval/ensemble_f1 0.73896
wandb:           train/avg_f1 0.8095
wandb:      train/ensemble_f1 0.8095
wandb:         train/mil_loss 0.62752
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fragrant-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kyzc4ajb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075739-kyzc4ajb/logs
wandb: ERROR Run kyzc4ajb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1yfr9gsm with config:
wandb: 	actor_learning_rate: 2.270757708754449e-06
wandb: 	attention_dropout_p: 0.39994428224574224
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 182
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8834612396050322
wandb: 	temperature: 1.613840000954131
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075912-1yfr9gsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1yfr9gsm
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▆█
wandb: best/eval_avg_mil_loss █▆▆▃▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▆▆█
wandb:            eval/avg_f1 ▇▇▅▅▆▃▆▅▄▇▅▅▆▃▁▇▇▃▃▅█▆▄▆▆█▅▇▆▆▄▄▇▃▅▇▇▇▅▆
wandb:      eval/avg_mil_loss ▆▇▆▇▅▇▅▇▅▇█▄▆▆▆▇▇▆▅▅▁▄▄▇█▅▆▇▄▅▅▇▅█▇▅▅▆▆▅
wandb:       eval/ensemble_f1 ▆▆▆▆▇▆█▆█▁▄█▄▄▁▄▇▅▄▆▆▅▆▆▄▇▆▁▄▇▂▇▄▂▇▄▆▆▆▄
wandb:           train/avg_f1 ▄█▂▄▄▄▇▅▅▇▄▄▇▄▅▃▅▆▂▂▅▅▄▄▃▆▄▄▄▃▃▄▆▅▄▃▄█▁▄
wandb:      train/ensemble_f1 ▆▅▇▆▅▇▆▄▅▄█▄▅▇▆▆▃▆▇▇▅▅▅▆▃▆▂█▇▄▆▃▄▇▁▇▆▄▅▄
wandb:         train/mil_loss ▅▅▆▅▅▆▃▂▄▆▆█▆▄▅▁▇▅▄▇▃▅▅▂▃▄▄▇▆▅▃▂▂▃▇▅▄▂▃▄
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▂▇▄▅▄▄▇▁▄▄▄▇▁▇▄█▄▄▁▁▄▁▁▄▁▁▄▇▄▁▅▁▁▄▁▁▇▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79965
wandb: best/eval_avg_mil_loss 0.47361
wandb:  best/eval_ensemble_f1 0.79965
wandb:            eval/avg_f1 0.63061
wandb:      eval/avg_mil_loss 0.77761
wandb:       eval/ensemble_f1 0.63061
wandb:           train/avg_f1 0.5796
wandb:      train/ensemble_f1 0.5796
wandb:         train/mil_loss 0.8269
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run revived-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1yfr9gsm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075912-1yfr9gsm/logs
wandb: ERROR Run 1yfr9gsm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: t91amaoe with config:
wandb: 	actor_learning_rate: 1.4930936989444544e-05
wandb: 	attention_dropout_p: 0.0973077814358948
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 162
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.16700733849246818
wandb: 	temperature: 7.178915469985723
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080315-t91amaoe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t91amaoe
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▅▆▇█
wandb: best/eval_avg_mil_loss █▅▇▇▃▅▁
wandb:  best/eval_ensemble_f1 ▁▁▃▅▆▇█
wandb:            eval/avg_f1 ▅▇▁█▇▇▅▆▆▆▆▃█▇▇▇▇▇█▆▇██▇▇██▇▆███▆█▁▇▇▆▇▇
wandb:      eval/avg_mil_loss ▄▆▅▇█▁▁▄▄▆▇▇▆▆▆▂▆▂█▃▄▇▆▅▂▅▄▆▆▃▃▅▁▆▅▅▄▅██
wandb:       eval/ensemble_f1 ▇██▇▆▇▇▆▃▁▇▆▄▇█▇▇▇██▅▇██▇██▃▇███▇▇▇███▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▆▅▇▅▅▄▅█▆▇█▅▆▃▆▅▇▅▄▁▇█▅▅▅▇▅▄▆▄██▆▅▆▆▅▄▅
wandb:      train/ensemble_f1 █▅▅▅▅▆▅▇▁▆▇█▅█▅▆█▇▁▇▆▅▃▆▆▃▂▅█▆▃▄█▇▇▆▅▇▆▅
wandb:         train/mil_loss ▃▃▄▆█▆▃▆▂▇▇▂▄▄▂▄▃▆▄▃▃▁▃▄▄▇▅▂▅▄▄▅▅▅▅▆▂▃▃▂
wandb:      train/policy_loss ████████▁███████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▁████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.86005
wandb: best/eval_avg_mil_loss 0.36325
wandb:  best/eval_ensemble_f1 0.86005
wandb:            eval/avg_f1 0.79179
wandb:      eval/avg_mil_loss 0.399
wandb:       eval/ensemble_f1 0.79179
wandb:            test/avg_f1 0.8819
wandb:      test/avg_mil_loss 0.39007
wandb:       test/ensemble_f1 0.8819
wandb:           train/avg_f1 0.73957
wandb:      train/ensemble_f1 0.73957
wandb:         train/mil_loss 0.60054
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smart-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t91amaoe
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080315-t91amaoe/logs
wandb: Agent Starting Run: my6gpn5a with config:
wandb: 	actor_learning_rate: 2.5148053886172943e-05
wandb: 	attention_dropout_p: 0.3159659241036897
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9849405118515484
wandb: 	temperature: 3.028208781973821
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080615-my6gpn5a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/my6gpn5a
wandb: uploading wandb-summary.json
wandb: uploading history steps 110-120, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇▇▇▇█
wandb: best/eval_avg_mil_loss ▅▇█▁▆▄▂
wandb:  best/eval_ensemble_f1 ▁▂▇▇▇▇█
wandb:            eval/avg_f1 ▆▂▆▄▆▆▆▁██▆▇▇▆▂█▂▇▅▆▇▇▄▆▆█▇▆▆█▅▆▆▆▆▆▇▇▄▇
wandb:      eval/avg_mil_loss ▄▅█▇▂▄▆▁▆▅▃█▂▂▇▃▅▃▃▅▂▅▅▄▅▅▇▃▅▄▂▂▃▄▂▃▂▂▅▁
wandb:       eval/ensemble_f1 ▆▆▅▅▇█▇▅▆▆▆█▄▁▄▅█▆█▇▇▆▆█▇▇▃▆▆▆▅▃▇▆█▇▇▃█▇
wandb:           train/avg_f1 ▄▁▅▄▃▂▃▆▁▆▃▃▅█▂▄▄▂▅▆▄▃▄▇▃▆▃▃▂▄▆▃▃▃▄▃▃▃▄▇
wandb:      train/ensemble_f1 ▃▃▇▅▅▆▂▃▃▃█▂▅▄▅▄▂▆▄▄▄▇▂▇▃▆▂▃▂▃▃▄▁▄▆▂▁▅▁▄
wandb:         train/mil_loss ▅▅▄▂▄▄▇▅▅▃▅▂▅▂▃▄▂▄▅▇▁▄▃▄▃▆▃▄▄▇▃█▂▅▂▂▄▄▃▄
wandb:      train/policy_loss █▄▄▄▁▄▁█▄▄▄▄▁▄▄█▄█▄▄▄██▁█▁▄▄▄█▄▁█▄▁▄▁▄▁▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83433
wandb: best/eval_avg_mil_loss 0.45671
wandb:  best/eval_ensemble_f1 0.83433
wandb:            eval/avg_f1 0.77295
wandb:      eval/avg_mil_loss 0.62956
wandb:       eval/ensemble_f1 0.77295
wandb:           train/avg_f1 0.75695
wandb:      train/ensemble_f1 0.75695
wandb:         train/mil_loss 0.61207
wandb:      train/policy_loss -0.19849
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.19849
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fresh-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/my6gpn5a
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080615-my6gpn5a/logs
wandb: ERROR Run my6gpn5a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: y4s58js3 with config:
wandb: 	actor_learning_rate: 3.862068653587336e-06
wandb: 	attention_dropout_p: 0.4425566982378252
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 54
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7663119882083661
wandb: 	temperature: 7.405698201715564
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080835-y4s58js3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y4s58js3
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 42-54, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▆▇▇▄▇▇▇▅▅▆▅▆▅▇▅▅▅▅▇▇▅▆▆▁▅██▆██▇█▅▆▇▇▅▅▆
wandb:      eval/avg_mil_loss ▁▁▅▄▇▂▂▂█▃▁▇▂▆▃▆▆▂▁▂▂▁▃▁▆▁▅▆▁▁▁▂▆▇▂▅▂▁▇▆
wandb:       eval/ensemble_f1 ▇▆▇▇▇▇▆▅▇▆▆▅▆▅▅▇▇▇▅▆▆▁▇▅█▆█▇▇██▅▆▇▇▅█▅▆▅
wandb:           train/avg_f1 ▅▁▄▄▂▂▄▅▆▂▅▇▃▅▃▅▃▇▅▆█▆▆▆▆▅▅▂▅▁▅▆▅▂▂▇▂▄▂▆
wandb:      train/ensemble_f1 ▇▇▃▆▆▁▄▆▇▇▆█▅▆▆▆█▅▇▇█▆▇▇▇▆▆▄▇▃▆▇▇▆▄█▄▆▄▇
wandb:         train/mil_loss ▄▄▆▆▆▄▇▇▇▄▄▆▄▇▆▇▆▃▁▆▅▇▅▅▅▂▃▅▇▆▄▂▆▅▅█▄▇▆▅
wandb:      train/policy_loss ███▆███████████████████▁████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████▁███████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83242
wandb: best/eval_avg_mil_loss 0.4532
wandb:  best/eval_ensemble_f1 0.83242
wandb:            eval/avg_f1 0.7435
wandb:      eval/avg_mil_loss 0.71626
wandb:       eval/ensemble_f1 0.7435
wandb:           train/avg_f1 0.79426
wandb:      train/ensemble_f1 0.79426
wandb:         train/mil_loss 0.58059
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run firm-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y4s58js3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080835-y4s58js3/logs
wandb: ERROR Run y4s58js3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: g0kw4x5s with config:
wandb: 	actor_learning_rate: 9.379449082020227e-06
wandb: 	attention_dropout_p: 0.461426717103547
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 114
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.294444136262378
wandb: 	temperature: 4.312380486630504
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080943-g0kw4x5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g0kw4x5s
wandb: uploading history steps 110-114, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂█
wandb: best/eval_avg_mil_loss ▅█▁
wandb:  best/eval_ensemble_f1 ▁▂█
wandb:            eval/avg_f1 ▇▇▁▇█▅▅▇▇█▄▄▇█▇▇▇█▇▄▆▃▄▆▇▇▇▆▅███▃▅█▄█▅▇▇
wandb:      eval/avg_mil_loss ▃▇▆▃█▅▄▆▃█▃▅▃▂▃▅▃▃▅▆▆▄▃▆▁▃▄▃▇▃▄▂▄▂▆▅▄▄▃▄
wandb:       eval/ensemble_f1 ▅▅▂▇▅█▅▇▄▅▁██████▇█▆▅▆██▅▇█▇▇█▇▇▅▅█▇▇█▅█
wandb:           train/avg_f1 ▄▄▄▁▂▃▂▃▂▂▆▁▅▂▄▄▄▅▁▁▃▄▆▁▆▄▃▄▃▁▅▄▅▄▂▄▃▂█▂
wandb:      train/ensemble_f1 ▆▄▃▅▃▆▄▂▃▂▁▂▅▅▅▆▄▅▄▇▁▂▂▅▃█▅▄▇▇▄▅▅▄▆▇▂▄▂▅
wandb:         train/mil_loss ▁▄▃▃▄▅▁█▇▅▇▅█▁▅▂▅▆▃▅▇▆▇▅▃▆▇▇▃▄▃▂▁▃▅▃▄▃▃▅
wandb:      train/policy_loss ▄▁▄▄▄▄▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁████▂██████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80545
wandb: best/eval_avg_mil_loss 0.50035
wandb:  best/eval_ensemble_f1 0.80545
wandb:            eval/avg_f1 0.77217
wandb:      eval/avg_mil_loss 0.60821
wandb:       eval/ensemble_f1 0.77217
wandb:           train/avg_f1 0.68086
wandb:      train/ensemble_f1 0.68086
wandb:         train/mil_loss 0.5953
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run mild-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g0kw4x5s
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080943-g0kw4x5s/logs
wandb: ERROR Run g0kw4x5s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wltsglpt with config:
wandb: 	actor_learning_rate: 0.0001853060268348073
wandb: 	attention_dropout_p: 0.4967422519566934
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8055805468093156
wandb: 	temperature: 8.559038123096933
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081154-wltsglpt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wltsglpt
wandb: uploading history steps 83-93, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆███
wandb: best/eval_avg_mil_loss █▅▁▁▁
wandb:  best/eval_ensemble_f1 ▁▆███
wandb:            eval/avg_f1 ▃█▇▇▄▆▅▆▅▅▇▃▆▂▆▄▄▆▄▃▇█▃▆▆▅▆▇▁▆█▄█▅▄▄▄▇▄▇
wandb:      eval/avg_mil_loss ▇▅▁▇▇▆▅█▄▅▅▅▁▃▃▄▂▂▄▅▅▃▄▃▇▄▃▂▅▅▄▃▅▆▇▃▆▃▁▄
wandb:       eval/ensemble_f1 █▆▁▆▆▅▆▄▇▄▆▁▆▃▆▆▇▄▃▆▄▆▂▇▆▆▇██▆▂▄▃▅▂▆▆▃█▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▄▃▆▇▃▄▅▇▄▄▅▆▄▅▅▅▄▅▃▆▆▅▁▆▅▅▅▄▄▅▆▅▅▄▅▄▄▃█
wandb:      train/ensemble_f1 █▇▄▅█▅▅▇▅▆▃▅▆▆▄▃▆▅▆▄▁▇▇▆▄▆▆▆▆▇▆▆▅▂▄▅▅▃▅▅
wandb:         train/mil_loss ▅▆▄▅█▄▆▄▅▆▅██▄▃▄▅▄▇▃▆▃▄▆▆▄▆▆█▄▄▅▇▅▁▅▆▅▆▇
wandb:      train/policy_loss ▁███████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁███████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81526
wandb: best/eval_avg_mil_loss 0.49959
wandb:  best/eval_ensemble_f1 0.81526
wandb:            eval/avg_f1 0.7257
wandb:      eval/avg_mil_loss 0.63757
wandb:       eval/ensemble_f1 0.7257
wandb:            test/avg_f1 0.77766
wandb:      test/avg_mil_loss 0.5063
wandb:       test/ensemble_f1 0.77766
wandb:           train/avg_f1 0.73605
wandb:      train/ensemble_f1 0.73605
wandb:         train/mil_loss 0.63464
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run soft-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wltsglpt
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081154-wltsglpt/logs
wandb: Agent Starting Run: 0gs7kay8 with config:
wandb: 	actor_learning_rate: 6.881657154660066e-05
wandb: 	attention_dropout_p: 0.3407973886148896
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.46840305256766535
wandb: 	temperature: 3.7099651707345127
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081357-0gs7kay8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0gs7kay8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅█
wandb: best/eval_avg_mil_loss ▃█▅▁
wandb:  best/eval_ensemble_f1 ▁▄▅█
wandb:            eval/avg_f1 ▇▇▆▃▇▇▇▇█▇▅▅▇▄▇▇▅▆▁▆▅▇▅▆▆▇▂▆▅▇▇█▆█▆▇▇█▆▅
wandb:      eval/avg_mil_loss ▄▆▆▃▇▄▄▄▄▄▅▅▂▆▃▅▆▆█▃▆▅▃▃▄▄█▇▆▄▄▁▅▄▄▄▂▄█▃
wandb:       eval/ensemble_f1 ▇▆▅▃▆▆▇▆▇▆▅▄▄▇▄▄▆▁▆▆▄▆▅▅▇▅▇▆▂▆█▆▇▇▆▆▆▇▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆▆█▄▇▃▅▅▆▆▇▁▅▇▃▆▃▆▇▆▃█▃▅▇▅▃▅▆▅▄▅▆▅▆▆▅▆▆
wandb:      train/ensemble_f1 ▅▆▆▆█▆▇▃▅▅▆▇▆▇▁▇▄▆▆▇▆▃█▃▅▇▅▃▅▆▅▄▆▅▆▆▆▅▆▆
wandb:         train/mil_loss ▅▃▆▁▅█▅▃▄▅▄▆▄▆▅▅▅▄▅▇▅▅▃▃▄▁▁▄▃▇▆▄▄▇▆▆▆▂▂▂
wandb:      train/policy_loss ██████████████████████████▁█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████▁█████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.84856
wandb: best/eval_avg_mil_loss 0.4752
wandb:  best/eval_ensemble_f1 0.84856
wandb:            eval/avg_f1 0.66335
wandb:      eval/avg_mil_loss 0.56653
wandb:       eval/ensemble_f1 0.66335
wandb:            test/avg_f1 0.70975
wandb:      test/avg_mil_loss 0.61232
wandb:       test/ensemble_f1 0.70975
wandb:           train/avg_f1 0.74405
wandb:      train/ensemble_f1 0.74405
wandb:         train/mil_loss 0.56808
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lyric-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0gs7kay8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081357-0gs7kay8/logs
wandb: Agent Starting Run: df8cptln with config:
wandb: 	actor_learning_rate: 6.224983334565223e-05
wandb: 	attention_dropout_p: 0.3817428557440229
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22819900470698007
wandb: 	temperature: 6.307201986511886
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081459-df8cptln
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/cjbrxv00
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/df8cptln
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▄▆▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 █▄██▇█▇▇▇▅▆▂▇▁▇▇█▄▇▆▆██▇▇▆▇▇▁▇▇▇▇▄▇██▃▇▇
wandb:      eval/avg_mil_loss ▁▄▁▄▂▄▄▄▆▃▂▄▁▁▅▁▄▁▁▅▃▄▄▄▅▅█▃▆▁▄█▄▃▃▆▃▅▃▁
wandb:       eval/ensemble_f1 █▇▅█▇█▇▇▇▇█▂██▂▃▇▇▁▄▇█▇▄▇▆▇▃▇▇▇▇█▄▆▃▇▅█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▅▄▂▇▅▇▄▆▅▁▆▄▄▂█▁▄▃▃▅▆▆▆▆▇▃▁▆▅▄▅▂▅▅▇▆▆▆
wandb:      train/ensemble_f1 ▆▂▁▆▄█▆▄▅█▅▆▆▅▁▄▄▄▆▇▅▁▃▃▅▆█▄▃▄▆▆▄▆▄▅▄▆▄▆
wandb:         train/mil_loss ▅▃▆▄▆▆▄▅▂▅▄▅▅▅▂▄▇▆▃▆▃█▄▄▅▇▆▃▄▄▅▇▆▅▅▅█▅▅▁
wandb:      train/policy_loss ██████████▁█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83952
wandb: best/eval_avg_mil_loss 0.44551
wandb:  best/eval_ensemble_f1 0.83952
wandb:            eval/avg_f1 0.81952
wandb:      eval/avg_mil_loss 0.46211
wandb:       eval/ensemble_f1 0.81952
wandb:            test/avg_f1 0.48822
wandb:      test/avg_mil_loss 0.62559
wandb:       test/ensemble_f1 0.48822
wandb:           train/avg_f1 0.76939
wandb:      train/ensemble_f1 0.76939
wandb:         train/mil_loss 0.47864
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lyric-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/df8cptln
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081459-df8cptln/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 9ujqn91z with config:
wandb: 	actor_learning_rate: 2.018138350104916e-05
wandb: 	attention_dropout_p: 0.11603230113860352
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 151
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5786748463428447
wandb: 	temperature: 4.8584246181783355
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081706-9ujqn91z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9ujqn91z
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇██
wandb: best/eval_avg_mil_loss ▁██▆▄▃
wandb:  best/eval_ensemble_f1 ▁▇▇▇██
wandb:            eval/avg_f1 ▄▄██▄█▄▇▄▇▄▇▆▇▄▁▄▁▁▄███▁██▅▅▄▄▇█▇▇█▄▇▆▆▅
wandb:      eval/avg_mil_loss ▃▅▁▅▇▄▆▄▄█▄▃▁▂▂▅▇▁▃▃▁▂▁▃▄▅▆▅▅▁▄▃▇▂▂▅▅▄▃▂
wandb:       eval/ensemble_f1 ▅▄█▆▄█▄█▄▅▄▇█▆█▅██▁▁█▁█▄▅▅▆▄▅▄▇█▇▆▅▅██▄▅
wandb:           train/avg_f1 ▄▄▃▆▄▁▅▄▅▄▅▄▃▆▃▆▄▆█▄▄▄▆▂▅▆▄▅▃▄▅▅▆▅▄▆▇▅▄▆
wandb:      train/ensemble_f1 ▂▅▄▄▅▆▄▄▆▄▃▃▁█▃▄▃▇▃▅▂▂▃▆▃▆▃▄▄▅▄▄▄▃▄▆▃▅▄▅
wandb:         train/mil_loss █▄▅▆▆▄▇▄▅▄▇▅▂▅▆▅▆▃▄▃▅▃▅▄▂▄▆▄▅▃▄▄▃▆▅▅▄▁▄▆
wandb:      train/policy_loss ██████████████████████████████████▁█████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78741
wandb: best/eval_avg_mil_loss 0.80601
wandb:  best/eval_ensemble_f1 0.78741
wandb:            eval/avg_f1 0.62464
wandb:      eval/avg_mil_loss 0.68672
wandb:       eval/ensemble_f1 0.62464
wandb:           train/avg_f1 0.66358
wandb:      train/ensemble_f1 0.66358
wandb:         train/mil_loss 0.72847
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run whole-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9ujqn91z
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081706-9ujqn91z/logs
wandb: ERROR Run 9ujqn91z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: reg2k3a5 with config:
wandb: 	actor_learning_rate: 3.3857475849458705e-06
wandb: 	attention_dropout_p: 0.21653102379262995
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 171
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9769447636332076
wandb: 	temperature: 1.5900301267078998
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081952-reg2k3a5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/reg2k3a5
wandb: uploading wandb-summary.json
wandb: uploading history steps 168-171, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇▇██
wandb: best/eval_avg_mil_loss ▆█▇▁▆▅
wandb:  best/eval_ensemble_f1 ▁▂▇▇██
wandb:            eval/avg_f1 ▂▅▃█▄▄▄▅▃▅▄▄▄▃█▅▄▅▂▅▂▄▄█▄▃▃▄▄▃█▅▄▅▄▁▅▆▄▅
wandb:      eval/avg_mil_loss ▃▂█▅▆▅▆█▄▂▆▅▃▇▄▆▂▆▆▆▆▃▂▂▇▇▂▂▄▃▅▆▃▅▂▁▄▄▆▂
wandb:       eval/ensemble_f1 ▅▃▅▁▄▄▅▄▅▆▄█▅▂▅▂▅▄▅▅▄▅▂▄▆▆▅▄▅▄▃▇▄▅▆▇▆▄▄▃
wandb:           train/avg_f1 ▅▆▂▁▇▆█▂▁▄▂▅▃▅▂▆▆▆▂▄▃▆▃▅▂▄▁█▆▆▄█▄█▆▆▆▄▆▇
wandb:      train/ensemble_f1 ▁▄▄▃▇▃▅▃▆▆▅▁▄▅▅▂▁▁▇▂▃▃▃▃▁▇▃▃▃█▃▄▄▆▅▆▃▃▄▆
wandb:         train/mil_loss ▆▃█▂▇▄▃▅▄▅▇▄▃▄▄▆▅▅▄▆▆▃▆▁▅▇▇▃▅█▅▃▆▄▆▄▄▃█▃
wandb:      train/policy_loss ███████████████████▁███▄████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▁████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.76859
wandb: best/eval_avg_mil_loss 1.11135
wandb:  best/eval_ensemble_f1 0.76859
wandb:            eval/avg_f1 0.60021
wandb:      eval/avg_mil_loss 0.77975
wandb:       eval/ensemble_f1 0.60021
wandb:           train/avg_f1 0.61685
wandb:      train/ensemble_f1 0.61685
wandb:         train/mil_loss 1.2606
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wild-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/reg2k3a5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081952-reg2k3a5/logs
wandb: ERROR Run reg2k3a5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 3g2j62qz with config:
wandb: 	actor_learning_rate: 4.9191573477206685e-05
wandb: 	attention_dropout_p: 0.13191204556474012
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 157
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3303342310234296
wandb: 	temperature: 5.142908450317762
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082247-3g2j62qz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3g2j62qz
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆▇█
wandb: best/eval_avg_mil_loss █▃▄▄▂▁
wandb:  best/eval_ensemble_f1 ▁▄▆▆▇█
wandb:            eval/avg_f1 ▄█▄▃▅▇▂█▄▄▇▇▄▄▃▅▆▆▆▇▄▇▁▇▆▇▆▇▆▆▇█▇▇▇▆█▅▄▇
wandb:      eval/avg_mil_loss ▄▂▄▆█▁▅▆▃▂▂▅▅▅▃▃▃▂▃▂▃▆▄▄▄▂▃▄▃▃▃▂▅▃▃▂▃▄▁▅
wandb:       eval/ensemble_f1 ▅▅▃▅▁▆▃▄▃▃▃▅▂▂▄▄▃█▄▃▅▃▁▆▆▄▆▆▆▄▄▆▂▄▄▃▆▆▄▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▂▄▃▂▁▂▃▁▃▂▃▅▃▃▄▄▅▃▅▃▅▄▄▃▃▄▅▅▅▆▄▅▅▆▇█▅▅▇
wandb:      train/ensemble_f1 ▄▁▄▆▄▅▅▇▅▆▄▅▅▅▅▆▄▄▅▅▆▅▅▅▆█▄▆▆▆▆▇▆▆██▆▆▆▅
wandb:         train/mil_loss ▆▄▄▅▇▇▆▅▇▄█▅▃▆█▆▄▄▆█▄▄▁▂▇▅▃▄▆▃▄▃▄▂▁▃▄▃▄▃
wandb:      train/policy_loss ███████████████████████████████████████▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇███████████████▇██████████████████████▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77608
wandb: best/eval_avg_mil_loss 0.59667
wandb:  best/eval_ensemble_f1 0.77608
wandb:            eval/avg_f1 0.58314
wandb:      eval/avg_mil_loss 1.18678
wandb:       eval/ensemble_f1 0.58314
wandb:            test/avg_f1 0.67612
wandb:      test/avg_mil_loss 0.88327
wandb:       test/ensemble_f1 0.67612
wandb:           train/avg_f1 0.65052
wandb:      train/ensemble_f1 0.65052
wandb:         train/mil_loss 0.94247
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run silver-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3g2j62qz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082247-3g2j62qz/logs
wandb: Agent Starting Run: avq9xvwv with config:
wandb: 	actor_learning_rate: 2.7562457338733458e-05
wandb: 	attention_dropout_p: 0.3039106938556306
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 98
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6681279175197148
wandb: 	temperature: 8.872633380520675
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082602-avq9xvwv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/avq9xvwv
wandb: uploading history steps 89-98, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▃▃▂█▄▁▄▂▄▃▅▆▃▅▄▅▅▃▇▇▄▅▂▄▆▅▃▅▄▅▅█▄▇▅▄▆▆▆▄
wandb:      eval/avg_mil_loss ▅▄▃▅▇▆▆▅▆▃▅▇▇▆▅▆▇█▅▅▄▄▄▇▄▅▄▄▇▁▅▄▁▇▃▅▄▆▄▆
wandb:       eval/ensemble_f1 ▂█▅▄▁▂▃▃▅▆▇▆▄▆▅▄▅▃▇▄▃▆▄▄▆▆▄▆▅▄▃▅▅█▄▅▆▅▆▇
wandb:           train/avg_f1 ▂▃▁▂▁▁▃▄▃▂▅▄▅▄▄▃▂▁▅▃▅▃▅▆▅▇▄▆▄▄▆▆▇▇█▆██▇▆
wandb:      train/ensemble_f1 ▄▃▃▄▃▃▃▅▃▄▁▂▅▁▅▅▃▆▅▆▆▄▅▄▅▇▇▆▅▇▆▆▇▇▇▇███▆
wandb:         train/mil_loss ▅▆█▅▄▅▆▃▄▃▃▃▅▄▂▃▄▃▂▃▃▃▄▃▂▂▁▄▂▂▂▂▃▃▄▃▁▃▂▃
wandb:      train/policy_loss ████▅███▁████████████████████████▅██████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄████▇█▁█████▆██▇███████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77874
wandb: best/eval_avg_mil_loss 0.71146
wandb:  best/eval_ensemble_f1 0.77874
wandb:            eval/avg_f1 0.75011
wandb:      eval/avg_mil_loss 0.73898
wandb:       eval/ensemble_f1 0.75011
wandb:           train/avg_f1 0.70039
wandb:      train/ensemble_f1 0.70039
wandb:         train/mil_loss 0.86461
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run brisk-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/avq9xvwv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082602-avq9xvwv/logs
wandb: ERROR Run avq9xvwv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9bqys6xp with config:
wandb: 	actor_learning_rate: 5.115795878236665e-05
wandb: 	attention_dropout_p: 0.4140007015211401
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 155
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.435275378610492
wandb: 	temperature: 6.8150254174644935
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082807-9bqys6xp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9bqys6xp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄▄▂▁▃▅▇▂▄▅▃▂▄▂▃▅█▃▄▅▄▆▄▄▂▆▃▄▃▅▆▆▁▂▅▂▅▃▂▃
wandb:      eval/avg_mil_loss ▆▇▅▆▄▄█▂▃▃▂▂▇▁▄▄▁▅▄▃▄▆▄▆▃▅▅▅▄▃▁▄▄▄▄▄▃▆▂▃
wandb:       eval/ensemble_f1 ▇▂▇▅▅▁▂▄▃▅▇▅▆▅▅▄▄▆▃▆▆▆▅▂▄█▆▆███▃▆▅▄█▁▅▇▂
wandb:           train/avg_f1 ▂▂▂▃▁▂▃▅█▂▅▃▄▄▇▃▃▇▅▃▅▂▃▃▆▃▂▅▃▅▄▅█▃▅▅▅▆▃▆
wandb:      train/ensemble_f1 ▂▂▃▄▂▄▃█▁▆▂▃▄▃▃▅▅▄▄▃▂▄▃▇▅▃▂▄▄█▅▆▅▃▃▇▇▃▄▄
wandb:         train/mil_loss ▇▅▆▅▄▆▃▅▃▂▆▅▂▃▆▂█▄▂▃▃▄▂▃▄▄▃▄▅▃▁▃▅▄▃▃▄▅▄▅
wandb:      train/policy_loss █████████████████████████████████▁█▃████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.73433
wandb: best/eval_avg_mil_loss 1.01464
wandb:  best/eval_ensemble_f1 0.73433
wandb:            eval/avg_f1 0.55626
wandb:      eval/avg_mil_loss 0.96111
wandb:       eval/ensemble_f1 0.55626
wandb:           train/avg_f1 0.62557
wandb:      train/ensemble_f1 0.62557
wandb:         train/mil_loss 1.08266
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run apricot-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9bqys6xp
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082807-9bqys6xp/logs
wandb: ERROR Run 9bqys6xp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ivvnkbod with config:
wandb: 	actor_learning_rate: 1.6584819216084023e-05
wandb: 	attention_dropout_p: 0.35999739781142337
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 129
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6413353729064096
wandb: 	temperature: 4.469342859375179
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083057-ivvnkbod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ivvnkbod
wandb: uploading history steps 125-130, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb: best/eval_avg_mil_loss ██▆▇▇▇▆▇▆▆▆▅▅▆▅▅▅▅▄▅▄▄▄▄▄▃▃▃▃▃▂▂▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:            eval/avg_f1 ▂▁▃▂▃▂▃▃▄▂▄▄▃▅▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇██▇████
wandb:      eval/avg_mil_loss ▇▇▇█▇▇▇█▆▇▅▅▅▅▅▅▄▅▆▄▅▄▄▄▄▃▃▂▃▃▃▃▃▂▂▃▂▂▂▁
wandb:       eval/ensemble_f1 ▂▁▂▂▃▂▃▄▃▄▄▄▄▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▆▇▆▇██▇▇▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▁▂▂▃▃▂▃▄▄▄▅▅▅▄▅▆▆▆▆▆▆▆▆▇▇▇▆▆▇▇██▇▇█▇███
wandb:      train/ensemble_f1 ▁▂▂▂▂▃▂▃▃▃▃▃▄▄▄▄▄▄▆▅▅▅▆▆▇▆▆▇▇▇▇▇▇▇▇█████
wandb:         train/mil_loss ██▇█▇▆▇▆▅▅▅▅▅▄▄▄▄▃▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▃▂▁▂
wandb:      train/policy_loss ▅▅▄▃▅▅▂▅▅▃▁▅▅▅█▅▅▅▅▅▅▅▅▅▄▅▄▅▅▅▅▅▄▅▅▃▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▄▅▂▅▅▃▅▅▅█▃▄▅▇▁▃▅▄▅▄▅▅▅▅▅▄▅▃▅▅▄▄▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77311
wandb: best/eval_avg_mil_loss 0.52332
wandb:  best/eval_ensemble_f1 0.77311
wandb:            eval/avg_f1 0.74526
wandb:      eval/avg_mil_loss 0.60674
wandb:       eval/ensemble_f1 0.74526
wandb:            test/avg_f1 0.78603
wandb:      test/avg_mil_loss 0.51093
wandb:       test/ensemble_f1 0.78603
wandb:           train/avg_f1 0.75994
wandb:      train/ensemble_f1 0.75994
wandb:         train/mil_loss 0.57885
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run different-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ivvnkbod
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083057-ivvnkbod/logs
wandb: Agent Starting Run: voginadh with config:
wandb: 	actor_learning_rate: 2.608841689826192e-05
wandb: 	attention_dropout_p: 0.3707146818237003
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 143
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.44407592363845183
wandb: 	temperature: 5.240057033038456
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083337-voginadh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/voginadh
wandb: uploading history steps 135-135, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▅▇█
wandb: best/eval_avg_mil_loss ▄█▁▃▁▂
wandb:  best/eval_ensemble_f1 ▁▃▅▅▇█
wandb:            eval/avg_f1 ▇▇▃▅█▆▆▇▅▇▆▇▆▇▅▇▆▆▅▆▅▅▆▅▁▅▂▆▆▇▆▅▄▅▇▂▄▄▆▆
wandb:      eval/avg_mil_loss ▅▃▂▄▃▂▄▁▄▅▂▃▃▂▂▃▂▄▃▃▃▇▂▃▅▄█▃▄▃▂▂▄▅▆▄▅▂▇▂
wandb:       eval/ensemble_f1 ▅▇▃▅█▆▅▄▄▆▄▆▅▇█▅▆▅▄▆▄▅▅▇▆▄▇▃▄▁▅▂▃▆▅▃▆▁▁▅
wandb:           train/avg_f1 ▃▅▅▃▃▃▂▄▃▂▂▃▃▃▆▃▂▇▄█▅▄▅▄▅▄▅▄▃▁▆▃▄▄▂▄▆▆▆▃
wandb:      train/ensemble_f1 ▅▅▂▆▁▃▅▇▄▃▃▃▅▃▃▅▄█▂▄▅▄▅▅▃▄▄▄▅▄▅▁▃▄▇▄▄▂▃▆
wandb:         train/mil_loss ▂▄▆▃█▄▅▃▆▃▇▇▇▄▄▇▄▂▂▅▂▄▃▁▆▃▄▄█▄▆▃▄▄▄▄▆▁█▃
wandb:      train/policy_loss ▄▁▄▄▄█▄▄▁▄█▁█▄▄▄▄▄▆▆█▄█▄▄▄▄█▄▄▁▄▄▄▄▄▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄█▄▄▄▄█▄▄██▁██▄█▄▄▄▄▁▁█▄▄▄▄▄▁▄▄▄▁▆▃▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75145
wandb: best/eval_avg_mil_loss 0.90199
wandb:  best/eval_ensemble_f1 0.75145
wandb:            eval/avg_f1 0.62208
wandb:      eval/avg_mil_loss 0.95743
wandb:       eval/ensemble_f1 0.62208
wandb:           train/avg_f1 0.57821
wandb:      train/ensemble_f1 0.57821
wandb:         train/mil_loss 0.96339
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run jolly-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/voginadh
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083337-voginadh/logs
wandb: ERROR Run voginadh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: h3bs9eqz with config:
wandb: 	actor_learning_rate: 0.00025359355038866225
wandb: 	attention_dropout_p: 0.1903798273908935
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6266042360899733
wandb: 	temperature: 1.438174648197944
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083616-h3bs9eqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/h3bs9eqz
wandb: uploading history steps 90-92, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅▆▆▆▇█
wandb: best/eval_avg_mil_loss ▇█▅▅▆▂▃▁▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅▆▆▆▇█
wandb:            eval/avg_f1 ▅▄▆▄▅▁▄▅▄▅▅▃▄▄▆▄▅▆▆▅▅▁▅▅▆▅▇▆▄▆█▅▆▅▇▇▇▄▇▇
wandb:      eval/avg_mil_loss ▃▄█▃▃▃▃▄▃▃▃▅▂▃▃▅▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▆▁▁▇▄▁▁▂
wandb:       eval/ensemble_f1 ▁▃▄▄▅▄▄▄▄▅▃▄▅▂▅▄▄▅▆▆▆▆▅▁▅▇▆▆▆▄▅▆█▅▆▅▇▂▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▂▂▃▃▃▂▄▄▃▂▃▅▄▄▅▄▃▅▄▅▆▆▆▆▆▆▇▅▆▆▆▇▇▇█▇██
wandb:      train/ensemble_f1 ▁▁▂▁▂▂▂▁▄▂▄▄▅▄▅▅▄▄▄▅▅▆▅▆▆▇▅▇▆▇▆▇▇▇▆▇▇▇██
wandb:         train/mil_loss ▇▆▅▆▅▄▄▆▇▆▆▆▆▄▇▅▆▅▄▆▆▄▄▃▃▆▅▁▃▃▅▃▅█▃▃▃▃▃▂
wandb:      train/policy_loss ▃█▅█▁█▆██████████▅▅████████▆███▃████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃█▆▂█████▄███▅████▃████▁█████▄███▆▂█████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.65822
wandb: best/eval_avg_mil_loss 0.72002
wandb:  best/eval_ensemble_f1 0.65822
wandb:            eval/avg_f1 0.62557
wandb:      eval/avg_mil_loss 0.76136
wandb:       eval/ensemble_f1 0.62557
wandb:            test/avg_f1 0.62047
wandb:      test/avg_mil_loss 0.60015
wandb:       test/ensemble_f1 0.62047
wandb:           train/avg_f1 0.62442
wandb:      train/ensemble_f1 0.62442
wandb:         train/mil_loss 0.76209
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run flowing-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/h3bs9eqz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083616-h3bs9eqz/logs
wandb: Agent Starting Run: 5xym1b8b with config:
wandb: 	actor_learning_rate: 1.687945720944979e-05
wandb: 	attention_dropout_p: 0.1535917488789194
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 109
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7086927834985323
wandb: 	temperature: 2.2269951977645155
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083756-5xym1b8b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5xym1b8b
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁██
wandb: best/eval_avg_mil_loss ▁▂▄█
wandb:  best/eval_ensemble_f1 ▁▁██
wandb:            eval/avg_f1 ▃▂█▂▇▄▃▁▄▄██▇▄▃▁▅▇▄▇█▂█▃▄▃▃▇▄▃▅▃▄█▅█▄▄▅▅
wandb:      eval/avg_mil_loss ▃▄▄▆▂▃▆▄▄▁▃█▆▁▃▅▄▃▅█▅▁▃▁▃▁▁▆▃▃█▃▂▄▃▇▅▆▁▄
wandb:       eval/ensemble_f1 █▅▄█▅▇▅▅▅█▅███▅▅▄▆▅▇█▄▅▅█▅▁█▇█▅▅█▅▅▅▆█▇▅
wandb:           train/avg_f1 ▅▃▃▂█▄▁▅▃▆▆▅▃█▅▃▅▇▃▃▃▃▆▃▆▄▇▆▆▆▅▃▇▅▄▅▆▆▇█
wandb:      train/ensemble_f1 ▂▅▅▄▄▂▁▇▃▅▃▄▅▆▄▄▂▃▆▇▆▆▄▅▆█▅▆▆▆▆▄▄▆▅▅▄▇▄▇
wandb:         train/mil_loss ▆▃▄▄▆▄▇▂▄▃▃▄▅▅▆▅▂▅▃▂▂▂▅▇▅▄▄▁▃▄▃▆▂▃▄▂█▄▄▅
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▆██████████████████▇███████▁███████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77608
wandb: best/eval_avg_mil_loss 0.87864
wandb:  best/eval_ensemble_f1 0.77608
wandb:            eval/avg_f1 0.75605
wandb:      eval/avg_mil_loss 0.84482
wandb:       eval/ensemble_f1 0.75605
wandb:           train/avg_f1 0.69936
wandb:      train/ensemble_f1 0.69936
wandb:         train/mil_loss 0.89268
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smooth-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5xym1b8b
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083756-5xym1b8b/logs
wandb: ERROR Run 5xym1b8b errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: btomoawn with config:
wandb: 	actor_learning_rate: 5.950067682296237e-06
wandb: 	attention_dropout_p: 0.3429526705718988
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 165
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3537178368163504
wandb: 	temperature: 4.254425354725129
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084000-btomoawn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/btomoawn
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▆▆▆▆█
wandb: best/eval_avg_mil_loss ▇▅█▆▆▁▃▄
wandb:  best/eval_ensemble_f1 ▁▄▄▆▆▆▆█
wandb:            eval/avg_f1 ▆▇▃▆▅▇▇▅▆▄▅▄▇▆▁▇▆█▆▂█▆▅▂▄▅█▂▇▅▃▄▁▃▆▄▅▄▇▂
wandb:      eval/avg_mil_loss ▂▅▂▁▆▄▄▃▅▆▆▂▃▆▄▅▂▅▇▆▃█▅▃▆▅▄▄▆▄▃▃▆▄▃▇▄▃▃▄
wandb:       eval/ensemble_f1 ▆▆▃▂▆▇▆▃▇▇▄▄▅▇▄▂▄█▄▄▄▆▂▇▃▇▆▇█▄▆▇▁▂█▄▅▅▆▄
wandb:           train/avg_f1 ▅▃▅▅▆▅▅▄▄▆▄▃▅▅▄▅▇▆▄▅▂▅▅▄▄▄▅▁▇▇▄▇▃█▅▅▅▅▇▅
wandb:      train/ensemble_f1 ▄▃▃▅▃▃▄▄▃▁▄▄▃▄▂█▃▄▄▃▄▄▃▅▃▆▃▃▄▃▄▃▆▄▃▃▄▅▂▄
wandb:         train/mil_loss ▅▅▆▅▆█▄▅█▃▇▄▆▅▃▃▄▆▅▅▅▆▄▃▆▄▅▆▆▆▄▁▂▄▁▃▆▂▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▄▁▄▄▁▄▄█▆▄▁▄█▆█▄█▄█▁█▄▄█▄▁▄▁█▁▁▄▄▁▄▁▁▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.67067
wandb: best/eval_avg_mil_loss 0.96335
wandb:  best/eval_ensemble_f1 0.67067
wandb:            eval/avg_f1 0.4897
wandb:      eval/avg_mil_loss 1.10977
wandb:       eval/ensemble_f1 0.4897
wandb:           train/avg_f1 0.60129
wandb:      train/ensemble_f1 0.60129
wandb:         train/mil_loss 1.08171
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cool-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/btomoawn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084000-btomoawn/logs
wandb: ERROR Run btomoawn errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 40q1nin8 with config:
wandb: 	actor_learning_rate: 3.2311273413047864e-06
wandb: 	attention_dropout_p: 0.2664206604671621
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5165295454231946
wandb: 	temperature: 9.366840696462068
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084327-40q1nin8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/40q1nin8
wandb: uploading history steps 87-92, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▄▅▅▆▆█
wandb: best/eval_avg_mil_loss ▄▄█▅▆█▅▃▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▃▄▅▅▆▆█
wandb:            eval/avg_f1 ▅▅▅▃▃▄▁▆▅▅▆▅▆▅▆▂▆▆▄▇▅▄▃▇▅▆▄▆▆▅▇▃▅▅▆█▅▅█▇
wandb:      eval/avg_mil_loss ▄▅▄▄▅▃▂▆▄█▃▅▃▇▄▃▅▆▄▄█▅▆▂▁▂▄▃▅▅▅▂▁▄▂▂▃▂▃▂
wandb:       eval/ensemble_f1 ▄▅▃▅▄▆▅▇▇▆▃▄▄▆▄▄▅▆▆▄▄▃▇▁▆▆▄▇▃▅▄▆▅▇▄▅█▄▆▇
wandb:           train/avg_f1 ▅▁▂▃▂▃▃▃▄▃▃▁▃▅▄▆▃▄█▆█▅▅▆▇▆▇▆▅▇▆▅▇▇▇▅▅▆▄█
wandb:      train/ensemble_f1 ▆▁▃▃▃▃▂▃▃▃▁▂▄▃▅▃▄▆▇█▅▇▃▇▅▇▅▆▇▅▆▇█▇▇▅▅▇▇█
wandb:         train/mil_loss ▅▅▇▆▂█▄▄▅▆▁▂▅▁▂█▃▅▄▇▄▄▃▂▆▂▆▃▃▄▂▆▄▂▃▅▂▂▄▅
wandb:      train/policy_loss ▄▅▅▅▇▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▃▅▅█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▅▅▅▁▄▅▅▅█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.73678
wandb: best/eval_avg_mil_loss 0.64969
wandb:  best/eval_ensemble_f1 0.73678
wandb:            eval/avg_f1 0.6627
wandb:      eval/avg_mil_loss 0.7464
wandb:       eval/ensemble_f1 0.6627
wandb:           train/avg_f1 0.64771
wandb:      train/ensemble_f1 0.64771
wandb:         train/mil_loss 0.93032
wandb:      train/policy_loss 0.32537
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.32537
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dazzling-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/40q1nin8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084327-40q1nin8/logs
wandb: ERROR Run 40q1nin8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: iudzn8sg with config:
wandb: 	actor_learning_rate: 4.07608509571466e-06
wandb: 	attention_dropout_p: 0.165503877325335
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 84
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6557377003527957
wandb: 	temperature: 3.236467066391804
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084532-iudzn8sg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iudzn8sg
wandb: uploading history steps 73-84, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄▇██
wandb: best/eval_avg_mil_loss ▁█▅▇▁▇▄
wandb:  best/eval_ensemble_f1 ▁▃▃▄▇██
wandb:            eval/avg_f1 ▂▂▁▄▂▄▅▁▃▄▂▃▁▄▁▄▄▄▅█▄▄▆▃▅▄▅▇▆▆▄▄▅▇▇▅▅▄▆▂
wandb:      eval/avg_mil_loss █▆▇▇▆▃▃▃▅▃▆▃▃▄▅▃▄▄▃▄▃█▄▅▄▁▄▆▄▄▁▅▅█▁▃▁▄▄▅
wandb:       eval/ensemble_f1 ▁▁▄▃▄▄▄▃▃▁▁▃▄▅▃▅▄▄▆▄█▅▇▅▅▆▅▄▃▃▆▇▅▅▅▆▅▅▇▅
wandb:           train/avg_f1 ▄▁▁▄▅▃▂▄▃▅▄▆▁▅▄▆▆▄▆▅▅▅▅▆▄▆▂▃▆▆▇▇▆▇▆▇▆▅▇█
wandb:      train/ensemble_f1 ▄▁▂▂▄▄▂▃▄▄▄▅▂▅▄▅▅▄▆▆▄▆▅▆▄▅▆▄▂▄▅▆▆▇▇▆▆▇▇█
wandb:         train/mil_loss ▃▄▅▅▅▃▄▄▃▂▄▆█▆▅▃▄▃▂▄▄▄▃▁▄▄▂▃▂▅▂▄▃▃▂▂▃▃▁▂
wandb:      train/policy_loss ████████████▁███████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.66312
wandb: best/eval_avg_mil_loss 0.94268
wandb:  best/eval_ensemble_f1 0.66312
wandb:            eval/avg_f1 0.59394
wandb:      eval/avg_mil_loss 0.79768
wandb:       eval/ensemble_f1 0.59394
wandb:           train/avg_f1 0.62108
wandb:      train/ensemble_f1 0.62108
wandb:         train/mil_loss 1.02165
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pretty-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iudzn8sg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084532-iudzn8sg/logs
wandb: ERROR Run iudzn8sg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: j696u8pn with config:
wandb: 	actor_learning_rate: 1.3498796547995312e-05
wandb: 	attention_dropout_p: 0.2501377135291625
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 188
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2213403557494229
wandb: 	temperature: 0.8378854319354634
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084705-j696u8pn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/j696u8pn
wandb: uploading history steps 176-189, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇█
wandb: best/eval_avg_mil_loss ▅▆▆▆▆▆▆█▅▅▄▂▄▄▂▁▂▁
wandb:  best/eval_ensemble_f1 ▁▁▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇█
wandb:            eval/avg_f1 ▄▃▄▄▆▄▃▅▄▅▃▅▅▃▆▇▃▃▆▆▅▁▄▆▇▅▇▇▆▄▇██▇▆▇▇▆██
wandb:      eval/avg_mil_loss ▅▃▆▅▃▆▄▅█▃▂▅▇▄▆▆▃▄▃▅▂▂▃▄▄▃▃▁▄▃▄▁▃▃▁▄▂▅▂▂
wandb:       eval/ensemble_f1 ▂▃▃▃▃▁▅▄▂▅▄▄▄▂▂▇▃▆▇▆▇▆▇▆▇▇█▇▄▇▇▅▃▆██▇██▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▂▃▂▁▃▃▂▄▄▄▅▄▃▄▄▄▅▅▆▆▅▇▇▆▆▇▆▅▆▇▇▆▇▇▇▇██▇
wandb:      train/ensemble_f1 ▁▁▃▃▃▃▃▃▃▄▃▄▃▄▄▄▆▅▅▅▄▅▄▄▆▆▆▇▆▅▇▇▆▇█▆▇▇█▇
wandb:         train/mil_loss ▆▇▅▇▆▅▄▅█▄▄▅▆▅█▅▅▆▄▅▄▄▆▅▄▁▃▃▄▂▄▂▂▁▃▂▂▂▃▃
wandb:      train/policy_loss ▅▅▆▃▆▆▆▆▆▁▅▆▆▆▆▆▅▆▆▆▆▆▆▅▆▆▃█▆▆▆▆▆▆▄▅▆▆▅▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▄▅▅▅▅▅▅▅▅█▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.72558
wandb: best/eval_avg_mil_loss 0.65413
wandb:  best/eval_ensemble_f1 0.72558
wandb:            eval/avg_f1 0.54209
wandb:      eval/avg_mil_loss 0.87637
wandb:       eval/ensemble_f1 0.54209
wandb:            test/avg_f1 0.70652
wandb:      test/avg_mil_loss 0.47283
wandb:       test/ensemble_f1 0.70652
wandb:           train/avg_f1 0.66689
wandb:      train/ensemble_f1 0.66689
wandb:         train/mil_loss 0.66777
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rare-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/j696u8pn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084705-j696u8pn/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6rfysf7f with config:
wandb: 	actor_learning_rate: 9.031360147312014e-05
wandb: 	attention_dropout_p: 0.372825622572535
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 196
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9433697907810052
wandb: 	temperature: 9.064383942850029
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085032-6rfysf7f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6rfysf7f
wandb: uploading wandb-summary.json
wandb: uploading history steps 185-196, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▆██
wandb: best/eval_avg_mil_loss ▅▄▇▁██
wandb:  best/eval_ensemble_f1 ▁▆▆▆██
wandb:            eval/avg_f1 ▅▃▅▄█▃▃▄▁█▅█▆▅▄▅█▄▄▅▄▄▄▅▅▅▅▁▆█▅▅▄▆█▅▅▄▄▅
wandb:      eval/avg_mil_loss ▂▃▂▇▆▂▃▂▃▃▃▅▂▂▅▃▂▄▅▄▂▂▂▅▄▄▅▇▆▄▃█▃▁▂▅▂▄▆▄
wandb:       eval/ensemble_f1 ▆▅▄▄▂▅▅▅▄▄█▅▅▁▃▅▆█▄▂▅▄▅▆▅▅▄█▅▅▆▄▃▅▄▅▄█▄▄
wandb:           train/avg_f1 ▁▁▂▇▁▃▅▅▄▃▄▄▃▄▂▃▄▄▄▅▄▃▄▄▃▆▄▅▅▃█▃▆▄▅▅▇▅▅▃
wandb:      train/ensemble_f1 ▅▃▁▇▁▄▇▄▅▆▂▃▇▅▅▃▆▆▄▅▆▅▅▆▁▆▃▅▃▅▅▆▅▆▅▇█▇▆█
wandb:         train/mil_loss ▅▇▅▃▆▅▂▂▅▆▃▆▅▃▄▅▂█▄▅█▄▅▃▇▆▃▇▄▄▂▃▂▂▂▁▁▇▂▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅█▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82725
wandb: best/eval_avg_mil_loss 0.86676
wandb:  best/eval_ensemble_f1 0.82725
wandb:            eval/avg_f1 0.53836
wandb:      eval/avg_mil_loss 1.0071
wandb:       eval/ensemble_f1 0.53836
wandb:           train/avg_f1 0.68361
wandb:      train/ensemble_f1 0.68361
wandb:         train/mil_loss 0.98745
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run solar-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6rfysf7f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085032-6rfysf7f/logs
wandb: ERROR Run 6rfysf7f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: nl84rp0w with config:
wandb: 	actor_learning_rate: 3.1104351295398443e-06
wandb: 	attention_dropout_p: 0.032939212659177985
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 115
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.49149599751849526
wandb: 	temperature: 8.234518073273788
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085408-nl84rp0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nl84rp0w
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss ▃█▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 ▆▅▆▅▅▅▅█▅▄▄▆▄▅▅▅▆▅▅▆▅▅▅▁▅▅▅▁▆▇▆▅▆█▅▄▇▅▅▆
wandb:      eval/avg_mil_loss ▄▅▅▆█▅▄▃▃▇▆▂▃▃▃▂▇▂▄▂▄▃▂▂▃▂▆▃▄▂▄▃▁▂▁▄▃▂▅▄
wandb:       eval/ensemble_f1 ▄▄▅▄▂▅▄▅▅▅▅▅▅▃▁▄▅▅▆▅▅▅▄▅▄▄▅▄▂▅▇▅▅▅▄█▆▄▅▅
wandb:           train/avg_f1 ▄▂▄▇▃▅▁▆▂▅▄▅▄▄▆▃▆▃▁▅▇▅█▇▇▆▆▅▇▆▃▆▅▄▆█▇▄▅▆
wandb:      train/ensemble_f1 ▄▅▂▃▁▅▂▄▂▆▄▄▃▇▆▂▅▅▂▄█▄▄▆▁▄▆▇▂▆▇▃▇▅▅▇▆▄▃▅
wandb:         train/mil_loss ▃▅▄▃▄▃▅▆██▅▅▃▃▄▄▆▄▃▂▄▅▅▂▄▄▄▅▂▆▆▄▃▅▁▁▄▃▅▄
wandb:      train/policy_loss ▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▄▁▅▅▅▅▅▅▅▅▅▆▅▅▆▄▅▅▅▅▅▅▅▅█▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79614
wandb: best/eval_avg_mil_loss 1.00426
wandb:  best/eval_ensemble_f1 0.79614
wandb:            eval/avg_f1 0.59688
wandb:      eval/avg_mil_loss 1.12662
wandb:       eval/ensemble_f1 0.59688
wandb:           train/avg_f1 0.61451
wandb:      train/ensemble_f1 0.61451
wandb:         train/mil_loss 1.00328
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run iconic-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nl84rp0w
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085408-nl84rp0w/logs
wandb: ERROR Run nl84rp0w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 7rjn2rao with config:
wandb: 	actor_learning_rate: 1.4479263503872402e-06
wandb: 	attention_dropout_p: 0.22688396282913015
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9086383774597472
wandb: 	temperature: 8.45921366803799
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085618-7rjn2rao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7rjn2rao
wandb: uploading wandb-summary.json
wandb: uploading history steps 102-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▇█
wandb: best/eval_avg_mil_loss █▆▃▄▁
wandb:  best/eval_ensemble_f1 ▁▅▅▇█
wandb:            eval/avg_f1 ▃▄▃▂▇▇▇▃▄▄▄▁▃▄▃▂▃▅▁▅▆▄▆▄▃▃▄█▃▄▄▆▄▁▃▅▇▅▅▄
wandb:      eval/avg_mil_loss █▃▅▅▄▁▄▅▇▃▅▆▄▅▇█▆▅▇▆▅▄▃▅▆▃▇▇▃▃▅█▃▇▆▄▄▄▅▃
wandb:       eval/ensemble_f1 ▅▄▃▄▃▄▄█▄▅█▄▅▁▃▆▆▁▁▄▃▆▄▄▄▄▂█▃▄▅▇▅▃▅▇▅██▅
wandb:           train/avg_f1 ▁▂▄▂▃▂▃▆▄▆█▃▁▃▅▃▇▃▃▅▃▄▂▃▃▂▃▄▃▄▃▃▂▆▆▅▃▅▆▅
wandb:      train/ensemble_f1 ▂▅▂▅█▅▄▄▆▆▄▃▄▇▁▇▆▂▂▆▆▆▃▇▄▂▅▅▅▅█▃▆▅▃▅▇▆▅▆
wandb:         train/mil_loss ▆▇▅▆▅▇▃▅▅▄▆▄▃▂▃▅▅▃▆▄▅▇▆▃▆▆▄▇▅▅▄▄▅▁▅▃▅▅▄█
wandb:      train/policy_loss █████▁██████████████████████▂███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁██████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80554
wandb: best/eval_avg_mil_loss 0.94375
wandb:  best/eval_ensemble_f1 0.80554
wandb:            eval/avg_f1 0.56188
wandb:      eval/avg_mil_loss 0.91036
wandb:       eval/ensemble_f1 0.56188
wandb:           train/avg_f1 0.58906
wandb:      train/ensemble_f1 0.58906
wandb:         train/mil_loss 1.12393
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sparkling-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7rjn2rao
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085618-7rjn2rao/logs
wandb: ERROR Run 7rjn2rao errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wo9nzot2 with config:
wandb: 	actor_learning_rate: 2.0414310739394768e-05
wandb: 	attention_dropout_p: 0.02157157893353079
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 118
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5227326969444275
wandb: 	temperature: 9.786288770051469
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085822-wo9nzot2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wo9nzot2
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss ▁██
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 ▄█▄▄▂▂▅▅▄▁▅▃▂▄▃▃▅▅▃▅▅▄▄▇▅▅▅▅▅▅▅▄▄▄▄▄▄▅▆▅
wandb:      eval/avg_mil_loss ▅▇▃▃▆▆▂▄▆▆▆▅▅▄█▄▃▄▃▇▂▄▇▄▃▄▃▃▄▄▅▁▆▃▂▆▄▄▇▄
wandb:       eval/ensemble_f1 ▄▄▂█▄▄▄▃▅▄▄▁▁▄▃▄▅▃▃▃▃▅▅▃▄▄▅▄▄▅▄▅▄▅▅▄▅█▄▂
wandb:           train/avg_f1 ▄▁▂▃▄▂▁▄▃▅▆▂▃▂▆▂▃▄▂▃▅▇▄▆▄▄▆▆▃▆▆▆▅▄▆█▄▆▅▅
wandb:      train/ensemble_f1 ▁▄▃▃▅█▃▅▂▅▆▃▂▅▂▄▆▄▄▄▇▄▅▆▇▇▄▅▆▇▆▅▆▆▆▄▆▆█▄
wandb:         train/mil_loss ▅▄▅▆▃▆▄█▃▃▂▅▆▅▇▂▄▂▃▄▃▃▂▄▃▂▅▄▅▁▄▃▃▁▄▂▃▂▂▁
wandb:      train/policy_loss ▅▅▁▅▅▅▅▅▃▅▅▅█▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75605
wandb: best/eval_avg_mil_loss 0.99997
wandb:  best/eval_ensemble_f1 0.75605
wandb:            eval/avg_f1 0.6284
wandb:      eval/avg_mil_loss 0.91903
wandb:       eval/ensemble_f1 0.6284
wandb:           train/avg_f1 0.57099
wandb:      train/ensemble_f1 0.57099
wandb:         train/mil_loss 0.92503
wandb:      train/policy_loss -0.41591
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.41591
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bright-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wo9nzot2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085822-wo9nzot2/logs
wandb: ERROR Run wo9nzot2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ifk43s6g with config:
wandb: 	actor_learning_rate: 1.7170542624251165e-06
wandb: 	attention_dropout_p: 0.23364925299614725
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 139
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8015852998950692
wandb: 	temperature: 7.955316967838675
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090031-ifk43s6g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ifk43s6g
wandb: uploading history steps 134-139, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▃▃▄▄▅▅▆▇▇█
wandb: best/eval_avg_mil_loss ▆▇▅▅▆██▇▅▆▅▃▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▃▃▃▄▄▅▅▆▇▇█
wandb:            eval/avg_f1 ▂▃▃▂▃▁▁▄▄▃▄▂▃▆▄▂▅▂▅▅▄▆▅▄▂▆▅▆▇▅▂▅▆▇▆▅█▄▇▇
wandb:      eval/avg_mil_loss ▄▆▄▅▅▄▅▄▅▄▆▄▅▄▅▃▃▆▃▃▃█▃▂▃▂▂▄▅▁█▅▁▅▅▁▂▂▂▄
wandb:       eval/ensemble_f1 ▂▁▃▁▂▃▂▂▁▄▅▇▄▄▅▄▄▄▅▅▂▃▅▃▄▇▆▆▄▄▇▃▄▄▇▄▅█▇▅
wandb:           train/avg_f1 ▂▂▁▂▃▂▃▂▃▂▄▃▃▅▄▅▄▅▅▅▆▅▄▇▆▇▇▇▇▇▅▇██▇▇▇█▇█
wandb:      train/ensemble_f1 ▂▁▂▂▂▂▃▄▃▂▂▄▃▃▄▃▄▄▄▄▅▅▅▅▅▆▇▆▇▆▅█▆▆▇█▆▇▆▆
wandb:         train/mil_loss █▆▅▇▅▄▆▆▄▆▅▅▆▆▄▄▃▅▃▃▄▃▂▄▃▆▄▃▅▅▃▅▂▃▂▃▄▁▁▂
wandb:      train/policy_loss ▇██▇███████████▁██▅███▄█████▇███▇███████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▁▇▇▇▇█▇▇▇▇▇▇▅▇▇▇▇▅▇▂▇▇▇▇▇▅▇▇▆▇▃▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.76405
wandb: best/eval_avg_mil_loss 0.53032
wandb:  best/eval_ensemble_f1 0.76405
wandb:            eval/avg_f1 0.72463
wandb:      eval/avg_mil_loss 0.77653
wandb:       eval/ensemble_f1 0.72463
wandb:           train/avg_f1 0.69478
wandb:      train/ensemble_f1 0.69478
wandb:         train/mil_loss 0.6928
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run winter-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ifk43s6g
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090031-ifk43s6g/logs
wandb: ERROR Run ifk43s6g errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: dz9zblr4 with config:
wandb: 	actor_learning_rate: 1.705641738954369e-05
wandb: 	attention_dropout_p: 0.3554232788875363
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 110
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1893940382156396
wandb: 	temperature: 0.8884301980831999
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090332-dz9zblr4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dz9zblr4
wandb: uploading history steps 101-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▆▆▅▃█▁▄▁▃▂▅▄▅▄▂▄▅▅█▄▁▅▅▅▄▂▃▅▃▄▆▄▄█▁▅▅▂▆
wandb:      eval/avg_mil_loss ▂█▆▄▅▄▅▃▃▅▂▆▃▂▄▇▇▆▅▄▆▄▆▃▃▄▄▃▅▅▄▆█▅▁▂▅▆▇▅
wandb:       eval/ensemble_f1 ▄▇▆▄▅▃▇▁▃▂▂▃▄▆▄▆▂▄▃▅▂▅▂▅▃▄▃▂▇▅█▅▄▆▇▅▅▂▄▄
wandb:           train/avg_f1 ▃▃▁▁▆▂▅▃▅▅▂▅▂▄▄▆▄▂▅▆▆▅▆▅▅▄▇▆▅▅▄▄▄▄▄▆▅▆█▆
wandb:      train/ensemble_f1 ▁▁▃▄▁▄▆▄▃▅▅▆▄▅▃▅▅▃▃▅▅▅▆▆▅▅▇▆▅▆▆▄▄▅▇▆▅▆▆█
wandb:         train/mil_loss ▅▅▅▄▄█▃▄▆▅▅▅▃▅▅▄▄▄▄▄▅▇▄▅▆▆▅▄▂▆▄▄▅▃▄▁▄▄▅▄
wandb:      train/policy_loss █▁██████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄█▄▄▄▄▄▇▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75694
wandb: best/eval_avg_mil_loss 0.75726
wandb:  best/eval_ensemble_f1 0.75694
wandb:            eval/avg_f1 0.66839
wandb:      eval/avg_mil_loss 1.03662
wandb:       eval/ensemble_f1 0.66839
wandb:           train/avg_f1 0.61744
wandb:      train/ensemble_f1 0.61744
wandb:         train/mil_loss 1.09157
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run jumping-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dz9zblr4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090332-dz9zblr4/logs
wandb: ERROR Run dz9zblr4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: l3cf4qmj with config:
wandb: 	actor_learning_rate: 4.160424117553572e-05
wandb: 	attention_dropout_p: 0.184600104595599
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 197
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4965432423211979
wandb: 	temperature: 5.890881253075287
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090526-l3cf4qmj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l3cf4qmj
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 114-125, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▆█
wandb: best/eval_avg_mil_loss █▅▇▄▁
wandb:  best/eval_ensemble_f1 ▁▂▂▆█
wandb:            eval/avg_f1 ▅▅▅▅█▁▅▅█▅▅▂▅▅▆▅▄▅▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▁▆▆▆▇
wandb:      eval/avg_mil_loss ▅▃▄▄▄▃▄▃▁▃▃▄▆▃▄▃▃▃▂▃▄▃▁▃█▅▇▆▃▃▃▃▃▃▁▃▃▂▂▃
wandb:       eval/ensemble_f1 ▄▄▄▄▄▄▄▇▄▇▂▅▄▅▂▅▅▅▅▄▄▅█▆▅▅▁▅▆▅▅▅▅▁▅▇▅▅▁▅
wandb:           train/avg_f1 ▃▄▂▄▃▁▄▃▄▆▅▄▃▄▅▄▃▄▆▅▅▄█▆▇▅▅▅▄▅▇▆▅▆▅▇█▇▇▇
wandb:      train/ensemble_f1 ▂▃▂▄▁▃▅▁▃▅▅▂▅▄▆▂▅▄▃▃▄▅▇▆▅▄▄▅▅▅▅▇▅▆▇█▇▇▆▇
wandb:         train/mil_loss ▅▃▄▃▄▄▅▄▆▄▃▅▄▃▆▄█▂▂▅▆▄▄▂▃▃▃▂▄▃▃▂▂▃▃▃▃▃▂▁
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▇▅█▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▇▇▇▇▇▇▇▇▇▇▇▇▆▇▁▇▂▆▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77068
wandb: best/eval_avg_mil_loss 0.67331
wandb:  best/eval_ensemble_f1 0.77068
wandb:            eval/avg_f1 0.58921
wandb:      eval/avg_mil_loss 1.03931
wandb:       eval/ensemble_f1 0.58921
wandb:           train/avg_f1 0.62443
wandb:      train/ensemble_f1 0.62443
wandb:         train/mil_loss 0.70235
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glad-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l3cf4qmj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090526-l3cf4qmj/logs
wandb: ERROR Run l3cf4qmj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: lwhrhy54 with config:
wandb: 	actor_learning_rate: 2.541895063211129e-06
wandb: 	attention_dropout_p: 0.12973794545440714
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 185
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.34766716266856434
wandb: 	temperature: 6.22383609100563
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090745-lwhrhy54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lwhrhy54
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▇▇▇██
wandb: best/eval_avg_mil_loss ▂█▄▃▁▆▂
wandb:  best/eval_ensemble_f1 ▁▃▇▇▇██
wandb:            eval/avg_f1 ▁▅▇▁▂▅▁█▄▁█▂▆▂▂▇▁▂▅█▁▄█▂▆▂▁▂▁▂▁▁▁▂█▄▄▁▃▂
wandb:      eval/avg_mil_loss ▇█▄▃▃▆▃▅▆▅▆█▇▇▅▄▄▆▃▅▃▆▅█▅▅▄▁▅▅▆▅▃▃▅▄▃▄▅▅
wandb:       eval/ensemble_f1 ▃▅▂▄▁▁▇▂█▄▁▅▁▂█▁█▇▂█▃▃▆▂▂▂▁▁▄█▂▁▄▂▁▁▁▂▃▂
wandb:           train/avg_f1 ▇▆▃▄▄▄█▂▅▇▂▅▁▃▇▃▄▇▂▄▅▄▅▄▂▆█▆▆▆▅▆▃▆▆▃▂▄▃▅
wandb:      train/ensemble_f1 ▅▃▅▅█▆▂▆▄▇▆▅▃▃▃▁▇▅▃▆▄▆▅▆▃▃▆▄▄▄▄▅▆▅▄▃▄▃▆▅
wandb:         train/mil_loss ▅▆▃▃▆▄▃▄▃▄▅▄▆▅▄▃▂▃▅▃▂▃▆▄▆▂▂█▁▅▁▃▄▁▃▂▄▂▃▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80719
wandb: best/eval_avg_mil_loss 0.69766
wandb:  best/eval_ensemble_f1 0.80719
wandb:            eval/avg_f1 0.61913
wandb:      eval/avg_mil_loss 1.01995
wandb:       eval/ensemble_f1 0.61913
wandb:           train/avg_f1 0.58896
wandb:      train/ensemble_f1 0.58896
wandb:         train/mil_loss 1.06468
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run kind-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lwhrhy54
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090745-lwhrhy54/logs
wandb: ERROR Run lwhrhy54 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 7abdu3ou with config:
wandb: 	actor_learning_rate: 9.77230538810012e-06
wandb: 	attention_dropout_p: 0.031168693209303155
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.040595618177793336
wandb: 	temperature: 6.998742318498177
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091047-7abdu3ou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7abdu3ou
wandb: uploading wandb-summary.json
wandb: uploading history steps 85-87, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss █▂▁
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 ▃█▅▄▆▄▅▅▅▇▅▅▅█▅▅▆▅▅▅▅▅▇▅▅▅▄▇▅▅▁▅▅▅▅▅▅▆█▅
wandb:      eval/avg_mil_loss ▄▄▃▂▃▃▃▃▃▂▂▁▁▄▃▅▅▆▂█▃▂▄▆▄▃▂▅▂▃▂▃▄▁▂▂▃█▄▂
wandb:       eval/ensemble_f1 ▃▅▄▅▅▄▄▄▅▇▅▅▇▅▄▅▅▅▄▅▆█▅▅▅▅▅▅▆▅▅▆▅▁▅▇▄▆▅▅
wandb:           train/avg_f1 ▃▅▄▆▄█▄▄▄▄▄▃▆▆▃█▅▆▆▃▆▆▆▃▅▄▇▅▆▂▆▅▇▁▅▆▇▅▄▅
wandb:      train/ensemble_f1 ▃▃▄▃▄▁▇▅▄▁▅▃▄▄▃▂▃▇▄▄▆▆▇▄▄▅▃▅█▄▄▆▆▆▅▆▅▇▃▆
wandb:         train/mil_loss ▅▅▆▅▆▆█▅▆▇▅▇▄▅▅▅▅▄▂▇▇▄▇▂▄▆▂▄▃▅▇▆▂▃▆▃▅▆▅▁
wandb:      train/policy_loss ▄▄▄▄▁▄▄▄▄█▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▂▄▄▂▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▄▄▄▁▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7712
wandb: best/eval_avg_mil_loss 0.99623
wandb:  best/eval_ensemble_f1 0.7712
wandb:            eval/avg_f1 0.55745
wandb:      eval/avg_mil_loss 0.891
wandb:       eval/ensemble_f1 0.55745
wandb:           train/avg_f1 0.59491
wandb:      train/ensemble_f1 0.59491
wandb:         train/mil_loss 0.71292
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vague-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7abdu3ou
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091047-7abdu3ou/logs
wandb: ERROR Run 7abdu3ou errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: gbut464q with config:
wandb: 	actor_learning_rate: 4.523196057174809e-06
wandb: 	attention_dropout_p: 0.4447634124719856
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 101
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7808055828251474
wandb: 	temperature: 7.496158829616808
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091225-gbut464q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gbut464q
wandb: uploading history steps 100-101, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss █▁▅
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 █▄▄▃▆▇▄▄▄▂▆▃▇█▆▁▆▄▃▃▅▅▄▆▃▃▆▂▅▅▂▃▆▄▄▆▃▆▃▂
wandb:      eval/avg_mil_loss ▃▅▅▄▁▂▅██▄█▄▆▄▂▄▃▇▃▃▄▄▁▇▅▃▃▂▃▆▅▁▂▂█▂▃▃▂▁
wandb:       eval/ensemble_f1 ▄▄▃▄▅▆▂▅▇▅▆▁█▃▃▅▃▄▄▁▇▆▄▆█▃▅▄▇▄▅▄▃▄▃▃█▆▄▅
wandb:           train/avg_f1 ▄▂▂▄▃▂▃▃▅▃█▅▃▆▅▅▄▅▁▃▄▅▄▂▇▁▅▄▆▆▁▂▅▄▆▅▄▇▅▆
wandb:      train/ensemble_f1 ▄▁▃▅▃▃█▄▄▆▂▄▄▂▃▂▅▁▄▄▅▃▅▃▅▁▅▄▃▆▄▅▆▃▆▆▄▅▅▄
wandb:         train/mil_loss ▄▅▆▄▆▄▃▄▄▆▃▄▃▄▄▄▄▃▄▅▅▂▆▃▃▄█▅▄▆▄▄▁▆▄▄▄▇▅▃
wandb:      train/policy_loss ▄▄▄▁▄▄▄▄▄▄▄▄▄▃██▄▄▄▇▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7637
wandb: best/eval_avg_mil_loss 0.85752
wandb:  best/eval_ensemble_f1 0.7637
wandb:            eval/avg_f1 0.53133
wandb:      eval/avg_mil_loss 0.84769
wandb:       eval/ensemble_f1 0.53133
wandb:           train/avg_f1 0.60011
wandb:      train/ensemble_f1 0.60011
wandb:         train/mil_loss 0.95123
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run drawn-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gbut464q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091225-gbut464q/logs
wandb: ERROR Run gbut464q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: udhii7if with config:
wandb: 	actor_learning_rate: 5.640209138728446e-06
wandb: 	attention_dropout_p: 0.013521188686490436
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 137
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.03729968270343376
wandb: 	temperature: 8.725353228522476
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091419-udhii7if
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/udhii7if
wandb: uploading history steps 128-138, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▇▇██
wandb: best/eval_avg_mil_loss ▄▂▆█▂▁
wandb:  best/eval_ensemble_f1 ▁▁▇▇██
wandb:            eval/avg_f1 ▅▅▅▇▁▇▅▅▅▅▅▅▆▇▇▆▆▆▃▃▇█▅▅▇▅▄▂▆▆▆▅█▆▇▆█▆█▇
wandb:      eval/avg_mil_loss ▄▃▅█▆▅▅▆▅▇▅▆▇▆▅▄▆▄▃▄▄▄▁▄▅▃▆▃▅▆▂▃▅▃▆▄█▅▃▆
wandb:       eval/ensemble_f1 ▃▅█▅▁▆▆▂▁█▇▅▆█▅▃▇▇█▅█▄▇▄▄▆▆▆▇▆▆▅█▄▅▆▆▆▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▄▅▅▂▆▁▃▁▄▃▆▄▂▅▄▅▆▅▃▅▇▆▃▄▅▄▃▅▆▆▇▇▅▅▇█▇█▆
wandb:      train/ensemble_f1 ▆▁▄▅▂▁▅▆▄▁▅▆▅▃▄▆▄▄▃▆▅▆▄▃▅▆▆▃▅▅▆▇▆▆▇█▇█▆▅
wandb:         train/mil_loss ▅▅█▅▆▆█▅▆▃▄▆▄▁▆▃▅▆▃▃▆▃▅▄▂▆▄▇▆▆▅▃▄▂▅▁▃▃▃▂
wandb:      train/policy_loss ▁█▁█▁▅█▅█▁▅▁▁▅▅▁▅▅▅▅▅▅▅▁▅█▅▅▅▅▅█▅▅▁▅▁▅▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁████████████████████████████▅█████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77859
wandb: best/eval_avg_mil_loss 0.83344
wandb:  best/eval_ensemble_f1 0.77859
wandb:            eval/avg_f1 0.73751
wandb:      eval/avg_mil_loss 1.02147
wandb:       eval/ensemble_f1 0.73751
wandb:            test/avg_f1 0.71205
wandb:      test/avg_mil_loss 0.54379
wandb:       test/ensemble_f1 0.71205
wandb:           train/avg_f1 0.70162
wandb:      train/ensemble_f1 0.70162
wandb:         train/mil_loss 0.98013
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run revived-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/udhii7if
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091419-udhii7if/logs
wandb: Agent Starting Run: z7sjuue6 with config:
wandb: 	actor_learning_rate: 3.903093400542032e-06
wandb: 	attention_dropout_p: 0.3052354570563609
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.47772072931121745
wandb: 	temperature: 5.8052266660922305
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091650-z7sjuue6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z7sjuue6
wandb: uploading history steps 85-94, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▅▅▅██
wandb: best/eval_avg_mil_loss ▆▃▅▄▅█▃▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▃▅▅▅██
wandb:            eval/avg_f1 ▁▆▅▅▄▇▅▅▇▄▅▇▆▅▆▆▆▇▆▅▇▅▆▆▅▅▆▆▆▅▇▆█▆▅▇▆█▆█
wandb:      eval/avg_mil_loss ▂▅▇▃▅▄▄▆▅▇▆▅▅▃▃▄█▅▄▆▄▅▃▂▄▇▁▃▃▄▄▃▄▄▄▂▂▂▄▁
wandb:       eval/ensemble_f1 ▅▁▅▅▅▅▅▅▇▅▆▅▇▆▆▆▆▆▆▅▆▆▅▇▅▇▆▆▆▄▆▆▇█▇▇▆▆▆█
wandb:           train/avg_f1 ▄▃▁▄▂▅▃▄▅▅▄▅▅▄▅▅▃▆▆▆▆▆▄▃▅▄▄▇▅▅▅▅▇▇█▆█▆▇█
wandb:      train/ensemble_f1 ▂▂▁▁▁▄▁▂▄▃▃▅▄▄▄▅▅▅▅▅▅▃▆▆▇▄▅▄▆▆▆▇▇▆▆▆█▄▆█
wandb:         train/mil_loss ▅█▇▇▆▇▄█▆▅▃▇▇█▆▇▂▄▇▃▅▆▅▄▂▇█▅▁▅▅▂▄▆▅▁▄▅▅▄
wandb:      train/policy_loss ▂████████▆█████▇▆████▆████████▁████▃████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████▅███████▅████████████████▁█████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74006
wandb: best/eval_avg_mil_loss 0.5823
wandb:  best/eval_ensemble_f1 0.74006
wandb:            eval/avg_f1 0.7328
wandb:      eval/avg_mil_loss 0.62311
wandb:       eval/ensemble_f1 0.7328
wandb:           train/avg_f1 0.66481
wandb:      train/ensemble_f1 0.66481
wandb:         train/mil_loss 0.88894
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z7sjuue6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091650-z7sjuue6/logs
wandb: ERROR Run z7sjuue6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9x73cbd8 with config:
wandb: 	actor_learning_rate: 0.0001485885557102545
wandb: 	attention_dropout_p: 0.401031398005572
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 102
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1844424657718403
wandb: 	temperature: 1.4143312789348728
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091854-9x73cbd8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9x73cbd8
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇███
wandb: best/eval_avg_mil_loss ▅█▆▆▅▁▄
wandb:  best/eval_ensemble_f1 ▁▅▆▇███
wandb:            eval/avg_f1 ▄▆▇▃▄▂▂▁▂▂▄▃▄▂█▂▆▁▃▂▆█▃▇▇▂▃▁▆▁▄▄▂▁▇▁▆▄▃▃
wandb:      eval/avg_mil_loss █▅█▂▂▆▃▄▇▇▃▇▆▃▆▁▁▅▇▃▂▂▄▃▄▄▆▄▁▆▂▄▆▆▅▂▄▂▃▁
wandb:       eval/ensemble_f1 ▁▆▆▆▅▅▄▅▅▅▅▆▄█▄▅▅▅█▆▇▄▅██▅█▅▅▆▆▆▅▆▅▆▅▇▆▆
wandb:           train/avg_f1 ▂▅▆▄▄▅▂▅▅▂▅▄▃▆▃▃▃▄▄▅▂█▄▃▄▃▂▁▂▃▄▃▆▅▅▅▃▆▅▄
wandb:      train/ensemble_f1 ▅▁▃▃▅▅▃▄▂▃▅▆▆▇▅▄▄▅▅▁█▄▃▃▃▇▄▂▄▄▃▅▅▄▆▅▄▆▆▄
wandb:         train/mil_loss █▄▆▅▅▃█▄▆▃▇▄▅▆█▃▂▄▁▂▅▄▄▃▃▄▅▅█▅▅▁▆▄▂▅▃▆▅▇
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▅▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁██████████████▄██████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77413
wandb: best/eval_avg_mil_loss 0.90505
wandb:  best/eval_ensemble_f1 0.77413
wandb:            eval/avg_f1 0.60842
wandb:      eval/avg_mil_loss 1.12195
wandb:       eval/ensemble_f1 0.60842
wandb:           train/avg_f1 0.6245
wandb:      train/ensemble_f1 0.6245
wandb:         train/mil_loss 1.02889
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hopeful-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9x73cbd8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091854-9x73cbd8/logs
wandb: ERROR Run 9x73cbd8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: zf0uffj4 with config:
wandb: 	actor_learning_rate: 4.27082434257544e-05
wandb: 	attention_dropout_p: 0.24828987041547723
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 98
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9856528205907664
wandb: 	temperature: 9.901334665227616
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092048-zf0uffj4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zf0uffj4
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅█
wandb: best/eval_avg_mil_loss █▁▇▂▆
wandb:  best/eval_ensemble_f1 ▁▄▄▅█
wandb:            eval/avg_f1 ▅▇▅▄▂▅▇▅▅█▆▅▁▂▁▁▇▄▇▆▅▆▆▆▇██▆▇▆▆▄▆▇▆█▆██▆
wandb:      eval/avg_mil_loss ▃▆█▄▄▅▆▃█▆▄▅▂▃▃▅▄▃▆▄▄▂▃█▇▆▃▄▄▇▁▆▄▁▃▄▃▃▃▆
wandb:       eval/ensemble_f1 ▄▅▃▁▆▅▆▃▅▅▁▇▇▃▃▆▆▅▃▄▆▅▆█▆▆▆▅▅▆█▃▅▇▆▅██▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▂▂▂▅▂▃▁▃▄▄▁▄▁▄▃▄▄▅▅▅▃▃▆▅▆█▅▇▇▇▅▂▅▅▅▇▆▄
wandb:      train/ensemble_f1 ▃▂▃▂▂▅▄▄▁▄▁▃▄▅▅▁▄▃▄▃▅▂▆▆▃▄▇▄▇▆▄▅█▂▅▅▇▆▇▅
wandb:         train/mil_loss ▅▆▅▅▃▅▅▅▄▃█▅▄▄▄▅▄▄▄▃▄▅▄▄▄▆▃▃▂▃▄▂▄▃▅▂▃▁▂▄
wandb:      train/policy_loss ▅▅▅▅▂▅▅▅▅▅▃▅▅▅▅▅▁▅▅▅▄▅▅▅▅▅▅▅▄▅▂▅▅▅▅▅▅█▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████▃███████████▁█████████▅▄██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.70329
wandb: best/eval_avg_mil_loss 0.97688
wandb:  best/eval_ensemble_f1 0.70329
wandb:            eval/avg_f1 0.6225
wandb:      eval/avg_mil_loss 0.86793
wandb:       eval/ensemble_f1 0.6225
wandb:            test/avg_f1 0.67225
wandb:      test/avg_mil_loss 0.69654
wandb:       test/ensemble_f1 0.67225
wandb:           train/avg_f1 0.60012
wandb:      train/ensemble_f1 0.60012
wandb:         train/mil_loss 0.93276
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wandering-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zf0uffj4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092048-zf0uffj4/logs
wandb: Agent Starting Run: 03czczto with config:
wandb: 	actor_learning_rate: 1.1123659574207702e-06
wandb: 	attention_dropout_p: 0.4814243801614711
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6844440692783557
wandb: 	temperature: 2.7987127969393977
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092258-03czczto
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/03czczto
wandb: uploading wandb-summary.json
wandb: uploading history steps 168-175, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▇▇██
wandb: best/eval_avg_mil_loss █▄▃▃▂▂▁▂
wandb:  best/eval_ensemble_f1 ▁▂▂▃▇▇██
wandb:            eval/avg_f1 ▃▂▃▃▃▃█▄▄▂▅▅▃▂▄▅▅▄▄▄▄▆▄▄▄▅▅▄▅▅▅▅▅▅▆▂▁▆▆▆
wandb:      eval/avg_mil_loss ▄▄▅▄▂▄▃▃▄▃▁▄▄▃▃▅▆▄█▃▃▁█▄▃▃▂▄▃▂▃▄▃▂▃▃▂▂▂▂
wandb:       eval/ensemble_f1 ▄▇▁▅▄▅▄▅▅▅▅▅▄█▅▅▄▅▆▅█▅▅▅▅▅▅▆▃▆▆▆▅▆▅▆▆▆▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▁▁▁▂▂▃▃▁▃▅▃▃▃▂▃▄▄▄▄▆▅▅▅▆▅▆█▇▇▇▇▇▇▆▆▇▄▆█
wandb:      train/ensemble_f1 ▁▂▅▆▅▂▃▄▂▄▃▅▅▅▅▇▅▄▅█▆▇█▇▆▆▇▇▇█▆▇█▇▇▇▇▇█▇
wandb:         train/mil_loss ▅▅▅▅▇▄▅▆▃▅█▃▆▅▄▄▂▄▃▂▅▄▃▅▅▃▅▃▅▃▅▆▁▃▂▄▂▁▂▂
wandb:      train/policy_loss ▁▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74559
wandb: best/eval_avg_mil_loss 0.83027
wandb:  best/eval_ensemble_f1 0.74559
wandb:            eval/avg_f1 0.63947
wandb:      eval/avg_mil_loss 0.90803
wandb:       eval/ensemble_f1 0.63947
wandb:            test/avg_f1 0.66456
wandb:      test/avg_mil_loss 0.54976
wandb:       test/ensemble_f1 0.66456
wandb:           train/avg_f1 0.61396
wandb:      train/ensemble_f1 0.61396
wandb:         train/mil_loss 0.69873
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lively-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/03czczto
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092258-03czczto/logs
wandb: Agent Starting Run: 77hf3fip with config:
wandb: 	actor_learning_rate: 3.595612597021158e-06
wandb: 	attention_dropout_p: 0.4532057882143906
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 149
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.12235617059477576
wandb: 	temperature: 6.055624417311764
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092614-77hf3fip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/77hf3fip
wandb: uploading wandb-summary.json
wandb: uploading history steps 126-135, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▃▄█
wandb: best/eval_avg_mil_loss ▄▄▃▄▁█▆
wandb:  best/eval_ensemble_f1 ▁▂▂▃▃▄█
wandb:            eval/avg_f1 ▁▂▂▂▂▂▂▁▂▂█▂▂▂▄▃▂▃▂▃▄▄▃▄█▃▄▄▅▂▃▄▄▃▃▃▄▄▄▄
wandb:      eval/avg_mil_loss ▄▃▃▃▃▃▃▁█▃▃▁▇▃▅▃▃▃▂▃▃▂▃▁▃▄▃▂▃▂▅▂▂▂▇▃▂▂▂▄
wandb:       eval/ensemble_f1 ▁▂▂▂▃▃▂▃▂▃▃▃▄▄▃▅▃▃▄▅▅▄▃▃▃▃▅▄▃▄▄▃▃▄█▅▅▅▃▄
wandb:           train/avg_f1 ▄▄▂▄▃▁▃▂▁▃▁▃▅▆▄▅▃▅▄▃▄▆▅▅▄▄▅▆▅▆▅▅▆▇▆▆▇▆█▇
wandb:      train/ensemble_f1 ▅▄▅▁▄▂▃▄▂▄▂▄▆▄▃▅▅▅▅▅▄▆▆▅▅▇█▇▇▆▆▇▆▇▇▆▇█▆█
wandb:         train/mil_loss ▅▇█▄▄█▃▄█▇▅▅▄▅▄▄▃▅█▁▇▄▅▄▆▂▅▃█▂▁▂▂▃▄▆▂▂▂▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅▅▅▁▅▅▅▅▅▅█▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.70868
wandb: best/eval_avg_mil_loss 1.04034
wandb:  best/eval_ensemble_f1 0.70868
wandb:            eval/avg_f1 0.59515
wandb:      eval/avg_mil_loss 0.80714
wandb:       eval/ensemble_f1 0.59515
wandb:           train/avg_f1 0.58891
wandb:      train/ensemble_f1 0.58891
wandb:         train/mil_loss 0.76623
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run gallant-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/77hf3fip
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092614-77hf3fip/logs
wandb: ERROR Run 77hf3fip errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: vzrjj05j with config:
wandb: 	actor_learning_rate: 4.8370040960537324e-06
wandb: 	attention_dropout_p: 0.4452112617181314
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 158
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.171010445828646
wandb: 	temperature: 8.568907195627618
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092854-vzrjj05j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vzrjj05j
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇██
wandb: best/eval_avg_mil_loss ██▁▅▄
wandb:  best/eval_ensemble_f1 ▁▆▇██
wandb:            eval/avg_f1 ▅▆█▄▆▂▅▇▅▆▆▆▁▃█▄▅▇▅▆▄▄▅▆█▃▆▄▇▅▄▄▅▅▅▆▅▇▅▅
wandb:      eval/avg_mil_loss ▃▆▃▂▄▅▃▄▁▃▆▅▂▄▅▃▃▂█▆▅▅▇▆▃▁▂▃▄▅▂▃▂▃▂▅▂▁▄▅
wandb:       eval/ensemble_f1 █▇▅▁▇▆▇▇▆▄▅▄▆▁▄▅▇▇▆▄█▆▇▇▅▄▆▇▅▆▅▇▆▇▇▅▇▅▅▅
wandb:           train/avg_f1 ▄▅▃▄▅▁▃▆▃▄▃▅▃▆▃▃▁▃▄▆▇▆▆▃▅▁▆▃▇▃▄█▆▄▅▅▃▇▄▆
wandb:      train/ensemble_f1 ▃▁▄▃▃▂▅▄▆▄▃▄▃▂▂▄▂▃▅▃▁▄▆▂▃▃▃▄▅▂▅▁▄▄▆█▂█▄▄
wandb:         train/mil_loss ▅▅▆█▄▆▄▃▆▄▃▅▅▃▄▅▁▁▄▆▄▂▃▆█▃▄▂▂▃▂▃▄▂▅▃▂▄▄▁
wandb:      train/policy_loss ▄▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅█████████▄████▃█████████████████████▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80482
wandb: best/eval_avg_mil_loss 0.79636
wandb:  best/eval_ensemble_f1 0.80482
wandb:            eval/avg_f1 0.58947
wandb:      eval/avg_mil_loss 0.86199
wandb:       eval/ensemble_f1 0.58947
wandb:           train/avg_f1 0.6792
wandb:      train/ensemble_f1 0.6792
wandb:         train/mil_loss 0.73635
wandb:      train/policy_loss -0.4979
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.4979
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run twilight-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vzrjj05j
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092854-vzrjj05j/logs
wandb: ERROR Run vzrjj05j errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: pi3n9im8 with config:
wandb: 	actor_learning_rate: 0.00019781176566738352
wandb: 	attention_dropout_p: 0.1947507634810293
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.971538230397636
wandb: 	temperature: 3.612425319374534
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093150-pi3n9im8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pi3n9im8
wandb: uploading wandb-summary.json; uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▅▆█
wandb: best/eval_avg_mil_loss █▅▆▅▂▆▃▁
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▅▆█
wandb:            eval/avg_f1 ▁▃▆▂▁▂▂▂▅▄▅▄▁▄▄▃▄▄█▃▃▅▃█▅▄▄▅▃▃▂▄▆▅▄▂▇▆▅▄
wandb:      eval/avg_mil_loss █▆▄▇▄▇▃▅▄█▅▆▆▆▇▅▃▃▆▂▅▅▅▃▄▄▅▁▄▃▆▆▄▆▆▃▃▅▃▇
wandb:       eval/ensemble_f1 ▁▃▂▁▄▂▂▂▄▅▄▄▄▂█▄▃▄▃▄▅▄▅▃▅▆▃▅▄▄▄▆▂▅▄▄▂▅▄▃
wandb:           train/avg_f1 ▄▄▂▃▃▃▄▃▄▅▁▄▁▄▄▄█▄▄▅▅▇▄▄▇▆▃▃▄█▅▅▅▆▇▆▅▆▅▆
wandb:      train/ensemble_f1 ▄▄▅▂▄▃▃▄▄▃▄▇▁▅▄▄▄▅▆▅▅█▃▇▇▄▇▇▅▅▅▅▆▅█▆█▄▅▅
wandb:         train/mil_loss ▃▅▅▆▄▅▃▅▂▅▅█▃▄▄▃▄▄▅▃▃▃▅▂▁▃▂▄▄▁▂▃▄▃▂▂▄▃▂▄
wandb:      train/policy_loss ██▄██████████▇███████████▁██████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄████▁██████▇██████████████████████▆███
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7328
wandb: best/eval_avg_mil_loss 0.98614
wandb:  best/eval_ensemble_f1 0.7328
wandb:            eval/avg_f1 0.58791
wandb:      eval/avg_mil_loss 1.20088
wandb:       eval/ensemble_f1 0.58791
wandb:           train/avg_f1 0.64231
wandb:      train/ensemble_f1 0.64231
wandb:         train/mil_loss 1.10943
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run polar-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pi3n9im8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093150-pi3n9im8/logs
wandb: ERROR Run pi3n9im8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xu5wgfw1 with config:
wandb: 	actor_learning_rate: 0.00024346470273844357
wandb: 	attention_dropout_p: 0.2935332031041669
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 128
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.09914793612345608
wandb: 	temperature: 5.789381068841894
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093349-xu5wgfw1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xu5wgfw1
wandb: uploading history steps 123-129, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▄▄▄▇▇██
wandb: best/eval_avg_mil_loss ▄██▄▂▅▃▇▄▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃▄▄▄▄▇▇██
wandb:            eval/avg_f1 ▄▃▅▅▅▆▅▅▅▄▅▆▅▅▅▆▁▆▅▄▆▆▆▆▆▆▇▇▇▇▄▇█▇▆█▇▆██
wandb:      eval/avg_mil_loss █▅▃▅▇▄▄▆▅▇▇█▅▅▄▆▄▄█▃▄▃▃▄▃▄▃▃▃▃▄▅▁▃▂▃▄▂▂▂
wandb:       eval/ensemble_f1 ▁▂▃▃▃▃▂▂▆▁▃▂▄▃▃▄▄▅▄▄▆▆▂▅▆▅▄▆█▅▃▃▆█▆▆█▇▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▂▁▂▂▂▃▂▃▃▂▃▄▅▃▃▄▄▄▃▃▄▄▆▅▅▅▃▄▅▆▆▅▇▆▇▇▅▇█
wandb:      train/ensemble_f1 ▂▂▂▁▂▃▂▃▃▄▄▅▃▄▄▄▃▄▃▄▅▅▇▆▅▆▆▆▆▆▇▇█▆▆▄▅▇▇▇
wandb:         train/mil_loss █▇▆▅▇▆▆▅█▅▆█▆▅▆▅▇▅▅▅▃▃▆▂▅▆▅▃▄▇▃▂▅▂▄▁▂▅▃▄
wandb:      train/policy_loss █▅██▆███▁██████████▅███▆████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▅█████▁█▄██▅████████▁████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.69562
wandb: best/eval_avg_mil_loss 0.74134
wandb:  best/eval_ensemble_f1 0.69562
wandb:            eval/avg_f1 0.63787
wandb:      eval/avg_mil_loss 0.79404
wandb:       eval/ensemble_f1 0.63787
wandb:            test/avg_f1 0.60942
wandb:      test/avg_mil_loss 0.65735
wandb:       test/ensemble_f1 0.60942
wandb:           train/avg_f1 0.68949
wandb:      train/ensemble_f1 0.68949
wandb:         train/mil_loss 0.75321
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run comic-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xu5wgfw1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093349-xu5wgfw1/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: pl4w75ey with config:
wandb: 	actor_learning_rate: 4.9501572682046364e-06
wandb: 	attention_dropout_p: 0.23240860552872533
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.31157789816577164
wandb: 	temperature: 2.2901151841595446
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093624-pl4w75ey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pl4w75ey
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▅▆█
wandb: best/eval_avg_mil_loss █▄▃▅▃▁
wandb:  best/eval_ensemble_f1 ▁▂▂▅▆█
wandb:            eval/avg_f1 ▆▅▇▁▄▆▄▆▇▇▄▅▆▅▇█▅█▆▆▇█▇▇▇▆█▆▇▇▆▅▇█▇▆▆█▆█
wandb:      eval/avg_mil_loss ▅▅▄▅▇▃▄▅▄▅▇▇▃▄▆▃▂▄▆▄▄▆▂▃▁▄█▅▄▅▂▅▆▄▃▄▆▄▂▅
wandb:       eval/ensemble_f1 ▆▆▇▁▆▆▅▅▅▅▄▅▆▅▆▇▅▇▆▅▆▅▆▆▇▇▄▄▆▆█▇█▇▆▆█▅██
wandb:           train/avg_f1 ▅▄▃▁▄▂▅▃▆▄▅▅▆▅▆▄▅▆▅▆▅▆▅▆▆▆▇█▆▆▅▇▆▆▅▆▆▆▇▆
wandb:      train/ensemble_f1 ▄▄▃▃▅▁▂▄▅▄▅▅▆▄▇▇▆▅▅▆▆▆▇▅▆▆▆▆▇▆▇▇▇██▇▆▇▇▇
wandb:         train/mil_loss ▅▅▆▆█▄▇▄▆▆▆▆▅▅▄▅▃▂▆█▄▅▄▂▄▂▂▅▆▂▃▁▄▅▁▅▂▁▂▂
wandb:      train/policy_loss ▃▃▃▂▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅█████████▁█████████████████████▄██████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.68706
wandb: best/eval_avg_mil_loss 0.87226
wandb:  best/eval_ensemble_f1 0.68706
wandb:            eval/avg_f1 0.59982
wandb:      eval/avg_mil_loss 0.9066
wandb:       eval/ensemble_f1 0.59982
wandb:           train/avg_f1 0.59622
wandb:      train/ensemble_f1 0.59622
wandb:         train/mil_loss 0.99924
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bright-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pl4w75ey
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093624-pl4w75ey/logs
wandb: ERROR Run pl4w75ey errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: gjx85i06 with config:
wandb: 	actor_learning_rate: 0.0003720412875816403
wandb: 	attention_dropout_p: 0.2434818692305859
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9339898382865768
wandb: 	temperature: 0.7021993518829994
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093914-gjx85i06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gjx85i06
wandb: uploading history steps 84-91, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▅██
wandb: best/eval_avg_mil_loss █▁▄▇█▇
wandb:  best/eval_ensemble_f1 ▁▃▅▅██
wandb:            eval/avg_f1 ▆▃▇▆▅▆▄▇▄▅▅▇▇▄▄▅▄▇▆▇▅▅▅▆▇▄▄█▅▅▁▇▅█▆▇▅▅▅▇
wandb:      eval/avg_mil_loss ▁▃▃▄▅▃▄▃▃▃▅▄▄█▃▄▅▆▃▄▆▂▄▃▅▃▆▃▂▃▄▃▄▂▃▂▃▃▄▁
wandb:       eval/ensemble_f1 ▃▂▅▄▃▆▄▄▃▅▅▅▁▂▂▃▆▃▃█▃▅▅▃▄▃▅▇▆▅▃▆▄▇▄▇▆▃▆▅
wandb:           train/avg_f1 ▃▂▄▃▁▅▅▅▄▆▃▅▃▅▃▄██▆▆▅▆▂▆▃▅▄▆▆█▅▆▄▆▅▆▇▇▇▅
wandb:      train/ensemble_f1 ▂▁▅▃▁▄▄▅▄▂▂▄▅▄▂▄▄▄█▅▅▇▄▄▄▂▃▄▆▆▃▄▆▆▆▆▇▇▆▅
wandb:         train/mil_loss ▆▄▇▅▅▆▂▇▄▄█▅▅▅▄▆█▆▃▂▅▅▅▄▄▇▃▃▃▂▃▃▃▄▃▁▃▆▃▅
wandb:      train/policy_loss █▆▆▆▆▆▆▃▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▄▆▆▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▅▆▅▆▆▃▆▆█▆▇▆█▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.76965
wandb: best/eval_avg_mil_loss 0.96034
wandb:  best/eval_ensemble_f1 0.76965
wandb:            eval/avg_f1 0.67111
wandb:      eval/avg_mil_loss 0.85424
wandb:       eval/ensemble_f1 0.67111
wandb:           train/avg_f1 0.64268
wandb:      train/ensemble_f1 0.64268
wandb:         train/mil_loss 0.97003
wandb:      train/policy_loss -0.59912
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.59912
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run efficient-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gjx85i06
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093914-gjx85i06/logs
wandb: ERROR Run gjx85i06 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 3jo7uk0c with config:
wandb: 	actor_learning_rate: 5.880076663428172e-06
wandb: 	attention_dropout_p: 0.06166355563474085
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6922507097019394
wandb: 	temperature: 5.712049954342765
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094057-3jo7uk0c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3jo7uk0c
wandb: uploading history steps 42-53, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▆█
wandb: best/eval_avg_mil_loss ▁█▄▇▃
wandb:  best/eval_ensemble_f1 ▁▄▄▆█
wandb:            eval/avg_f1 ▆▅▁▆▇▆▆▇▆▇▄▆▆▇▃▆▇▇▇▇▆▆▆▇▆▁▅▄▆█▇▂▂▇▇▇▇▇▆▇
wandb:      eval/avg_mil_loss ▁▃▂▃▃▂▃▆▁▂▆▃▁█▁▁▃▃▁▁▃▆▂▁▁▇▄▄▁▂▂▇▄▄▁▁▁▁▃▂
wandb:       eval/ensemble_f1 ▆▅▁▆▇▆▆▆▇▆▇█▄▆▆▃▇▆▆▆▇▇▇▆▆▆▇▁▅▄█▇▂▂▇▇▇▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▆▂▃▄▃▄▃▄▂▁▅▂▃▄▃▃▃▂▃▅▄▄▄▂▃▆▃▄▄▅█▄▅▄▅▃▅▅▄
wandb:      train/ensemble_f1 ▃▆▂▃▄▃▄▃▄▂▁▅▂▃▄▃▃▃▂▃▄▅▄▄▄▅▃▄▆▃▄▅▅█▅▄▅▃▅▄
wandb:         train/mil_loss ▃▄▃▁▆▃▅▅▂█▂▄▅▂▃▃▃▃▄▄▇▃▃▇▄▁▂▃▄▄▅▂▂▄▇▃▅▂▇▅
wandb:      train/policy_loss ▄▆█▆▄▆▆▁▆▆▆▆▄▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆█▆▄▆▆▆▁▆▆▆▆▄▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.61944
wandb: best/eval_avg_mil_loss 0.83438
wandb:  best/eval_ensemble_f1 0.61944
wandb:            eval/avg_f1 0.5684
wandb:      eval/avg_mil_loss 0.85617
wandb:       eval/ensemble_f1 0.5684
wandb:            test/avg_f1 0.60144
wandb:      test/avg_mil_loss 0.57504
wandb:       test/ensemble_f1 0.60144
wandb:           train/avg_f1 0.57213
wandb:      train/ensemble_f1 0.57213
wandb:         train/mil_loss 0.93232
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run volcanic-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3jo7uk0c
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094057-3jo7uk0c/logs
wandb: Agent Starting Run: ykmj1rxx with config:
wandb: 	actor_learning_rate: 3.0430598407409713e-05
wandb: 	attention_dropout_p: 0.4276283352407362
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 135
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2506343316389621
wandb: 	temperature: 2.7981042580645754
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094159-ykmj1rxx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ykmj1rxx
wandb: uploading history steps 126-135, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▇█
wandb: best/eval_avg_mil_loss ▂█▇▁
wandb:  best/eval_ensemble_f1 ▁▃▇█
wandb:            eval/avg_f1 █▃▄▄▃▃▃▅▁▅█▄▆▆▇▇▆▆▅▄▆▅▆▃▂▆▅▆▅▅▇▅▆█▆▂█▅▆▆
wandb:      eval/avg_mil_loss ▃▃▆▄▃▄▄▆▄▇▂▄▃▅▃█▄▃▃▅▄▆▃▅▃▄▃▃▇▇▃▄▂▄▄▃▁█▄▆
wandb:       eval/ensemble_f1 ▄▅▇▄▅▃▆▄▂▁▃▅▃▆▇▁▃▆▅▃▅▃▇▄▃▄▅▅▅█▃▇▅▇▆▇▄▇▆▅
wandb:           train/avg_f1 ▁▂▅▁▁▃▁▄▃▅▂▃▄▆▄▆▂▅▅▄▂▆▄▆▅▁▅▃▃▃▅▃▄▅▄▄▅█▆▆
wandb:      train/ensemble_f1 ▄▄▃▂▆▃▅▁▅▄▄▂█▃▅▄▂▇▆▄▆▅▇▃▇▅▇▆▅▄▄▅▅▆▄▄▅▅▆▆
wandb:         train/mil_loss ▂▇▅▅▇▅▅▃▄▄▅▇▂▆▅▃▁▆▆▄▆▁▆▃▅▁▃▃█▁▅▁▇▆█▃▄▇▅▆
wandb:      train/policy_loss █▁█▆█▄████████▆█████████▆███████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78858
wandb: best/eval_avg_mil_loss 0.74504
wandb:  best/eval_ensemble_f1 0.78858
wandb:            eval/avg_f1 0.69545
wandb:      eval/avg_mil_loss 0.71492
wandb:       eval/ensemble_f1 0.69545
wandb:           train/avg_f1 0.67815
wandb:      train/ensemble_f1 0.67815
wandb:         train/mil_loss 0.82881
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rich-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ykmj1rxx
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094159-ykmj1rxx/logs
wandb: ERROR Run ykmj1rxx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: czgiddr8 with config:
wandb: 	actor_learning_rate: 1.2738128449268476e-06
wandb: 	attention_dropout_p: 0.08338596829894196
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5123198154108991
wandb: 	temperature: 7.3228822490734515
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094439-czgiddr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/czgiddr8
wandb: uploading history steps 42-53, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss █▇▄▆▁
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▂▃▆▂▃▄▁▁▃▇▇▃▃▂▂▃▂▁▂▂▂▃▇▂▄▃▂▂▅▄▁▅▅▃▆▂▅█▃▁
wandb:      eval/avg_mil_loss ▇▆▄▃▅▃▅▅▇▇▅▅▅▆▇▂▅▃▅▂▆▅▆▂▃▆▆▅▆▅▄█▃▆▄▄▅▁▄▁
wandb:       eval/ensemble_f1 ▂▃▆▃▃▁▅▁▇▃▃▃▂▂▃▂▁▂▂▂▃▇▂▄▂▂▂▄▁▅▂▅▅▃▆▂▅█▃▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▃▆▂▃▂▂▄▂▅▄█▁▄▅▄▇▅▄▆▆▃▄▄▄▄▄▄▁▃▆▄▇▂▄▃▅▅▁▃
wandb:      train/ensemble_f1 ▃▄▆▃▃▃▅▃▄▃▃▄█▂▄▅▅▇▅▄▆▆▄▄▅▁▄▄▅▃▇▃▅▃▂▆▅▁▂▃
wandb:         train/mil_loss ▄▃▂▄▄▃▂▄▃▃█▄▅▄▄▂▄▃▆▄▂▄▁▃▅▅▅▂▃▆▄▂▄▅▃▅▄▆▁▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74839
wandb: best/eval_avg_mil_loss 0.69317
wandb:  best/eval_ensemble_f1 0.74839
wandb:            eval/avg_f1 0.52677
wandb:      eval/avg_mil_loss 0.72487
wandb:       eval/ensemble_f1 0.52677
wandb:            test/avg_f1 0.58697
wandb:      test/avg_mil_loss 0.84564
wandb:       test/ensemble_f1 0.58697
wandb:           train/avg_f1 0.58355
wandb:      train/ensemble_f1 0.58355
wandb:         train/mil_loss 0.98014
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run good-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/czgiddr8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094439-czgiddr8/logs
wandb: Agent Starting Run: fi4hsov6 with config:
wandb: 	actor_learning_rate: 0.00013966997883742144
wandb: 	attention_dropout_p: 0.055328945349481795
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 144
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.35377132621311336
wandb: 	temperature: 6.1402403191119586
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094542-fi4hsov6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fi4hsov6
wandb: uploading history steps 140-144, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆██
wandb: best/eval_avg_mil_loss ▃▃█▃▁
wandb:  best/eval_ensemble_f1 ▁▃▆██
wandb:            eval/avg_f1 ▄▅▄▇█▅▆▅▂▅▅▃▇▆▃▆▃▆▇▄▅█▅▅▃▁▆▆▂▇▇▃▃▇▄▅▅▇▄▆
wandb:      eval/avg_mil_loss ▂▅▃▂▃▃▃▆▃▃▆▃▃▅▃▆▅▅▃▂▃▅█▂▂▆▂▁▂▅▂▃▂▃▆▂▂▄▁▁
wandb:       eval/ensemble_f1 ▅▄▅▃▂▆▂▅▅▃▅▅▆▄▃▁▄▃▇▆▁▅▃▅▂▃▄▃▆█▇▄▅▂▇▇█▆▇█
wandb:           train/avg_f1 ▂▃▄▄▃▃▂▄▅▃▁▅▂▅▆▄▄▁▆▃▄▅▄▅▄▃▆▄▄▄▆▄▇▃█▃▅▄▆▇
wandb:      train/ensemble_f1 ▂▁▃▃▂▃▂▅▄▂▂▅▄▄▅▁▃▅▇▅▆▄▅▅▅▄▄█▆▇▇▅▃▄▇█▆▇▆█
wandb:         train/mil_loss ▅▄█▆▇▅▇▇▂▇█▆▅▆▅▃▂▃▆▄▂▆▃▇▅▃▂▄▄▆▅█▁▄█▇▂▆▃▅
wandb:      train/policy_loss ▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▃▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▆▁▁▁▁▂▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.63739
wandb: best/eval_avg_mil_loss 0.72894
wandb:  best/eval_ensemble_f1 0.63739
wandb:            eval/avg_f1 0.62043
wandb:      eval/avg_mil_loss 0.80581
wandb:       eval/ensemble_f1 0.62043
wandb:           train/avg_f1 0.59999
wandb:      train/ensemble_f1 0.59999
wandb:         train/mil_loss 0.79349
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dauntless-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fi4hsov6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094542-fi4hsov6/logs
wandb: ERROR Run fi4hsov6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 6obbwcbq with config:
wandb: 	actor_learning_rate: 1.1099356983779151e-05
wandb: 	attention_dropout_p: 0.25134345460941576
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 147
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4575060300495362
wandb: 	temperature: 1.1043680294992977
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094822-6obbwcbq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6obbwcbq
wandb: uploading history steps 97-110; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▃█
wandb: best/eval_avg_mil_loss █▅▁▅▆
wandb:  best/eval_ensemble_f1 ▁▁▃▃█
wandb:            eval/avg_f1 ▅▁▅█▅▅▃▅▆▅▆▅▆▆█▆▅▅▆█▆▆▆▆▆▇▅▇▆▇▇▇▆█▇▆▆▇▆▆
wandb:      eval/avg_mil_loss ▇▅▇▄▄▄█▄▄▄▅▃▆▄▆▄▃▁▄▂▃▃▅▆▃▃▃▆▃▃▃▃▆▃▄▃▄▄▅▅
wandb:       eval/ensemble_f1 ▃▁▄▄▄▄▄▃▄█▄▅▆▅▄▅▄▅▅▁▅▅▅▃▆▅▅▃▅▅▅█▅▇▃▆▅▆▅▅
wandb:           train/avg_f1 ▂▂▁▃▅▇▄▅▅▁▃▂▃▂▁▅▅▃▅▃▅▃▆▄▄▆▆▅▅▄▇▅▆██▆▃▆▆▅
wandb:      train/ensemble_f1 ▁▃▂▂▃▄▅▄▅▂▂▃▅▃▅▅▃▆▆▆▅▆▆▄▇▅▅█▅▆▅▄▃▅▇▆▅▅▅▅
wandb:         train/mil_loss █▂▄▆▄▅▅▅▃▄▆▅█▄▃▃▄▃▃▁▄▃▃▅▅▃▁▅▅▁▃▂▄▂▂▄▃▂▅▁
wandb:      train/policy_loss ▁█████████▅█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█████▃████████▅████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.68611
wandb: best/eval_avg_mil_loss 0.8636
wandb:  best/eval_ensemble_f1 0.68611
wandb:            eval/avg_f1 0.56339
wandb:      eval/avg_mil_loss 0.93384
wandb:       eval/ensemble_f1 0.56339
wandb:           train/avg_f1 0.63219
wandb:      train/ensemble_f1 0.63219
wandb:         train/mil_loss 0.75943
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fine-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6obbwcbq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094822-6obbwcbq/logs
wandb: ERROR Run 6obbwcbq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: uj9g431k with config:
wandb: 	actor_learning_rate: 0.00014064557220792788
wandb: 	attention_dropout_p: 0.28502424060252374
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22243346812384523
wandb: 	temperature: 7.258905457221233
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095037-uj9g431k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uj9g431k
wandb: uploading history steps 180-185, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅█
wandb: best/eval_avg_mil_loss ▆█▁▇
wandb:  best/eval_ensemble_f1 ▁▁▅█
wandb:            eval/avg_f1 ▄▄▆▃▄▁▆▇▅▇▅█▅▆▄▆▅▄▆▄▆▃█▃▄▂▄▆▇▆▆▃▄▃▄▇▅▅█▇
wandb:      eval/avg_mil_loss ▅▆▅▇▇▅▃▅▅▂▃█▃▄▄█▅▄▃▃▂▆▁▄▄▁▃▂▆▆▂▁▁▄▄█▆▁▂▆
wandb:       eval/ensemble_f1 █▁▇▇▃▅▄▄▇▇▇▁▅▅▇▄█▆▆▁▆▅▄▇▂▄▅▆▇▆▆▆▆▅▆█▆▄▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▁▂▃▃▂▁▃▄▂▂▅▁▅▃▂▃▄▂▅▄▄▄▅▃▃▅▆█▅▅▅▆▆▅▆▅▆▇
wandb:      train/ensemble_f1 ▂▂▁▃▃▃▃▃▄▄▂▃▅▄▂▄▄▄▃▄▅▅▄▄▆▄▇▅▄▅█▆▆▅▅▅▆▇▅▅
wandb:         train/mil_loss ▄█▅▂▇▆▄▇█▅▄▆▅▅▅▅▄▆▆▄▆▅▁▅▆▄▂▅▇▂▁▇▇▆▂▄▃▃▆▄
wandb:      train/policy_loss ████████▇██████████████████▆██████▄███▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████▇███████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78916
wandb: best/eval_avg_mil_loss 0.91036
wandb:  best/eval_ensemble_f1 0.78916
wandb:            eval/avg_f1 0.78301
wandb:      eval/avg_mil_loss 0.78479
wandb:       eval/ensemble_f1 0.78301
wandb:            test/avg_f1 0.77969
wandb:      test/avg_mil_loss 0.47248
wandb:       test/ensemble_f1 0.77969
wandb:           train/avg_f1 0.74325
wandb:      train/ensemble_f1 0.74325
wandb:         train/mil_loss 0.8074
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run floral-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uj9g431k
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095037-uj9g431k/logs
wandb: Agent Starting Run: 3dxyoqa7 with config:
wandb: 	actor_learning_rate: 0.0005990505678833434
wandb: 	attention_dropout_p: 0.32452557975317575
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 98
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.055882446917507655
wandb: 	temperature: 2.937513744671504
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095404-3dxyoqa7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3dxyoqa7
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss ▂▇▁█
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▅▄▃▆▂▅▅▄▄▁▂▇▁▄▅▁▇▇▄▅▄▇▁▅▁▄▂▄▂▅▂▅▂▄▁█▅▇▄▇
wandb:      eval/avg_mil_loss ▂▅▆▄▄▅▇▂▃█▆▄▄▂▅▄▄▄▄▃▃▄▃▅▄▅▃▄▇▁▄▅▅▄▅▃▅▄▄▄
wandb:       eval/ensemble_f1 ▄▅▇▅▂▆▅▅▁▂▁▅▅▅▄▅▅▅▆█▂▅▅▂▅▄▆▅▅▅▅▅▅▁▄▆▇▅▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▆▄▂▃▄▅▅▄▅▆▅▃▅▂▅▆▅▃▂█▆▅▃▅▇▄▅▃▂▆▄▄▄█▃▂▅▃▁
wandb:      train/ensemble_f1 ▃▄▃▃▁▅▅▄▄▄█▆▆█▃▂▄▅▄▅█▇▅▃▄▂▄▅▆▄▆▄▄▆▄▃▃▆▁▇
wandb:         train/mil_loss █▅▃▅▇▄▇▆▄█▃▅▆▆▃▆▇▄▇▆▅▃▄▄▃▁▃▅▅▆▅▄▃▄▅▅▄▆▄▇
wandb:      train/policy_loss █████████████████████████████████████▁██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█▅██████████████████████████████▅████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79863
wandb: best/eval_avg_mil_loss 0.87296
wandb:  best/eval_ensemble_f1 0.79863
wandb:            eval/avg_f1 0.52005
wandb:      eval/avg_mil_loss 0.88463
wandb:       eval/ensemble_f1 0.52005
wandb:            test/avg_f1 0.55568
wandb:      test/avg_mil_loss 0.94183
wandb:       test/ensemble_f1 0.55568
wandb:           train/avg_f1 0.63371
wandb:      train/ensemble_f1 0.63371
wandb:         train/mil_loss 0.95965
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lunar-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3dxyoqa7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095404-3dxyoqa7/logs
wandb: Agent Starting Run: 0fvdm8aw with config:
wandb: 	actor_learning_rate: 5.658359525346602e-06
wandb: 	attention_dropout_p: 0.22146013298757328
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22810854013789628
wandb: 	temperature: 4.216469686761645
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095559-0fvdm8aw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0fvdm8aw
wandb: uploading history steps 176-185, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▁▂▂▄▄▄▅▅▅▆▆▇██
wandb: best/eval_avg_mil_loss █▅▅▅█▆▆▃▂▃▂▁▁▄▃▁
wandb:  best/eval_ensemble_f1 ▁▁▁▂▂▄▄▄▅▅▅▆▆▇██
wandb:            eval/avg_f1 ▁▃▂▂▃▄▄▅▄▃▃▄▄▃▄▃▄▅▆▅▆▆▄▅▆▆▆▇▅▇▇▇▇▇▇▇▇▆▆█
wandb:      eval/avg_mil_loss █▇▆▆▄▅▄▅▅▆▇▅▆▅▃▅▄▄▄▆▅▃▆▆▄▂▂▅▃▂▂▅▃▅▃▅▂▄▁▁
wandb:       eval/ensemble_f1 ▁▃▃▁▁▂▁▃▃▃▃▃▃▄▄▄▄▅▄▃▅▅▆▇▅▆▇▆▆▆▆█▆▇▇▇▅█▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▁▃▂▃▂▃▂▃▃▃▄▄▃▅▄▅▅▆▇▆▆▇▇▇▆▇▇▇▇█▇█▇▇█▇█▇
wandb:      train/ensemble_f1 ▂▁▂▂▂▃▃▂▃▄▃▄▄▄▄▅▅▆▆▆▆▆▆▆▆▇▆▇▆▇▆▇▇▇▇▇▇▇▆█
wandb:         train/mil_loss ▇█▆▆▅▅▅▅▅▆▅▃▅▅▅▄▄▂▃▃▄▂▄▃▂▃▃▂▃▂▃▂▂▃▁▁▂▂▂▁
wandb:      train/policy_loss ▃▁▆█▄▄▄▄▂▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▆▄▇▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃█▄████████▅███████▅██████████▄███▇▁███▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80906
wandb: best/eval_avg_mil_loss 0.5553
wandb:  best/eval_ensemble_f1 0.80906
wandb:            eval/avg_f1 0.78603
wandb:      eval/avg_mil_loss 0.52064
wandb:       eval/ensemble_f1 0.78603
wandb:            test/avg_f1 0.81536
wandb:      test/avg_mil_loss 0.40115
wandb:       test/ensemble_f1 0.81536
wandb:           train/avg_f1 0.74639
wandb:      train/ensemble_f1 0.74639
wandb:         train/mil_loss 0.63686
wandb:      train/policy_loss 0.32503
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.32503
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devoted-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0fvdm8aw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095559-0fvdm8aw/logs
wandb: Agent Starting Run: 7vzf5jji with config:
wandb: 	actor_learning_rate: 5.696969380257409e-06
wandb: 	attention_dropout_p: 0.3204446174267133
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6430429323438486
wandb: 	temperature: 9.93832160248928
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100001-7vzf5jji
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7vzf5jji
wandb: uploading history steps 109-112, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▅▆▇▇▇█
wandb: best/eval_avg_mil_loss ▄▃▇▅▄▁▅█▁
wandb:  best/eval_ensemble_f1 ▁▁▄▅▆▇▇▇█
wandb:            eval/avg_f1 ▇▅▅▆▂▁▂▅▅▇▇▃▄▄▄█▄▄█▇▇▃▄▄▅▆▄▃▅▂▅▆█▅▅▄▃▅▄▆
wandb:      eval/avg_mil_loss ▄▆▄▂▆▄▃▇▇▂▆▆▄█▄▃▃▆▃▅▅▁▄▄▄▄▆▃▄▃▅▅▄▂▁▅▂▃▄▄
wandb:       eval/ensemble_f1 ▄▇▅▅▃▁▅▅▇▄▄▃▄▂▅▄▄▇▆▅▃▅▅▃▂▆▅▃▅▆█▇▄▅▅▄▆█▃▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▃▄▁▁▂▄▄▅▃▆▅▅▅▃▄▃▅▃▅▆▇▅▆▆▄▆▆▆█▇▄▅▇▅▇██▇█
wandb:      train/ensemble_f1 ▄▄▃▄▃▄▅▃▁▇▂▅▅▄▄▆▄▅▅▆▆█▆▄▅▆▆▇▅▆▇▄█▇▅▇▆█▆█
wandb:         train/mil_loss ▄▄▆▂▂▄▃▅▅▂▃▄▃█▄▃▃▅▅▃▂▂▂▁▁▁▄▂▃▃▅▇▃▁▁▃▂▃▃▂
wandb:      train/policy_loss ▄████▂█▃████▆█▄█▅█████████████████████▁▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄████▂████████▆██████████████████████▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.72083
wandb: best/eval_avg_mil_loss 0.83319
wandb:  best/eval_ensemble_f1 0.72083
wandb:            eval/avg_f1 0.62772
wandb:      eval/avg_mil_loss 0.98959
wandb:       eval/ensemble_f1 0.62772
wandb:            test/avg_f1 0.59967
wandb:      test/avg_mil_loss 0.67243
wandb:       test/ensemble_f1 0.59967
wandb:           train/avg_f1 0.63258
wandb:      train/ensemble_f1 0.63258
wandb:         train/mil_loss 1.02771
wandb:      train/policy_loss -0.5997
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.5997
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bumbling-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7vzf5jji
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100001-7vzf5jji/logs
wandb: Agent Starting Run: aor4g451 with config:
wandb: 	actor_learning_rate: 6.070045399443527e-05
wandb: 	attention_dropout_p: 0.4694050463049989
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.06658742639366544
wandb: 	temperature: 0.23199572003929855
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100211-aor4g451
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aor4g451
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄▄▄▅▅▅▅▆▆▇██
wandb: best/eval_avg_mil_loss █▇▆▄▅▄▅▄▄▃▃▃▁▃▁
wandb:  best/eval_ensemble_f1 ▁▃▃▄▄▄▅▅▅▅▆▆▇██
wandb:            eval/avg_f1 ▁▃▁▂▁▃▄▃▂▄▅▄▅▅▆▆▆▆▆▆█▆▆▆▇▇▇▇▆▅▆▇▇▆▇▆▆▇██
wandb:      eval/avg_mil_loss ▇▆█▇▇▆▆▅▅▆▅▅▅▅▄▃▃▄▃▄▂▂▂▂▁▂▂▂▂▃▁▁▂▁▄▄▆▁▃▁
wandb:       eval/ensemble_f1 ▂▁▁▃▂▁▃▃▃▃▄▅▅▄▅▅▅▆▆▇▆█▆▇▇▆▇▅▇▇▇▇▆▇▆▇▆██▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▄▄▄▅▆▅▆▆▅▇▆▆▆▆▆▆▇▇▇▆▇▇▇▇█▆▇▇▇▇█▇██▆███
wandb:      train/ensemble_f1 ▁▂▃▃▃▃▄▄▄▃▅▅▅▆▅▆▅▆▆▇▆▆▇▆▇▇▇█▇▇▇▇▇▇█▇▇███
wandb:         train/mil_loss █▆▇█▆█▆▆▆▆▆▄▄▄▅▄▃▃▄▃▃▃▂▂▃▂▃▂▃▃▂▁▃▂▂▂▁▂▁▂
wandb:      train/policy_loss ▃▅▄▄▄▃▄▂▄▄▄▄▁▃▃▄▄▄▄▂▄▄▄▄▃▄▄▄▄▄▄▄█▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▂▂▇▇▆▆▂▆▆▃▆▆▆▆▄▇▆▆▆▆▆▆▆▆▆▆▁▆▆▆▄▆▆▆▄█▆▅▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7641
wandb: best/eval_avg_mil_loss 0.60413
wandb:  best/eval_ensemble_f1 0.7641
wandb:            eval/avg_f1 0.72747
wandb:      eval/avg_mil_loss 0.58479
wandb:       eval/ensemble_f1 0.72747
wandb:            test/avg_f1 0.75145
wandb:      test/avg_mil_loss 0.5271
wandb:       test/ensemble_f1 0.75145
wandb:           train/avg_f1 0.76009
wandb:      train/ensemble_f1 0.76009
wandb:         train/mil_loss 0.56875
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run tough-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aor4g451
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100211-aor4g451/logs
wandb: Agent Starting Run: 22lw5ons with config:
wandb: 	actor_learning_rate: 4.459821450181388e-06
wandb: 	attention_dropout_p: 0.17653166134939907
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 79
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.28334569156907563
wandb: 	temperature: 1.0465501065161251
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100624-22lw5ons
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/22lw5ons
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss ▁▄▆█
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 ▆▃▇▄▄▅▆▂▄▇▆▆▁▆▅▆▄▅▅▆▁▁▅▆▅▄▁▁▇▅▅▅▂▆▅▄▅▅█▅
wandb:      eval/avg_mil_loss ▄▇▇▅▄▅▄▄▁▃▃▂▂▄▅▃▂▆▆▇▃▆█▄▄▂▆▇▆▅▂▁▆▃▁▂▇▂▅▂
wandb:       eval/ensemble_f1 ▆▃█▅▄▅▇▇▇▅▆▇▄▇▇▁▆▆▇▇▆▇▆▄▁▆▇▄▆▃▅▇▂▇▆▁▅▆▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅▄▆▆▄▆▆█▆▄▆▄▅█▇▄▅▅▅▅▁▅▃▁▅▇▆▂▂▅▄▆▄▅▆▆▆▅▄
wandb:      train/ensemble_f1 ▇▇▇▂▆▃▆▅▇▇▆▅▅▅▆▂▄▄█▅▆▅▆▅▁▇▆▅▇█▄▆▅▁▇▅▆▆▄▄
wandb:         train/mil_loss ▅▄▅▃▂▅▃▃▆▂▄▄█▂▅▂▄▄▂▅▃▃▁▄▂▅▄▄▃▅▃▅▃▅▅▅▃▃▂▅
wandb:      train/policy_loss █▃▅▅▅▃▁▅▅▅▆▁▁█▅▃▃▅▃▅▅▅█▅▅▁▅▅▆█▅▆▁▅▃▃▅▃█▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅▅▁▃▅▅▅▅▆▃▅▅▅▃▅█▅▅▁▁▅█▅▅▅▆▁▅▃▃█▃▃▅█▃█▅█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77443
wandb: best/eval_avg_mil_loss 0.98538
wandb:  best/eval_ensemble_f1 0.77443
wandb:            eval/avg_f1 0.56167
wandb:      eval/avg_mil_loss 1.00949
wandb:       eval/ensemble_f1 0.56167
wandb:            test/avg_f1 0.54379
wandb:      test/avg_mil_loss 1.03357
wandb:       test/ensemble_f1 0.54379
wandb:           train/avg_f1 0.56794
wandb:      train/ensemble_f1 0.56794
wandb:         train/mil_loss 1.0703
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run easy-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/22lw5ons
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100624-22lw5ons/logs
wandb: Agent Starting Run: wo3fdx0q with config:
wandb: 	actor_learning_rate: 5.17797322880416e-05
wandb: 	attention_dropout_p: 0.05240537638145765
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 69
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8950853210188011
wandb: 	temperature: 3.156762757153694
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100757-wo3fdx0q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wo3fdx0q
wandb: uploading history steps 55-68, summary; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss █▁▁
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 ▁█▅▄▂▂▅▃▃▆▂▂▃▂▃▃▂▃▄▃▄▁▂▂▂▃▃▂▁▅█▄▃▂▃▇█▄▂▃
wandb:      eval/avg_mil_loss ▇▆▃▄▇▄▅▄▂▅▇▆▃▄▆▄▄▄▅▄▅▃▇▄▃▂▂▄▁▄█▂▅▅▇▂█▆▅▃
wandb:       eval/ensemble_f1 ▁█▁▆▄▃▂▃▄▃▂▂▄▃▃▂▃▂▃▄▄▂▂▂▃▁▅▂▃▁█▃▄▂▃▅▃▄█▃
wandb:           train/avg_f1 ▅▃▁▆▄▅▇▄▅█▄▄▆▅▇▅▆▅▆▆▄▄▃▄▃▃▇▆▇▆▅▅▆▇█▇▄▅▄▇
wandb:      train/ensemble_f1 ▃▆▅▁▅▆▅█▄▄▅▅▇▄▅▆▅▅▄▄▄▆█▅▇▇▆▅▅▆▅▅█▄▇▇▅▄▇█
wandb:         train/mil_loss ▆▇▆▆▄▄▃▂▅▇▅▃▄▅▃▄▆▄▃█▆▄▃▃▁▃▅▄▅▂▁▅▃▂▄▄▅▂▄▂
wandb:      train/policy_loss ▅▅▅▃▅▅▃▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▂▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▅██████████████████████████████▁███
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78649
wandb: best/eval_avg_mil_loss 1.00507
wandb:  best/eval_ensemble_f1 0.78649
wandb:            eval/avg_f1 0.59394
wandb:      eval/avg_mil_loss 0.86418
wandb:       eval/ensemble_f1 0.59394
wandb:           train/avg_f1 0.63156
wandb:      train/ensemble_f1 0.63156
wandb:         train/mil_loss 0.8331
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run whole-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wo3fdx0q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100757-wo3fdx0q/logs
wandb: ERROR Run wo3fdx0q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 76l4i06o with config:
wandb: 	actor_learning_rate: 0.0008817483315671197
wandb: 	attention_dropout_p: 0.31914139552335596
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 83
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4312980041339616
wandb: 	temperature: 0.17991670590482456
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100921-76l4i06o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/76l4i06o
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▅▆▇█
wandb: best/eval_avg_mil_loss ▇▄▃▄▁█▂
wandb:  best/eval_ensemble_f1 ▁▁▄▅▆▇█
wandb:            eval/avg_f1 ▄▁▄▄▄▄▄▄▆▄▅▄▇▆▄▅▇▄▆▅▆▄▅▆▅▃▃▄▅▇▅▅▇▅▄▅███▆
wandb:      eval/avg_mil_loss ▇▇▄▅▄▃▃▅▅▃▇▄▁▃▇▃▇▆█▅▆▃▇▆▂▇▅█▄▅▇▃▃▂▅▄▄▂▁▄
wandb:       eval/ensemble_f1 ▄▁▄▄▄▄▄▅▄▇▄▄▆▅▄▆▅▄▅▅▆▅▅▆▃▃▄▆▅▅▄▅▅▆▆███▆▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▆▄▁▃▃▃▂▄▁▅▄▇▄▄▅▅▅▅▂▄▄▅▅▅▇▆▇▅▅▇▆▆▆▄▇▇▇▆█
wandb:      train/ensemble_f1 ▄▆▁▃▂▁▄▃▃▂▄▃▅▄▄▆▅▃▂▆▅▆▆▅▅▅▇▄▅▅▆▆▆▆▄▇▇▇▅█
wandb:         train/mil_loss ▆▄▇▅▇▆▃▇▄▆▅▇▅▆▆▆▆▅▅▆▄▅▅▆▃█▂▅▅▄▂▄▂▄▁▇▄▁▃▄
wandb:      train/policy_loss ▅▅▅█▅▅▅▅▅▅▅▇▅▁▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅▇▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅█▅▅▅▅▅▇▅▅▅▆▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.71847
wandb: best/eval_avg_mil_loss 0.81858
wandb:  best/eval_ensemble_f1 0.71847
wandb:            eval/avg_f1 0.61665
wandb:      eval/avg_mil_loss 0.85955
wandb:       eval/ensemble_f1 0.61665
wandb:            test/avg_f1 0.65051
wandb:      test/avg_mil_loss 0.62246
wandb:       test/ensemble_f1 0.65051
wandb:           train/avg_f1 0.67088
wandb:      train/ensemble_f1 0.67088
wandb:         train/mil_loss 0.69717
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fluent-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/76l4i06o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100921-76l4i06o/logs
wandb: Agent Starting Run: th2ec5h8 with config:
wandb: 	actor_learning_rate: 0.00015831497676235058
wandb: 	attention_dropout_p: 0.39184270339945304
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 101
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.34051967122088267
wandb: 	temperature: 1.9071523752938944
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101059-th2ec5h8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/th2ec5h8
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▄██
wandb: best/eval_avg_mil_loss ▁▃▁█▂▄▃
wandb:  best/eval_ensemble_f1 ▁▂▂▃▄██
wandb:            eval/avg_f1 ▅▅▅▄▅▅▆▁█▄▄▅▄▅▅▅▅▅▅▂▂▄▄█▅▅▅▆▅▅▅▅▅▅▅▃▆▆▆▅
wandb:      eval/avg_mil_loss ▅▄▄▅█▄▄▅▅▆▁▇▄▄▃▆▇▄▄▅▅▄▃▅▄▃▆▅▇▅▄▄▂▅▄▄▄▁▇▅
wandb:       eval/ensemble_f1 ▄▄▁▄▅▅▅▄▆▄▁▆▅▅▄▄▁▄▄█▆▅▃▅▄▅▅▄▆▅▆▆▅▄▆▆▂▆▁▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▂▅▃▆▃▇▅▅▇▃▄▃▅▇▆▄▃▆▆▁▄▅▇▇▅▇▅▅██▆▅▇▇▇█▇▆▇
wandb:      train/ensemble_f1 ▂▃▄▅▂▅▇▆▄▃▆▅▁▅▄▄▅▅▆▇▅▅▅▅▅█▅█▆▅▅▆█▇▇▇▇█▆▇
wandb:         train/mil_loss ▅▅▄▄██▅▆▃▄▇▂█▆▆▆▅▄▅▄▆▆▃▅▄▃▂▅▅▄▅▅█▂▂▅▃▅▁▁
wandb:      train/policy_loss ▁▇▅█████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▇▅██████████████████████▇██████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74684
wandb: best/eval_avg_mil_loss 0.98602
wandb:  best/eval_ensemble_f1 0.74684
wandb:            eval/avg_f1 0.58809
wandb:      eval/avg_mil_loss 0.98279
wandb:       eval/ensemble_f1 0.58809
wandb:            test/avg_f1 0.54745
wandb:      test/avg_mil_loss 0.58708
wandb:       test/ensemble_f1 0.54745
wandb:           train/avg_f1 0.62406
wandb:      train/ensemble_f1 0.62406
wandb:         train/mil_loss 0.78659
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fresh-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/th2ec5h8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101059-th2ec5h8/logs
wandb: Agent Starting Run: lm92krkm with config:
wandb: 	actor_learning_rate: 5.811517423595428e-05
wandb: 	attention_dropout_p: 0.39593681655695057
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5972251448453461
wandb: 	temperature: 7.935357273651864
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101258-lm92krkm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lm92krkm
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▇▇▇█
wandb: best/eval_avg_mil_loss ██▅█▄▃▂▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▇▇▇█
wandb:            eval/avg_f1 ▂▅▂▃▂▁▃▃▂▂▅▃▂▄▃▄▃▁▃▄▆▃▄▅▅▄▅▄▃▆▅▄▆▇▄▄▆▇▅█
wandb:      eval/avg_mil_loss ▄▄▄▄▄▄▆▄▅▃▅▃▃▃▅▃▃▃▃▃▃▃▃█▅▂▃▃▂▃▅▅▃▂▂▃█▁▄▁
wandb:       eval/ensemble_f1 ▁▃▃▄▃▃▂▃▁▃▆▃▄▆▄▅▅▇▄▄▆▅▇▅▅▅▇▆▇▆▇▇▆█▇▇█▇▇▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▂▂▃▃▄▃▃▃▃▄▄▅▅▆▅▅▅▅▆▇▅▆▆▆▇▆▆▆▆▆▇▇▇▇▇██▇
wandb:      train/ensemble_f1 ▁▁▂▁▁▁▁▃▃▃▃▄▄▄▄▅▅▄▄▅▄▇▆▅▆▅▆▆▆▇▇▇▇▇█▇▇█▆█
wandb:         train/mil_loss ▅▅▅▅▅▆█▆▆▆▅▅▅▅▄▃▅▃▃▃▃▆▃▃▂▄▃▃▄▄▇▂▄▂▄▃▇▂▂▁
wandb:      train/policy_loss ▁▃▆▆▆▆▆▃▆▆▆▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▂▆▆▆▆▆▆▆▆▆▆█▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁███████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74559
wandb: best/eval_avg_mil_loss 0.62636
wandb:  best/eval_ensemble_f1 0.74559
wandb:            eval/avg_f1 0.6858
wandb:      eval/avg_mil_loss 0.85493
wandb:       eval/ensemble_f1 0.6858
wandb:            test/avg_f1 0.74243
wandb:      test/avg_mil_loss 0.52692
wandb:       test/ensemble_f1 0.74243
wandb:           train/avg_f1 0.71173
wandb:      train/ensemble_f1 0.71173
wandb:         train/mil_loss 0.62155
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run blooming-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lm92krkm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101258-lm92krkm/logs
wandb: Agent Starting Run: frjgpc1p with config:
wandb: 	actor_learning_rate: 0.00013654830006410289
wandb: 	attention_dropout_p: 0.1644646655841045
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 165
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5372934480379667
wandb: 	temperature: 6.359807617048218
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101635-frjgpc1p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b47tidcn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/frjgpc1p
wandb: uploading history steps 165-166, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇▇███
wandb: best/eval_avg_mil_loss █▄▂▃▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇▇███
wandb:            eval/avg_f1 ▇▄▆▅█▆▆▆▆▅▆▄▇▇▁▅▆▆▇█▇▆▆█▇▂▆▅▂▇▆▃▆█▆▅▇▆▆▇
wandb:      eval/avg_mil_loss ▅▅▅▄▃▁▅▄▃▆▅▃▁▂▄▂▄▃▃▅▅▅▄▄▃█▄▆▅▄▄▃▃▄▂▂▄▄▂▁
wandb:       eval/ensemble_f1 ▁▇▅▅▆▅▅▅▇▅▆▆▆▆█▆▅▂█▆▇▇▄▅▅▆▇▂▅█▇▆▅▆▅▆▆▇▇▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▃▆▆▄▇▅▆▆█▄▅▆▃▅▅▅▃▆▆▅▅▇▆█▆▄▅▇▅▅▆▄▄▄▅▅▁▅▅
wandb:      train/ensemble_f1 ▇▇▅▃▆▇▇▄▇▄▅▄▆▇▄▄▅▅█▇▅▄▆▇▄▅▆▆▅▅▇▆▄█▆▅▆▅▁▅
wandb:         train/mil_loss ▃█▆▆▇▅▇▅▇▆▅▅▄▃▇█▆▆▃▆▆▇▄▅▄▅▆▄▂▂▂▄▃▆█▁▃▅▃▅
wandb:      train/policy_loss ▁██▃████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▁████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80047
wandb: best/eval_avg_mil_loss 0.82341
wandb:  best/eval_ensemble_f1 0.80047
wandb:            eval/avg_f1 0.57961
wandb:      eval/avg_mil_loss 0.78084
wandb:       eval/ensemble_f1 0.57961
wandb:            test/avg_f1 0.69104
wandb:      test/avg_mil_loss 0.59887
wandb:       test/ensemble_f1 0.69104
wandb:           train/avg_f1 0.63647
wandb:      train/ensemble_f1 0.63647
wandb:         train/mil_loss 1.0499
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fast-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/frjgpc1p
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101635-frjgpc1p/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 8xy37jb7 with config:
wandb: 	actor_learning_rate: 0.00020311083779030905
wandb: 	attention_dropout_p: 0.33910018273922893
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 106
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9072957464043128
wandb: 	temperature: 1.2277324820491742
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102018-8xy37jb7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8xy37jb7
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▇▇███
wandb: best/eval_avg_mil_loss ▆█▃▄▃▃▃▁
wandb:  best/eval_ensemble_f1 ▁▁▅▇▇███
wandb:            eval/avg_f1 ▄▄▇█▇▂▄█▇▆██▃█▂█▇▂▃▁▄▄█▇█▄▆▂██▄▆▁▂▇▁▂▂▁█
wandb:      eval/avg_mil_loss ▃▅▆▄▃▄▃█▃▆▇▁█▃▆▆▃▃▄▄▄▆▅▅▃▄▅▅█▃▆▆▆▁▅▄▆▄▃▄
wandb:       eval/ensemble_f1 ▇▄▇▇▆▄▇▆▂▇▂▁▇▃▃█▄▂▂▁▂▄▁▇█▇▄▇▇▆▂▂█▇▁▁▂▂▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▆▇▂▅▃██▆▁▅▅▆▆▂▅▄▃▆▃█▅▃▆▆▆▃▅▂▇▄█▄▇▅▆▃▇▆▅
wandb:      train/ensemble_f1 ▅▄▅▃█▇▆▇▆▅▆▅▇▆▆▅█▅▅▅▆▇█▆▄▆▆▅▅▄▄█▅▅▆▆▁▃█▅
wandb:         train/mil_loss ▄▃▃▁▅▃▄▃▄▆▆▆▅▆▄▃▃▅▄▇▃▅▇▅▂▆▄▁█▅▃▃▇▄▄▅▂▂▂▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77957
wandb: best/eval_avg_mil_loss 0.67757
wandb:  best/eval_ensemble_f1 0.77957
wandb:            eval/avg_f1 0.75488
wandb:      eval/avg_mil_loss 0.8145
wandb:       eval/ensemble_f1 0.75488
wandb:            test/avg_f1 0.29016
wandb:      test/avg_mil_loss 1.18298
wandb:       test/ensemble_f1 0.29016
wandb:           train/avg_f1 0.56129
wandb:      train/ensemble_f1 0.56129
wandb:         train/mil_loss 0.82421
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run serene-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8xy37jb7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102018-8xy37jb7/logs
wandb: Agent Starting Run: cw993lup with config:
wandb: 	actor_learning_rate: 1.893631630209292e-06
wandb: 	attention_dropout_p: 0.21591566646620064
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8771605705265287
wandb: 	temperature: 2.6726849020722887
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102212-cw993lup
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cw993lup
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆█
wandb: best/eval_avg_mil_loss ███▁▄
wandb:  best/eval_ensemble_f1 ▁▄▆▆█
wandb:            eval/avg_f1 ▅▅█▆▅▅▆▇▇█▅▆▇▄▅▆▆▅▆▇▅▄▅▆▇▇▄▄▁▆▆▅▁▇▅▇█▇▇▆
wandb:      eval/avg_mil_loss ▄▅▄▅▅▃▅▄▃▇▅▄▇█▅▆▇▆▆█▂▄▆▆▄▃▁▄▄▁█▂▅▄▄▃▂▄▂▅
wandb:       eval/ensemble_f1 ▅▅█▆▇▆▅▆▇█▆▇▆▆▇▄▇▆▅▇▁▅▇▇▄▄▄▄▇▆▆▅▁▆▇▇▅█▅▇
wandb:           train/avg_f1 ▃▅▆▅▂▅▅▄▅▃▅▆▆▇▃▅▆▅▆▅▅▅▃▃▃▄▂▅▄▄▁▄▄▆▆▃▇█▅▅
wandb:      train/ensemble_f1 ▂▆▁▇▂▆▅▂▃▂▃▃▃▄▄▂▆▆▆▂▅▆▃▃▆▄▁▅▃▄▂▅▄█▆█▆▆▃▄
wandb:         train/mil_loss ▄▄▅▄▅▆▇▇▅▅▂█▄▂▇▅▅▄▅▅▄▃▃▆▅▅▇▁▃▅▅▄▄▄▅▅▄▆▄█
wandb:      train/policy_loss ▁▁█▅▅▅▁█▅▅▁▅▁▅▅▅▅▅▁▅▁▅▅▅█▅▅▅▁▁▅▅▁▁▅█▅█▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄██▁▄▄▄▁▁▄▁▄▄▄█▄▄▄▄█▄▆▄▄▁▄▄▄▄▄▁█▄▄▄▄█▄▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7504
wandb: best/eval_avg_mil_loss 0.68252
wandb:  best/eval_ensemble_f1 0.7504
wandb:            eval/avg_f1 0.73027
wandb:      eval/avg_mil_loss 0.75062
wandb:       eval/ensemble_f1 0.73027
wandb:           train/avg_f1 0.6454
wandb:      train/ensemble_f1 0.6454
wandb:         train/mil_loss 0.74013
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run woven-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cw993lup
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102212-cw993lup/logs
wandb: ERROR Run cw993lup errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jcnkjsxe with config:
wandb: 	actor_learning_rate: 0.0005680480528232748
wandb: 	attention_dropout_p: 0.4349626880982901
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.831123323424398
wandb: 	temperature: 6.742113577677533
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102421-jcnkjsxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jcnkjsxe
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▅▅▆▇▇█
wandb: best/eval_avg_mil_loss ▇▇█▅▂▁▁▇▁
wandb:  best/eval_ensemble_f1 ▁▃▃▅▅▆▇▇█
wandb:            eval/avg_f1 ▇▇▆█▁▇▇█▆▇▆▇█▆█▇▆█▇▇▇▅▅▄▇▇▇▇█▇▆▇▇▆▇▇▄█▇▇
wandb:      eval/avg_mil_loss ▆▅▆▆▆█▅▆▆▆▅▆▆▄▅▆▄▆▆▆▅▃▅▅▅▃▅▅▁▅▅▆▆▁▅▂▅▃▅▄
wandb:       eval/ensemble_f1 ▇█▁▇▇▇▇▇▇▄▇▅▇▇▇▇███▆▇▇▅▄▇▇█▇▇▂▇▇███▇▇▇▇▃
wandb:           train/avg_f1 ▇▄▆▄▅▆▆▆▅▅██▄▄▇▄▅▃▆▇▃▃▅▆▅▄▄▄▇▄▆▆█▃▇▅█▁▇▂
wandb:      train/ensemble_f1 ▅▆▆▃▄▆▃▄▄▆▆▇▅▅▄▅▂▆▃▄▄▆▄▇▃▅▆▅▅▇▆█▄▇▃▁▆▃▄▆
wandb:         train/mil_loss ▆▄▃▃▂▄▅▃▂▃▄▆▇▃▂▆▃▂█▇▄▄▂▆▄▄▆▄▄▂▂▁▅▃▂▂▂▅▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81824
wandb: best/eval_avg_mil_loss 0.4775
wandb:  best/eval_ensemble_f1 0.81824
wandb:            eval/avg_f1 0.50888
wandb:      eval/avg_mil_loss 0.9676
wandb:       eval/ensemble_f1 0.50888
wandb:           train/avg_f1 0.74377
wandb:      train/ensemble_f1 0.74377
wandb:         train/mil_loss 0.68308
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rural-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jcnkjsxe
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102421-jcnkjsxe/logs
wandb: ERROR Run jcnkjsxe errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: mid3z3f1 with config:
wandb: 	actor_learning_rate: 0.0004224427707664127
wandb: 	attention_dropout_p: 0.3654207629456081
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 66
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7965465166014982
wandb: 	temperature: 0.24889894312098315
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102748-mid3z3f1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mid3z3f1
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 61-67, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▇█
wandb: best/eval_avg_mil_loss █▃▄▃▁
wandb:  best/eval_ensemble_f1 ▁▆▆▇█
wandb:            eval/avg_f1 ▇█▄▄▅▇▆█▇▇█▄▇▇▇▄▇██▇▇▇▇▇██▇▇▇▆▄▇▄▇▇█▇▁▇▆
wandb:      eval/avg_mil_loss ▅█▅▆▇▂▂▂▅▂▃▅▅▇▂▅▂▃▄▂▁▅▂▆▂▄▃▂▄▂▇▄▅▄▆▅▁▃█▄
wandb:       eval/ensemble_f1 ▇█▄▄▄██▇█▇█▆▄▇▄▇▁▁▇█▇▇▇▇███▇▇▇▆▇▄▇▇█▇▇▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▅▅▇▅▆▄▅▄▅▅▄▁▄▅▆▅▆█▂▆▇▂▇▅▅█▅▅▅▇▇▄▆▆▆▆▄▅
wandb:      train/ensemble_f1 ▅▄▅▅▅▆▅▄▃▅▄█▄▆▁▆▂▆▄▄▇▆▄▅▇▃▇▆▅▅▄▅▇▄▆▆▆▆▄▄
wandb:         train/mil_loss ▇██▆▆▅▆▇▅▂▇▇▅▇▂▅▅▅▆▄▆▂▂▆▇▆▂▅▅▅▃▁▇▆▂▄▄▅▇▅
wandb:      train/policy_loss █████████████████████████████▁██████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████████████▁██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80628
wandb: best/eval_avg_mil_loss 0.60595
wandb:  best/eval_ensemble_f1 0.80628
wandb:            eval/avg_f1 0.71164
wandb:      eval/avg_mil_loss 0.89758
wandb:       eval/ensemble_f1 0.71164
wandb:            test/avg_f1 0.67486
wandb:      test/avg_mil_loss 0.57557
wandb:       test/ensemble_f1 0.67486
wandb:           train/avg_f1 0.72507
wandb:      train/ensemble_f1 0.72507
wandb:         train/mil_loss 0.76817
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lively-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mid3z3f1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102748-mid3z3f1/logs
wandb: Agent Starting Run: slrg5qyl with config:
wandb: 	actor_learning_rate: 6.338237322641096e-05
wandb: 	attention_dropout_p: 0.2877706760176988
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9361427344323094
wandb: 	temperature: 3.216494228313681
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102900-slrg5qyl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/slrg5qyl
wandb: uploading wandb-summary.json
wandb: uploading history steps 76-90, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▁▆▂
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▄▁▇▄▃▄▅▄▄▅█▄▄▄▄▇▄▄▅▂▅▄▂▄█▄▆▅▇▄▅▅▇▂▆▇▅▄▂▅
wandb:      eval/avg_mil_loss ▅▆▅▅▃▃▂▆▆▄▃▁▄▂▄▃▂▄▅▆▂▄▃▂▆▅▂█▃▄▄▂▂▆▁▄▂▃▂▄
wandb:       eval/ensemble_f1 ▇▅▆▄▁▇▄▄▃▆█▅██▃█▄█▂▄▇▄▂▅▇▅▆▄▇▅▅▇▅▅▃▇▆▁▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▂▄▂▄▅▃▂▆▆▅▃▆▄▂█▃▆▄▃▃▂▂▇▅▆▁▃▅▅▂▂▅▄▄▄▇▆▃
wandb:      train/ensemble_f1 ▅▆▆▆▂█▂▄▅▃▇▃▄█▅▇▄▅▆▂▃▄▆▁▄▄▆█▆▂▃▅▄▅▅▄▅█▄▄
wandb:         train/mil_loss ▅▂▃▄▅▅▅▂▂▃▂▅▄▅▁▃▃█▃▃▄▄▁▅▃▄▃▃▅▃▄▃▆▄▆▄▇▄▂▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78695
wandb: best/eval_avg_mil_loss 0.62593
wandb:  best/eval_ensemble_f1 0.78695
wandb:            eval/avg_f1 0.55534
wandb:      eval/avg_mil_loss 0.92564
wandb:       eval/ensemble_f1 0.55534
wandb:            test/avg_f1 0.82526
wandb:      test/avg_mil_loss 0.65193
wandb:       test/ensemble_f1 0.82526
wandb:           train/avg_f1 0.60614
wandb:      train/ensemble_f1 0.60614
wandb:         train/mil_loss 0.75848
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run likely-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/slrg5qyl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102900-slrg5qyl/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: a36mq69l with config:
wandb: 	actor_learning_rate: 1.1389240205608054e-06
wandb: 	attention_dropout_p: 0.3963841618632497
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 93
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7512265660344656
wandb: 	temperature: 9.27225327120913
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103044-a36mq69l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a36mq69l
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▇▇▇▇██
wandb: best/eval_avg_mil_loss ██▄▃▁▃▄▄
wandb:  best/eval_ensemble_f1 ▁▃▇▇▇▇██
wandb:            eval/avg_f1 ▇▇▇▇▆▆▇▄▄▇▇▇▇▇▇▆▄▆▇▇▇▄▇▇▇█▇▄▇▇▆▅▃▁█▆▄▄▆▇
wandb:      eval/avg_mil_loss ▅▅▃▅█▃▇▃▂▃▆▆▃▂▇▄▇▃▅▃▃▃▃▆▃█▄▂▁▄▃▆▆▇▃▇▂▆▆▃
wandb:       eval/ensemble_f1 ▄▅▇▅▇▇▆██▇▇▇▇▄▇▅▅▇█▇▇█▇██▅▅▇▇█▅▁▇▁█▄▇▇█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅█▅▃▆▄▄▃▆▃▆▄▂▃▄▅▇▅▁▅▅▅▇▇▅▇▄▄▄▇▆▅▅▆▆▆▅▄▇▅
wandb:      train/ensemble_f1 ▇▅▅▅▃▄▅▅▄▃▄█▂▅▄▅▄▅▆▆▅▄▅▃▃▆▄▁▆▃▄▅▅▅▇▄▅▅▄▄
wandb:         train/mil_loss ▄▇▂▅▅▄▆▄▁▄▃▆▅▇▄▅▅▆▄▅▄█▆▆▆▄▅▅▅▃▄▅▅▃▄▃▅▄▇▆
wandb:      train/policy_loss ▄▄▄▁▄▁▄▄▄▄▆▁▄▄█▄▄▄▁▄▄▄▄▁█▄▄▄▁█▄▄▁▄██▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▁▄▁▄▄▄▁▄▄█▄▄▄▁▄██▄▄▄▄▄█▁█▄▄▁▄▄▄█▄█▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78162
wandb: best/eval_avg_mil_loss 0.79629
wandb:  best/eval_ensemble_f1 0.78162
wandb:            eval/avg_f1 0.6383
wandb:      eval/avg_mil_loss 0.87586
wandb:       eval/ensemble_f1 0.6383
wandb:            test/avg_f1 0.79791
wandb:      test/avg_mil_loss 0.42809
wandb:       test/ensemble_f1 0.79791
wandb:           train/avg_f1 0.69368
wandb:      train/ensemble_f1 0.69368
wandb:         train/mil_loss 0.72742
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a36mq69l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103044-a36mq69l/logs
wandb: Agent Starting Run: a4gw1lba with config:
wandb: 	actor_learning_rate: 2.682785858931994e-06
wandb: 	attention_dropout_p: 0.25917499839834296
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 180
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.15603559906809228
wandb: 	temperature: 1.6176800453267104
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103222-a4gw1lba
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a4gw1lba
wandb: uploading history steps 167-180, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅▆▆█
wandb: best/eval_avg_mil_loss █▆▅▅▅▄▁
wandb:  best/eval_ensemble_f1 ▁▃▄▅▆▆█
wandb:            eval/avg_f1 ▄▄▄▅▄▆▅▄▅▄▄▆▄▄▄▄█▃▆▄▇▃▆▆▄▅▇▃▆▃▆▄▅▆▄▆▆▁▃▄
wandb:      eval/avg_mil_loss ▆▆▂▄▄▆▅▆▅▅▇▆▂▇▆▇▆▆▇▃▆▆▇▂▇▄▇▇▂▄▇▃▄▄▅▁▅▅█▂
wandb:       eval/ensemble_f1 ▅▅▅▅▅▅▄▆▇▇▅▁█▅▇▅▅▇▅▅▄▅▅▅▆▄▇▅▁▅▆█▇▄▁▆▅█▇▅
wandb:           train/avg_f1 ▂▂▁▅▄▃▄▄▆▅▃▂▁▂▆▅▆▃▅▃▅▁▅▆▃█▅▄▃▄▃▄▆▅▅▅▃▆▁▃
wandb:      train/ensemble_f1 ▅▃▄▃▇▄▅▃▄▄▄▂▄▇▂▃▄▆▆▁▄▇▆▅▆▅▆▄█▄▄▄▃▃▄▅▄▄▇▆
wandb:         train/mil_loss ▅▅▅▄▅▄▃▄▃▃▄▅▅▄▅▂▄▂▃▅▃▁▂▆▄▅▅▃▂▃▁█▃▃▄▃▃▁▄▅
wandb:      train/policy_loss ▄███▁▄█▄█▄██▄▄▄▄▄▄▄▄▄▁█▄▃█▄▄▄▄▄▁▄▄█▄▁▆▄▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁▅▅▅█▁▅▁▅▁▅▅█▅▁█▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79577
wandb: best/eval_avg_mil_loss 0.64621
wandb:  best/eval_ensemble_f1 0.79577
wandb:            eval/avg_f1 0.56785
wandb:      eval/avg_mil_loss 0.98194
wandb:       eval/ensemble_f1 0.56785
wandb:           train/avg_f1 0.63406
wandb:      train/ensemble_f1 0.63406
wandb:         train/mil_loss 0.84669
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wobbly-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a4gw1lba
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103222-a4gw1lba/logs
wandb: ERROR Run a4gw1lba errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: sqbcexpv with config:
wandb: 	actor_learning_rate: 0.0004367040989125103
wandb: 	attention_dropout_p: 0.3841427345701155
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8039484354491085
wandb: 	temperature: 3.4319020168379257
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103605-sqbcexpv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sqbcexpv
wandb: uploading wandb-summary.json
wandb: uploading history steps 74-88, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss ██▇▁
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ▆▇▇▅▇▄▆▇▇▄▆▇▂▇█▇█▇▇▆▇▁█▅▇▇▇▁█▇▆▇▇▇▂▁▆▂▇▆
wandb:      eval/avg_mil_loss ▆▆▃▅▅▇▅▅▂▅▇▃▄▄▆█▅▅▆▂▄▄▄▄█▅▄▅▁▅▇▄▆▄▂▂█▅▇▂
wandb:       eval/ensemble_f1 ▆█▇▃▄▇▄▁▇█▇▇▁▅▇▄▁▇█▃▇▄▇▆▇▇▁█▆▇▇█▇▇▇▁▇▇▁▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▅▅▅▃▄▅▇▂▅▅▇▆▄█▁▅▆▆▄▄▅▆▆▅▂▇▁▃▆▄▄▄▄▄▄▃▄▅▄
wandb:      train/ensemble_f1 ▁▂▃▆▅▄▃▆▄▆▇▂▆▇▆▆▆▁▇▄▆▅▃▅▆▅▁▂▆▅▅▄▇▃▄█▅▄▃▁
wandb:         train/mil_loss ▅▄▃▆▃▃▅▄▅▃▁▄▂▃▃▃▃▆▄█▆▄▃▅▄▂▄▆▄▂▁▄▅▃▅▃▃▄▄▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80127
wandb: best/eval_avg_mil_loss 0.61182
wandb:  best/eval_ensemble_f1 0.80127
wandb:            eval/avg_f1 0.6516
wandb:      eval/avg_mil_loss 1.00329
wandb:       eval/ensemble_f1 0.6516
wandb:            test/avg_f1 0.76277
wandb:      test/avg_mil_loss 0.53452
wandb:       test/ensemble_f1 0.76277
wandb:           train/avg_f1 0.6753
wandb:      train/ensemble_f1 0.6753
wandb:         train/mil_loss 0.70634
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glorious-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sqbcexpv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103605-sqbcexpv/logs
wandb: Agent Starting Run: kahyebl5 with config:
wandb: 	actor_learning_rate: 0.0006125561113135179
wandb: 	attention_dropout_p: 0.06533753799707309
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 75
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.894747562107777
wandb: 	temperature: 2.8042131623730726
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103742-kahyebl5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kahyebl5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▆█
wandb: best/eval_avg_mil_loss █▄▆▅▁
wandb:  best/eval_ensemble_f1 ▁▃▄▆█
wandb:            eval/avg_f1 ▆▃▇▄▅▄▄▇▃▅▇█▇▅▆▅▄▆▆▆▁██▄▄▃▁▇▅▇█▆▇▅▆▅▆▆▅▇
wandb:      eval/avg_mil_loss ▄▆▃▆▅▆▃▂▂▅▅▄▂▆▆▃▃▂▁▂▂▄▆█▄▆▂▃▂▂▂▁▂▃▅▅▄▃▄▁
wandb:       eval/ensemble_f1 ▆▆▄▅▄▆▄▇▇▇█▇▆▆▅▆▄▄▆▇▇▆█▄▇▅▄▅▁▁▇▅▅▆▇▄▆▅▅▇
wandb:           train/avg_f1 ▂▂▄▁▃▄▄▂▆▄▃▅▅▄▆▄▁▅▅▆▃█▅▄▇▅▅█▇▄█▆▆▆▆▅▅▆▅▂
wandb:      train/ensemble_f1 ▁▁▄▃▄▆▆▅▃▄▅▄▃▄▃▄▂▆██▄▂▆▄▄▇▇▄▅▇▆▆▅▂▄▄▃▁▅█
wandb:         train/mil_loss ▄▅▃▄▄▂▂▂▂▄▃▃▃▃▄▃▄▃█▂▂▃▃▄▁▁▅▃▂▃▂▂▁▂▁▂▁▂▃▁
wandb:      train/policy_loss █▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████▁████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7712
wandb: best/eval_avg_mil_loss 0.67532
wandb:  best/eval_ensemble_f1 0.7712
wandb:            eval/avg_f1 0.74429
wandb:      eval/avg_mil_loss 0.65602
wandb:       eval/ensemble_f1 0.74429
wandb:           train/avg_f1 0.74995
wandb:      train/ensemble_f1 0.74995
wandb:         train/mil_loss 0.66913
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run avid-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kahyebl5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103742-kahyebl5/logs
wandb: ERROR Run kahyebl5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 08ya5gsp with config:
wandb: 	actor_learning_rate: 3.168213634149289e-05
wandb: 	attention_dropout_p: 0.4303203524861222
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 158
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5582455201494523
wandb: 	temperature: 5.424779417423371
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103919-08ya5gsp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/08ya5gsp
wandb: uploading wandb-summary.json
wandb: uploading history steps 149-159, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▆▇▇██
wandb: best/eval_avg_mil_loss ▄▃█▅▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▅▆▇▇██
wandb:            eval/avg_f1 ▅▂▅▄▃▄▂▃▂▅▅▅▃▄▄▂▇▃▃▅▃▄▅▃▂▇▃▅▅▃▆▅▆▄▄▃▁▄█▃
wandb:      eval/avg_mil_loss ▆▆█▇▃▃▂▇▆▇▆▇▇▆▃▅▆▁▆▆▆▇▆▁▅▇▆▃▆▆▇▆▆▇▅▅▄▇▇▇
wandb:       eval/ensemble_f1 ▇▆█▆▇▆▇▆▅▇▇▅▁▆▇▇▇▅▆▅▆█▇▂▆▅▆▅▇▆▆▇▆▆▇▇█▆▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▅▂▇▁▄▄▄▅▂▄█▆▅▄▅▅▆▆▄▄▃▄▆▅▄▄▃█▄▆▇▄▄▃▅▆█▃
wandb:      train/ensemble_f1 ▆▇▃▆▂▃▆▃▄█▆▅▄▆▅▆▃▄▅▂▃▅▂▄▁▄▁█▄▅▆▄▂▄▄▆█▆▆▃
wandb:         train/mil_loss ▄▅▃▃▂▅▄▁▄▃▃▂▄▃▂▅▃▅▁▃▄▆▁▃▄▅▂▄▃▄▂▇▂▂▃▁▂█▂▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78916
wandb: best/eval_avg_mil_loss 0.85344
wandb:  best/eval_ensemble_f1 0.78916
wandb:            eval/avg_f1 0.75792
wandb:      eval/avg_mil_loss 0.89262
wandb:       eval/ensemble_f1 0.75792
wandb:            test/avg_f1 0.82996
wandb:      test/avg_mil_loss 0.29193
wandb:       test/ensemble_f1 0.82996
wandb:           train/avg_f1 0.72831
wandb:      train/ensemble_f1 0.72831
wandb:         train/mil_loss 0.69059
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pretty-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/08ya5gsp
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103919-08ya5gsp/logs
wandb: Agent Starting Run: elzooic9 with config:
wandb: 	actor_learning_rate: 0.0009477663284922084
wandb: 	attention_dropout_p: 0.3087695026447319
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 85
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7503315518219895
wandb: 	temperature: 8.090694317856492
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104204-elzooic9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/elzooic9
wandb: uploading wandb-summary.json
wandb: uploading history steps 75-85, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▁▅
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▇▇▇▇▅▇█▇▇▇▁▆▁█▇██▇▆▇▁▇▆▇▇▄▆▇▇▇▇▆▇▇▇▇▇█▇▇
wandb:      eval/avg_mil_loss ▂▄▅▅▄▅▄▄▆▂▆▅▂█▅▅▄▅▄▅▃▁▄▅▅▅▆▄▅▅▇▅▇▅▅▅▅▄▄▅
wandb:       eval/ensemble_f1 ██▇█▅▁▇▇▇▂▂▇▇▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇
wandb:           train/avg_f1 ▄▅▇▄▆▆▆▄▂▇▆▆▂▃▂▅▄▆▅▃▆▆▃▅▇▅▆▄▅▄▅▁▄▂▄▅▅▅▅█
wandb:      train/ensemble_f1 ▅▆▇▂▅▆▇▆▅▇▁▇▆▇▇▃▅▆▆▄▅▆▇▅▇█▅▄▆▆▆▃▅▅▃▃▆▆▅█
wandb:         train/mil_loss ▅▅▆▅▅▆▅▅▇▃▄▅▆▅▅▅▄▃▆▃▃▃▅▄▆▄▆▃▃▅▂▇▅▁▄▄▄█▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77954
wandb: best/eval_avg_mil_loss 0.7365
wandb:  best/eval_ensemble_f1 0.77954
wandb:            eval/avg_f1 0.72477
wandb:      eval/avg_mil_loss 0.87478
wandb:       eval/ensemble_f1 0.72477
wandb:           train/avg_f1 0.70682
wandb:      train/ensemble_f1 0.70682
wandb:         train/mil_loss 0.67262
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run clean-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/elzooic9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104204-elzooic9/logs
wandb: ERROR Run elzooic9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 37zl5i7t with config:
wandb: 	actor_learning_rate: 0.0002193306352939097
wandb: 	attention_dropout_p: 0.47669989731942886
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 55
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2633206155444541
wandb: 	temperature: 0.35399717848255796
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104338-37zl5i7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/37zl5i7t
wandb: uploading wandb-summary.json
wandb: uploading history steps 45-55, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▆█
wandb: best/eval_avg_mil_loss █▃▁▂▄
wandb:  best/eval_ensemble_f1 ▁▆▆▆█
wandb:            eval/avg_f1 ▁▆▆▆▆▆▆▆▂▅▆█▆▇▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅
wandb:      eval/avg_mil_loss █▁▂▃▄▃▃▂▂▅▄▂▃▂▃▂▃▃█▂▃▂▁▃▃▂▂▃▃▂▁▂▇▃▃▂▂▁▂█
wandb:       eval/ensemble_f1 ▁▆▆▆▆▆▆▂▅▆█▆▇▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▇▆▅
wandb:           train/avg_f1 ▅▁▅▆▄▂▇▄▃▆▂▇▆▅▇▄▄▅▄▅▂▂▂▆▅▂█▇▆▅▆▅▆▂▂▅▇▃▇▄
wandb:      train/ensemble_f1 ▅▁▅▃▆▄▂▇▆▃▅▆▆▅▇▆▄▄▅▄▅▂▂▆▅▄▂█▇▆▅▆▅▂▂▅▇▃▇▄
wandb:         train/mil_loss ▂▃▄▃▅▅▂▃▅▃▃▃▄▂▄▄█▁▂▁▃▄▅▃▁▆▂▃▃▄▃▄▄▄▄▃▄▂▂▇
wandb:      train/policy_loss ████████▁███████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79228
wandb: best/eval_avg_mil_loss 0.95659
wandb:  best/eval_ensemble_f1 0.79228
wandb:            eval/avg_f1 0.68706
wandb:      eval/avg_mil_loss 1.11978
wandb:       eval/ensemble_f1 0.68706
wandb:           train/avg_f1 0.69616
wandb:      train/ensemble_f1 0.69616
wandb:         train/mil_loss 0.75512
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run leafy-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/37zl5i7t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104338-37zl5i7t/logs
wandb: ERROR Run 37zl5i7t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: yvxdc87v with config:
wandb: 	actor_learning_rate: 6.647197319805467e-06
wandb: 	attention_dropout_p: 0.357592953288732
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 105
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6526721734793772
wandb: 	temperature: 8.246322742944146
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104452-yvxdc87v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yvxdc87v
wandb: uploading history steps 102-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▅▆█
wandb: best/eval_avg_mil_loss █▄▁▅▄▄
wandb:  best/eval_ensemble_f1 ▁▄▅▅▆█
wandb:            eval/avg_f1 ▅▆▅▇▇▄▃▃▇▇▇▇▃▂▆█▅▆▆▃▆▇▂▃█▆▆▇▁█▇▆▇▇▂▆▇▇▇▆
wandb:      eval/avg_mil_loss ▂▃▄▂▆▁▄▅▁▅▇▄▆▅▂▃▃▂▄█▂▇▁█▄▄▃▃▃▄▃▄▄▃▃▃▃▃▃▃
wandb:       eval/ensemble_f1 ▆▄▅▆▄▅▇▇▆▆▅▄▆▆▂▆▂▅▂▅▆▅▁▃▅▇▇▆▆▅▃▆▇▁▆▃▂█▆▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▅▄▃▄▄▃▃▃▃▄▄▂▄▅▄▅▄▄▅▅▁▆▆▇▂▄▆▇▂▅█▅▃▄█▅▇▄▆
wandb:      train/ensemble_f1 ▂▂▆▃▅▂▅▂▆▆▄▁▅▄▄▄▅▃▅▃▂▅▅▇▃▇▁▄▅▃▃▄▃█▄▅█▃▃▇
wandb:         train/mil_loss ▄█▂▆▅▆▄▅▂▃▃▄▅▃▅▄▃▄▆▄▄▃▅▅▅▅▃▂▃▃▅▂▅▂▅▂▃▁▅▃
wandb:      train/policy_loss ▁▅▅▁▁▅▅▅▅▅▁▅█▅▁▅▅▁▅█▁▅▅▅█▁▅█▅▅██▅▅▅▁▁▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7776
wandb: best/eval_avg_mil_loss 0.72623
wandb:  best/eval_ensemble_f1 0.7776
wandb:            eval/avg_f1 0.7144
wandb:      eval/avg_mil_loss 0.76886
wandb:       eval/ensemble_f1 0.7144
wandb:            test/avg_f1 0.64936
wandb:      test/avg_mil_loss 0.67986
wandb:       test/ensemble_f1 0.64936
wandb:           train/avg_f1 0.72557
wandb:      train/ensemble_f1 0.72557
wandb:         train/mil_loss 0.71718
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run soft-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yvxdc87v
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104452-yvxdc87v/logs
wandb: Agent Starting Run: 3203csj2 with config:
wandb: 	actor_learning_rate: 6.582833837666234e-06
wandb: 	attention_dropout_p: 0.45108032396366365
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 90
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5269390404489835
wandb: 	temperature: 9.969550142987758
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104645-3203csj2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3203csj2
wandb: uploading history steps 90-91, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▅█
wandb: best/eval_avg_mil_loss █▇▆▁▄
wandb:  best/eval_ensemble_f1 ▁▃▅▅█
wandb:            eval/avg_f1 ▆▃▅▇▇▇▇▆▄▇▆▆▇▆▇▆▇▄▇▇▇▇▆█▇▅▆▆▁▇▇▇▇▄▇▇▇▇▇▅
wandb:      eval/avg_mil_loss ▆▄▄▄▅▃▃▅▄▄▄▃▃▄▄▃▄▄▂▅▅▂▁▄█▄▄▂▂▅▄▇▃▄▂▃▂▃▅▂
wandb:       eval/ensemble_f1 ▄▇▇▇▇▇▅▇▄▄▇▇▇▅▆▇▇▇▇▇▇▇▅▇▁▇█▄▇▇▇▇▄▇█▇▇▇▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▁▄▄▄▅▅▄▅▂▄▅▅▅▃▆▄▄▇▅▅▅▅▆▅▄▄▃▃▃▄▄▄▃▄▄▅▃█▄
wandb:      train/ensemble_f1 ▄▄▃▂▄▂▃▄▄▄▁▃▄▃▅▅▃▄▆▆▅▅▄▅▃▃▄▃▄▃▃▄▃▂▃▄▄▂█▅
wandb:         train/mil_loss ▄█▃▄▄▄▄▄▄▇▃▃▅▆▂▃▅▃▄▇▄▃▄▄▅▄▂▄▁▇▄▃▂▄▄▃▄▅▃▄
wandb:      train/policy_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄█▄▄▄▄▄█▁███▁▄▄███▄▁▄▁▄▄▁▄█▁▄▄▄█▁██▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75828
wandb: best/eval_avg_mil_loss 0.76745
wandb:  best/eval_ensemble_f1 0.75828
wandb:            eval/avg_f1 0.64198
wandb:      eval/avg_mil_loss 0.84227
wandb:       eval/ensemble_f1 0.64198
wandb:            test/avg_f1 0.76702
wandb:      test/avg_mil_loss 0.54998
wandb:       test/ensemble_f1 0.76702
wandb:           train/avg_f1 0.66927
wandb:      train/ensemble_f1 0.66927
wandb:         train/mil_loss 0.73957
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hardy-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3203csj2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104645-3203csj2/logs
wandb: Agent Starting Run: 3yktrwa7 with config:
wandb: 	actor_learning_rate: 1.7733408616203395e-05
wandb: 	attention_dropout_p: 0.4039972306121519
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 80
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5039796831676452
wandb: 	temperature: 6.014999978392242
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104824-3yktrwa7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3yktrwa7
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃█
wandb: best/eval_avg_mil_loss █▇▅▁
wandb:  best/eval_ensemble_f1 ▁▂▃█
wandb:            eval/avg_f1 █▄▁▁▅█▃█▁█▇▅▄▁▂▅█▁▅▂██▁▄▄▄▁▄▁▂▂█▇▇▂▂▇█▇▁
wandb:      eval/avg_mil_loss ▄▅▇▅▄▇▄▇▄▇▅▆▆▇▇▄▅▄▆▅▄▄▇▃█▆▆▆▅▄▄▆▇▅▄▄▄▁▇▄
wandb:       eval/ensemble_f1 ▄▅▄▇▁▂▂▇▁▅▁▇██▇██▅▅██▁██▁▇▄▁█▇█▇▇▁▂█▇▇▁▇
wandb:           train/avg_f1 ▃▆▆▆▄▅▄▃▇▃▃▅▇▆▇▆▄▆▃▅▆▆▅▅▃█▇▄▅▇▄▂▁▆▆▄▅▆▂▄
wandb:      train/ensemble_f1 ▆▅▇▄█▅▃▇▄▆▅▆▄▁█▇▆▅▆▆▃██▃▆▅▄▄▂▄▆▆▆▂▆▂▆▅▄▅
wandb:         train/mil_loss ▅▃▅▆▅█▄▃▆▃▆▄▅▂█▃█▄▃▆▄▃▄▁▅▇▄▄▅▅▄▆▄▆▇▂▆▅▅▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77766
wandb: best/eval_avg_mil_loss 0.81862
wandb:  best/eval_ensemble_f1 0.77766
wandb:            eval/avg_f1 0.69957
wandb:      eval/avg_mil_loss 0.85721
wandb:       eval/ensemble_f1 0.69957
wandb:           train/avg_f1 0.5759
wandb:      train/ensemble_f1 0.5759
wandb:         train/mil_loss 0.78496
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run avid-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3yktrwa7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104824-3yktrwa7/logs
wandb: ERROR Run 3yktrwa7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: fa3xi77n with config:
wandb: 	actor_learning_rate: 2.8331744367150142e-06
wandb: 	attention_dropout_p: 0.4320334194958862
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 75
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7213400578215255
wandb: 	temperature: 9.345691965274527
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104951-fa3xi77n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fa3xi77n
wandb: uploading history steps 75-75, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅█
wandb: best/eval_avg_mil_loss █▄▁▄▄
wandb:  best/eval_ensemble_f1 ▁▃▄▅█
wandb:            eval/avg_f1 ▅▁▃▂▂▂▃▂▄▂▆▃▂▃▃▃▃▃▂▅▂▁▅▅▅▅▆▄▄▃▂▃▅▅█▄▁▆▃▁
wandb:      eval/avg_mil_loss ▇▇▃▆▄▄█▄▄▆▃▅▄▂▅▁▅▅▃▆▃▄▅▃▄▄▆▄▅▄▇▂▅▅▃▃▅▂▃▅
wandb:       eval/ensemble_f1 ▆▄▂▇▇▄▅▂▅▅▇▇▃▂▃▄▃▄▂▃▅▆▆▂▆█▄▃▁▃▆▆▂▃▃▆▄▁▇▅
wandb:           train/avg_f1 ▃▄▄▅▅▄▃▇▄▃▄▂▄▅▅▄▅▂▅▅▅▁▆▅▄▃▆▃▅▄▇▂▅▅▃█▅▃▇▆
wandb:      train/ensemble_f1 ▃▄▄▂▄▄▄▂▄▄▅▄▅▅▄▂▅▁▆▆▆▄▄▅▆▇▅▄▅▇▅▅▅▃▄█▅▃▆▆
wandb:         train/mil_loss ▆▆▄▄█▃▅▅█▇▃▄▅▄▄▇▄▅▅▃▃▄▅▃▂▄▄▃▄▄▄▃▅▄▂▁▁▄▂▃
wandb:      train/policy_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77789
wandb: best/eval_avg_mil_loss 0.76451
wandb:  best/eval_ensemble_f1 0.77789
wandb:            eval/avg_f1 0.69712
wandb:      eval/avg_mil_loss 0.79502
wandb:       eval/ensemble_f1 0.69712
wandb:           train/avg_f1 0.73748
wandb:      train/ensemble_f1 0.73748
wandb:         train/mil_loss 0.63746
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devout-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fa3xi77n
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104951-fa3xi77n/logs
wandb: ERROR Run fa3xi77n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 29b67lri with config:
wandb: 	actor_learning_rate: 0.0002723474076462765
wandb: 	attention_dropout_p: 0.15545012925158902
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2144981128108097
wandb: 	temperature: 5.801022212779532
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_105141-29b67lri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/29b67lri
wandb: uploading wandb-summary.json
wandb: uploading history steps 104-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▁▇▇██
wandb: best/eval_avg_mil_loss █▅▄▁▃▁▂
wandb:  best/eval_ensemble_f1 ▁▁▁▇▇██
wandb:            eval/avg_f1 ▅▄▂█▅▄█▅█▇▄▇▅▆▄▅▁▄▇▄█▄▄▄▅▅▄▇▄▅▅▁▅▇█▆▁▄▄▄
wandb:      eval/avg_mil_loss ▆▄▂▅▃▃▃▄▆▃▃▃▃▁▂▃▃▄▄▃▃▃▄▂▇▃▇▃▃▃▃▄▃▃▃▅█▃▃▅
wandb:       eval/ensemble_f1 ▁▄▆▄▅▄▅▄▅▄▆▄▄▇█▄▁▄▄█▁▅▁▄▄█▄▄▁▅▅██▄▅▂▄▄▄▄
wandb:           train/avg_f1 ▂▂█▄▄▆▅▆▃▄▃▅█▃▆▄▆▃▆▅▂▄▅▃▄▄▆▄▁▄▃▅▆▂▆▃▄▄▇▅
wandb:      train/ensemble_f1 ▄▅▃▄▆█▆▃▄▂▂▂▅▃▅▅▄▂▆▄▄▃▄▁▆▆▄▄▄▃▅▂▁▄▅▃▂▂▆▄
wandb:         train/mil_loss ▅▃▃▆▅▃▆▃▇▅▃▄█▄▄▁▃▂▃▄▂▂▂▄▅▆▇▅▅▄▇▃▆▇▅▆▄▃▄█
wandb:      train/policy_loss ██████████████████████████████████████▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████████████▁██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79179
wandb: best/eval_avg_mil_loss 0.80175
wandb:  best/eval_ensemble_f1 0.79179
wandb:            eval/avg_f1 0.50172
wandb:      eval/avg_mil_loss 0.97772
wandb:       eval/ensemble_f1 0.50172
wandb:           train/avg_f1 0.60906
wandb:      train/ensemble_f1 0.60906
wandb:         train/mil_loss 0.81593
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run daily-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/29b67lri
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_105141-29b67lri/logs
wandb: ERROR Run 29b67lri errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 16nl1pre with config:
wandb: 	actor_learning_rate: 0.0001847199042644929
wandb: 	attention_dropout_p: 0.017846898657085997
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 195
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6228159471511661
wandb: 	temperature: 7.0396713822486525
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_105343-16nl1pre
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/16nl1pre
wandb: uploading history steps 190-195, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▆▆▆▆▇▇█
wandb: best/eval_avg_mil_loss ▄█▆▅▅▄▅▅▁▁
wandb:  best/eval_ensemble_f1 ▁▅▅▆▆▆▆▇▇█
wandb:            eval/avg_f1 ▂▆▅▆▁▆▆▆▅▅▂▆█▃▃▆█▅▆▆▂▁▅▆▆▃▇▇▇▄▄▇▅▅▆▅▅▆█▆
wandb:      eval/avg_mil_loss ▄▆▆▆▇▅▄▃▆▂▆▄▇▅▄▄▅▅▃▇▅▅▄▅▁▅▆▅▄▄█▄▅▄▅▁▄▄▅▄
wandb:       eval/ensemble_f1 ▆▆▆▅▅▁▃▆▆▄▇▇▇▂▃▂█▅▁▆▃▅▆▇█▆▅▆▆▆▇▆▂▇▆▅▆▆▆▁
wandb:           train/avg_f1 ▅▄▄▁▄▂▁▆▅▁▃▃▅▄▄▄▅▄▁▅▅▆▆▅▅▆▂▆▃▃▅▆▆▅▆█▄▆▄▆
wandb:      train/ensemble_f1 ▄▆▅█▅▃▃▁▅▆▅▆▅▄▆▄▆▅▆▆██▅▆▆▃▆▅▇▇▇▇▄▆▅▇▅▇▃█
wandb:         train/mil_loss █▄█▄▆▅▅▅▃▄▄▃▆▂▆▂█▆▃▆▃▄▄▇▂▅▃▇▄▅▃▅▄▄▄▁▄▂▄▃
wandb:      train/policy_loss ███████████████████████████████▁████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8098
wandb: best/eval_avg_mil_loss 0.60202
wandb:  best/eval_ensemble_f1 0.8098
wandb:            eval/avg_f1 0.61377
wandb:      eval/avg_mil_loss 0.8538
wandb:       eval/ensemble_f1 0.61377
wandb:           train/avg_f1 0.73762
wandb:      train/ensemble_f1 0.73762
wandb:         train/mil_loss 0.6349
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run honest-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/16nl1pre
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_105343-16nl1pre/logs
wandb: ERROR Run 16nl1pre errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 99t22fu2 with config:
wandb: 	actor_learning_rate: 0.00015161804055415298
wandb: 	attention_dropout_p: 0.10029349807739818
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 57
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8791125131063192
wandb: 	temperature: 9.398760283427768
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_105709-99t22fu2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/99t22fu2
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss █▅▁
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▄▇▄█▁▇▇▄▄▁▇▁▄█▇▆▄▃▇▇▇▄▇▇▇▇▇▇▇▇▁█▇▁█▇▄▃▁▁
wandb:      eval/avg_mil_loss ▅▁▅▅▆▄██▅▅▅▇▄▅▇▅▅▇▆▃▄▂▄▃▆▅▂▄▅▄▅█▆█▃▇▄▇█▇
wandb:       eval/ensemble_f1 █▄▇▄▁▇▇▄▄▁▇▁▇█▇▇▄▃▇▇▇▄▇▇▇▇▇▇▇▇█▁█▇▁▄▄▄▁▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅▆█▆▄▅▅▅▆▃▃▅▇█▅▆▆▅▃▅▄▁▆▇▄▃▇▃▃▇▇▃▅▆▂▆▄▄▇
wandb:      train/ensemble_f1 ▄▇▅▆█▆▄▄▅▅▃▃▅▃▆█▄▅▆▅▃▆▂▄▁▄▇▅▄▇▅▇▂▅▆▆▄▄▄▇
wandb:         train/mil_loss ▆▃▆▆▄▅▄▅█▁█▄▃▅▃▆▆▇█▅▅▃▅▃▇▂▇▇█▄▅▅▅▆▇▄▄▆▃▁
wandb:      train/policy_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78007
wandb: best/eval_avg_mil_loss 0.76348
wandb:  best/eval_ensemble_f1 0.78007
wandb:            eval/avg_f1 0.35969
wandb:      eval/avg_mil_loss 1.02826
wandb:       eval/ensemble_f1 0.35969
wandb:            test/avg_f1 0.76822
wandb:      test/avg_mil_loss 0.53072
wandb:       test/ensemble_f1 0.76822
wandb:           train/avg_f1 0.71262
wandb:      train/ensemble_f1 0.71262
wandb:         train/mil_loss 0.62624
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/99t22fu2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_105709-99t22fu2/logs
wandb: Agent Starting Run: qr48gkjp with config:
wandb: 	actor_learning_rate: 1.8679134987148124e-06
wandb: 	attention_dropout_p: 0.002820885993351563
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 154
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5083562040663996
wandb: 	temperature: 1.1409474016829135
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_105816-qr48gkjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qr48gkjp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▃▆█
wandb: best/eval_avg_mil_loss ▆█▃▂▁▂
wandb:  best/eval_ensemble_f1 ▁▁▃▃▆█
wandb:            eval/avg_f1 ▇█▅▃▅▃▅▆▅▇▆▃▅█▅▄▇▅▂▅▅▆▆▂▅█▁▄▅▅▁█▅▅▇▄█▄▅▆
wandb:      eval/avg_mil_loss ▅█▅▂█▅██▅▇▆▁▅▅█▃▃▇▄▄▃▃▅▄▅█▄▅▂▇▆▁▆▄▇▄▇▅█▅
wandb:       eval/ensemble_f1 ▅▄▄▆▃▆▃▅▆▆▂▅▃▅▄▃▅▃▆▅▅▄▆▅▆▆▄▃▄▃▆▅▆▅▆▄█▃▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▅▅▆▆▄▅▅▄▆▄▅▆▅▅▃▅▅█▅▅▄▄▄▅▆▄▃▃▇▆▇▆▇▄▆▇▄▆▅
wandb:      train/ensemble_f1 ▂▅▂▄▃▅▃▂▄▆▄▄▁▄▄█▃▂▇▄▄▃▃▆▅▂▆▃▅▅▇▂▆▃▇▄▅▃▃▃
wandb:         train/mil_loss ▆█▄▇▃▆▅▅▅▄█▅▅▅▆▅▇▆▅▄▃▆▄▇▁▄▆▂▅▃▆▃▂▂▄▃▂▄▃▂
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▁▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████▆██████████████▁██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80277
wandb: best/eval_avg_mil_loss 0.68339
wandb:  best/eval_ensemble_f1 0.80277
wandb:            eval/avg_f1 0.71405
wandb:      eval/avg_mil_loss 0.75619
wandb:       eval/ensemble_f1 0.71405
wandb:            test/avg_f1 0.7779
wandb:      test/avg_mil_loss 0.53531
wandb:       test/ensemble_f1 0.7779
wandb:           train/avg_f1 0.67463
wandb:      train/ensemble_f1 0.67463
wandb:         train/mil_loss 0.72234
wandb:      train/policy_loss -0.03993
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.03993
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fast-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qr48gkjp
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_105816-qr48gkjp/logs
wandb: Agent Starting Run: 7bxxwcu6 with config:
wandb: 	actor_learning_rate: 4.77650094846435e-05
wandb: 	attention_dropout_p: 0.14110969556243663
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 148
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6434362303385125
wandb: 	temperature: 7.986073943723348
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_110133-7bxxwcu6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7bxxwcu6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss █▆█▁
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ▅▄▄▆▅▆▄▇▄▇▆▆▄▆▅▄▇▁▄▆▆▇▄▅▆▃▃█▅▇▆█▆▅▅█▃▄▅█
wandb:      eval/avg_mil_loss ▅▄▄▃▅▇▄▃█▄▂▃▅▃▃▆▇▇▆▅▄▄▆▄▁▅▂▅▂▂▅▂▃▂▄▂▅▂▄▂
wandb:       eval/ensemble_f1 ▄▄▆▆▆▇▆▃▄▇▇▃▄▆▂▂▇▁▅▂▃▆▆▇█▃▅█▅▅▆█▄▆▄▂▃▄▅▆
wandb:           train/avg_f1 ▄▄▅▄▃▅▃▃▄▄▃▁▄▄▄▂▁▅█▅▃▇▆▃▅▃▅▅▅▇▄▅▆▆▄▄▂▄▇▂
wandb:      train/ensemble_f1 ▆▄▁▆▂▆▄▃▃▅▄▄▆▁▅▁▆▆▆▇▆█▆▃▅▆▃▇▂▄█▇▄▅▄▂▆▅▆▃
wandb:         train/mil_loss █▅▅▅▅▆▄▇▄▇▆█▆▇▆▃▄▅▆▇▆▅▅▅▂▆▅▄▅▃▄▂▁▃▂▂▂▄▃▁
wandb:      train/policy_loss ▂▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▁▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.82306
wandb: best/eval_avg_mil_loss 0.5621
wandb:  best/eval_ensemble_f1 0.82306
wandb:            eval/avg_f1 0.79946
wandb:      eval/avg_mil_loss 0.55801
wandb:       eval/ensemble_f1 0.79946
wandb:           train/avg_f1 0.79226
wandb:      train/ensemble_f1 0.79226
wandb:         train/mil_loss 0.61737
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run summer-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7bxxwcu6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_110133-7bxxwcu6/logs
wandb: ERROR Run 7bxxwcu6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: cs3jbakd with config:
wandb: 	actor_learning_rate: 8.234288622922646e-05
wandb: 	attention_dropout_p: 0.422386011144781
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 128
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8123784290659486
wandb: 	temperature: 8.930691431827082
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_110444-cs3jbakd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cs3jbakd
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 115-128, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss ▂█▆▄▁
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▇▃▆▁▄▄▄▇▄▇█▇▇▅▇▇█▆▄▇▁▇▇▁▄▇▇▄▁▇▄▁▇▇▇▆▇█▅▇
wandb:      eval/avg_mil_loss ▄▆▂█▇▄▅▃▆▄▄▅▇▅▄▅▂▂▃▃▄▁▄▅▆▄▅▃▅▄▆▂▆▁▄▂▃▅▄▃
wandb:       eval/ensemble_f1 ▇▄▇▃▃▄▄▇▄▇█▄▇▄▅▄▄▃▇▅▇▁█▇▇▄▇▄▃▇▇▅▇▇▇▁▅▁▅▁
wandb:           train/avg_f1 ▆▅▄▅▃▇▃▄▆▆▁▆▆▇▇▅█▅▇▄▅▃▆▇▅▅▇▅▇▆▄▆▅▃▅▇▅▄▃▅
wandb:      train/ensemble_f1 ▄█▂▆▅▅▃▅▅▄▆▆▆▇▅▄▇▇▄▅▅▄▂▅▅▅▁▆▅▄█▅▄▅▅▄▅▅▄▄
wandb:         train/mil_loss ▅▁▃▅▅▄▅▆▅▄▂▄▃▅▁▃▃▂▄▃▂▃▄▅▆▃▄█▅▆▄▅▄▄▃▅▇▇▂▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78135
wandb: best/eval_avg_mil_loss 0.67702
wandb:  best/eval_ensemble_f1 0.78135
wandb:            eval/avg_f1 0.69289
wandb:      eval/avg_mil_loss 0.75342
wandb:       eval/ensemble_f1 0.69289
wandb:           train/avg_f1 0.62978
wandb:      train/ensemble_f1 0.62978
wandb:         train/mil_loss 0.79199
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run comic-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cs3jbakd
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_110444-cs3jbakd/logs
wandb: ERROR Run cs3jbakd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: p0mlbumn with config:
wandb: 	actor_learning_rate: 0.000320513317001505
wandb: 	attention_dropout_p: 0.3946167510670654
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.42608753290153223
wandb: 	temperature: 9.88560765970126
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_110704-p0mlbumn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p0mlbumn
wandb: uploading history steps 85-93, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆▇▇██
wandb: best/eval_avg_mil_loss █▄▄▃▂▂▁▁
wandb:  best/eval_ensemble_f1 ▁▄▆▆▇▇██
wandb:            eval/avg_f1 ▄▃▄▃▅▇▇▅▄▆▄▅▂▄▃▃█▅▁▄▅▂▃▄▅▂▂▄▂▄▅█▆▅▃▅▃▄▅▄
wandb:      eval/avg_mil_loss ▄▄▆▄▃▄▄▃▃▇▂▄▅▂▄▆▅▅▂▂▆█▄▆▅▆▄▅▅▃▇▃▃▂▃▅▂▄▆▁
wandb:       eval/ensemble_f1 ▅▄▇▃▅▇▅▃▅▅▄▅▅▃█▅▆▆▄█▃▄▆▅▅▃▃▃▄▇▇▅▆▆▅▁▇▄▅▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▂▂▇▃▄▅▄▂▆▄▆▅▃▃▆▃▄▅▄▇▁▂▁▄▇█▄▄▄▄▅▁▃▆▄▂█▂▃
wandb:      train/ensemble_f1 ▅▄▃█▇▇▅▄▅▄▅▇▄▄▄▅▂▄▁▅▅▆▇▆█▅▅▅▅▆▆▄▆▇▇▆▅▇█▄
wandb:         train/mil_loss ▅▆▄▄▃▅▃▆█▅▅▄▄▅▅▄▇▂▄▂▅▂▆▃▄▄▃▃▃▆▄▆▃▆▃▁▁▃▄▅
wandb:      train/policy_loss █▁▄█▁█▁▁█▁▁██▁▁██▁▄▁▁▁▄▄▄▁█████▄█▄█▄▄▄▄▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄█▁▁█▁█▄▁█▁▄██▄▁▁▁▄▁▄▄▄████████▁█▁▁▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77348
wandb: best/eval_avg_mil_loss 0.68426
wandb:  best/eval_ensemble_f1 0.77348
wandb:            eval/avg_f1 0.72741
wandb:      eval/avg_mil_loss 0.64318
wandb:       eval/ensemble_f1 0.72741
wandb:            test/avg_f1 0.80937
wandb:      test/avg_mil_loss 0.43623
wandb:       test/ensemble_f1 0.80937
wandb:           train/avg_f1 0.66329
wandb:      train/ensemble_f1 0.66329
wandb:         train/mil_loss 0.75904
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rare-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p0mlbumn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_110704-p0mlbumn/logs
wandb: Agent Starting Run: yvbs1rzk with config:
wandb: 	actor_learning_rate: 4.131872460124541e-05
wandb: 	attention_dropout_p: 0.4964179025130905
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 146
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.30951588291326737
wandb: 	temperature: 9.76081140810511
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_110903-yvbs1rzk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yvbs1rzk
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▇▇█
wandb: best/eval_avg_mil_loss █▆▃▁▂▄▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄▇▇█
wandb:            eval/avg_f1 ▁█▆▆▆▄▂▅▄▅▅▇▃▅▅▅▃█▃▅▅▃▄▆▄▃▄▄▅▃▂▆▃▃▁▂▅▅▅▅
wandb:      eval/avg_mil_loss ▅▂▄▇▄▄▄▅▅▄▃▄▄▆▃▄▂▃▁▂▆▁▂▄▆▄▅█▃▁▅▅▁▄▅▄▄▂▂▂
wandb:       eval/ensemble_f1 ▆▃▃█▆▇▇▇▃▆▇▆▆▄▆▃▁▅▆▃▄▆▅▅▇▇▇▆▅▄▅▅▆█▅▆▆▆▆▅
wandb:           train/avg_f1 ▅▁▃▄▂▂▃▅▂▆▄▃▃▁▃▄▂▅▆▄▅▅▄▃▁▅▁▄▂▇▆█▂▅▁▃▆▅▇▄
wandb:      train/ensemble_f1 ▅▅▃▆▇▂▄▃▄▄▂▃▃▅▆▅▅▃▇▇▆▂▄▆▄▁▅▅▃▃▅▄█▄▃▆▃▅▄▅
wandb:         train/mil_loss ▆▃▂▆▄▅▅▄▄▃▄▄▃▁▃▄▁▃█▃▃▄▅▆▅▃▂▄▂▃▂▄▁▂▅▄▇▂▁▃
wandb:      train/policy_loss ▅▁▅▅▅▅▁▅▅██▁▁█▁▅█▆▅▅█▁█▅▅▁▁▁▅█▁▁█▅▁█▅▁▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████████████████████▁██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77909
wandb: best/eval_avg_mil_loss 0.70162
wandb:  best/eval_ensemble_f1 0.77909
wandb:            eval/avg_f1 0.68238
wandb:      eval/avg_mil_loss 0.80205
wandb:       eval/ensemble_f1 0.68238
wandb:           train/avg_f1 0.66157
wandb:      train/ensemble_f1 0.66157
wandb:         train/mil_loss 0.78447
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run flowing-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yvbs1rzk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_110903-yvbs1rzk/logs
wandb: ERROR Run yvbs1rzk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 12ck52xn with config:
wandb: 	actor_learning_rate: 3.806720463906307e-05
wandb: 	attention_dropout_p: 0.4151227662250063
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 60
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7111861102455009
wandb: 	temperature: 8.997523031360073
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_111209-12ck52xn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/12ck52xn
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇▇██
wandb: best/eval_avg_mil_loss ▅█▅▆▆▄▁
wandb:  best/eval_ensemble_f1 ▁▄▅▇▇██
wandb:            eval/avg_f1 ▃▅▅▅▃▆▇▆▅▅▇▃▅▁▅▅▅▃▃▄▄█▂▆▆▃▄▃▃▄▇▆▅▃▅▄▅▆▆▅
wandb:      eval/avg_mil_loss ▄▆▄▅▅▄▅▆▆▅▂▅▆▆▆▄▆▅▆▆▅██▁█▅▄▆▅▆▆▄▂▅▆▄▅▃▅▆
wandb:       eval/ensemble_f1 ▅▂▅▃▅▅▅█▇▃▅▁▅▃▅▅▅▃▃▄▄█▃▄█▆▅▆▃▃▃▃▄▇▅▅▄▄▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▄▆▄▃▆▁▆▃▃▄▆▃▄▆▄▅▅▂▇▆▄▃▄▃▆█▄▄▄▅▂▅▃▄▄▇▄▇▅
wandb:      train/ensemble_f1 ▇▃▇▆▂▄▆▄▆▃▂▇▆▂▆▁▃▇▅▄▂▅▆▄▂▂▄▅█▃█▃▄▄▂▃▇▇▃▄
wandb:         train/mil_loss ▅▂▅▆▃▇▅█▅▁▄▃▇▃▃▇▁▄▃▆▅▄▇▂▆▆▂▄▂▄▅▇▄▆▅▁▂▄▁▅
wandb:      train/policy_loss ████████████████▁███████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████▁██████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75808
wandb: best/eval_avg_mil_loss 0.54025
wandb:  best/eval_ensemble_f1 0.75808
wandb:            eval/avg_f1 0.59394
wandb:      eval/avg_mil_loss 0.95178
wandb:       eval/ensemble_f1 0.59394
wandb:            test/avg_f1 0.59671
wandb:      test/avg_mil_loss 0.80926
wandb:       test/ensemble_f1 0.59671
wandb:           train/avg_f1 0.65922
wandb:      train/ensemble_f1 0.65922
wandb:         train/mil_loss 0.86337
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run summer-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/12ck52xn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_111209-12ck52xn/logs
wandb: Agent Starting Run: j6mugzp9 with config:
wandb: 	actor_learning_rate: 0.0002628578174973789
wandb: 	attention_dropout_p: 0.07875883308117582
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 53
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4288190590676928
wandb: 	temperature: 4.776523637867456
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_111317-j6mugzp9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/j6mugzp9
wandb: uploading history steps 44-54, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▄█
wandb: best/eval_avg_mil_loss ▅▇█▁▅
wandb:  best/eval_ensemble_f1 ▁▂▂▄█
wandb:            eval/avg_f1 ▆▆▆▆▇▆▇▄▆▆▆▇▅▇▇▆▂▆▁▅▅▄▆▂▆█▆▅▅▅▃▅▃▆▃▄▇▅▅▆
wandb:      eval/avg_mil_loss ▃▅█▄▄▄▄▄▄▅▃▄▁▄▄▁▄▄▆▄▄▅▅▆▇▃▄▅▄▄▆▄▇▃▇▃▃▃▃▄
wandb:       eval/ensemble_f1 ▇▇▂▇▆▇█▆▇▄▇▆▇▇███▆▇▂▁▆▆▄▇▇▇▇▆▆▃▆▃▇▃▇▆▆▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▁▃▆▄▂▃▂▂▃▅▁▄▅▃▆▆▆▂▆▄▆▃▆▆█▃▅▇▇▇▅▄▂▃▃▆▅▆▅
wandb:      train/ensemble_f1 █▃▅▆▅▄▄▄▅▄▅▆▄▃▅▄▁▆▇▇▆▅▆▄▆▇█▄▆▇▇▆▅▅▄█▅▆▆▅
wandb:         train/mil_loss █▆▆▁▇▄▄▅▆▄▅▅▁▄▃▄▇▂▆▃▄▅▅▅▆▄▅▆▅▃▄▃▅▄▆▅▃▄▄▂
wandb:      train/policy_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78135
wandb: best/eval_avg_mil_loss 0.72979
wandb:  best/eval_ensemble_f1 0.78135
wandb:            eval/avg_f1 0.71329
wandb:      eval/avg_mil_loss 0.78075
wandb:       eval/ensemble_f1 0.71329
wandb:            test/avg_f1 0.7122
wandb:      test/avg_mil_loss 0.77374
wandb:       test/ensemble_f1 0.7122
wandb:           train/avg_f1 0.71889
wandb:      train/ensemble_f1 0.71889
wandb:         train/mil_loss 0.60559
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rich-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/j6mugzp9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_111317-j6mugzp9/logs
wandb: Agent Starting Run: 792dnpke with config:
wandb: 	actor_learning_rate: 0.00014671719315256904
wandb: 	attention_dropout_p: 0.22256959775834695
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 76
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05771697791716912
wandb: 	temperature: 7.509491939618618
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_111419-792dnpke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/792dnpke
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▇█
wandb: best/eval_avg_mil_loss █▆▆▃▁
wandb:  best/eval_ensemble_f1 ▁▃▅▇█
wandb:            eval/avg_f1 ▁▃▅▁▁▃▅▄▅▂▅▄▁▄▅▄▂▇▃▂█▅▄▅▄▃▇▆▂▅▅▃▂▃▂▅▇▆▆▆
wandb:      eval/avg_mil_loss ▅▂▆▅▄▇▅▅▄█▆▅▆▄▆▇▅▆▃▅▁▄▅▃▅▇▆▇▇▅▅▆▅▃▄▅▃▄▅▃
wandb:       eval/ensemble_f1 ▂▄▆▂█▅▅▅▃▃▄▆▂▅▅▆▆▅▄█▇▄▇▆▆▅▄█▃▅▇▄▂▄▅▁▄▅▃▅
wandb:           train/avg_f1 █▅▅▆▇█▃▃▂▁▇▃▅▇▃▃▇█▇▂▁▅▅▆▅▄▃▅▅▅▅▅▅▄▃▅█▆▄▅
wandb:      train/ensemble_f1 ▆▄▃▄▅▇▃▃▂▁▇▆▇▃▄▅▃▃▃▃▅▆▂▂▄▁▄▄▄▄▃▅▄▃█▄▆▅▃▄
wandb:         train/mil_loss ▃▄▂▃▅▃▄▃█▅▇█▄▆▃▇▁▄▄▂▅▆▅▇▂▄▃▅▃▃▄▄▃▄▄▂▅▂▆▁
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▁████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75934
wandb: best/eval_avg_mil_loss 0.6445
wandb:  best/eval_ensemble_f1 0.75934
wandb:            eval/avg_f1 0.68248
wandb:      eval/avg_mil_loss 0.77413
wandb:       eval/ensemble_f1 0.68248
wandb:           train/avg_f1 0.64089
wandb:      train/ensemble_f1 0.64089
wandb:         train/mil_loss 0.80716
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fast-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/792dnpke
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_111419-792dnpke/logs
wandb: ERROR Run 792dnpke errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jqodd6xw with config:
wandb: 	actor_learning_rate: 9.596592975233752e-05
wandb: 	attention_dropout_p: 0.08160140281063033
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 96
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2307350951078101
wandb: 	temperature: 1.107353038882929
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_111557-jqodd6xw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jqodd6xw
wandb: uploading wandb-summary.json
wandb: uploading history steps 86-96, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▅▆▆▆▇█
wandb: best/eval_avg_mil_loss ▆▃▅▆▆▆█▁▂
wandb:  best/eval_ensemble_f1 ▁▁▅▅▆▆▆▇█
wandb:            eval/avg_f1 ▇▄█▆▄▇▂▂▂▄▁▆▆▇█▂▇▇▆▇▇▁▆▃▆█▅▆▇█▁▆▆▃▂▁▆▇▇▆
wandb:      eval/avg_mil_loss ▂▁▄▅▂▃▅▄▅▅▃█▃▅▄▃▃▃▃▅▂▃▂▂▄▄▅▂▃▅▂▂▂▃▄▄▁▃▃▂
wandb:       eval/ensemble_f1 ▆▆▇▁▇▄▇▂▂▅▆▁▂▂▆▃▆▂▆▁▆▆▆▃▂▆█▇▇▇▁▆▄▆▂▁▁▆▂█
wandb:           train/avg_f1 ▆▄▄▄▁▇▃▄▅▅▃▁▃▃██▇▄▆▄▄▅▃▆▂▄▆▃▅▄▆▄▂▇▂▆▂▃▅▄
wandb:      train/ensemble_f1 ▆▅▄▃█▆▄█▆▅▃▁▄▃▆▅▇▅▅▆▅▅▆▄▄▆▆▅▇▅▇█▂▁▆▆▂▃▆▄
wandb:         train/mil_loss ▃▇▄▄▆▂▂▅▃▅▃▃██▅▄▂▃▆▅▄▂▃▄▄▇▃▅▃▅▄▄▅▄▆▅▆▄▆▁
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.81014
wandb: best/eval_avg_mil_loss 0.70157
wandb:  best/eval_ensemble_f1 0.81014
wandb:            eval/avg_f1 0.79529
wandb:      eval/avg_mil_loss 0.74952
wandb:       eval/ensemble_f1 0.79529
wandb:           train/avg_f1 0.66005
wandb:      train/ensemble_f1 0.66005
wandb:         train/mil_loss 0.84552
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glamorous-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jqodd6xw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_111557-jqodd6xw/logs
wandb: ERROR Run jqodd6xw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: l9un3429 with config:
wandb: 	actor_learning_rate: 3.907957483581186e-06
wandb: 	attention_dropout_p: 0.10443178895915356
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 76
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2547723879873809
wandb: 	temperature: 3.1688069949166744
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_111747-l9un3429
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l9un3429
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇██
wandb: best/eval_avg_mil_loss █▃▁▂▂
wandb:  best/eval_ensemble_f1 ▁▇▇██
wandb:            eval/avg_f1 ▃▇▂▅█▃▃▇▄▃▂▆▆▂▃▅▅▆▅▅▆▄▇▃▁▆█▅▅▅▂▂▃▂▇▆▅▂▃▃
wandb:      eval/avg_mil_loss ▃▁▆▃▄▂▅▅▅▂▃▃▅▃▄▂▅▃▅▄▄▄▃▅▁▂▄▄▆▃██▆▂▃▂▅▁█▂
wandb:       eval/ensemble_f1 ▃█▄▃▁▁▇▄▇▆▃▅▅█▅▄▅▆▅▄▄▁█▆▆▅▇▁▂▂▃▂▇▆▆▇▄▂▇▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▇▅▅▃▅▅▄▃▄▂█▃▁▂▄▃▆▅▄▇▆▆▄▆▄▃▅▅▅▅▅▃▆█▃▂▅▆
wandb:      train/ensemble_f1 ▅▅▆▅▅▅▅▅▄▄▄▄▆▃▇▁▂▃▂▃▆▅▅▄▆▅▄▅▅▅▂▅█▃▆▆▂▅▅▆
wandb:         train/mil_loss ▅▃▆▃█▂▅▅▅▃▆▂▃▅▂▅▅▇▅▁▄▄▅▅▃▂▅▄▄▁▄▄▃▅▅▅▄▄▃▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁█▃█████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7631
wandb: best/eval_avg_mil_loss 0.70669
wandb:  best/eval_ensemble_f1 0.7631
wandb:            eval/avg_f1 0.63739
wandb:      eval/avg_mil_loss 0.90481
wandb:       eval/ensemble_f1 0.63739
wandb:            test/avg_f1 0.62772
wandb:      test/avg_mil_loss 0.87045
wandb:       test/ensemble_f1 0.62772
wandb:           train/avg_f1 0.71227
wandb:      train/ensemble_f1 0.71227
wandb:         train/mil_loss 0.67907
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lyric-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l9un3429
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_111747-l9un3429/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 3vb10y4s with config:
wandb: 	actor_learning_rate: 0.00012126726088296412
wandb: 	attention_dropout_p: 0.2023104833388895
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.02134500401233841
wandb: 	temperature: 2.021313762645738
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_111935-3vb10y4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3vb10y4s
wandb: uploading wandb-summary.json
wandb: uploading history steps 101-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇███
wandb: best/eval_avg_mil_loss █▃▄▁▄▄
wandb:  best/eval_ensemble_f1 ▁▆▇███
wandb:            eval/avg_f1 ▇▁▆█▄▅▇▇▇█▇▇▄▁▂▃▄▄▅▄▆▃▇▄▄▄▅▆▃▆▂▅▅▄▅█▄▄█▇
wandb:      eval/avg_mil_loss ▆█▂▆▅▂▃▆▃▄▁▃▅▄▃▆▆▁▆▂▅▅▄▅▆▂▆▆▅▄▄▆▄▃▂▄▅▃▃▅
wandb:       eval/ensemble_f1 ▄▁▄▄▇▇▃▇▆▇▄▆▇▃▄▂▇▃▃▄▄▅▃▇▄▃█▃▇▅▅▇▆▅▇▇▄▇▄▁
wandb:           train/avg_f1 ▇▃▅▄▇▆▆▇▃▄▅▅▆▇▆▄▆▇▆▆▅▆▄▆▁▄▆▅▅▃▅█▅█▄▆▆▅▅▄
wandb:      train/ensemble_f1 ▅▆▅▃▂▆▄▆▆▅▃▇█▅█▇▆▆▇▇▇▄▅▁█▃▇▆▇▃▄▄▆█▅▅▇▅▄▄
wandb:         train/mil_loss ▂▅▆▂█▂▃▄▄▃▄▃▂▄▄▄▂▄█▄▄▆▂▆▅▄▄▁▂▇▂▇▄▁▅▅▇▇▄▁
wandb:      train/policy_loss ██▁███▅▅▅▅▅▆▁▅▆▃▅▅▁▁▅▅█▅▅▅▅▁▅▅▅▅██▁▅▅█▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅███▅▅█▅▅▃▁▅▅▅▅▁▅▆▅▅▅█▁▅▅▅▅█▁▅▅▅▅▁█▅▁█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7515
wandb: best/eval_avg_mil_loss 0.72418
wandb:  best/eval_ensemble_f1 0.7515
wandb:            eval/avg_f1 0.50984
wandb:      eval/avg_mil_loss 0.92304
wandb:       eval/ensemble_f1 0.50984
wandb:           train/avg_f1 0.62508
wandb:      train/ensemble_f1 0.62508
wandb:         train/mil_loss 0.7345
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3vb10y4s
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_111935-3vb10y4s/logs
wandb: ERROR Run 3vb10y4s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5eepd20f with config:
wandb: 	actor_learning_rate: 0.00011653076638057892
wandb: 	attention_dropout_p: 0.021836274727049132
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 140
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8562537897845437
wandb: 	temperature: 0.5288552494288379
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_112147-5eepd20f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5eepd20f
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▇█
wandb: best/eval_avg_mil_loss ▁███▁
wandb:  best/eval_ensemble_f1 ▁▁▄▇█
wandb:            eval/avg_f1 ▅█▅▆▄▅▄▇▅▇█▅▇▃██▇▃▅▇▇▇█▄▇▁▆▆▇▄▄▄▂▅▅█▂▁█▆
wandb:      eval/avg_mil_loss ▅▅▅▅▆▆▅▃▃▅▄▅▄▃▅█▅▃▆▇██▅▄▅▆▅▆▄▄▇▂▁▇▇▄▄▃▄▇
wandb:       eval/ensemble_f1 ▇▆██▅▆▇▅▇▆▇▅▆▅▇▇▅▅██▆▁▁▆▇▆█▆▅▅▆▇▅▅▄▆▅▇█▆
wandb:           train/avg_f1 ▃▄▅▅▄▆▇▆▃▅▆█▄▇▇▆▆▅▇▃▅▆▆▄▃▆▆▄▆▅▇▅▇▄▇▃▄▇▁▇
wandb:      train/ensemble_f1 ▅▅▄▆▄▄█▅▇▅▇▃▄▃▅▅▇▆▄▆▆▃▄▆▃▆▅▇▃▄▅▇▅▄▇▅▃▅▁▅
wandb:         train/mil_loss ▇▅▅▂▃▃▄▂▇▄▄▁▃▆▇▆▃▁▆▄▃█▅▄▅▆▁▃▂█▃▁▅▄▄▆▃▃▄▅
wandb:      train/policy_loss ▁▄▄█▄▁█▄▁▄▁▄▄▁▄▁▄▁▁▄▁▄█▄▄██▄▁█▄▁▄██▄▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▄▄▄█▁▁▄▄▁█▁▄▁▄▁█▄▄▄███▁▄█▄▄▄▄▁▄▃▆▄▁▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79648
wandb: best/eval_avg_mil_loss 0.60093
wandb:  best/eval_ensemble_f1 0.79648
wandb:            eval/avg_f1 0.64688
wandb:      eval/avg_mil_loss 0.99007
wandb:       eval/ensemble_f1 0.64688
wandb:           train/avg_f1 0.66756
wandb:      train/ensemble_f1 0.66756
wandb:         train/mil_loss 0.81009
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glorious-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5eepd20f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_112147-5eepd20f/logs
wandb: ERROR Run 5eepd20f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: tmp9gst1 with config:
wandb: 	actor_learning_rate: 1.3390030345210491e-06
wandb: 	attention_dropout_p: 0.1685925872095299
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1254837708370723
wandb: 	temperature: 5.250417727014517
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_112402-tmp9gst1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tmp9gst1
wandb: uploading history steps 120-125, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss █▂▄▁
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 ▆▆▄▄▃▂▃▂▄▂▂▆▂▂█▃▄█▄▅█▇▆▄▄▂▄▄▃▁▃▆▅▃▅▃▃▆▄▅
wandb:      eval/avg_mil_loss ▂▃▆▄▇▂▄▅▆▅▃▆▇█▄▄▃▅▆▄▅▁▃▃▆▆▅▆▅▃▅▄▆▃▂▃▄▄▅▂
wandb:       eval/ensemble_f1 ▆▆▅▁▃▃▄▂█▅▂▃▁▃▄▂▃▂▃▁▄▄▇▃▄▂▄▁▃▁▅▅▄▂▂▄▅▄▄▄
wandb:           train/avg_f1 ▆▆▄▅▃▅▁▆▇▃▃▃▆▅▅▅▅▅▅▄▆▅▄▄▄▄▆▄▄▅▃▄█▆▄▄▄▅▂▄
wandb:      train/ensemble_f1 ▆▇▅▄▇▆▂▄▃▃█▆▇▅█▅▆▇▄▅▆▃█▄▅▇▄▇▅▅▃▃▂▃▃▆▂▇▁▃
wandb:         train/mil_loss ▇█▅▃▅▃▃▅▄▅▅▆▃▆▆▁▂▅▆▅▂▂▃█▅▄▃▄▄▄▅▅▄▃▇▅▅▅▆▄
wandb:      train/policy_loss ▅▁█▁▁█▁▅▅▁▁▅▁▁▅▁▁▅██▅▅▅██▁█▁█▁▅▅▅▅▁█▅▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▁▁▅▁▅█▅▁██▅█▅▅█▅▁█▅█▅▅█▅██▁█▅▅▅▁█▁█▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.72062
wandb: best/eval_avg_mil_loss 0.6853
wandb:  best/eval_ensemble_f1 0.72062
wandb:            eval/avg_f1 0.59111
wandb:      eval/avg_mil_loss 0.75097
wandb:       eval/ensemble_f1 0.59111
wandb:           train/avg_f1 0.59405
wandb:      train/ensemble_f1 0.59405
wandb:         train/mil_loss 0.75126
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pleasant-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tmp9gst1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_112402-tmp9gst1/logs
wandb: ERROR Run tmp9gst1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: d5ribwe6 with config:
wandb: 	actor_learning_rate: 1.334224585349956e-06
wandb: 	attention_dropout_p: 0.4676989756463748
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 134
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.724170975443014
wandb: 	temperature: 8.561375129972436
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_112652-d5ribwe6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d5ribwe6
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇▇▇▇██
wandb: best/eval_avg_mil_loss █▅▃▁▅▅▄▂▂
wandb:  best/eval_ensemble_f1 ▁▇▇▇▇▇▇██
wandb:            eval/avg_f1 ██▄▅▆▅▅▅▄▅▅▇▁▃▇▄▇█▅▃▃▁▇▇▅▅▄▃▇█▇▄▇▃▄▆▆▄▁▅
wandb:      eval/avg_mil_loss ▄▄▃▄▄▄▅▅▃▇▅▃▃▁▂▄▆▃▆▅▁▄▃▇▆▃▅▄▅▅▆▂▄▁▇█▅▃▄▃
wandb:       eval/ensemble_f1 ▇▅▄▂▇▁▄▄▅▂▇▇▆▁▃▇▇▅▄▆▄▄▆▄▄▄▆▇▅▃▄▄█▄▁▄▇▄▄▅
wandb:           train/avg_f1 ▃▃▃▂▅▃▃▁▅▃▄▃▆▄▃▁▂▄▁▅▃▃▄▅▄▅▂▂▁▅▂▂▃▃▁█▆▄▆▃
wandb:      train/ensemble_f1 ▄▆▄▅▄▄▄▅▁▃▃▅▁▆▅▇▆▆▂▄▆▅▄▆▆▇▅█▂█▄▂▅▅▁▇▄▇▅▇
wandb:         train/mil_loss ▅▆▅▇▄▁▄▇▅▄█▅▄▇▆▆▅▄▅▅▅▇█▄▅▃▃▁▇▅▂▆▃▃▄▄▄▁▂▅
wandb:      train/policy_loss ▄█▁▁▄███▄▄▄▄▁█▄▄▁▁▁█▄▄▁▄▁▄▄▄▁▄▄▄█▄▄▁▄█▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁▄▁▁▆▄█▄██▄▄▄▄▄▄▄▄▁▄▁▄▄▄██▄▄▄▄▄▁█▄▄▄█▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79529
wandb: best/eval_avg_mil_loss 0.68379
wandb:  best/eval_ensemble_f1 0.79529
wandb:            eval/avg_f1 0.64917
wandb:      eval/avg_mil_loss 0.77906
wandb:       eval/ensemble_f1 0.64917
wandb:           train/avg_f1 0.6994
wandb:      train/ensemble_f1 0.6994
wandb:         train/mil_loss 0.75374
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run resilient-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d5ribwe6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_112652-d5ribwe6/logs
wandb: ERROR Run d5ribwe6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: aclc2x1t with config:
wandb: 	actor_learning_rate: 0.00011089479661490406
wandb: 	attention_dropout_p: 0.29640859009938203
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 73
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.08246133339716244
wandb: 	temperature: 7.843136826478361
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_112922-aclc2x1t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aclc2x1t
wandb: uploading history steps 72-73, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 █▄▅▃▃▆▆▅▇▄▂▆▃▆▄▇▅▃▆▅▁▆▆▇▁▇▆▄▃▄▆▆▆▅▄▆▅▆▇▅
wandb:      eval/avg_mil_loss ▅▅▅█▅▃▇█▅▇▄▆▆▅▄▄▅▄▅▂▇▅▅▄▆█▆▁▇█▅▅▆█▄▅▃▆▂▄
wandb:       eval/ensemble_f1 ▅▃▅▃▃▄▆▆█▂▆▅▄▂▄▆▇█▅▆▆▄▅▁▅▇▁▆█▆▄▅▇▅▄▅▇▅▇▆
wandb:           train/avg_f1 ▆▂▂▄▂▆▆▇▄▂▄▃▇▆▄▃▂▄▃▅▂▄▅▄▂▇▄▂▆▄▄▆█▃▁▃▆▆▄▄
wandb:      train/ensemble_f1 ▇▆▃▆▅▄▄▆▅▇▁█▅▄▅▅▄▅▅▅▅▆▄▅▆█▆▇▇▆▃▆▇▄▇▄▇▆▇▆
wandb:         train/mil_loss ▄▆▅▅▄▄▆█▇▆▅▃▃▆█▄▅▆▄▅█▃▂▄▄▃▅▃▄▂▂▅▁▂▄▄▄▅▅▂
wandb:      train/policy_loss █▅▅▁▅▁▁▅▅▁▅█▁█▅▁▁▁▁███▁▅▅▅█▁██▁▁▅▅▁▅▁▅█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁█▁▄▄▁▁▁▄█▄▁▁▁█▄█▁█▄▄█▁▄▄█▁▄▄█▁▁▁▄▁▄▄██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77766
wandb: best/eval_avg_mil_loss 0.70978
wandb:  best/eval_ensemble_f1 0.77766
wandb:            eval/avg_f1 0.7023
wandb:      eval/avg_mil_loss 0.74924
wandb:       eval/ensemble_f1 0.7023
wandb:           train/avg_f1 0.69261
wandb:      train/ensemble_f1 0.69261
wandb:         train/mil_loss 0.67975
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run toasty-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aclc2x1t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_112922-aclc2x1t/logs
wandb: ERROR Run aclc2x1t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: waz3j73h with config:
wandb: 	actor_learning_rate: 9.914492885477284e-05
wandb: 	attention_dropout_p: 0.05717764593604313
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 186
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.25417834238774883
wandb: 	temperature: 7.172871357359332
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_113101-waz3j73h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/waz3j73h
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▆█
wandb: best/eval_avg_mil_loss ▆▅▅█▄▁
wandb:  best/eval_ensemble_f1 ▁▂▃▄▆█
wandb:            eval/avg_f1 ▇▅▂▁▂▄▂▃▄▅▁▆▄▇▂▁█▄▃▃▄▅▁▄▁▄▃▂▂▇▂▅▂▅▃▅▁█▄█
wandb:      eval/avg_mil_loss ▇▃▅▄▅▇▄▇▄▅▇▇▇▅▆▇▅▆▃▇▆▁▅▇▄▃▅▄▄▅▅▅▄█▅▄▆▆▇▄
wandb:       eval/ensemble_f1 ▃▅▆▃▃▅▄▆▅▃▃▆█▃▅▄▃▃▅▅▆▃▇▄▃▅▂▄▅▄▅▂▁▆▅▅▅▅▄▅
wandb:           train/avg_f1 ▅▂▅▂▂▄▅▄▆▄▂▄▃▄▅▃▅▄▄▅▄▃▆▅▄▄▁▄▂▃█▅█▃▆▄▃▂▃▂
wandb:      train/ensemble_f1 ▄▃▆▄▃▃▅▃▃▄▅▅▅▄▅▃▃▂▄▆▅▂▃▅▃▅▁▅▂▆▆█▃█▅▅▃▄▃▄
wandb:         train/mil_loss ▅▄▅▄▅▆▆▃▆▄█▄▅▆▃▂▄▅▄▄▄▁▃▄▆▄▃▃▂▄▅▄▅▅▃▂▅▃▅▃
wandb:      train/policy_loss ▅▅▅▅▅▁▁▁█▅▅▅█▅█▁▁█▅▅▅█▅▅▅▅▁██▅▅█▅█▁█▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄████▁▄▄▄▄█▄█▄▄▁▄▁██▄▄███▄▁▁▁█▁▄▄▁▁▁▄▄▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80699
wandb: best/eval_avg_mil_loss 0.54072
wandb:  best/eval_ensemble_f1 0.80699
wandb:            eval/avg_f1 0.68038
wandb:      eval/avg_mil_loss 0.84545
wandb:       eval/ensemble_f1 0.68038
wandb:           train/avg_f1 0.65564
wandb:      train/ensemble_f1 0.65564
wandb:         train/mil_loss 0.71857
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run revived-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/waz3j73h
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_113101-waz3j73h/logs
wandb: ERROR Run waz3j73h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9dr2pmmv with config:
wandb: 	actor_learning_rate: 0.00010775080520053044
wandb: 	attention_dropout_p: 0.4414154089630698
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 148
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.28622836175001676
wandb: 	temperature: 3.482258201356472
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_113357-9dr2pmmv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9dr2pmmv
wandb: uploading history steps 144-148, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▅██
wandb: best/eval_avg_mil_loss █▄▇▆▄▅▁
wandb:  best/eval_ensemble_f1 ▁▄▄▅▅██
wandb:            eval/avg_f1 ▃▅▂▄▅▄▃▄▃▃▅▆▁▄▆▇▅▄▆▄▅█▆▇▆▄▄▃▇▄▁▃▄▄▄▃▂▄▃▅
wandb:      eval/avg_mil_loss ▁▃▅▇▃▇▄▆▂▃█▇▂▂▆▃▃▃▄▄▃▇▃▃▅▂▃▄▁▁▂▁▅▁▃▁▂▁▂▄
wandb:       eval/ensemble_f1 ▄▄▄▅▄▇▂▄▅▄▂▂▄▄▅▆▄▃▆▆▂▄▂▅▇▃▇█▆▅▁▄▃▅▅▃▃▂▃▄
wandb:           train/avg_f1 ▆▄▄▄▄▁▄▄▄▄▃▄▅▆▂▃▄▃▃▂▃▂▂▂▃▂▄▅▇▅▆▂▃▅▄█▆▆▆▄
wandb:      train/ensemble_f1 ▁▃▅▅▄▃▃▄▅▄▆▅▄▇▄▃▃▄▆▄▆▆▂▄▃▅▆▆▆█▅▅▅▇▃▇▆▇▃▄
wandb:         train/mil_loss ▇▄▄▄▂▄█▂▄▂▃▃▇▄▂▃▄▄▅▄▂▅▂▂▂▁▄▁▃▄▁▁▃▃▂▄▂▁▃▃
wandb:      train/policy_loss ███▁▄▄██▄█▁▄█▁▄▁██▄▁▁▁▄▄██▄▄█▁▄█▄█▁▁▁▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████▁██████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.74953
wandb: best/eval_avg_mil_loss 0.61409
wandb:  best/eval_ensemble_f1 0.74953
wandb:            eval/avg_f1 0.65267
wandb:      eval/avg_mil_loss 0.67723
wandb:       eval/ensemble_f1 0.65267
wandb:           train/avg_f1 0.63647
wandb:      train/ensemble_f1 0.63647
wandb:         train/mil_loss 0.85469
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run distinctive-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9dr2pmmv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_113357-9dr2pmmv/logs
wandb: ERROR Run 9dr2pmmv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ysci1wt4 with config:
wandb: 	actor_learning_rate: 1.5578598513624328e-06
wandb: 	attention_dropout_p: 0.4479274620082512
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 196
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.27385334347573564
wandb: 	temperature: 8.800491600362486
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_113708-ysci1wt4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ysci1wt4
wandb: uploading history steps 196-197, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▅▅▆▆▆▇▇▇▇█
wandb: best/eval_avg_mil_loss █▄▅▄▄▅▄▁▄▂▂▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▅▅▆▆▆▇▇▇▇█
wandb:            eval/avg_f1 ▆▅▆▃▆▄▇▄▅▄▆▄▇▇▅▅▇▃▇▇▆▆▃▁▆▃▄▆██▇▇▇▆▄▆▆▃▆▁
wandb:      eval/avg_mil_loss ▅▅▅▅▆▅▅▅▅▅▅▅▃▅▅▄▆▄▅▇▄▄▃▃▄▁▃█▃▃▅▃▄▁▃▂▃▂▂▄
wandb:       eval/ensemble_f1 ▄▆▆▆▇▇▄▇▅▇▆▄▆▆▄▅▇▂▇▇▄▇▆▂▇▆▆▇▇▆▇▄▁▃█▇█▇▇▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▄▃▄▅▆▃▄▃▆▅▄▄▄▄▇▄▃▁█▅▃▅▇▅▁▄▆▄▄▅▅▄▄▄▇▅▄▃▇
wandb:      train/ensemble_f1 ▅▃▄▁▅▂▃▂▄▄▆▂▅▅▅▄▃▃▃▃▆▅▅▄▂▃▇▆▃▇▇▆▅█▅▅▅▄▅▇
wandb:         train/mil_loss ▅▆█▇▆▄▇▅▆▃▄▄▄▄▅▄▅▄▃▅▄▆▃▄▇▄▂▅▃▄▂▂▁▄▁▃▄▂▃▂
wandb:      train/policy_loss ▁▄▄▄▄▄▄██▁▄▁▄█▄▄▁█▁██▁▄▄▄▄▄▄▄▄▄▄▄▁▁█▄▄██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▁██████████████████████████▃███████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79965
wandb: best/eval_avg_mil_loss 0.66203
wandb:  best/eval_ensemble_f1 0.79965
wandb:            eval/avg_f1 0.66316
wandb:      eval/avg_mil_loss 0.75836
wandb:       eval/ensemble_f1 0.66316
wandb:            test/avg_f1 0.76564
wandb:      test/avg_mil_loss 0.51453
wandb:       test/ensemble_f1 0.76564
wandb:           train/avg_f1 0.70443
wandb:      train/ensemble_f1 0.70443
wandb:         train/mil_loss 0.64392
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run apricot-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ysci1wt4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_113708-ysci1wt4/logs
wandb: Agent Starting Run: psqe6nnp with config:
wandb: 	actor_learning_rate: 4.520585387775661e-06
wandb: 	attention_dropout_p: 0.3625792410658736
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 119
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6225386509294787
wandb: 	temperature: 7.828541467794768
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_114045-psqe6nnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/psqe6nnp
wandb: uploading history steps 97-105, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▇█
wandb: best/eval_avg_mil_loss █▇▁▃
wandb:  best/eval_ensemble_f1 ▁▁▇█
wandb:            eval/avg_f1 ▅▇█▆▄▆▆▇▅▂▆▇▇▇▇▇▆▅▇▄▁▂▆▃▄▅▁▄▅▄▆▄▆▇▃▇▃▅▆▇
wandb:      eval/avg_mil_loss ▅▄▁▂▄▄▅▅▆▁▃▅▃▃▄▄▂▄▇▇▆▄▇▃▁█▅▅▅▆▅▄▄▄▅█▄▂▄▃
wandb:       eval/ensemble_f1 ▇▂█▃▄▆▆█▂▇▄▃▇▆▆▅█▅▂▅▅▅▅▄▄▃▁▂▆▃▇▄▄▆▄▃▅▇▄▅
wandb:           train/avg_f1 ▄▂▄▆▂▄▅▅█▅▄▇▆▁▅▇▅▄▄██▆▆▆▄▄▄▆▄▆▄█▅▄▄▆▃▅▆▃
wandb:      train/ensemble_f1 ▅▇▆▁▅▅▇▇▇▅▇▃▇▆▆█▆▇▃▅▆▅▇▅▇▅▅▇▇▅▅▅▆▄▆▆██▆▆
wandb:         train/mil_loss ▁▃▅▅▃▇▅▆▅▇▅▄▃▅▅▄▅▇▆▃▃▇▇▁▄▅▇▃▄▄▄▆▆▄█▄▇▄▄▅
wandb:      train/policy_loss ██▁█████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80203
wandb: best/eval_avg_mil_loss 0.6848
wandb:  best/eval_ensemble_f1 0.80203
wandb:            eval/avg_f1 0.73207
wandb:      eval/avg_mil_loss 0.76889
wandb:       eval/ensemble_f1 0.73207
wandb:           train/avg_f1 0.66025
wandb:      train/ensemble_f1 0.66025
wandb:         train/mil_loss 0.83585
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rose-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/psqe6nnp
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_114045-psqe6nnp/logs
wandb: ERROR Run psqe6nnp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: kgi3tggd with config:
wandb: 	actor_learning_rate: 4.858605086081431e-06
wandb: 	attention_dropout_p: 0.2513867940715429
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 76
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3060654333915255
wandb: 	temperature: 1.323962492259504
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_114246-kgi3tggd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kgi3tggd
wandb: uploading wandb-summary.json
wandb: uploading history steps 70-77, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁██
wandb: best/eval_avg_mil_loss ██▁▁
wandb:  best/eval_ensemble_f1 ▁▁██
wandb:            eval/avg_f1 ▇▂▇▅▄▇▇▇▄▇▄▁▇▄▄▄▇▂▄▇▇▄▄▇▇▇▂▅▅▅▄▇▄▄▅▇▇▄▇█
wandb:      eval/avg_mil_loss ▄█▄▃▄▅▁▃▂▁▅▇▄▆█▃▇▄▃▁▃▂▆▃▄▇▄▆▃▃▃▄▄▇▅▄▃▅▃▁
wandb:       eval/ensemble_f1 ▅█▇▄▇▇▇▇▇▅▆▄▅▇▇▄▇█▇▃▃▇▄▇▇▅▅▃▄▄▇▇▄▄▇▇▁▄█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▅▅▄▇▃▂▇▅▅▇▄▄▁▇▄▆▅▄▂▅▄▇▃▄▆█▇▄▃▆▆▄▄▄▅▄▄█
wandb:      train/ensemble_f1 ▄▁▃▄▅▅▃▆▇█▁▅▆▅▃▄▃▃▃▄▄▅▄▄▁█▃▁▆▇▅▄▁▆▆▃▃▄█▃
wandb:         train/mil_loss ▄▄▄▁▃▅▄▃▄▅▂▁▅▃▂▂▆▇█▄▃▄▂▃▆▂▄▂▆▃▇▃▃▅▁▃▃▆▅▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.78671
wandb: best/eval_avg_mil_loss 0.71045
wandb:  best/eval_ensemble_f1 0.78671
wandb:            eval/avg_f1 0.78671
wandb:      eval/avg_mil_loss 0.71045
wandb:       eval/ensemble_f1 0.78671
wandb:            test/avg_f1 0.34754
wandb:      test/avg_mil_loss 0.82797
wandb:       test/ensemble_f1 0.34754
wandb:           train/avg_f1 0.71567
wandb:      train/ensemble_f1 0.71567
wandb:         train/mil_loss 0.72106
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vibrant-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kgi3tggd
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_114246-kgi3tggd/logs
wandb: Agent Starting Run: 4gl5n1uv with config:
wandb: 	actor_learning_rate: 7.712219556461768e-05
wandb: 	attention_dropout_p: 0.2656840457522922
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 186
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5238269769249163
wandb: 	temperature: 0.4917059964884174
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_114413-4gl5n1uv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4gl5n1uv
wandb: uploading history steps 178-186, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅▅▆▆█
wandb: best/eval_avg_mil_loss █▇▂▇█▅▆▅▁
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅▅▆▆█
wandb:            eval/avg_f1 ▆▄▅▆▁▂▆▆▆▄▃▄▆▇▁▆▃▃▅▅█▅▆▆▄▃█▂▄▇▆▆▆▅▅▄▆▂▅▄
wandb:      eval/avg_mil_loss ▆▆▅▆▇█▅▆▄▅▄▅▇█▆▂▄▅▃▆▅▆▃▆▃▅▆▃▃▅▆▆▃▄▃▁▃▆▅▃
wandb:       eval/ensemble_f1 ▆▅▅▅▁▇▆▂▅▆▃▆▅▅▅▄▄▆█▅▅▅▆█▇▅▇▅▅█▇▆▅▆▆▄▅▇▆▇
wandb:           train/avg_f1 ▄▄▄▄▂▁▆▂▃▅▆▁▆▃▇▅▃▁▅▃▃▄▄▅▃▅▄▇█▇▄▄▃▃▅▆▄▄▄▄
wandb:      train/ensemble_f1 ▆▆▄▅▁▃▃▅▂▃▇▃█▄█▅▆▆▃▆▆▆▄▅▇▇▆▂▄▄▅▃▃▄▆▄▆▄▄▁
wandb:         train/mil_loss ▄▆▆▅▅▃▄▂▅█▅▃▇▆▄▆▇▃▁▄▆▃▂▅▃▃▇▅▄▃▄▃▃▃▄▄▂▄▆▃
wandb:      train/policy_loss █▅▁▁▅▅▁▅█▅▅▅▁█▁▅█▁▁█▁▁▁▁▁████▃█▅▅█▅▅▅▅█▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8166
wandb: best/eval_avg_mil_loss 0.53036
wandb:  best/eval_ensemble_f1 0.8166
wandb:            eval/avg_f1 0.72174
wandb:      eval/avg_mil_loss 0.76285
wandb:       eval/ensemble_f1 0.72174
wandb:           train/avg_f1 0.74198
wandb:      train/ensemble_f1 0.74198
wandb:         train/mil_loss 0.61833
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run quiet-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4gl5n1uv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_114413-4gl5n1uv/logs
wandb: ERROR Run 4gl5n1uv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 41lshgo2 with config:
wandb: 	actor_learning_rate: 7.455311604894341e-06
wandb: 	attention_dropout_p: 0.14116425922646209
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 53
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05064497656574918
wandb: 	temperature: 4.55246967054535
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_114817-41lshgo2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/41lshgo2
wandb: uploading wandb-summary.json; uploading history steps 41-53, summary
wandb: uploading history steps 41-53, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss █▁▃
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 ▇▁▅▁▇▇▇▅▆▁▇▇▆▆▆▆▇█▇▇▇▇▇▆▇▇▆▆▇▁▇▇▅▇▆▇█▆▆▇
wandb:      eval/avg_mil_loss ▅▃▅▁▃▂▂▅█▅▄▅▄▄▆▁▃▄▄▅▃▆▆▅▅▅▇▆▄█▆▆▂▁▂▆▅▂▅▁
wandb:       eval/ensemble_f1 ▁▅▇▆▇▅▆▇▁▁▇▆▆▆▆██▇▇▇▇▆▇▇▇▆▇▁▄▇▅▇▆▇█▇▇█▆▇
wandb:           train/avg_f1 ▃▄▄▅▂▃█▅▅▆▅▇▄▃▁▅▂▃▆▅▄▇▄▃▆▃▅▆▆▅▂█▄▂▇▃▃▃▆▆
wandb:      train/ensemble_f1 ▃▃▃▄▁▅▄▂█▄▅▇▅▇▄▄▄▄▃▅▃▃▃▆▅▅▆▆▅▃▇▄▁▆▅▂▅▃▆▃
wandb:         train/mil_loss ▃▅▅▄▁▄▅▄▃▅▆▄▃▂▃▃▃▃▃▁▇▃▆▄▄▃▃▄▅█▆▃▄▁▃▁▅▅▄▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80398
wandb: best/eval_avg_mil_loss 0.78155
wandb:  best/eval_ensemble_f1 0.80398
wandb:            eval/avg_f1 0.75948
wandb:      eval/avg_mil_loss 0.85162
wandb:       eval/ensemble_f1 0.75948
wandb:           train/avg_f1 0.73979
wandb:      train/ensemble_f1 0.73979
wandb:         train/mil_loss 0.69172
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vocal-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/41lshgo2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_114817-41lshgo2/logs
wandb: ERROR Run 41lshgo2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: kkzmdhpa with config:
wandb: 	actor_learning_rate: 2.6899423631830757e-05
wandb: 	attention_dropout_p: 0.2896253672690912
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4678967070048192
wandb: 	temperature: 6.895588455127292
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_114924-kkzmdhpa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kkzmdhpa
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅▅▆▆▇▇█
wandb: best/eval_avg_mil_loss ██▄▄▃▅▁▄▂▄
wandb:  best/eval_ensemble_f1 ▁▃▄▅▅▆▆▇▇█
wandb:            eval/avg_f1 ▇▄▇▇▅▅▂▄▄▇▆▅▆▇▅▇▅▅▁▅█▅▄▄▇▇▄█▅▆▆▆▁▇▃▅█▆▄▅
wandb:      eval/avg_mil_loss ▄▅▆▄▁▅▅▃▆▅▂▆▆█▅▂▄▅▃▃▃▇▅▄█▁▆▄▄▅▂▇▄▄▄▅▃▅▄▆
wandb:       eval/ensemble_f1 ▃▄▄▆▃▇▆▇▄▁▁▆▇▄▄▅▆▁▅▅▆▅▃▆▇▃▅▃▁▅▆▃▇▄▆█▂▃▄▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▃▄▃▃▅▁▄▅▄▅▄▂▄▄█▇▆▅▄▂▃▇▄▇▇▃▃█▇▄▅▇█▃▂▅▄▄▆
wandb:      train/ensemble_f1 ▄▂▄▅▃▁▄▄▄▄▄▄▄▆▄▄▁▄▃▆▅▃▆▄▃▄▅▆▆▄▇█▂█▄▆▄▃▆▆
wandb:         train/mil_loss ▄▄█▃▃▅▄▅▄▄▃▅▅▃▅▄▄▃▄▄▄▃▃▄▅▃▅▁▄▃▁▄▂▄▄▃▅▃▅▄
wandb:      train/policy_loss █▁▄▄█▁▄▄█▄▄█▄█▄▁▁▄▄▄█▄▄▄▄▄▁██▄▄▁█▁▄█▄▄█▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄▄▄▁█▄▄▄▄▄█▄▄▄▁█▄██▄█▄█▁▁█▁█▄▄██▄▄█▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.76405
wandb: best/eval_avg_mil_loss 0.79663
wandb:  best/eval_ensemble_f1 0.76405
wandb:            eval/avg_f1 0.6284
wandb:      eval/avg_mil_loss 0.97136
wandb:       eval/ensemble_f1 0.6284
wandb:            test/avg_f1 0.61135
wandb:      test/avg_mil_loss 0.64849
wandb:       test/ensemble_f1 0.61135
wandb:           train/avg_f1 0.68453
wandb:      train/ensemble_f1 0.68453
wandb:         train/mil_loss 0.75842
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run volcanic-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kkzmdhpa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_114924-kkzmdhpa/logs
wandb: Agent Starting Run: 4te7ecxp with config:
wandb: 	actor_learning_rate: 1.429150410352577e-06
wandb: 	attention_dropout_p: 0.09033814272625684
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 141
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.03797600198993212
wandb: 	temperature: 3.9286967266533046
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_115139-4te7ecxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4te7ecxp
wandb: uploading history steps 140-142, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▆█
wandb: best/eval_avg_mil_loss █▄▃▂▃▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▆█
wandb:            eval/avg_f1 ██▄▁▅▇▃▅▁▅▃▆▇▅▅▆▃▄▄▃█▅▇▃▆▄▆▅▆▄▆▃▂▆▅▁▄▆▄▅
wandb:      eval/avg_mil_loss ▅▂▃▅██▃▆█▆▅▅▇▅▇▁▅▇▇▅▆▇▆▆▅▁▇▅▃▅▆▅▅▃▅▇▆█▄▃
wandb:       eval/ensemble_f1 █▇▃▁▄▇▃▃▆▇▁▄▅▅▆▄▅▃▄▄▇▄▆▃▂▃▅▅▄▆▅▆▆▅▆▃▅▃▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▆▅▅▅▄▄█▆▅▇▆▆▃█▇▅▅▆▇▁▅▅▄▆▆▁▄▅▄▁▇▆▅█▆▅▇▆
wandb:      train/ensemble_f1 ▁▃▆▆▄▇▆▇▅▃▅▇▄▃▆▄█▅▄▅▇▄▇▄▅▄▃▅▃▅▅▇▅▆▂▆▅▅▇▆
wandb:         train/mil_loss ▄▄▅▂▃▄▄▂▂▄▃█▃▆▄▅▄▁▄▁▄▅▁▅▄▄▆▂▃▂▁▂▄▄▄▂▃▃▄▆
wandb:      train/policy_loss ▄▄▄▁▄███▄▁▁▄▄▄▆▄█▁▄██▁▆▄█▁▁▁█▄▁█▃▁▄▆▄▁▁▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁██▄▁███▄▁▃▄█▄▄▄█▄█▆▄▁▃▄█▁██▁█▃██▁▄█▁█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7597
wandb: best/eval_avg_mil_loss 0.64532
wandb:  best/eval_ensemble_f1 0.7597
wandb:            eval/avg_f1 0.55109
wandb:      eval/avg_mil_loss 0.94224
wandb:       eval/ensemble_f1 0.55109
wandb:            test/avg_f1 0.55825
wandb:      test/avg_mil_loss 0.87999
wandb:       test/ensemble_f1 0.55825
wandb:           train/avg_f1 0.58502
wandb:      train/ensemble_f1 0.58502
wandb:         train/mil_loss 0.88817
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run deep-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4te7ecxp
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_115139-4te7ecxp/logs
wandb: Agent Starting Run: arkm7fkf with config:
wandb: 	actor_learning_rate: 0.0006801536505879161
wandb: 	attention_dropout_p: 0.003920080228726586
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 73
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9455906450033404
wandb: 	temperature: 2.9320271045688475
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_115446-arkm7fkf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/arkm7fkf
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅█
wandb: best/eval_avg_mil_loss █▅▇▁
wandb:  best/eval_ensemble_f1 ▁▄▅█
wandb:            eval/avg_f1 ▇▄▇▇▂▅▇▇▇▇▆▆▁▇▆▇▆▇▇▆▇▇▆▇▇▇▇▆▁▇▆▆▆▆▇█▂▇▆▇
wandb:      eval/avg_mil_loss ▄▆▂▄▄▆▅▆▅▂▆▄▅▆▄▁▄▆▄█▆▆▆▅▄▅▁▅▄▆▁▄▅▄▅▂█▆▄▅
wandb:       eval/ensemble_f1 ▇▃▅▇▇█▆▇▇▇▆▇▆▇▃▇▇▃▇▇▇▇▇▇▆█▇▇▇▃▇▇▁▇▇▇▄█▇▇
wandb:           train/avg_f1 ▅▄▄▄▃▄▄▃▅▅▇▆▄▆▃█▆▄▄▄▃▅▃▆▄▃▅▆▅▄▁▅▅▄▅▄█▅▄▅
wandb:      train/ensemble_f1 ▄▅▅▄▃▃▂▄▄▃▄▅▄▆▇▅█▃▄▃▅▄▅▁▅▂▄▄▄▆▄▄▃▅▅▄▄▃▄▄
wandb:         train/mil_loss ▃▃█▄▇▄▄▃▂▂▇▃▂▄▆▂▅▃▆▃▃▁▂▅▇▇▅▃▁▄▂▄▃▃▄▂▂▃▆▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.80277
wandb: best/eval_avg_mil_loss 0.68566
wandb:  best/eval_ensemble_f1 0.80277
wandb:            eval/avg_f1 0.77225
wandb:      eval/avg_mil_loss 0.88484
wandb:       eval/ensemble_f1 0.77225
wandb:           train/avg_f1 0.73611
wandb:      train/ensemble_f1 0.73611
wandb:         train/mil_loss 0.62574
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run silver-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/arkm7fkf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_115446-arkm7fkf/logs
wandb: ERROR Run arkm7fkf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: p6zfv8xk with config:
wandb: 	actor_learning_rate: 2.832113253113707e-06
wandb: 	attention_dropout_p: 0.010831874504487748
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 144
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4268106255998936
wandb: 	temperature: 5.16665890693374
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_115620-p6zfv8xk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p6zfv8xk
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▅███
wandb: best/eval_avg_mil_loss ███▄▁▄▄
wandb:  best/eval_ensemble_f1 ▁▂▃▅███
wandb:            eval/avg_f1 ▅▅▇▇▃▅█▄▇▆▅▆▆▅▇▆▅▇▁▅▅▄▇▇▂▆▇▇▆▂▇▇▄▇█▃▆▅▅█
wandb:      eval/avg_mil_loss ▂▆▆▇▇▄█▃▅▅▆▆▂▆█▅▄█▅▆▅▅▂▃▆▆▂▄▄▄▅▃▄▄█▄▆▄▃▁
wandb:       eval/ensemble_f1 ▅▆▂▄▆█▄▇▃▄▇▆▆▅▄▃▅▃▃▄▅▂▄▂▃▆▆▇▇▄▁▄▇█▇▅▆▆▆▆
wandb:           train/avg_f1 ▆▃▆▂▄▄▁▁▂▅█▁▆▅▂▃▆▆▆▄▅▆▅▅▆▇▂▄▃▄▃▇█▂▇▃█▄▆▇
wandb:      train/ensemble_f1 ▅▅▅▂▆▄▁▃▃▂▃▇█▆▅▆▂▅▅▄▆▅▅▅▅▂▆▄▇▃▇▅▆▃▂▆▂▄▅▃
wandb:         train/mil_loss █▅▆▇▃▄▅▄▆▆▄▄▅▃▇▂▄▆▂▅▆▅▄▃▁▃▅▁▄▅▃▁▁▂▄▃▅▃▂▄
wandb:      train/policy_loss ████████████████████████▆██████████▁████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████▆█████████▇█▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79614
wandb: best/eval_avg_mil_loss 0.67729
wandb:  best/eval_ensemble_f1 0.79614
wandb:            eval/avg_f1 0.75965
wandb:      eval/avg_mil_loss 0.54563
wandb:       eval/ensemble_f1 0.75965
wandb:           train/avg_f1 0.71602
wandb:      train/ensemble_f1 0.71602
wandb:         train/mil_loss 0.7137
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run solar-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p6zfv8xk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_115620-p6zfv8xk/logs
wandb: ERROR Run p6zfv8xk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 19qrgpsb with config:
wandb: 	actor_learning_rate: 0.00011104639353038775
wandb: 	attention_dropout_p: 0.45976195409416154
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 83
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1868877457176208
wandb: 	temperature: 2.5055020313251997
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_115930-19qrgpsb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/19qrgpsb
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▇█
wandb: best/eval_avg_mil_loss █▁▃▇
wandb:  best/eval_ensemble_f1 ▁▁▇█
wandb:            eval/avg_f1 ▇▇▇▇█▅▆▇▅▇██▆▆█▇▇█▇█▇▇▅▇█▆▆██▁▇▇██▆▇█▆▇▅
wandb:      eval/avg_mil_loss ▂▂▃▁▄█▆▃▅▄▅▂▅▄▂▄▄▄▄▄▄▄▇▄▂▄▅▄▃▅▁▁▄▄▄▄▃▃▃▄
wandb:       eval/ensemble_f1 █▆▆█▂▃▇▆▂▆▅▇▆▅▇▅▆▆▇█▆▆█▇▂▅▇▁▄█▃▇▆▇▇▆▃▆▆▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▆▅▂▄▃▂▄▄▃▅▁▁█▃▁▁▄▄▅▂▃▆▅▂▆▃▅▇▂▆▅▅▃▄▅▃▄▅
wandb:      train/ensemble_f1 ▅▆▁▄▅▇▅▆▃▅▄▅▅▄█▆▇▅▄▅▆▅▅▄▅▇▄▇▆▄▃▇▇▃▅▅▅▅█▄
wandb:         train/mil_loss ▄▄▆▇▆█▁▄▅▂▇▄▇▆▃▄▃▃▆█▄▁▆█▅▁▆▅▅▅▅▇▅▂▇▄▆▃▆▄
wandb:      train/policy_loss ▄▄▁▁▁▁▄█▄▄▁▄▄▄▄▄▄█▄▄▃▄▄█▁▄▄▄▁▄▄█▄▁▄▄▄▄█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████▁████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79367
wandb: best/eval_avg_mil_loss 0.80949
wandb:  best/eval_ensemble_f1 0.79367
wandb:            eval/avg_f1 0.62877
wandb:      eval/avg_mil_loss 0.78042
wandb:       eval/ensemble_f1 0.62877
wandb:            test/avg_f1 0.8083
wandb:      test/avg_mil_loss 0.4276
wandb:       test/ensemble_f1 0.8083
wandb:           train/avg_f1 0.71893
wandb:      train/ensemble_f1 0.71893
wandb:         train/mil_loss 0.69064
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run autumn-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/19qrgpsb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_115930-19qrgpsb/logs
wandb: Agent Starting Run: 4jrma0fm with config:
wandb: 	actor_learning_rate: 9.71370659587846e-05
wandb: 	attention_dropout_p: 0.03359855264001388
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 171
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.815206627968991
wandb: 	temperature: 1.703116853270792
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_120103-4jrma0fm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4jrma0fm
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇████
wandb: best/eval_avg_mil_loss █▆▄▄▄▁▂▂
wandb:  best/eval_ensemble_f1 ▁▇▇▇████
wandb:            eval/avg_f1 ▇▆▅▇▆▇▃▇▇▁▄▅▆▅▇▅▇▄▇▄▃▄▄▆█▅█▄▃▇▇▂██▇▆▇▇▆▃
wandb:      eval/avg_mil_loss ▄▁▃▃▂▂▇▃▆▄▃█▄▂▂▄▆▅▄▂▆▄▃▁▆▄▆▆▃▅▃▂▅▄▁▃█▆▆▅
wandb:       eval/ensemble_f1 ▃▆▅▇▃▁▃▄▆▇▄▇▄▄▅▇▇▆▇▄▄█▇▆▅█▆▃▇▆▆▆▄▁▇█▃▇▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▄▁▃▄▄▅▂▃▂▃▅▃▄▃▃▁█▄▂▆▅▃▄█▅▄▄▅▆▅▄▂▆▆█▂▅▄
wandb:      train/ensemble_f1 ▄▅▃▁▂▆▃▄▂▅▄▃▆▃▅▃▂▄█▄▃▃▅▆▁▃▅▆▃▃▅▆▅▂▆▆▃▃▅▁
wandb:         train/mil_loss ▅▇▃▅▇▆▃▄▆▅▃▄▂▇▃▄▅▄▄█▃▄▃▃▇▅▃▄▄▅▂▂▃▄▃▅▄▄▁▃
wandb:      train/policy_loss ▄▄███▄▁█▄▄▄▄▄█▄▄█▁▄▁▁▄▄▄█▄▄▁▁▄█▄▄▄█▄▄▁▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7692
wandb: best/eval_avg_mil_loss 0.68744
wandb:  best/eval_ensemble_f1 0.7692
wandb:            eval/avg_f1 0.64164
wandb:      eval/avg_mil_loss 0.81515
wandb:       eval/ensemble_f1 0.64164
wandb:            test/avg_f1 0.76965
wandb:      test/avg_mil_loss 0.56143
wandb:       test/ensemble_f1 0.76965
wandb:           train/avg_f1 0.6742
wandb:      train/ensemble_f1 0.6742
wandb:         train/mil_loss 0.6495
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lively-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4jrma0fm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_120103-4jrma0fm/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: eo86ziws with config:
wandb: 	actor_learning_rate: 1.7341228511187306e-06
wandb: 	attention_dropout_p: 0.3221895885093186
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 168
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.11041880983322072
wandb: 	temperature: 0.22093170338566392
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_120433-eo86ziws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eo86ziws
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▇█
wandb: best/eval_avg_mil_loss ▁▆▅█▁▂
wandb:  best/eval_ensemble_f1 ▁▂▃▆▇█
wandb:            eval/avg_f1 ▇▄▇▇▇▄▇▇█▇▇▇█▄▅▇█▇▇▇▃█▄▄▇▇▇▇▄▅▇█▄▁▄▇▄▄▇▇
wandb:      eval/avg_mil_loss ▂▄▄▅▄▅▅▂▆▅▂▂▅▂▄▆▂▂▅▁▃▅▅▂█▅▄▄▄▃▄▅▆▃▂▃▃▃▅▄
wandb:       eval/ensemble_f1 ▁▆▆▆▁▆▆█▆▆▇▁▁▁▆▆▆▆▁▆▂▂▁▇▆▁▇▇▃█▇▃█▇▆▁▂▇▁▅
wandb:           train/avg_f1 █▄▃▂▇▄▄▃▆▄▄▄▆▄▂▇▅▇▃▆▃▆▆▆▁▆▇▆▇▆▃▇▄▅▄▆▅▇▆▇
wandb:      train/ensemble_f1 ▄▆▇▆▅▅▇▃▄▄▅▅▄▃▄▂▅▆▇▂▅▂▇▁▇█▅▄▄█▁▅▃▃▅▂▃▅▇▄
wandb:         train/mil_loss ▅▄▁▇▄▅▆▄▆▄█▇▄▅▇▅▅▅▃▅▆▆▅█▆▃▅▅▄▃▅▃▅▃▄▆▂▆▇▅
wandb:      train/policy_loss █████████████████████████████▁██████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████▁████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7883
wandb: best/eval_avg_mil_loss 0.774
wandb:  best/eval_ensemble_f1 0.7883
wandb:            eval/avg_f1 0.50776
wandb:      eval/avg_mil_loss 0.82686
wandb:       eval/ensemble_f1 0.50776
wandb:           train/avg_f1 0.672
wandb:      train/ensemble_f1 0.672
wandb:         train/mil_loss 0.72306
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eo86ziws
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_120433-eo86ziws/logs
wandb: ERROR Run eo86ziws errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: k1l97o0a with config:
wandb: 	actor_learning_rate: 1.808157727459299e-05
wandb: 	attention_dropout_p: 0.20849141964059945
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9482437309895512
wandb: 	temperature: 3.312836214308544
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_120743-k1l97o0a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k1l97o0a
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss ▁█▂
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 ▇▄▂▆▆▃▄▆▆▃█▅▆▇▅▄▅▆█▇▅▃▂▆▅▆▆▅▆▅▄▁▇▇█▅▄▅▄▇
wandb:      eval/avg_mil_loss ▅█▆▆▇█▆▆▇▆▅▃▆▅▂▃▇▆▃▄▅▅▃▆▃▅▂▅▆▂▆▄▆▃▃▄▂▁▄▁
wandb:       eval/ensemble_f1 ▃▁▂▆▂▅▂▆▃▅▃▄▂▁▅▅▄▅▇▆▃▂▆▁█▄▅▆▅▇▅▅▅▇▆▄▃▆▃▆
wandb:           train/avg_f1 ▂▂▂▃▃▂▅▆▂▄█▃▇▂▃▂▅▃▆▂▂▆▃▄▂▁▄▆▇▄▃▂▆▄▄▅▅▄▃▃
wandb:      train/ensemble_f1 ▄▃▁▃▃▄▄▃▃▅▃▃█▇▄▅▃▅▄▄▄▅▄▂▃▃▅▂▅▅▇▇▃▆▄▅▅▃▆▆
wandb:         train/mil_loss ▅▄▃▅▅▃▅▃▄█▄▄▄▂▆▃▂▂▃▂▂▄▁▂▄▃▄▄▃▂▃▃▂▂▂▃▁▁▂▃
wandb:      train/policy_loss █████████▃██████████▁███████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.7776
wandb: best/eval_avg_mil_loss 0.72167
wandb:  best/eval_ensemble_f1 0.7776
wandb:            eval/avg_f1 0.76233
wandb:      eval/avg_mil_loss 0.6855
wandb:       eval/ensemble_f1 0.76233
wandb:           train/avg_f1 0.75249
wandb:      train/ensemble_f1 0.75249
wandb:         train/mil_loss 0.64005
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run chocolate-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k1l97o0a
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_120743-k1l97o0a/logs
wandb: ERROR Run k1l97o0a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: eovjxikb with config:
wandb: 	actor_learning_rate: 9.36559052271791e-06
wandb: 	attention_dropout_p: 0.3783155194897519
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7567684413781505
wandb: 	temperature: 8.945249125990367
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_120947-eovjxikb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/9pmgzkhh
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eovjxikb
wandb: uploading wandb-summary.json
wandb: uploading history steps 83-89, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇█▇▅▁▅▇█▅▁▅▇▄▅▇█▅▄▇▇▅▇▅▅▇█▇▇█▇▄▅▄▇▇█▇▇▇▅
wandb:      eval/avg_mil_loss ▁▅▄▅▄█▄▅█▅▃▅▁▆█▂▆▄▅▃▅▅▅▃▄▅▅▄▇▂▆▅▅▅▅▄▄▅▄▆
wandb:       eval/ensemble_f1 █▇▅▁▇▇█▇▄▅▇▇▇▅▄▄▄▄▇▄▇▅▇▇▇▄▇▁▄▇▇▇▅▇▇▇▄▇▇▄
wandb:           train/avg_f1 ▆▄▇▄█▆▅▂▆▇▂▆▆▅▇▃▅▃▁▇▂▇▄▅▆▄█▅▄▅▆▆▅▅▅▇▇▇▄▄
wandb:      train/ensemble_f1 ▇▂▂▇▅▅▆▄█▅▅▅▂▄▂▃▁▆▄▄▂▆▃▅▄▅▂▅▅▃▇▆▅▄▄▆▅▆▅▅
wandb:         train/mil_loss ▃▄▃▅█▇▁▄▅▄▅▅█▅▆▄▄▃▄▇▆▆▃▄▅▇▄▅▇▄▇▅▅▂▅▄▇▇▆▆
wandb:      train/policy_loss ████████████████████████████████▁███████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.77789
wandb: best/eval_avg_mil_loss 0.85578
wandb:  best/eval_ensemble_f1 0.77789
wandb:            eval/avg_f1 0.55811
wandb:      eval/avg_mil_loss 0.94075
wandb:       eval/ensemble_f1 0.55811
wandb:           train/avg_f1 0.64439
wandb:      train/ensemble_f1 0.64439
wandb:         train/mil_loss 0.84375
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run treasured-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eovjxikb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_120947-eovjxikb/logs
wandb: ERROR Run eovjxikb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
