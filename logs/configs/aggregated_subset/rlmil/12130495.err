wandb: Agent Starting Run: dxu7nlen with config:
wandb: 	actor_learning_rate: 1.0114903462943684e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.204954865807938
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6809698594551483
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041100-dxu7nlen
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dxu7nlen
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading history steps 135-152, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ████▁▁▁▁▁▁▁███████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      eval/avg_mil_loss ████▇▇▇▇▇▆▆▆▆▆▆▄▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       eval/ensemble_f1 █████▁▁▁██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▄▅▃█▄▃▅▃▄▃▅▂▄▆▄▂▃▅▁▆▆▅▄▂▅▅▅▁▇▁▃▆▂▅▄▄▃▃
wandb:      train/ensemble_f1 ▄▁▅▅▄▆▁▅▅▅▅▆▂▄▅▄▄▇▇▇▆▅▄▆▅▅▁▆▄█▄▇▅█▅▅▃▆▅▄
wandb:         train/mil_loss ▆▇▆▄▃▅▅▅▂▆▄▆▇▄▄█▁▅▅▃▂▄▂▄▄▃▃▃▂▄▃▃▂▂▅▂▃▄▅▄
wandb:      train/policy_loss ▅▅▆▆█▅▆▄▆▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91161
wandb: best/eval_avg_mil_loss 0.26532
wandb:  best/eval_ensemble_f1 0.91161
wandb:            eval/avg_f1 0.90799
wandb:      eval/avg_mil_loss 0.25994
wandb:       eval/ensemble_f1 0.90799
wandb:            test/avg_f1 0.93026
wandb:      test/avg_mil_loss 0.18622
wandb:       test/ensemble_f1 0.93026
wandb:           train/avg_f1 0.89204
wandb:      train/ensemble_f1 0.89204
wandb:         train/mil_loss 0.23608
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run blooming-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dxu7nlen
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041100-dxu7nlen/logs
wandb: Agent Starting Run: kwnk4sv0 with config:
wandb: 	actor_learning_rate: 0.001643926624859414
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.08825444999186682
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17878115525502203
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041304-kwnk4sv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kwnk4sv0
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 █▃▁▃▃▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:      eval/avg_mil_loss ▁▁▁▁▆████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       eval/ensemble_f1 ██▆▃▁▂▂▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇█▁▁▂▃▂▁▃▃▃▂▃▃▃▃▃▄▄▄▃▃▄▄▄▃▄▄▃▅▄▃▃▄▃▃▄▄▄▃
wandb:      train/ensemble_f1 ▇█▁▃▃▂▄▂▃▄▃▃▄▃▄▃▃▃▄▄▃▄▄▃▃▄▄▃▃▄▄▄▄▄▃▄▄▅▄▄
wandb:         train/mil_loss ▁▂▄▇██▇▇▇▆▆█▆▇▆▆▇▇▅▅▆▇▆▇▆▆▇▇▇▆▄▇▇▄▅▆▇▅▅▆
wandb:      train/policy_loss ▄▄▄▆▁▄▄▄▄▄▇▄▄▄▄▄▄▄▆▄▄▄▄▃▄▄▃▄▄▄▃█▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████▁███████████████████▆███████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.6142
wandb: best/eval_avg_mil_loss 1.08827
wandb:  best/eval_ensemble_f1 0.6142
wandb:            eval/avg_f1 0.52544
wandb:      eval/avg_mil_loss 2.03657
wandb:       eval/ensemble_f1 0.52544
wandb:            test/avg_f1 0.58151
wandb:      test/avg_mil_loss 1.08223
wandb:       test/ensemble_f1 0.58151
wandb:           train/avg_f1 0.52288
wandb:      train/ensemble_f1 0.52288
wandb:         train/mil_loss 1.74676
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dazzling-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kwnk4sv0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041304-kwnk4sv0/logs
wandb: Agent Starting Run: o6auwq0w with config:
wandb: 	actor_learning_rate: 0.0012435827663524468
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.7351287970299747
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.283535765885111
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041426-o6auwq0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/o6auwq0w
wandb: uploading history steps 100-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 █████████████▇▅▄▂▃▃▁▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:      eval/avg_mil_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▄▅▆▆▆▆▆▆▆▆▆▆▆▆███████
wandb:       eval/ensemble_f1 ████████████████▇▇▃▃▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▇▇▆█▆▆▇███▅▇▇█▅▄▅▆▄▅▅▃▄▃▂▃▃▂▂▂▃▃▃▄▃▂▁▃
wandb:      train/ensemble_f1 ▇▆▆▇▆█▆█▆██▆█▇▆▄▄▄▅▃▂▃▂▂▁▃▂▂▃▃▃▃▃▄▂▂▂▃▁▃
wandb:         train/mil_loss ▄▄▄▆█▄▄▆▆▇▄▃▆▅▅▇▆▅▆▇▅█▅▇▁▄▄▇▆▅▃▅▄▅▇▇▄█▃▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75694
wandb: best/eval_avg_mil_loss 1.00698
wandb:  best/eval_ensemble_f1 0.75694
wandb:            eval/avg_f1 0.72463
wandb:      eval/avg_mil_loss 1.25448
wandb:       eval/ensemble_f1 0.72463
wandb:            test/avg_f1 0.75531
wandb:      test/avg_mil_loss 0.62107
wandb:       test/ensemble_f1 0.75531
wandb:           train/avg_f1 0.7309
wandb:      train/ensemble_f1 0.7309
wandb:         train/mil_loss 0.41697
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vocal-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/o6auwq0w
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041426-o6auwq0w/logs
wandb: Agent Starting Run: zpgn4sb8 with config:
wandb: 	actor_learning_rate: 1.0967546482720011e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.22156747185289127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7535308522815813
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041549-zpgn4sb8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zpgn4sb8
wandb: uploading history steps 200-219, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▅▆▇█
wandb: best/eval_avg_mil_loss █▅▅▃▃▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃▅▆▇█
wandb:            eval/avg_f1 ▁▁▁▁▁▃▃▂▂▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆███████████████
wandb:      eval/avg_mil_loss ██▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       eval/ensemble_f1 ▁▁▁▂▃▂▂▂▂▂▅▅▅▆▆▆▆▆▆▆▆███████████████████
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▁▂▅▂▂▅▄▃▁▇█▇▅▂▃▆▅▆▇▆█▅▄▄▄▃▅▅▅▅▁▆▄▇▃▄▅▄▆
wandb:      train/ensemble_f1 ▃▃▅▂▄▄▇▁▄▆▅▅▅▆▅▆▆▅█▃▅▆▇▇▇▃▇▃▅▆▄▇▆▇▄█▆▆▄▄
wandb:         train/mil_loss ▄▆█▆▅▇▄▄▆▄▅▂▅▃▂▄▅▅▃▃▁▄▅▆▆▄▄▃▃▄▄▄▄▄▃▃▇▄▄▄
wandb:      train/policy_loss █▆▇█▇▄▄▄▁▂▁▂▁▂▃▃▂▂▂▂▇█▇█▆▆▇█▇▆▇▇█▇█▇▆▆▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92573
wandb: best/eval_avg_mil_loss 0.24343
wandb:  best/eval_ensemble_f1 0.92573
wandb:            eval/avg_f1 0.92573
wandb:      eval/avg_mil_loss 0.23449
wandb:       eval/ensemble_f1 0.92573
wandb:            test/avg_f1 0.93052
wandb:      test/avg_mil_loss 0.16641
wandb:       test/ensemble_f1 0.93052
wandb:           train/avg_f1 0.91814
wandb:      train/ensemble_f1 0.91814
wandb:         train/mil_loss 0.24285
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fresh-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zpgn4sb8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041549-zpgn4sb8/logs
wandb: Agent Starting Run: 8xbwpsjp with config:
wandb: 	actor_learning_rate: 1.2922591358175116e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.7142994087259202
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9176233457316456
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041839-8xbwpsjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8xbwpsjp
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 219-237, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅▇█
wandb: best/eval_avg_mil_loss ██▅▅▅▁
wandb:  best/eval_ensemble_f1 ▁▂▄▅▇█
wandb:            eval/avg_f1 ▁▂▂▂▄▄▄▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      eval/avg_mil_loss █▅▅▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁
wandb:       eval/ensemble_f1 ▁▂▂▄▄▄▄▄▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▂▅▃▁▆▄▅▂▃▇▇▄▃▆▄▄▄▂▄▅▇▇▅▆▆▆█▇▃▃▆▄▄█▅▃▄▆▆
wandb:      train/ensemble_f1 ▁▆▅▄▃▁▅▄▃▂▁▅▅▄▃▃▃▂▂▄▃▃▇▆▄▂▃▂▄▃██▃▃▃▃▅▅▃▅
wandb:         train/mil_loss ▄▇▆▇▅▁▁▄▄▃▂█▅▄▄▅▇▅▄▇▅▆▅▄▃▆▃█▇▅▃▆▅▃▆▅▄▄▄▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9103
wandb: best/eval_avg_mil_loss 0.2981
wandb:  best/eval_ensemble_f1 0.9103
wandb:            eval/avg_f1 0.90667
wandb:      eval/avg_mil_loss 0.29225
wandb:       eval/ensemble_f1 0.90667
wandb:            test/avg_f1 0.93824
wandb:      test/avg_mil_loss 0.15967
wandb:       test/ensemble_f1 0.93824
wandb:           train/avg_f1 0.91704
wandb:      train/ensemble_f1 0.91704
wandb:         train/mil_loss 0.23331
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dutiful-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8xbwpsjp
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041839-8xbwpsjp/logs
wandb: Agent Starting Run: 06zq01mf with config:
wandb: 	actor_learning_rate: 1.0102689780941506e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.06253849164866898
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.918274256922424
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042145-06zq01mf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/06zq01mf
wandb: uploading history steps 141-155, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▅▆▆▇▇█
wandb: best/eval_avg_mil_loss ██▅▅▅▅▃▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▅▆▆▇▇█
wandb:            eval/avg_f1 ▁▂▂▂▂▃▄▅▅▅▆▅▇▆▇█████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      eval/avg_mil_loss ██▇▇▆▅▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       eval/ensemble_f1 ▁▂▂▂▂▄▄▅▅▆▇▇▇████████▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▃▃▃▁▂▃▁▄▃▃▄▄▅▅▄▅▄▄▅█▆▆▆▅▄▅▇▅▅▆▄▆█▇▅▅▇▆▇
wandb:      train/ensemble_f1 ▃▃▅▁▁▁▄▄▂▄▄▃▄▃▂▆▅▅▄▄▄▆▆▆▆▆▆▇▆▄▄▆▇▆█▇▅▆▅█
wandb:         train/mil_loss █▄▆▆▆▄▆█▅▆▅▃▄▇▅▃▃▅▄▃▇▃▂▄▁▅▁▃▂▃▂▄▂▄▃▄▃▄▂▁
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄█▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92587
wandb: best/eval_avg_mil_loss 0.26431
wandb:  best/eval_ensemble_f1 0.92587
wandb:            eval/avg_f1 0.9186
wandb:      eval/avg_mil_loss 0.24787
wandb:       eval/ensemble_f1 0.9186
wandb:            test/avg_f1 0.93845
wandb:      test/avg_mil_loss 0.14668
wandb:       test/ensemble_f1 0.93845
wandb:           train/avg_f1 0.92705
wandb:      train/ensemble_f1 0.92705
wandb:         train/mil_loss 0.23003
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glad-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/06zq01mf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042145-06zq01mf/logs
wandb: Agent Starting Run: rragbcmv with config:
wandb: 	actor_learning_rate: 1.1235134032961316e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.32555706610367385
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9137348093319848
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042349-rragbcmv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rragbcmv
wandb: uploading output.log; uploading config.yaml; uploading history steps 218-237, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▅▆▆▇█
wandb: best/eval_avg_mil_loss ██▅▅▄▄▃▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▃▅▆▆▇█
wandb:            eval/avg_f1 ▁▁▁▁▂▄▄▆▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇███████▇▇▇▇▇▇▇▇▇▇
wandb:      eval/avg_mil_loss ███▇▇▇▆▆▅▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       eval/ensemble_f1 ▁▁▂▂▁▁▂▅▆▅▅▆▆▆▆▆▆▆▆▇▇▇█████████████▇▇▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▂▃▄▆▃▆▅▄▃▆▄▆▄▆▅▅▅▆▆▇▇▇█▇▇▇▅▅█▆█▆▅▆▇▆▇▇
wandb:      train/ensemble_f1 ▃▁▂▄▃▃▃▃▃▄▃▃▅▅▃▅▅▅▅▅▅▅▅▆▅▇▆▅▅▇█▇▇▅█▅▄▅▆▆
wandb:         train/mil_loss █▅▅▄▇▅▃▅██▄▇▃▅█▇▇▇▄▅▆▃▇▄▄▅▁▆▂▄▃▂▂▃▄▅▆▁▂▂
wandb:      train/policy_loss ▆█▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.25984
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.91069
wandb:      eval/avg_mil_loss 0.25336
wandb:       eval/ensemble_f1 0.91069
wandb:            test/avg_f1 0.93845
wandb:      test/avg_mil_loss 0.15607
wandb:       test/ensemble_f1 0.93845
wandb:           train/avg_f1 0.9148
wandb:      train/ensemble_f1 0.9148
wandb:         train/mil_loss 0.24765
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dashing-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rragbcmv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042349-rragbcmv/logs
wandb: Agent Starting Run: ookan6i0 with config:
wandb: 	actor_learning_rate: 1.0310912831235431e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 800
wandb: 	epsilon: 0.9702177089617744
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4006044068965934
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042655-ookan6i0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/hc1avi8a
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ookan6i0
