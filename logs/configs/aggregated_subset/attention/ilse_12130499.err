wandb: Agent Starting Run: 0yc9vqwq with config:
wandb: 	actor_learning_rate: 6.291062752571662e-05
wandb: 	attention_dropout_p: 0.35953906711409
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 159
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.44035924211942334
wandb: 	temperature: 0.5902833941900287
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041839-0yc9vqwq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0yc9vqwq
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 129-133, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▄▅▆▇▇█
wandb: best/eval_avg_mil_loss ▁▂▃█▃▆▃▂▁
wandb:  best/eval_ensemble_f1 ▁▃▄▄▅▆▇▇█
wandb:            eval/avg_f1 ▇█▆▃▅▄▆▃▃█▆▂▆▇▄▄▇▃▅▆▇▁▅▄▆▃▅█▆▃▄▆▄▄▄▂▄▄▃▅
wandb:      eval/avg_mil_loss ▂█▃▅▂▂▆▄▅▂▄▂▁▄▆▄▂▆▄▄▅▄▅▁▃▂▅▁▅▄▅▅▅▅▅▇▄▄▆▂
wandb:       eval/ensemble_f1 ▄▅▅▃▆▄▄▄▅▃▇▄█▆▅▆▇▆▃▆▄▅▄▃▄▃▁▆█▃▅▁▃▄▆▄▃▃▃▄
wandb:           train/avg_f1 ▄▅▄▆▄▄▅▅▄▆▄▇▆█▆▅▂▅▃▆▃▄▂▄▃▃▄▃▄▃▃▃▄▃▄▃▂▃▁▅
wandb:      train/ensemble_f1 ▆▄▅▄▇▂▅▅▆▄▇▆▆▇▁▁█▄▅▁▆▃▃▅▅▂▅▂▃▃▃▃▂▄▂▂▃▅▁▆
wandb:         train/mil_loss ▆▆▄▇▅▇▆▄▅▅█▅▇▄▅▆▇▅▆▄▅▅▅▄▄▄▁▁▄▅▅▄▄▂▆▂▄▄▅▂
wandb:      train/policy_loss ▆▇▆▆▆▆▆▆▆▆▆▆▁▆▅▆█▆▅▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆█▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91524
wandb: best/eval_avg_mil_loss 0.26847
wandb:  best/eval_ensemble_f1 0.91524
wandb:            eval/avg_f1 0.88281
wandb:      eval/avg_mil_loss 0.42302
wandb:       eval/ensemble_f1 0.88281
wandb:           train/avg_f1 0.86567
wandb:      train/ensemble_f1 0.86567
wandb:         train/mil_loss 0.25463
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run astral-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0yc9vqwq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041839-0yc9vqwq/logs
wandb: ERROR Run 0yc9vqwq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: an72l9ma with config:
wandb: 	actor_learning_rate: 1.1125310993939654e-06
wandb: 	attention_dropout_p: 0.04370361774402004
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.26249674594713335
wandb: 	temperature: 9.650019794208673
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042033-an72l9ma
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/an72l9ma
wandb: uploading history steps 89-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 ▄▄▅█▇▅▅▃▁▂▂█▆▃▄▅▆▄▅▄▅▆▇▅▇▃▃▂▅▄▇▃▃▄▅▅▅▄▆▅
wandb:      eval/avg_mil_loss ▆▂▆▅▅▅▃▃▄▄▃█▅▃▄▄▄▃▅▄▄▂▂▄▁▆▂▄▅▅▃▄█▄▃▃▃▄▄▆
wandb:       eval/ensemble_f1 █▄▇▄▅▂▃▄▅▂▄▃▄▅▄▅▆▅▃▆█▃▆▇▃▄▃▃▄▅▃▄▆▂▃▅▆▃▁▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▃▂▄▅▅▅▅▄▁▄▅▄▅▅▅▃▂▄▄▆▃▂▂█▆▆▄▂▆▆▄▃▃▃▄▅▂▃
wandb:      train/ensemble_f1 █▅▃▃▃▃▅▃▄▄▅▄▅▅▅▅▃▄▃▃▆▃▃▃▁█▆▂▃▃▆▃▃▄▄▅▂▆▅▄
wandb:         train/mil_loss ▇█▃▅▄▆▆▃▄▄▅▃▅▁▃▆▄▇▄▅▅▅▄▆▄▅▄▄▆▆▄▃▅█▄▅▇▄▅▅
wandb:      train/policy_loss ▄▄█▄▄▄▄▄▄▄▄▄▄▄▇▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▅▃▅█▁▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92236
wandb: best/eval_avg_mil_loss 0.25666
wandb:  best/eval_ensemble_f1 0.92236
wandb:            eval/avg_f1 0.88652
wandb:      eval/avg_mil_loss 0.35512
wandb:       eval/ensemble_f1 0.88652
wandb:            test/avg_f1 0.91226
wandb:      test/avg_mil_loss 0.22796
wandb:       test/ensemble_f1 0.91226
wandb:           train/avg_f1 0.88262
wandb:      train/ensemble_f1 0.88262
wandb:         train/mil_loss 0.2169
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run colorful-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/an72l9ma
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042033-an72l9ma/logs
wandb: Agent Starting Run: 78ut9eu7 with config:
wandb: 	actor_learning_rate: 2.6288586164078054e-06
wandb: 	attention_dropout_p: 0.03528171558985249
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8554459419204408
wandb: 	temperature: 4.0724431285983815
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042206-78ut9eu7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/78ut9eu7
wandb: uploading wandb-summary.json
wandb: uploading history steps 174-184, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▅▆▆█
wandb: best/eval_avg_mil_loss ▇▆▅▂▁▅▃█▄
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▅▆▆█
wandb:            eval/avg_f1 ▅▇▆▇█▄▅▁▃▄▇▄▆▅▇▆▇▇▆▅▆▆▆▇▆▄▇▅▅▇▆▅▂▇▆▄█▆▃▇
wandb:      eval/avg_mil_loss ▄▃▃▃▃▃▃▁▂▅▄▁▂▄▄▄▂█▄▂▃▁▃▃▃▂▃▇▄▄▄▅▅▄▃▃▄▃▂▄
wandb:       eval/ensemble_f1 ▃▂▅▄▄▅▃▃▃▃▂▄▂▄▅▆▇▃▅▃▁▅▅▆▁▃▅▄▆▃▅▆▃▃▅▆▆▅█▂
wandb:           train/avg_f1 ▃▅▄▅▄▅▆▄▆▁▅▄▄▅▄▃▄▅▄▃▅▄▄▃▃▅▅▃▅▅▅▅▆▆▄▅█▄▃▅
wandb:      train/ensemble_f1 ▆▄▃▇▆▅▄▂▅▇▆▆▁▆▂▅█▄▆▇▂▆▅▄▄▇▄▅▅▄▇▃▅█▆▆▇▅▆▆
wandb:         train/mil_loss ▇▆▆██▆▆▇▅▇▅▇▄▄▆▇▆▆▅▄▄▄▄▆▆▃▄▁▄▃▅▄▃▃▃▃▄▃▃▁
wandb:      train/policy_loss ▅▅▅▅█▅▅▅▅▃▅▅▅▅▆▅▅▅▂▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92249
wandb: best/eval_avg_mil_loss 0.32327
wandb:  best/eval_ensemble_f1 0.92249
wandb:            eval/avg_f1 0.86343
wandb:      eval/avg_mil_loss 0.41759
wandb:       eval/ensemble_f1 0.86343
wandb:           train/avg_f1 0.8833
wandb:      train/ensemble_f1 0.8833
wandb:         train/mil_loss 0.84831
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lucky-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/78ut9eu7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042206-78ut9eu7/logs
wandb: ERROR Run 78ut9eu7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: t5pg820r with config:
wandb: 	actor_learning_rate: 0.00011500806257085972
wandb: 	attention_dropout_p: 0.20852219208274136
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 189
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5510767760005835
wandb: 	temperature: 5.2847969509308115
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042436-t5pg820r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t5pg820r
wandb: uploading history steps 119-128, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃██
wandb: best/eval_avg_mil_loss ▅█▃▁
wandb:  best/eval_ensemble_f1 ▁▃██
wandb:            eval/avg_f1 ▆▆▆▆▄▄▇██▄▄▃▃▄▇▄▂▅▄▃█▅▄▅▁▂▅▄▆▇▁▅▂▄▅▁▁▄▃▅
wandb:      eval/avg_mil_loss ▄▃▄▃▃▆▃▄█▂▄▃▄▇▆▁▇▃█▃▅▆▂▃▄▇▄▅▆▇▆▃▆▂▆▇▆▆▆▅
wandb:       eval/ensemble_f1 ▆▆▅▆▆▄▄▇█▃▅▆▄▄▃▄▄▃▂▂▃▄▃▁▂▅▄▄▄▅▃▄▃▃▂▇▄▂▄▆
wandb:           train/avg_f1 ▇▇▇▆█▅▇▅▇█▇▆▄▅▅▅▂▅▁▃▅▃▅▄▃▅▃▂▅▃▃▄▃▅▅▄▃▃▃▁
wandb:      train/ensemble_f1 █▇█▇▅▇▇▇▅█▆▇▆▄▅▅▅▃▄▃▄▅▂▅▁▃▆▃▄▄▃▅▃▂▂▃▃▄▃▄
wandb:         train/mil_loss ▇█▇▇▇▃▅▆▃▅▇▄▁▁▃▃▂▂▄▅▃▃▃▂▂▁▄▄▄▃▄▄▄▄▆▂▃▄▃▃
wandb:      train/policy_loss ▃▃▃▃▃▃▄▃▃▆▃▅▃▃▅▃█▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▆▅▃▅▅▁▅█▅▅▅▅▅▅▅▅▆▅▅▅▄▅▅▅▅▅▅▅▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91203
wandb: best/eval_avg_mil_loss 0.24921
wandb:  best/eval_ensemble_f1 0.91203
wandb:            eval/avg_f1 0.89405
wandb:      eval/avg_mil_loss 0.3178
wandb:       eval/ensemble_f1 0.89405
wandb:           train/avg_f1 0.86448
wandb:      train/ensemble_f1 0.86448
wandb:         train/mil_loss 0.21083
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run woven-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t5pg820r
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042436-t5pg820r/logs
wandb: ERROR Run t5pg820r errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: upg3ueh6 with config:
wandb: 	actor_learning_rate: 1.8125674829039468e-06
wandb: 	attention_dropout_p: 0.07792873795977151
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 158
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.29278472409294976
wandb: 	temperature: 8.439462172260537
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042700-upg3ueh6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/upg3ueh6
wandb: uploading history steps 112-131, summary; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇▇█
wandb: best/eval_avg_mil_loss ▅█▆▅▆▁
wandb:  best/eval_ensemble_f1 ▁▄▅▇▇█
wandb:            eval/avg_f1 ▄▂▅▂▃▄▄▇▁▆▄▆▅▅▄▃▅▄▆▆▆▄█▃▇▄▄▆▄▁▆█▆▇█▆▆▅▆▅
wandb:      eval/avg_mil_loss ▃▅▅█▃▅▅▅▄▅▄▃▅▃▄▅▅██▆▂▃▃▃▆▂▁▂▃▆▆▂▂▃▆▄▅▃▂▃
wandb:       eval/ensemble_f1 ▁▅▅▅▄▄▄▆▁▁▃▆▃▇▆▅▅▃▅▄▅▄█▆▃▄▃▆▅▄▁▆▅▆█▄▅▆▄▅
wandb:           train/avg_f1 ▅█▄▄▄▅▅▅▅▆▆▄█▇▅▆▇▇▃▆▃▆▅▇█▇▆▆▄▇▆▅▄▅▄▄▆▆▁▅
wandb:      train/ensemble_f1 ▃▁█▄▃▅▄▄▅▄▅▆█▇▄▇▂▇▅▆▆▅▇▃▇▅▆▆▆▄▆▄▅▃█▅▅██▄
wandb:         train/mil_loss █▆▇▇▇█▇▅▆▆▆▆▆▅▆▅▆▄▄▄▄▄▄▄▃▅▃▂▃▃▂▂▃▂▃▂▂▃▂▁
wandb:      train/policy_loss ██████████████▁█████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄███▁████▁██████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91898
wandb: best/eval_avg_mil_loss 0.24357
wandb:  best/eval_ensemble_f1 0.91898
wandb:            eval/avg_f1 0.88859
wandb:      eval/avg_mil_loss 0.30259
wandb:       eval/ensemble_f1 0.88859
wandb:           train/avg_f1 0.8813
wandb:      train/ensemble_f1 0.8813
wandb:         train/mil_loss 0.33039
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dry-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/upg3ueh6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042700-upg3ueh6/logs
wandb: ERROR Run upg3ueh6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qgb34jqg with config:
wandb: 	actor_learning_rate: 1.0559514048828589e-06
wandb: 	attention_dropout_p: 0.3345354558380369
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9832337054974918
wandb: 	temperature: 4.500371143067468
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042854-qgb34jqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qgb34jqg
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 93-100, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▆█
wandb: best/eval_avg_mil_loss ▆▄█▄▁
wandb:  best/eval_ensemble_f1 ▁▅▅▆█
wandb:            eval/avg_f1 ▅▄▅▅▆▃▃▆▅▅▃▆█▆▅▁▂▃▄▃▃█▄▃▇▆▇▆▄▅▂▃▅▃▆▆▃▅▄▃
wandb:      eval/avg_mil_loss ▃▄▆▃█▅▅▅▅▁▆▇▅▄▂▄▆▇▃▄▄▅▇▇▇▆▅▇▆▃▁▅▃▂▃▆▆▆▆▄
wandb:       eval/ensemble_f1 ▁▄▄▄▃▃▃▅▄▃▅▄█▄▃▁▂▃▃▂▄▃▃▂▅▄▅▄▄▅▄▃▄▂▂▃▄▄▂▃
wandb:           train/avg_f1 ▂▆▅▇▆▆▄▃▃▂▅█▄▄▄█▄▅▆▃▅▄▃▆▂▃▆▅▃▃▂▅▄▅▅▃▁▄▃▂
wandb:      train/ensemble_f1 ▂▇▆▅▄▇▆▄▄██▃▇▇▄▆▆▂▇▃▅▃▅▄▄▅▅▃▂▅▅▅▂▃▂▁▃▃▂▃
wandb:         train/mil_loss ▃▆▅▅▇█▅▃▄▅▄▄▅▇▅▇▆▃▅▄▄▂▄▂▃▃▃▃▃▄▁▅▁▃▂▄▅▆▃▃
wandb:      train/policy_loss ▅▁▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▃▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▁▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94464
wandb: best/eval_avg_mil_loss 0.19056
wandb:  best/eval_ensemble_f1 0.94464
wandb:            eval/avg_f1 0.87937
wandb:      eval/avg_mil_loss 0.292
wandb:       eval/ensemble_f1 0.87937
wandb:           train/avg_f1 0.87797
wandb:      train/ensemble_f1 0.87797
wandb:         train/mil_loss 0.21931
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run misunderstood-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qgb34jqg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042854-qgb34jqg/logs
wandb: ERROR Run qgb34jqg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: y2asd65g with config:
wandb: 	actor_learning_rate: 0.0004409001634680523
wandb: 	attention_dropout_p: 0.22379582740335088
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 187
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.41900983165393046
wandb: 	temperature: 0.1731217222389858
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043021-y2asd65g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y2asd65g
wandb: uploading history steps 122-134, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▄▃▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▆█▆█▆▇▆▇▇▇▇▆█▇▆▇▆▅▄▆▆▅▆▆▅██▅▅▅▅▆▄▆▅▄▁▄▄▁
wandb:      eval/avg_mil_loss ▃▆▄▆▁▂▄▆▅█▄▃▄▆▅▄▄▁▄▄▃▄▁▂▅▄▄▄█▅█▅▅▄▅▅▅▅▆▇
wandb:       eval/ensemble_f1 ▆█▇▆▇▇▆▇▆▇█▇▆▄█▅▇▇▇▄▆▆█▆▅▇▄▆▅▆▅▅▆▃▅▄▅▃▃▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▆▇▇▆▆▆▇▇▅▇▆▆▅▄▅▅▇▆▄▅▃▅▄▃▅▅▅▂▄▃▃▂▅▂▃▂▁▂▁
wandb:      train/ensemble_f1 ▇▇▇▇▇▇▇▆▆▇▇██▅▇▆▅▆█▆▆▆▇▆▄▆▅▅▃▃▄▄▄▄▃▄▁▃▂▃
wandb:         train/mil_loss ▇▇▇▆▆▇▇▆▆█▄▅▅▄▆▄▅▅▄▄▃▅▃▂▄▂▄▃▃▃▃▃▂▂▂▅▃▁▃▃
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▆▄▄██▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▁▅▇▅▅▅▅▅▅▅▅▅▅▅▅█▆▇▇▅▅▅▅▅▄▆▅▅▆▅▅▅█▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91886
wandb: best/eval_avg_mil_loss 0.23464
wandb:  best/eval_ensemble_f1 0.91886
wandb:            eval/avg_f1 0.82836
wandb:      eval/avg_mil_loss 0.38642
wandb:       eval/ensemble_f1 0.82836
wandb:            test/avg_f1 0.8828
wandb:      test/avg_mil_loss 0.24406
wandb:       test/ensemble_f1 0.8828
wandb:           train/avg_f1 0.8432
wandb:      train/ensemble_f1 0.8432
wandb:         train/mil_loss 0.18844
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run silver-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y2asd65g
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043021-y2asd65g/logs
wandb: Agent Starting Run: bnjjk51c with config:
wandb: 	actor_learning_rate: 6.915713872394583e-05
wandb: 	attention_dropout_p: 0.1264486218508976
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 73
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0254451952924869
wandb: 	temperature: 1.4183696179718608
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043220-bnjjk51c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bnjjk51c
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅▆▇█
wandb: best/eval_avg_mil_loss ▃▄▆▁▂▂█
wandb:  best/eval_ensemble_f1 ▁▃▄▅▆▇█
wandb:            eval/avg_f1 ▃▅▄▅▄▄▃▃▃▆▅▆▅▁▄▆▃▃▄▆▇▄▇▂█▃▅▆▂▄█▃▆▅▃▄▄▅▁▅
wandb:      eval/avg_mil_loss ▃▃▅▅▆▄▃▄▃▇▆▃▅▅▁▄▄▁▂▇▄▄▂▃▃▃▅▂▃▄▂█▆▄▂▄█▂▄▃
wandb:       eval/ensemble_f1 ▂▄▃▅▁▄▅▂▃▃▃▅▁▄▅▃▄▃▄▅▆▆▂▇▅▄▄▆▃▅▅▃▃▄▄█▅▁▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▇▄▂▄▃▅▄▅▁▅▂▅▃▄▆▄▆▇▅▅▄█▅▃▆▅▇▆▅▆▆▆▇█▇▄▄▄▅
wandb:      train/ensemble_f1 ▄▇▄▂▄▅▄▄▁▅▄▅▃▄▄▆▅▆▅▅█▅▇▇▃▃▇▆▅▄▆▅▆▇▅█▇▄▄▅
wandb:         train/mil_loss ▁▃▅▃▆▆▄▆█▂▅▅▄▄▅▅▂▄▃▄▂▅▅▄▂▄▃▃▄▁▆▄▄▂▂▆▂▅▁▂
wandb:      train/policy_loss ▁███████████████████████████▁███▂▇██████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆█▂▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▂▆▆▆▅▄▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91556
wandb: best/eval_avg_mil_loss 0.46163
wandb:  best/eval_ensemble_f1 0.91556
wandb:            eval/avg_f1 0.87151
wandb:      eval/avg_mil_loss 0.30742
wandb:       eval/ensemble_f1 0.87151
wandb:            test/avg_f1 0.86934
wandb:      test/avg_mil_loss 0.32714
wandb:       test/ensemble_f1 0.86934
wandb:           train/avg_f1 0.86522
wandb:      train/ensemble_f1 0.86522
wandb:         train/mil_loss 1.89084
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run daily-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bnjjk51c
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043220-bnjjk51c/logs
wandb: Agent Starting Run: 9w4xz89a with config:
wandb: 	actor_learning_rate: 0.0002454524259680078
wandb: 	attention_dropout_p: 0.04952620026606441
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 81
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.01426285814089745
wandb: 	temperature: 0.672784488338598
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043327-9w4xz89a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9w4xz89a
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 75-81, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▆▆▇█
wandb: best/eval_avg_mil_loss █▃▂▁▅▁
wandb:  best/eval_ensemble_f1 ▁▂▆▆▇█
wandb:            eval/avg_f1 ▄▆▆▂▅▅▇▄▃▆▄▃█▇▆▂▄▆▇▆▆▅▆▆▃▂▅▂▂▄▁▂▅▅▁▇▁▁▆▅
wandb:      eval/avg_mil_loss ▇▃▄▅▅▃▅▂▂▆▅▅█▅▄▅▃▁▄▄▆▅▃▅▄▄▃▄▄▆█▃▃▂▃▅▃▃▄▃
wandb:       eval/ensemble_f1 ▃▄▇▅▄▇▃▅▄▅▅▄▅█▆▇▇▆▃▅▇▅▂▅▄▄▇▁▅▅▃▇▆▅▂▄▇▅▃▅
wandb:           train/avg_f1 ▇▅▇▅▅▇▅▇▇█▆▅█▆▆▆▆▄▆▅▂▃▂▅▂▄▂▂▅▆▁▁▃▂▅▆▇█▅▄
wandb:      train/ensemble_f1 ▄▃▅▄▃▄█▅▃▆▅▃▆▆▄▅▄▅▄▄▅▄▂▁▁▃▂▄▅▄▁▃▂▃▄▅▆▆▄▅
wandb:         train/mil_loss ▇▆▇▇▄▄▆▄▆▅▅▃▅▄▃▄▆▄▁▇▆▇█▃▆▃▇▇▄▃▇▃▂▃▅▆▄▄▂▅
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▁▄▄▄▄▁▄▄▄▄▄▄▄▁▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▇▃▃▃▃▁▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91546
wandb: best/eval_avg_mil_loss 0.29639
wandb:  best/eval_ensemble_f1 0.91546
wandb:            eval/avg_f1 0.89715
wandb:      eval/avg_mil_loss 0.28376
wandb:       eval/ensemble_f1 0.89715
wandb:           train/avg_f1 0.8831
wandb:      train/ensemble_f1 0.8831
wandb:         train/mil_loss 0.3303
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run ruby-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9w4xz89a
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043327-9w4xz89a/logs
wandb: ERROR Run 9w4xz89a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: opus2oyo with config:
wandb: 	actor_learning_rate: 0.00014389295952577802
wandb: 	attention_dropout_p: 0.3063701596271806
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 150
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.544688218241387
wandb: 	temperature: 9.19005055566129
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043441-opus2oyo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/opus2oyo
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▄▅▆▆▇▇█
wandb: best/eval_avg_mil_loss ▇▇█▁▆▄▃▇▄▁▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▄▅▆▆▇▇█
wandb:            eval/avg_f1 ▁▃▄▆▂▅▄▅▅▆▃▇▆▆▂▆▇▄▃▄▄▅▇▃▄▅██▄▅▄▆▆▃▅▆▄▅▅▆
wandb:      eval/avg_mil_loss ▄▂▂▂▂█▃▁▃▃▃▂▁▅▂▅▃▄▁▁▅▃▁▃▄▃▄▂▂▂▄▅▃▁▃▅▁▁▂▂
wandb:       eval/ensemble_f1 ▂▄▅▅▆▆▅▆▁▆▅▆▇▅▅▅▇▇▄▇▆▄█▆██▅▅▆▆▇▅▄▅▄▁▅▆▆█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▂▃▂▃▃▅▄▆▅▄▄▃▅▄▃▆▅█▅█▅▃▄▇▄▆▃▆▄▄▄▆▆▅▆▅▆▄
wandb:      train/ensemble_f1 ▃▁▃▃▃▃▆▅▂▆▁▃▆▄█▂▃▆▆▇▂▅▅▂▂▆▇▇▂▃▃▄▄▆▆▂▇▇▅▃
wandb:         train/mil_loss ▇█▇█▇▆▆▆▆▇▅▆▅▅▆▄▆▃▇▅▄▅▅▄▄▅▄▄▄▃▄▄▄▃▃▂▁▂▂▃
wandb:      train/policy_loss ▅▅█▁▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇▅▅▆▅▆▆▆▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▆▅▅▅▅▅▅▅▅▅▆▅▅▅▅▅▄▂▅█▅▁▅▅▅▅▆▅▅▅▅▅▅▇▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90344
wandb: best/eval_avg_mil_loss 0.32079
wandb:  best/eval_ensemble_f1 0.90344
wandb:            eval/avg_f1 0.83947
wandb:      eval/avg_mil_loss 0.31739
wandb:       eval/ensemble_f1 0.83947
wandb:            test/avg_f1 0.85481
wandb:      test/avg_mil_loss 0.32832
wandb:       test/ensemble_f1 0.85481
wandb:           train/avg_f1 0.85295
wandb:      train/ensemble_f1 0.85295
wandb:         train/mil_loss 2.03317
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run eager-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/opus2oyo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043441-opus2oyo/logs
wandb: Agent Starting Run: 5944t3rz with config:
wandb: 	actor_learning_rate: 3.094461915382063e-06
wandb: 	attention_dropout_p: 0.46123919654742246
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8167147172567719
wandb: 	temperature: 1.279069235919461
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043656-5944t3rz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5944t3rz
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▄▆█
wandb: best/eval_avg_mil_loss █▅▆▂▆▁
wandb:  best/eval_ensemble_f1 ▁▃▄▄▆█
wandb:            eval/avg_f1 ▅▇▅▇▄▅▅▅▆▆▇█▆▂██▃▆▅▄▇▅▄▂▅▇▃▃▆▄▆██▆▅▃▃▄▁▅
wandb:      eval/avg_mil_loss ▅▄▃▅▄▃▄▅▃▄▄▃▄▆██▄▃▆▆▄▄▃▄▅▅▇▄▄▅▃▅▃▄▁▅▄▄▃▄
wandb:       eval/ensemble_f1 ▇▆▇▄▆▃▄▅▆▆▁▃▄▃█▇▆█▃▄▆▆▄▇▇▅▄▇▃▄▄▆▄█▆▃▇▆▄▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▅▅▃▅▂▂▆▃▂▁▅▅▄▅▂▁▄▄▂▄▅█▂▆▂▄▇▅▄▃▂▆▇▄▄▅▇▅
wandb:      train/ensemble_f1 ▇▄▃▅▃▅▂▆▂▆▆▂▁▂▄▆▃▄▄▁▄▅▂█▁▅▇▅▂▂▂▄▂▄▆▄▅▇▄▅
wandb:         train/mil_loss ▄▅▃▃▆▄▅▄▄▄▄▃▆▅▂▃▅▂▄▁▂▂▃▄▂▃▃▃▄▂▂█▃▃▃▅▄▂▁▂
wandb:      train/policy_loss ▆▆▆█▆▆▆▆▆▆▆▄▆▆▇▆▆▆▇▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▁▂▁▁▁█▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.25677
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.88558
wandb:      eval/avg_mil_loss 0.30677
wandb:       eval/ensemble_f1 0.88558
wandb:            test/avg_f1 0.90768
wandb:      test/avg_mil_loss 0.2139
wandb:       test/ensemble_f1 0.90768
wandb:           train/avg_f1 0.88261
wandb:      train/ensemble_f1 0.88261
wandb:         train/mil_loss 0.36129
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run elated-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5944t3rz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043656-5944t3rz/logs
wandb: Agent Starting Run: 8pd68eb4 with config:
wandb: 	actor_learning_rate: 8.227980911354214e-06
wandb: 	attention_dropout_p: 0.24064869573924164
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 103
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7342444141786963
wandb: 	temperature: 5.379231181650482
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043824-8pd68eb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8pd68eb4
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▆▆▆█
wandb: best/eval_avg_mil_loss ▅▂█▅▅▆▁
wandb:  best/eval_ensemble_f1 ▁▄▄▆▆▆█
wandb:            eval/avg_f1 ▅▆▂▄▅▄▅▄▃▆▆▄▄▂▆█▁▄▄▄▁▁▅▅▁▂▄▅▃▄▁▆▇▂▄▇▃▃▃▅
wandb:      eval/avg_mil_loss ▂▄▄▅▃▅▄▃▂▅▅▆▅▁▄▆▃█▇▄▅▄▃▅▇█▇▄▄▄▅▄▅▆▆▅▅▆▄▅
wandb:       eval/ensemble_f1 ▅▇▇▂▆▇▆▇▅▇▂▇▂▃▄▇▆▇▂▆▂▄▂▆▃▆▃▆▅▁▄█▂▅▄▄▃▅▆▅
wandb:           train/avg_f1 ▇▄▂▇▄▂▆█▆▅▆▁▂▃█▅▂▅▅▃▄▆▄▅▄▇▆▅▁▆▇▅▁▃▃▆▆▅▂▇
wandb:      train/ensemble_f1 ▄▂▆▇▂▂▂▅▆▅▄▅▆▁▂▅▂▃▅▄▆▆▄▄▆▃▆▆██▅▁▇▃▁▄▁▆▆▃
wandb:         train/mil_loss ███▆▇▆▆▅▆▇▆▆▅▄▆▅▄▄▃▄▄▃▃▃▂▃▂▂▃▃▂▂▃▂▂▁▂▂▃▁
wandb:      train/policy_loss ▄▅▄▃▄▄▆▄▅▄▄▄▄▁▅▄▄▄▄▄▄▄▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▁▁▁▁▁▃▁▁▁▁▂▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91873
wandb: best/eval_avg_mil_loss 0.19847
wandb:  best/eval_ensemble_f1 0.91873
wandb:            eval/avg_f1 0.88859
wandb:      eval/avg_mil_loss 0.30728
wandb:       eval/ensemble_f1 0.88859
wandb:           train/avg_f1 0.89526
wandb:      train/ensemble_f1 0.89526
wandb:         train/mil_loss 0.28467
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wise-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8pd68eb4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043824-8pd68eb4/logs
wandb: ERROR Run 8pd68eb4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	size mismatch for task_model.mlp.0.weight: copying a param with shape torch.Size([256, 22]) from checkpoint, the shape in current model is torch.Size([64, 22]).
wandb: ERROR 	size mismatch for task_model.mlp.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
wandb: ERROR 	size mismatch for task_model.mlp.3.weight: copying a param with shape torch.Size([2, 256]) from checkpoint, the shape in current model is torch.Size([2, 64]).
wandb: ERROR 
wandb: Agent Starting Run: 6uji1qns with config:
wandb: 	actor_learning_rate: 6.852896350696967e-06
wandb: 	attention_dropout_p: 0.3815252515349368
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 152
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6901246311613068
wandb: 	temperature: 6.095774336702638
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043957-6uji1qns
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6uji1qns
wandb: uploading wandb-summary.json
wandb: uploading history steps 146-153, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▅▅▇█
wandb: best/eval_avg_mil_loss █▃▆▂▁▂▁
wandb:  best/eval_ensemble_f1 ▁▄▅▅▅▇█
wandb:            eval/avg_f1 ▄▄▄▆▆▄▆▇▅▆▆▆▆▆▇▄▃▆▆▅█▄▁█▄▅▇▆▅▃▄▅▇▆▅▄▆▅▆▄
wandb:      eval/avg_mil_loss ▄▃▄▁▂▁▂▄▃▃▃▃▃▂▃▁▄▂▇▃▅▆▄▄▄▄▃█▃▃▁▄▅▃▂▅▃▅▅▄
wandb:       eval/ensemble_f1 ▁▃▅▄▄▃▄▄▄▅▅▄█▄▆▅▃▅█▃█▄▆▃▇▂▇▄▃▄▅▇▄▄▇▄▆▂▃▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▁▃▃▄▄▂▄▅▇▄▁▅▅▃▅▅▃▃█▅▅▃▆▆█▆▅▄▃▇▅▆▅▆▅▂▃▄▆
wandb:      train/ensemble_f1 ▅▁▂▄▂▂▄▃▄▇▅▁▃▂▅▂▄▄▆▆▂▄▄▃▆▅▅▃▄▇█▅▄▅▄▅▄▄▂▆
wandb:         train/mil_loss ▁▁▂▃▃▃▄▄▄▄▅▅▆▅▅▅▆▆▅▆▅▇▇█▅▆▆▆▅▇▇▇▆▇▅▆▅▇▅▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.23619
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.87756
wandb:      eval/avg_mil_loss 0.37732
wandb:       eval/ensemble_f1 0.87756
wandb:            test/avg_f1 0.88246
wandb:      test/avg_mil_loss 0.21016
wandb:       test/ensemble_f1 0.88246
wandb:           train/avg_f1 0.8918
wandb:      train/ensemble_f1 0.8918
wandb:         train/mil_loss 1.72701
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run morning-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6uji1qns
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043957-6uji1qns/logs
wandb: Agent Starting Run: gkrnh9fe with config:
wandb: 	actor_learning_rate: 0.0001003568230083866
wandb: 	attention_dropout_p: 0.14379542034034298
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 132
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9416489373849286
wandb: 	temperature: 4.534581204532594
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044211-gkrnh9fe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gkrnh9fe
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 122-133, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆██
wandb: best/eval_avg_mil_loss ▆▆█▁▇
wandb:  best/eval_ensemble_f1 ▁▅▆██
wandb:            eval/avg_f1 ▄▆▆▁▅▃▅▄▄▂▅▆▃▄▇▃▅▃▂▄█▄▅▃▁▅▅▅▅▅▂▄▅▅▂▁▇▃▄▄
wandb:      eval/avg_mil_loss ▃▁▂▇▅▇▅▄▃▃▂▂▇▃▂▅▁▅▃▅▄▃▄█▃▃▄▁▇▄▄▄█▅▃▅▅▄▅▃
wandb:       eval/ensemble_f1 ▅▅▅▅▃▆▄▁▇▃▄▂▄▃▅▃▅▂▃▄▅▄▆▃▁▃▅▄▆▃▄▃▄▄▃▆▇▃█▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▃▄▅▄▄▄▆▅▅▄█▃▃▄▅▅▅▆▄▅▃▄▃▆▁▅▅▅▅▆▆▅▅▇▃▇█▃▆
wandb:      train/ensemble_f1 ▁▄▄▄▆▄▅▆▄▅▅▄▃▅▄▄▄▄▅▅▄▃█▃▆▃▅▁▅▅▃▅▄▆▂▅▄▃▃▅
wandb:         train/mil_loss ▄▆▅▇▆▅▇▆▁▇▄▆▆▆▄▆▆▅▄▅▆▅█▆▅▅▇▅▆▆▅▄▅▅▅▅▆▄█▅
wandb:      train/policy_loss ▁███████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9338
wandb: best/eval_avg_mil_loss 0.26846
wandb:  best/eval_ensemble_f1 0.9338
wandb:            eval/avg_f1 0.89657
wandb:      eval/avg_mil_loss 0.31328
wandb:       eval/ensemble_f1 0.89657
wandb:            test/avg_f1 0.884
wandb:      test/avg_mil_loss 0.2371
wandb:       test/ensemble_f1 0.884
wandb:           train/avg_f1 0.89587
wandb:      train/ensemble_f1 0.89587
wandb:         train/mil_loss 0.21844
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run confused-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gkrnh9fe
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044211-gkrnh9fe/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: lz2ga32a with config:
wandb: 	actor_learning_rate: 0.0004620503794487003
wandb: 	attention_dropout_p: 0.1313415040301335
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 164
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05624128409483808
wandb: 	temperature: 1.748239405417641
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044439-lz2ga32a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lz2ga32a
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆██
wandb: best/eval_avg_mil_loss █▇▇▁
wandb:  best/eval_ensemble_f1 ▁▆██
wandb:            eval/avg_f1 ▄▄█▄▄▆▄▆▄▅▇█▅▅▅▇▆▆▆▄▄▆▇▆▃▄▄▃▄▄▄▃▄▄▄▅▆▁▃▄
wandb:      eval/avg_mil_loss ▅▁▃▄▅▅▄▄▄▅▃▅█▃▅▇▄▅▅▆▆▅▅▃▆▆▇▄▆▄▄▇▇▄█▇█▆▆▆
wandb:       eval/ensemble_f1 █▅▃▄▇▄▃▄▂▃▅▆▄▆█▇▅▄▁▆▃▃▃▅▇▇▂▄▁▆▆▄▂▃▅▃▆▃▄▃
wandb:           train/avg_f1 ▄▄▄▄▅▂▂▅▇▆▁▅▁▃▇▃▃▁▆▃█▁▃▄▆▅▄▃▁▄▅▆▂▂▃▄▇▂▄▇
wandb:      train/ensemble_f1 ▅▄▆█▃▆▄▆▆▆▃▆▃▆▄▂▅▃█▆▂▄▂▃▃▅▅▃▁▆▅▅▂▆▆▃▇▃▃▅
wandb:         train/mil_loss █▅▄▇▅▆▄▄█▅▃▆▃▆▅▃▅▄▅▂▅▃▅▄▆▂▅▅▅▂▂▂▂▃▃▁▁▂▂▃
wandb:      train/policy_loss ▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▆▂▂█▂▂▂▂▂▂▂▂▄▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9186
wandb: best/eval_avg_mil_loss 0.25372
wandb:  best/eval_ensemble_f1 0.9186
wandb:            eval/avg_f1 0.8804
wandb:      eval/avg_mil_loss 0.35227
wandb:       eval/ensemble_f1 0.8804
wandb:           train/avg_f1 0.88638
wandb:      train/ensemble_f1 0.88638
wandb:         train/mil_loss 1.04993
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lz2ga32a
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044439-lz2ga32a/logs
wandb: ERROR Run lz2ga32a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 86picw9i with config:
wandb: 	actor_learning_rate: 7.509152130907468e-06
wandb: 	attention_dropout_p: 0.08804634022462171
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 109
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.006658138983044348
wandb: 	temperature: 5.993617388371186
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044710-86picw9i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/86picw9i
wandb: uploading wandb-summary.json
wandb: uploading history steps 92-109, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▇█
wandb: best/eval_avg_mil_loss █▃▃▄▁
wandb:  best/eval_ensemble_f1 ▁▃▅▇█
wandb:            eval/avg_f1 ▃▇▄▆▄▆▃▁▃▅▄▅▆▃▇▃▁▅▆▃▆▃▆▄▄▃▂▄▇▄▅▅▃▆██▄▁▃▃
wandb:      eval/avg_mil_loss ▂▄▄▃▁▅▃▃▃▄▂▄▄▄▃▅▁▆▃▅▃▃▇▄▄▁▅█▂▂▄▅▂▅▂▅▅▂▃▂
wandb:       eval/ensemble_f1 ▅▃▄▃▂▃█▃▅▂▂▄▆▅▅▂▅▄▂▄▅▂▅▆▄▄▃▃▅▆▄▂▆▂▃▇▁▆▃▂
wandb:           train/avg_f1 ▇▅▇▆▃█▆▄▂▇▆▅▁█▅▃▇▅█▆▃▄█▆▅▅▅▆▂▄▅▂▅▆▄▇▇▂█▅
wandb:      train/ensemble_f1 ▅▆▅▂▄▅▅▅▃▄▁▇▅▄▄▃▆█▄▇▅▃▃▃▇▅▃▃▃▄▄▁▇▅▇▆▂▆▇▄
wandb:         train/mil_loss ▅▄▇▆▅▆▃█▆▅█▆█▄▂▆▄▇▄▄▇▆▅▇▄▅▄▅▁▅▅▂▃▅▂▃▆▄▅▃
wandb:      train/policy_loss ████████████████████████████▁███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▁▅▅▁▁▁▅▁▁█▅▅▅▁█████▅▅██▅▅▅▅▅▅▁▅▅▅▅▁█▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93371
wandb: best/eval_avg_mil_loss 0.23892
wandb:  best/eval_ensemble_f1 0.93371
wandb:            eval/avg_f1 0.88859
wandb:      eval/avg_mil_loss 0.2896
wandb:       eval/ensemble_f1 0.88859
wandb:           train/avg_f1 0.89007
wandb:      train/ensemble_f1 0.89007
wandb:         train/mil_loss 0.2588
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run winter-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/86picw9i
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044710-86picw9i/logs
wandb: ERROR Run 86picw9i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: c3392oq0 with config:
wandb: 	actor_learning_rate: 1.866996988213252e-06
wandb: 	attention_dropout_p: 0.22359299528795945
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 77
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9062386183423524
wandb: 	temperature: 8.417074913544232
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044854-c3392oq0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/c3392oq0
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆█
wandb: best/eval_avg_mil_loss ▇█▁▃
wandb:  best/eval_ensemble_f1 ▁▄▆█
wandb:            eval/avg_f1 ▁▇▅▄▆▄▅▄▁▄▄▄█▅▃▂▅▆▄▄▃▆▆▆▅▂▅▅▃▅▅▃▄▃▅▇▃▃▄▄
wandb:      eval/avg_mil_loss ▂█▂▂▅▄▃▂▄▄▅▃▂▃▄▄▂▅▅▄▁▄▅▂▄▁▃▅▃▂▅▅▄▅▁▃▃▃▃▂
wandb:       eval/ensemble_f1 ▅▁▇▃▅▄▆▅▄▄▄▄█▅▃▂▅▄▄▄▅▇▂▄▄▄▃▃▃▅▄▄▅▂▄▅▅▃▃▅
wandb:           train/avg_f1 ▅█▆▃▁▄▅▆█▅▄▇▅▇█▇▄▃▇▄▆▅▂▂▂▄▃▂▂▆▅▅▂▅▅▆▅▃▃▄
wandb:      train/ensemble_f1 ▇▇▄▃▁▆█▅▇▅▇▇▃▇▇█▄▄▃▅▆▅▆▅▃▂▂▄▄▃▄▂▅▃▆▆▄▃▃▁
wandb:         train/mil_loss ▄▄▇▇▇▇▅▄▃▆▆▅▅█▅▇▅▆▆▃▄█▅▃▄▄▄▃▄▆▇▇▅▃▂▅▁▆▇▄
wandb:      train/policy_loss ▄▄▄▄▆▃▄▄▄▄▆▄▄▄▄█▄▁▄▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▆▅▅▅▅▅▅▅▇▆▅▅▅▅▅█▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92669
wandb: best/eval_avg_mil_loss 0.24768
wandb:  best/eval_ensemble_f1 0.92669
wandb:            eval/avg_f1 0.88245
wandb:      eval/avg_mil_loss 0.26583
wandb:       eval/ensemble_f1 0.88245
wandb:           train/avg_f1 0.88767
wandb:      train/ensemble_f1 0.88767
wandb:         train/mil_loss 0.18246
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wandering-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/c3392oq0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044854-c3392oq0/logs
wandb: ERROR Run c3392oq0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: emkvrdxk with config:
wandb: 	actor_learning_rate: 1.022795878600108e-06
wandb: 	attention_dropout_p: 0.4972683153927244
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 124
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7066211136869142
wandb: 	temperature: 3.661166960617317
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045016-emkvrdxk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/emkvrdxk
wandb: uploading history steps 119-124, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▁▅▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▂▃▃▃▅▄▃▇▂▆▅▅▂▃▅▆▄▆▅▅▂▆█▄▃▇▃▅▆▆▅▅▁▅▃▄▂▄▃▅
wandb:      eval/avg_mil_loss ▃▃▆▆▃█▅▇▁▅▆▄▅▃▄▅▅▃▃▆▃▄▄▂▂▅▅▄▅▃▅▃▄▃▁▃▂▄▃▂
wandb:       eval/ensemble_f1 ▃▁▄▆▂▄▅▄▂▅▇█▃▅▄█▅▄▃▆▅▆▂▄▅▆▆██▆▇▃▇▆▁▅▃▅▆▃
wandb:           train/avg_f1 █▂▆▂▄▅▅▆▅▄▆▄▅▄▄▃▂▃▄▃▃▃▆▂▅▅▅▄▂▅▄▄▅▇▁▅▅▃▄▆
wandb:      train/ensemble_f1 ▄▅▄▆▃▅▅▄▇▆▅▄▆▄▃▃▅▄▃▄▁▂▅▃▅▄▅▂▄▅▅▆▇▄▂█▇▃▆▇
wandb:         train/mil_loss ▅▄▄▃▆▅▅█▃▇▆▄▄▅▆▆▃▅▆▅▄▅▆▄▄▄▃▇▅▅▃▃▄▄▄▃▂▁▂▁
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▇▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▇██▆█████████████▁██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.2348
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.8742
wandb:      eval/avg_mil_loss 0.34167
wandb:       eval/ensemble_f1 0.8742
wandb:           train/avg_f1 0.89291
wandb:      train/ensemble_f1 0.89291
wandb:         train/mil_loss 1.40138
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run zany-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/emkvrdxk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045016-emkvrdxk/logs
wandb: ERROR Run emkvrdxk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ve8g48mh with config:
wandb: 	actor_learning_rate: 0.00016178370950941626
wandb: 	attention_dropout_p: 0.06802419398899728
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 79
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.33122802975258503
wandb: 	temperature: 0.0003135861169933918
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045220-ve8g48mh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ve8g48mh
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 73-79, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss █▃▁
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▅▁█▄▅▆▄▂▅█▅▆▄▅▁▆▆▅▄▄▄█▆▇█▆▅▅▆▅▅▄█▆▆▄▂▄█▅
wandb:      eval/avg_mil_loss ▅▆▄▅█▄▇▃▆▄▅▆▂▃▁█▅▅█▃▂▃▅▆▃▂▅▄▅▅▃▄▆▄▄▄▅█▆▃
wandb:       eval/ensemble_f1 ▅▁▄█▄▃▄▄▅▄▅▆▅▆▄▄▅▄▅▆▆▄▄▄▄▇▄▆▅▆▅▅▆▆▆▅▄▄▆▆
wandb:           train/avg_f1 ▇▆▅▅▇▄▁▅▄▅▆▅█▄▇▄▅▅▆▄▄▄▇▃▆▆▆▆▄▃▄▃█▅▇▁▅▅▅█
wandb:      train/ensemble_f1 ▇▇▆▅▅▃▅▁▂▅▆▇▆▅▇▅▆▆▄▄▃▆▅▆▆▆▅▃▅▄██▅▅▇▁▅▄▅▅
wandb:         train/mil_loss ▂▅▂▂▇▄▃▂▃▃▆▂▃▅▂▃▃▁█▄▂▅▃▄▂▃▃▃▆▃▃▁▃▃▂▃▅▃▃▂
wandb:      train/policy_loss ▅▅▁▅▁▅▅▅█▅▅▅█▅▁▅▅▅▅▅▁▁█▅▁▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▁█▅▅▅▅▅▅▅▁▅▅▅▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▁▅▅█▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.34521
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.86989
wandb:      eval/avg_mil_loss 0.25711
wandb:       eval/ensemble_f1 0.86989
wandb:           train/avg_f1 0.89493
wandb:      train/ensemble_f1 0.89493
wandb:         train/mil_loss 1.29114
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run summer-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ve8g48mh
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045220-ve8g48mh/logs
wandb: ERROR Run ve8g48mh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: sm4jndxn with config:
wandb: 	actor_learning_rate: 1.1964381064967684e-06
wandb: 	attention_dropout_p: 0.13380866316555912
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6805942510875852
wandb: 	temperature: 0.932871549416654
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045332-sm4jndxn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sm4jndxn
wandb: uploading output.log; uploading config.yaml
wandb: uploading history steps 100-112, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅▆█
wandb: best/eval_avg_mil_loss ▄▁▁▃█▄
wandb:  best/eval_ensemble_f1 ▁▂▄▅▆█
wandb:            eval/avg_f1 ▃▅▃▄▄▄▆▃▅▄▃▃▃▄▁█▆▆▄▅▃▆▅▄▄▁▃▄▆▄▅▄▁▂▄▆▂▃▃▂
wandb:      eval/avg_mil_loss ▄▃▆▂▃▂▄▂▄▄█▇▆▆▅▄▃▄▆▂▄▁▄▄▂▄█▄▁▂▃▂▅▂▃▆▆▄▄▂
wandb:       eval/ensemble_f1 ▃▅▄▂▄▄▆▄▄▃▃▃▄▅▅▅▁▄█▆▄▅▃▃▃▄▁▇▄▃▆▃▅▄▃▆▄▄▂▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆█▇▇▇▁▇▇▇▄▅▅▅▃▄▆▆▅▂▂▁▆▂▄█▄▃▇█▅▃▂▇▅▅▆▅▁▃
wandb:      train/ensemble_f1 ▅▅▅▇█▆▇▆▄▇▆▄▄▃▄▅▅▄▄▆▄▅▇▅▆▄▅▂▃▅▆▃▄▆▃▄▆▄▄▁
wandb:         train/mil_loss █▆▅▄█▆▄▂▆▇▅▄▃▄▄▃▅▅▇▄█▁▅▇▅▅▅▄▇▅▂▃▄▄▃▄▃▃▁▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅█▅▅▅█▁▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████▁██▅███████▆█▆███
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93025
wandb: best/eval_avg_mil_loss 0.32603
wandb:  best/eval_ensemble_f1 0.93025
wandb:            eval/avg_f1 0.88316
wandb:      eval/avg_mil_loss 0.40419
wandb:       eval/ensemble_f1 0.88316
wandb:            test/avg_f1 0.88642
wandb:      test/avg_mil_loss 0.23084
wandb:       test/ensemble_f1 0.88642
wandb:           train/avg_f1 0.88125
wandb:      train/ensemble_f1 0.88125
wandb:         train/mil_loss 0.24939
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sage-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sm4jndxn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045332-sm4jndxn/logs
wandb: Agent Starting Run: r3ibg2o4 with config:
wandb: 	actor_learning_rate: 2.9382889049187977e-06
wandb: 	attention_dropout_p: 0.37047042837944266
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 139
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2986288970984661
wandb: 	temperature: 4.642507486805445
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045521-r3ibg2o4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r3ibg2o4
wandb: uploading wandb-summary.json; uploading history steps 105-122, summary
wandb: uploading history steps 105-122, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▆▇▇█
wandb: best/eval_avg_mil_loss ▃▆█▁▄▂▂
wandb:  best/eval_ensemble_f1 ▁▁▅▆▇▇█
wandb:            eval/avg_f1 ▃▁▃▇█▆█▆▄▅▆█▃▇▅▇▆▆▄▄▆▅▆▃▆▄▅▆▄▆▄▃▇▆▁▅▄▆▅▆
wandb:      eval/avg_mil_loss ▃▅▅▄▇█▃▃▂▅▃▅▂▄▇▄▄▂▄▂▃▃▄▆▃▄▆▃▄▁▂▄▄▃▂▄▄▂▃▃
wandb:       eval/ensemble_f1 ▄▇▁▃▇█▆▆▄▅▅▃▃▅██▇▂▄▇▇▅▄▆▆▇█▆▇█▆▄▅▇▅▄▆▂▆▆
wandb:           train/avg_f1 ▆▂▅▄▄▄▄▄▆▅▃▄▂▄▃▅▅▄▆▄█▄▆▆▅▄▅▅▂▆▁▃▁▃▃▂▄▃▅▄
wandb:      train/ensemble_f1 ▅▄▆▇▂▄▆▆▄▆▇▆▅▄▅▇▄▆▄▆██▅▇▆▇▅▇▇▇▄▆▁▄▄▆▆▆▅▇
wandb:         train/mil_loss ▆█▆▆▇▄▅▄▇▅▅▇▇▆▃▃▆▆▆▅▅▅▅▇█▅▆▆▃▅▁▆▃▆▇▃▇▅▅▅
wandb:      train/policy_loss ██████▁█████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▁▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9338
wandb: best/eval_avg_mil_loss 0.28134
wandb:  best/eval_ensemble_f1 0.9338
wandb:            eval/avg_f1 0.91135
wandb:      eval/avg_mil_loss 0.25669
wandb:       eval/ensemble_f1 0.91135
wandb:           train/avg_f1 0.89391
wandb:      train/ensemble_f1 0.89391
wandb:         train/mil_loss 0.25053
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run exalted-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r3ibg2o4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045521-r3ibg2o4/logs
wandb: ERROR Run r3ibg2o4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hxzi0syz with config:
wandb: 	actor_learning_rate: 2.89732291761935e-05
wandb: 	attention_dropout_p: 0.23048036067173655
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 129
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04307743709916034
wandb: 	temperature: 4.31553530437623
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045720-hxzi0syz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hxzi0syz
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▇█
wandb: best/eval_avg_mil_loss ▅█▆▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▇█
wandb:            eval/avg_f1 ▅▅▇▇█▄▆█▄█▄▆▅▅▆▇▆▆▆▅▄▃▆▆▆▇▅▄▆▆█▃▅▅▅▆▃▅▁▁
wandb:      eval/avg_mil_loss ▃▄▂▆▃▅▅▅▃▃▅▁▅▅▆▄▅▅▄▆▄▃▆▄▆▅▄▇▇▅▄▆█▄▂▆▇█▇▇
wandb:       eval/ensemble_f1 ▆▆▇▇▅▅▇▅▄▅▄▅▅▅▆▁▄▅▅▃▄▇▆▆▇▅▂▄▇▃█▃▄▃▅▃▆▄▁▂
wandb:           train/avg_f1 ▆▆▆▆▆▂▆▇█▅▅▁▅▂▅▄▅▄▅▄▁▅▂▂▃▄▂▃▆▃▂▃▂▁▃▃▂▃▄▃
wandb:      train/ensemble_f1 ▆▆▇▄▆▆█▃▇▃▅▅▄▅▅█▇▄▅▅▅▅▅▅▄▆▅▄▅▅▃▄▃▃▃▄▃▁▃▁
wandb:         train/mil_loss ▆██▇█▆▆▇▆▆▅▅▆█▆▅▅▄▄▅▅▅▅▃▄▆▃▃▄▅▃▅▃▅▄▄▃▄▃▁
wandb:      train/policy_loss ▄▄▆▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▂▄▄▄▄▄▇▄▄▁▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▇▇▇▇▇▇▇█▇▇▇▇▇▆▇▇▇▇█▇▇▇▇▆▇▇▇▇▁▇▇▇█▇▇▆▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9229
wandb: best/eval_avg_mil_loss 0.23495
wandb:  best/eval_ensemble_f1 0.9229
wandb:            eval/avg_f1 0.84672
wandb:      eval/avg_mil_loss 0.4337
wandb:       eval/ensemble_f1 0.84672
wandb:           train/avg_f1 0.87697
wandb:      train/ensemble_f1 0.87697
wandb:         train/mil_loss 0.21209
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run unique-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hxzi0syz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045720-hxzi0syz/logs
wandb: ERROR Run hxzi0syz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: sdxx4wcz with config:
wandb: 	actor_learning_rate: 0.0003009193329160721
wandb: 	attention_dropout_p: 0.40958415603324505
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 136
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.32915035627431277
wandb: 	temperature: 8.626963770308206
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045935-sdxx4wcz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sdxx4wcz
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▆▇▇██
wandb: best/eval_avg_mil_loss █▁▅▁▃▂▆▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▆▇▇██
wandb:            eval/avg_f1 ▆▅▄▆▄▃▆▇▅▁▅▄▅▃▄▅▆▇▆▅▆█▅▂▂▄▇▆▅▄▆▅▆▄▇▄▅▆██
wandb:      eval/avg_mil_loss ▅▄▂▃▅▄▃▇▂▄▅▄▃▃▄▄▂▃▃▃▃▃▇▅▇▃▃▂▆▄█▄▄▃▆▁▆▃▃▄
wandb:       eval/ensemble_f1 ▅▇▅▄▂▅▇▇▄▄▇█▆▅▅▇▂▆▄▅▅▃▄██▁▇▂▆▅▅▅▄▅▁▇▅▂▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▄▁▃▄▅▆▄▄▅▇▃▁▆▆▅▂▅▇▃▅▅▄▅▅▃▅█▄▆▅▄▅▅▃▄▄▅▅
wandb:      train/ensemble_f1 ▄▅▅▄▆▄▃▃▂▅▁▇▃▄█▇▇▆▄▄▅▆▁▅▇▂▄▅▇▄▆▅▅▄▄█▄▄▄▆
wandb:         train/mil_loss ▅▃▅▅▅▄▅▃▂█▄▇▄▄▄▂▃▄▃▂▅▅▃▂▂▃▃▄▃▃▃▅▄▃▄▃▄▁▃▅
wandb:      train/policy_loss ▁███▅▅█▅█▅▁▅▅█▁▅▅█▅██▅▅▅▁▅▅▅▅▅▅█▁▅▅██▅▅█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▁██████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.24132
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.88619
wandb:      eval/avg_mil_loss 0.37647
wandb:       eval/ensemble_f1 0.88619
wandb:            test/avg_f1 0.92083
wandb:      test/avg_mil_loss 0.19213
wandb:       test/ensemble_f1 0.92083
wandb:           train/avg_f1 0.89762
wandb:      train/ensemble_f1 0.89762
wandb:         train/mil_loss 0.19071
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run light-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sdxx4wcz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045935-sdxx4wcz/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 0o1a733f with config:
wandb: 	actor_learning_rate: 0.0003133411372288962
wandb: 	attention_dropout_p: 0.05140761383383402
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 130
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.12632125029739416
wandb: 	temperature: 0.3385730978418311
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050211-0o1a733f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0o1a733f
wandb: uploading history steps 107-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄█
wandb: best/eval_avg_mil_loss ▅█▁▁
wandb:  best/eval_ensemble_f1 ▁▂▄█
wandb:            eval/avg_f1 ▄▇▄▃▂▂█▄▅▇▅▁▃▄▃▇▃▆▄▂▁▃▅▇▆▅▆▅▃▄▅█▁▆▄▃▆▄▄▄
wandb:      eval/avg_mil_loss ▆▃▃▄▆▆▄▄▃▇▇▅▇▄▆▆▁▆▃▄▂▃█▃▄▅▃▅▅▂▄▃▄▄▆▃▁▄▅▅
wandb:       eval/ensemble_f1 ▄▆▃▃▆▃▄▆▇▆▄▃▅▃▁▃▇▄▅▃▅▅▃▅▆▃▄▄▂▂▇▇▅▄▄▄▃▄█▂
wandb:           train/avg_f1 ▄▂▇▆▅▂▃▅▃▅▄▅▁▂▅▅▆▅▃▇▄▆▆▄█▆▂▄▃▄▃▄▅▆▃▂▄▅▃▃
wandb:      train/ensemble_f1 ▄▂▆█▄▂▃▄▃▄▂▄▄▃▁▁▂▅▄▂▅▅▄▆▂▃▄▆▅▁▄▂▅▃▄▃▂▃▄▅
wandb:         train/mil_loss ▂▄▇▆▇▆▄▄▆▇▄█▁▆▄▂▄▅▆▂▆▃▆▄▂█▄▄▂▆▄▂▅▆▃▅▄▅▃▃
wandb:      train/policy_loss ▃▃█▁▃▃▃▃▇▃▃▃▃▃▃▇▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅███████████████████▁██████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93766
wandb: best/eval_avg_mil_loss 0.18848
wandb:  best/eval_ensemble_f1 0.93766
wandb:            eval/avg_f1 0.89384
wandb:      eval/avg_mil_loss 0.44002
wandb:       eval/ensemble_f1 0.89384
wandb:           train/avg_f1 0.88858
wandb:      train/ensemble_f1 0.88858
wandb:         train/mil_loss 0.20392
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dainty-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0o1a733f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050211-0o1a733f/logs
wandb: ERROR Run 0o1a733f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ahebiqxd with config:
wandb: 	actor_learning_rate: 0.00013157033201442432
wandb: 	attention_dropout_p: 0.11333383003596686
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 114
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.03184492454298582
wandb: 	temperature: 4.419082873170228
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050354-ahebiqxd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ahebiqxd
wandb: uploading history steps 99-115, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▆▇█
wandb: best/eval_avg_mil_loss ▅▆█▃▁
wandb:  best/eval_ensemble_f1 ▁▂▆▇█
wandb:            eval/avg_f1 ▆▃▆█▆▅▅▃▅▃▄▄▅▂▄▅▅▁▃▅▄▂▅▅▂▄▃▄▆▄▄▃▆▆▅▃▃▅▄▆
wandb:      eval/avg_mil_loss ▃▂▃▄▃▄▂▃▃▇▁▂▂▅▃▄█▄▃▁▃▃▄▃▃▆▄▄▆▄▄▅▆▂▃▆▂▄▂▃
wandb:       eval/ensemble_f1 ▆▆▄▁▄▄█▅▅▄▄▄▃▃▂▅▅▂▆▄▅▃▆▅▃▄▆▅▂▆▄▄▄▄▆█▆▁▆▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▄█▄▄▅▇▅██▄▄▂▆▇▆▇▆▅▆▄▅█▄▅▃▇█▅▅▅▇▄▅▆▆▆▅▁
wandb:      train/ensemble_f1 ▄▅▄▄▅▄▃▇▆▅▆█▅▃▅█▄▂▇▇▆▅▆▆▆▅▄▅█▅▃▅▄▆▅▆▆▅▅▁
wandb:         train/mil_loss ▃▅▅▅▃▃▄▅▅▅▅▅▄▄▇▅▅▄▅▃▄▄▅▆▆▄▄▇▄▁▃▅▄▃▄▅▄▆▃█
wandb:      train/policy_loss ██████████████████████████████████████▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅█▅▁▁██▅█▅█▅▅▅▁▅▅▁▅▅▅█▁▅▁▅▅▁▅█▅█▅▅▅██▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.22078
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.90786
wandb:      eval/avg_mil_loss 0.30061
wandb:       eval/ensemble_f1 0.90786
wandb:            test/avg_f1 0.89036
wandb:      test/avg_mil_loss 0.24385
wandb:       test/ensemble_f1 0.89036
wandb:           train/avg_f1 0.86979
wandb:      train/ensemble_f1 0.86979
wandb:         train/mil_loss 0.26223
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glad-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ahebiqxd
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050354-ahebiqxd/logs
wandb: Agent Starting Run: lrxk5owb with config:
wandb: 	actor_learning_rate: 0.0005095351393627092
wandb: 	attention_dropout_p: 0.12358024986588428
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 76
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8860920621505756
wandb: 	temperature: 5.548725696104739
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050543-lrxk5owb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lrxk5owb
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▆▆█
wandb: best/eval_avg_mil_loss ▂█▂▁▅▂
wandb:  best/eval_ensemble_f1 ▁▃▅▆▆█
wandb:            eval/avg_f1 ▄▅▇▆▅█▇█▅▁▅▆▆▅▆▂▄▇▇▆▅▆▆▆▄▆▆█▆▄▄▅▇▆▅▆███▆
wandb:      eval/avg_mil_loss ▆▄▄▄▄▃▄▆▄▆▃▄▄▄▆▄▄▄▃▄▄▆▅██▄▁▃▆▅▅▃▃▃▄▅▃▅▃▃
wandb:       eval/ensemble_f1 ▄▅▅▆▆██▅▇▁▅▆▆▅▅▇▆▆▆▅█▆▅▆▄▄▆▆▄▅▅█▆▆▅▆▆█▆▆
wandb:           train/avg_f1 ▁▅▃▁▃▇▅▅▄▃▇▆▂▅▆▅▃▆▄▄▇▅▁▆█▅▄▆▅▃▇▃▇▆▅▇▄▃▄█
wandb:      train/ensemble_f1 ▇▅▁▃▄▃▃▁▃▆▃▄▆▄▃▄▂▆█▃▄▃▇▅▃▄▅▁▄▅▄▅▄▅▃▆▆▃▆▄
wandb:         train/mil_loss ▄▂▄▆▃█▄▃▄▄▃▄▂▃▃▅▅▄▄▂▄▁▃▅▄▃▃▃▄▃▃▄▂▃▂▄▄▁▃▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆█▆▆▆▆▆▆▁▃▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▄▆▆▁▆▃▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92281
wandb: best/eval_avg_mil_loss 0.25706
wandb:  best/eval_ensemble_f1 0.92281
wandb:            eval/avg_f1 0.89312
wandb:      eval/avg_mil_loss 0.32391
wandb:       eval/ensemble_f1 0.89312
wandb:           train/avg_f1 0.90169
wandb:      train/ensemble_f1 0.90169
wandb:         train/mil_loss 0.88008
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dry-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lrxk5owb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050543-lrxk5owb/logs
wandb: ERROR Run lrxk5owb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5f44ai80 with config:
wandb: 	actor_learning_rate: 0.000325477408250129
wandb: 	attention_dropout_p: 0.25994503861954
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 125
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5371744432008799
wandb: 	temperature: 8.69342629987569
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050710-5f44ai80
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5f44ai80
wandb: uploading history steps 112-126, summary; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▅▅▅▆█
wandb: best/eval_avg_mil_loss ▆▅▁▁▆█▃▂
wandb:  best/eval_ensemble_f1 ▁▁▄▅▅▅▆█
wandb:            eval/avg_f1 ▄▄▄▃▃▄▃▁▂▄▁▄▃▆▆▅▄▇▄▅▃▅▃█▃▄▁▆▂▂▄▂▆▄▄▄▃▆▄▃
wandb:      eval/avg_mil_loss ▃█▃▁▆▅▆▃▅▄▃▁▁▂▆▄▂▃▄▅▄▃▃▂▃▁▅▂▄▂▅▄▄▃▃▃▂▄▃▃
wandb:       eval/ensemble_f1 ▄▄▂▂▆▄▃▁▁▁▁▂▃▃▃▆▃▅▇▃▃▃█▃█▂▆▂▃▄▂▃▆█▅▄▃▃▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▄▅▅█▄▆▆▁▄▅▇▃▁▆▆▇▅▇▄▄█▆▆▅▄▄▅▅▇▆▅▅▆▄▇▅▆▅▄
wandb:      train/ensemble_f1 ▃▅▄▅▆▄▅▄▃▅▅▄▁▆▃▆▅▅▅▅▅▅▇█▅▄▅▄▄▇▅▅▆▇▆▇▅▆▅▇
wandb:         train/mil_loss █▇▆▇▆▇▇▅█▄▅▅▆▄▃▅▆▄▆▅▄▆▇▄▄▄▄▂▂▃▂▁▃▇▁▃▁▂▄▁
wandb:      train/policy_loss ███████████████▁████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅█▁▅▅▅▅▅▅▅▅▅▅▅▄▃▅▅▅▅▅▇▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.26531
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.88198
wandb:      eval/avg_mil_loss 0.34829
wandb:       eval/ensemble_f1 0.88198
wandb:            test/avg_f1 0.89399
wandb:      test/avg_mil_loss 0.26064
wandb:       test/ensemble_f1 0.89399
wandb:           train/avg_f1 0.89386
wandb:      train/ensemble_f1 0.89386
wandb:         train/mil_loss 2.23336
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devoted-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5f44ai80
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050710-5f44ai80/logs
wandb: Agent Starting Run: ieshku8u with config:
wandb: 	actor_learning_rate: 8.256782237222594e-05
wandb: 	attention_dropout_p: 0.21180393724278648
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.08653856123697168
wandb: 	temperature: 7.80095062808781
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050930-ieshku8u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ieshku8u
wandb: uploading wandb-summary.json
wandb: uploading history steps 104-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆█
wandb: best/eval_avg_mil_loss ▁█▃▁
wandb:  best/eval_ensemble_f1 ▁▃▆█
wandb:            eval/avg_f1 ▅▆▃▃▅▆▂▅▄█▄█▄▄▇▄▄▅▃▂▄▁▄▃▄▃▅▇▂█▆▂▅▁▆▃▆▄▅▄
wandb:      eval/avg_mil_loss ▇▄▂▆▂▃▅▄▅▄▆▃▃▇▄▃▂▁▇▂▂▃▂▂▅▄▃█▃▄▇▃▃▃▃█▃▂▃▆
wandb:       eval/ensemble_f1 ▃▆▅▅▂▃█▇▄▂▃▄▁▃▆▅▄▇▇▅▆▄▄▆▃▃▁▅▄▆▆▆▃▄▆▆▅▄▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▃▃▄▄▆▇▃▃▃▆▃█▆▇▅▇▆▄▆▆▅▆▁▅▄▅▄▆▇█▅▅▇▇▅▆▆▁
wandb:      train/ensemble_f1 ▄▆▁▃▃▇▃▅▃▆▆▆▄▅▂▇▄▅▆▆▆▆▅▃▆▅▅▄▆▆▆▅▅█▆▄▇▆▆▁
wandb:         train/mil_loss ▆██▇▆▅▅▇▅▃▄▆▂▃█▄▆▄▃▆▁▅▃▄▃▅▁▄▅▆▂▄▇▂▂▅▆▃▂▅
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅█▁█▅█▅▅▅▁██▅▅▅██▅▁▅▅▁█▅▅▅█▅▁▅█▅▅█▅▅█▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93744
wandb: best/eval_avg_mil_loss 0.24685
wandb:  best/eval_ensemble_f1 0.93744
wandb:            eval/avg_f1 0.90063
wandb:      eval/avg_mil_loss 0.37182
wandb:       eval/ensemble_f1 0.90063
wandb:            test/avg_f1 0.93494
wandb:      test/avg_mil_loss 0.1637
wandb:       test/ensemble_f1 0.93494
wandb:           train/avg_f1 0.88401
wandb:      train/ensemble_f1 0.88401
wandb:         train/mil_loss 0.33323
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run skilled-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ieshku8u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050930-ieshku8u/logs
wandb: Agent Starting Run: dxpv9rhf with config:
wandb: 	actor_learning_rate: 5.925720284160694e-05
wandb: 	attention_dropout_p: 0.4609475584589705
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8420633941556339
wandb: 	temperature: 9.73732868844049
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051109-dxpv9rhf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dxpv9rhf
wandb: uploading history steps 83-89, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss ▁▇█
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 ▆▂▄▆▆▄▄▇▁▃▅▃▃▂▇▆▆▅▄▄▅▆▅▃▅▆▆▄▆▂▅█▇▄▅▆▅▅▃█
wandb:      eval/avg_mil_loss ▂▁▂▆▁▄▁██▁▄▅▄▄▂▂▄▃▄▆▇▃▂▁▃▃▃▅▄▂▂▁▆▄▅▄▄▄█▂
wandb:       eval/ensemble_f1 ▆▅▃▅▆▆▇▁▇▄▃▄▆▂▂▅▇▅▆▃▄▁▄▅▅▅▄▅▄▇▅█▅▇▄▄▃▅▄▄
wandb:           train/avg_f1 ▄▇▂▅▇▆▄▆▅▆▅▆▃▄▆▅▄▂▅▅▂█▅▁▆▅▇▃▇▄▅▅▃▅▄▃▆▃█▄
wandb:      train/ensemble_f1 ▇▇█▇▅▅▆▇▃▇▇▅▆▇▄▆▆▆▃▆▇▁▆▆▅▄▆▄▄▄▃▆▃█▄▅▅▆▃▆
wandb:         train/mil_loss █▃▅▆▅▁▄▇▃▅▇▄▆▃▄▇▆▂▂▆▃▆▇█▃▃▇▄▇▄▇▆▇▃▅▅▅▄▆▆
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▁▆▆▆▆▆▆▆▆▇▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▄▆█▁▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92644
wandb: best/eval_avg_mil_loss 0.26349
wandb:  best/eval_ensemble_f1 0.92644
wandb:            eval/avg_f1 0.88631
wandb:      eval/avg_mil_loss 0.30601
wandb:       eval/ensemble_f1 0.88631
wandb:           train/avg_f1 0.89178
wandb:      train/ensemble_f1 0.89178
wandb:         train/mil_loss 0.17716
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smooth-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dxpv9rhf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051109-dxpv9rhf/logs
wandb: ERROR Run dxpv9rhf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: yyr0lsqo with config:
wandb: 	actor_learning_rate: 0.0007984255220901918
wandb: 	attention_dropout_p: 0.34310215630972635
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 55
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8886710937580262
wandb: 	temperature: 2.6674794847963765
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051252-yyr0lsqo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yyr0lsqo
wandb: uploading history steps 42-55, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss ███▁▅
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▁▄▆▅█▄▃▃▆▂▅▄▆█▄▅▆▃▄▂▄▄▄▃▃▄▅▇▆▃▅▄▂▅█▆▄▄▆▄
wandb:      eval/avg_mil_loss ▅▅▅▄▁▅▅▃▇▅▄▅▃▅▅▄█▃▆▆▅▅█▂▇▄▃▆▇▇▃▃▅▄▄▄▃▄▇▃
wandb:       eval/ensemble_f1 ▂▁▄▆▅▄▃▃▃▆▃▅▄█▄▅▃▄▂▂▄▄▃█▃▄▅▇▆▃▅▄▂▅▅▇▄▄▆▄
wandb:           train/avg_f1 ▄▅▄▃▃▅▄▆▄▄▇▅▃▅▄▄▃▃▅▅▆▇█▆▆▅▄▄▂▇▄▅▆▅▃▁▃▇▅▄
wandb:      train/ensemble_f1 ▄▅▄▃▃▆▅▄▆▄▇▅▃▅▆▃▃▅▅▅▇█▆▆▅▅▄▂▇▅▅▆▅▃▁▃▆▇▅▄
wandb:         train/mil_loss ▅█▆█▄█▄▇▅▅█▄▆▅▆▄▅▆▄▄▅▅▇▄▃▄▃▅▄▅▅▃▁▅▃▅▅▇▆▃
wandb:      train/policy_loss ██████████▅██████████▆███▇██▁███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████████▆████▇███▁██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.24551
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.89673
wandb:      eval/avg_mil_loss 0.23284
wandb:       eval/ensemble_f1 0.89673
wandb:           train/avg_f1 0.88904
wandb:      train/ensemble_f1 0.88904
wandb:         train/mil_loss 0.23177
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run likely-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yyr0lsqo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051252-yyr0lsqo/logs
wandb: ERROR Run yyr0lsqo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qr8je4x8 with config:
wandb: 	actor_learning_rate: 1.790296783313988e-05
wandb: 	attention_dropout_p: 0.29427925770702557
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 64
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1619103076552828
wandb: 	temperature: 5.521226063813235
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051400-qr8je4x8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qr8je4x8
wandb: uploading history steps 50-64, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄█
wandb: best/eval_avg_mil_loss ▅▇▁█▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄█
wandb:            eval/avg_f1 ▃▂▅▄▆▆▆▅▄▁▆▅▅▂▄▄▄▅▆█▅▂▄▇▆▇▅▇▅▆▅▂▆▅▃▆▅▅▅▆
wandb:      eval/avg_mil_loss ▄▇▂▇▃▄▃█▆▃▂▅▇▃▅▄▃▄▃▂▄▅▂▁▃▅▅▁▅▃▆▆▁▃▆▄▅▄▃▄
wandb:       eval/ensemble_f1 ▄▃▂▆▇▆▅▆▂▇▆▄▁▅▅▃▅▄▅▆▁▇█▆▆▇▇▇▇█▆▆▅▇▆▃▄▇▆█
wandb:           train/avg_f1 ▄▅▅▅▃▆▅▃█▁▅▃▄▆▄▂▂▂▃▂▄▂▅▅▃▃▄▅▄▅▃▄▃▃▂▄▄▄▄▅
wandb:      train/ensemble_f1 ▄▅▅▅▃█▁▆▃▄▂▃▃▂▅▅▇▁▄▃▅▃▄▄▅▅▄▄▃▂▃▄▃▅▄▃▅▄▄▅
wandb:         train/mil_loss ▅▆▃▅▃▆▄▂▆█▆▄▃█▄▅▄▆▃▃▂▃▅▅▂▅▂▃▂▅▂▇▂▃▁▃▄▃▄▃
wandb:      train/policy_loss ████████▆█████████████████████▁████████▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████████████████████▁██████▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94837
wandb: best/eval_avg_mil_loss 0.2153
wandb:  best/eval_ensemble_f1 0.94837
wandb:            eval/avg_f1 0.92634
wandb:      eval/avg_mil_loss 0.27122
wandb:       eval/ensemble_f1 0.92634
wandb:           train/avg_f1 0.90453
wandb:      train/ensemble_f1 0.90453
wandb:         train/mil_loss 0.35368
wandb:      train/policy_loss -0.13234
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.13234
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run volcanic-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qr8je4x8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051400-qr8je4x8/logs
wandb: ERROR Run qr8je4x8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jdlmcm53 with config:
wandb: 	actor_learning_rate: 4.255582177553559e-06
wandb: 	attention_dropout_p: 0.372890525849412
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 139
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1791435051982737
wandb: 	temperature: 4.216744433189375
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051507-jdlmcm53
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jdlmcm53
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▃▄▇█
wandb: best/eval_avg_mil_loss ▄█▁▃▂▂▂▁
wandb:  best/eval_ensemble_f1 ▁▂▂▃▃▄▇█
wandb:            eval/avg_f1 ▃▄▃▆▄▂▂▅▆▂▅▃▇▅▄▄▆█▅▄▅▇▇▄▆▂▅▆▆▂▅▅▁▂▂▃▂▄▄▃
wandb:      eval/avg_mil_loss ▅▅▆▄▅▄▅▅▅▂▅▃▃▅▆▁▃▂▃▄▄▄▃▃▃▆▄▆█▄▄▃▅█▄▃█▆▆▆
wandb:       eval/ensemble_f1 ▄▄▄▅▃█▃▆▂▂▅▄▁▅▄▄▇▆▇▄▃▅▄▃▅▂▂▅▃▄▅▄▅▃▄▄▂▂▂▄
wandb:           train/avg_f1 ▅▇▇▆▆█▆▄▆▅▄▇█▆▄▅▆▆▆▃▅▅▄▆▄▁▅▆▃▄▅▄▄▄▃▃▃▂▆▅
wandb:      train/ensemble_f1 ▅▆▇▅▅▃▅█▅▅▇▇▅▅▆▃▇▅▄▅▁▅▄▃▃▅▄▅▆▃▄▄▃▅▄▅▅▃▃▄
wandb:         train/mil_loss ▇▆▆▆██▇▆▅▆▅▄▅▅▆▅▅▅▅▅▃▄▄▄▄▄▅▅▄▅▁▅▃▄▄▂▃▃▁▄
wandb:      train/policy_loss ▅▅▅▅▅▅▆▅▅▁▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃█▃▃▃▃▁▃▃▃▃▃▃▃▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94499
wandb: best/eval_avg_mil_loss 0.239
wandb:  best/eval_ensemble_f1 0.94499
wandb:            eval/avg_f1 0.90498
wandb:      eval/avg_mil_loss 0.27178
wandb:       eval/ensemble_f1 0.90498
wandb:           train/avg_f1 0.8952
wandb:      train/ensemble_f1 0.8952
wandb:         train/mil_loss 0.21977
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run skilled-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jdlmcm53
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051507-jdlmcm53/logs
wandb: ERROR Run jdlmcm53 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: k0tjccjf with config:
wandb: 	actor_learning_rate: 2.9394101612829133e-05
wandb: 	attention_dropout_p: 0.25201742879562855
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 141
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7323285584821126
wandb: 	temperature: 8.336652917295943
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051747-k0tjccjf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k0tjccjf
wandb: uploading history steps 103-120, summary; uploading wandb-summary.json
wandb: uploading history steps 121-121, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁█
wandb: best/eval_avg_mil_loss ▆█▁
wandb:  best/eval_ensemble_f1 ▁▁█
wandb:            eval/avg_f1 ▃▂▇▅▂▃▆▆▆▃▅▂▃▂▅▁▄▄▁▅▅▅▄▂▅▇▁▂█▃▃▂▆▃▅▃▅▃▆▆
wandb:      eval/avg_mil_loss ▃▃▄▅▄▄▄▄▁▇▃▅▅▄▄▃▄▅▃▄▃▆█▄▅▂▂▄▃▄▅▅▄▅▅▄▆▄▇▃
wandb:       eval/ensemble_f1 ▇▄▄▆▃▆▅▄▂▆▃▃▅▂▃▂▆▅▄▅▄▄▄▁▅▅▂▆▇▆▄▃▄▆▃▅▃▆█▆
wandb:           train/avg_f1 ▅▆▅▇▇▇▆█▆█▆▆▅▆▅▃▄▅▅▆▃▆▁▄▅▅▄▆▅▃▄▄▂▅▄▆▆▄▃▄
wandb:      train/ensemble_f1 ▆▇▇▇█▇▇▅▄▆▇▄▇▆▇█▆▃▄▆▇▅▄▁▆▇▆▇▅▅▅▆▃▅▃█▅▄▃▄
wandb:         train/mil_loss ▆▆█▅▁▆▄▄▁▃▅▄▅▂▄▃▅▂▃▂▄▄▆▄▄▅▃▄▄▅▄▄▄▃▄▂▃▅▂▅
wandb:      train/policy_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁█▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93725
wandb: best/eval_avg_mil_loss 0.1728
wandb:  best/eval_ensemble_f1 0.93725
wandb:            eval/avg_f1 0.91898
wandb:      eval/avg_mil_loss 0.24496
wandb:       eval/ensemble_f1 0.91898
wandb:           train/avg_f1 0.8936
wandb:      train/ensemble_f1 0.8936
wandb:         train/mil_loss 0.28459
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run icy-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k0tjccjf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051747-k0tjccjf/logs
wandb: ERROR Run k0tjccjf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jd1sifgq with config:
wandb: 	actor_learning_rate: 2.335540986226972e-05
wandb: 	attention_dropout_p: 0.1286680784862908
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 162
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10357738805717263
wandb: 	temperature: 0.06657942494378455
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051941-jd1sifgq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jd1sifgq
wandb: uploading history steps 124-133, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▅▇▆▆▆▄▄▅▆▆▃▆▇▄▆▆▄▆█▆▄▆▄▄▄▄▁▅▅▄▇▅▃▄▄▁▅▂▄
wandb:      eval/avg_mil_loss ▅▂▅▃▄▃▅▄▆▃▁▅▄▂▂▄▃▃▃▇▅▃▅▅▅▅▆▄▆▄▆▇▄█▄▄▆▇▆▇
wandb:       eval/ensemble_f1 ▇▆▅▆▆▅▇▄▄▆▆█▄▄▄▅▆▄▄▄▄▅▂▅▃▃▅▁▅▅▄▅▇▅▅▅▃▁▄▇
wandb:           train/avg_f1 ▇▇▄▅▇▆▆▇▆█▇▆▇▆▆▆▇▅▅▅█▆▅▅▅▅▇▅▆▄▄▆▃▆▃▂▄▆▁▄
wandb:      train/ensemble_f1 ▆▅▄▅▅▇▇▅▅█▆▇▅▅▅▆▆▅▇▆▆▄▄▄▅▄▅▆▆▃▄▄▃▃▅▄▅▅▁▃
wandb:         train/mil_loss ▅█▆▅▅▅▇▆▄▄▆▅▅▄▄▃▄▆▃▃▅▄▂▃▃▄▄▃▂▂▂▃▄▁▂▂▂▁▂▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄█▄▁▄▄▄▃▄▄▄▄▄▄▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93361
wandb: best/eval_avg_mil_loss 0.19931
wandb:  best/eval_ensemble_f1 0.93361
wandb:            eval/avg_f1 0.89767
wandb:      eval/avg_mil_loss 0.25717
wandb:       eval/ensemble_f1 0.89767
wandb:           train/avg_f1 0.88576
wandb:      train/ensemble_f1 0.88576
wandb:         train/mil_loss 0.23622
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run playful-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jd1sifgq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051941-jd1sifgq/logs
wandb: ERROR Run jd1sifgq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wch2sjtx with config:
wandb: 	actor_learning_rate: 7.397642087625819e-05
wandb: 	attention_dropout_p: 0.28297057536052483
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 81
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7565764447327531
wandb: 	temperature: 3.8716918128963695
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052211-wch2sjtx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wch2sjtx
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄█
wandb: best/eval_avg_mil_loss █▇▁▇
wandb:  best/eval_ensemble_f1 ▁▃▄█
wandb:            eval/avg_f1 ▆▆▅▂▃▄▆▄▅▆▄▄▅▅▇▅▄▆▆▅▂▆▁▃▆▆█▅▅▆▃▇▃▃▇▅▁▃▄▅
wandb:      eval/avg_mil_loss ▇▆▃▆▇▃▂█▂▄▄▄▁▅▄▅▃▇▂▄▆▃▂▅▇▃▂▄▂▇▇▅▃▄▇▂▃▆▅▄
wandb:       eval/ensemble_f1 ▅▆▅▂▅▄▄▅▃▅▇▄▄▅▄▆▆▅▆▂▄▆▃▅▅▆▄█▄▃▄▅▇▃▃▅▃▅▁▄
wandb:           train/avg_f1 ▅▆▄▄▆▃▄▇▃▆▄▄▅▃▅▄▄▃█▅▂▃▅▆▃▅▅▂▅▆▅▅▅▇▄▁▅▂▅▂
wandb:      train/ensemble_f1 ▄▄▅▄▆▄▅▇▆▅▅▅▅▇▄▄▄▃█▄▆▃▅▃▃▄▅▅▄▂▅▄▂▅▇█▅▁▂▄
wandb:         train/mil_loss ▄▅▄▂▇▅▃▄█▃█▄▁▃▅▂▅▄▅▅▂▄▄▂▂▄▁▄▁▃▄▁▄▃▂▅▄▃▅▁
wandb:      train/policy_loss ▆▆▆▆▆▆█▆▆▆█▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▇█▆▅▅▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93032
wandb: best/eval_avg_mil_loss 0.26859
wandb:  best/eval_ensemble_f1 0.93032
wandb:            eval/avg_f1 0.88992
wandb:      eval/avg_mil_loss 0.28059
wandb:       eval/ensemble_f1 0.88992
wandb:           train/avg_f1 0.87941
wandb:      train/ensemble_f1 0.87941
wandb:         train/mil_loss 0.1983
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run decent-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wch2sjtx
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052211-wch2sjtx/logs
wandb: ERROR Run wch2sjtx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: tjotbtg0 with config:
wandb: 	actor_learning_rate: 0.0004594060718657372
wandb: 	attention_dropout_p: 0.20854230779824512
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 176
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.057852301719627075
wandb: 	temperature: 4.378795314763131
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052343-tjotbtg0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tjotbtg0
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂█
wandb: best/eval_avg_mil_loss ██▁
wandb:  best/eval_ensemble_f1 ▁▂█
wandb:            eval/avg_f1 ▇▇▆▅▇▆▄▅▇▄▇▅▄▅▅▅▇▄▇▆▇▄▅▅▂▆▁▃▃▇▃▄▆▄█▇▃▅▂▃
wandb:      eval/avg_mil_loss ▃▄▂▂▁▁▂▄▂▅▂▂▅▃▁▁▄▂▃▃▂▂▃▅▆▅▅▇▄█▄▃▄▂▄▃▆▅▇▃
wandb:       eval/ensemble_f1 ▇▆▅▆▅▃▇▃█▄▄▅█▆▆▃█▂▆█▄▄▂▁▃▁▂▃▂▃▁▄▄▅▆▁▆▆▅▂
wandb:           train/avg_f1 ▇█▅▆▅▅▅▆▇▅▇█▁▄▆▆▁▆▅▆▅▅▃▂▆▄▅▆▆▄▃▃▅▂▄▂▄▁▃▂
wandb:      train/ensemble_f1 ▇▅▆█▆▇█▆▆▇▇▃▅█▆▅▄▆▆▅▆▆▆▃▅▆▇▃▆▄▅▃▅▃▃▁▄▃▄▃
wandb:         train/mil_loss ▇█▃▅▆▅▅▆▅▅▅▆▅▆▅▆▃▃▃▅▅▆▆▆▆▅▂▆▂▄▅▃▁▅▄▂▅▂▂▅
wandb:      train/policy_loss ▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92653
wandb: best/eval_avg_mil_loss 0.22549
wandb:  best/eval_ensemble_f1 0.92653
wandb:            eval/avg_f1 0.87218
wandb:      eval/avg_mil_loss 0.30013
wandb:       eval/ensemble_f1 0.87218
wandb:           train/avg_f1 0.87953
wandb:      train/ensemble_f1 0.87953
wandb:         train/mil_loss 0.25247
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run warm-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tjotbtg0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052343-tjotbtg0/logs
wandb: ERROR Run tjotbtg0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: x6c473ej with config:
wandb: 	actor_learning_rate: 0.00015368120197235103
wandb: 	attention_dropout_p: 0.23452439511081904
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 63
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0408369405933412
wandb: 	temperature: 6.115067076215411
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052552-x6c473ej
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x6c473ej
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▇▇█
wandb: best/eval_avg_mil_loss █▇▃▄▃▁▃
wandb:  best/eval_ensemble_f1 ▁▄▄▅▇▇█
wandb:            eval/avg_f1 ▄▅▂▄▄▆▅▆▄▅▄█▄▄▆█▅▅▆▅▄▄▅▃▆▃▅▄▃▆▄▇▅▅▄▄▁▆▅▃
wandb:      eval/avg_mil_loss ▇▅▅▆▃█▅▃▅▃▂▇▁▂▄▄▆▃▃▄▆▂▇▅▁▆▅▆▃▃▄▃█▇▄▇▇▇▄█
wandb:       eval/ensemble_f1 ▄▅▂▄▄▅▄▆▆▄▅█▅▆▆▅▅▆▅█▄▄▄▅▃▄▆▃▃▅▅▃▆▄█▅▄▄▁▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅██▃▅▄▇▅▄▄▃▆▄▂▄▄▁▄▄▅▃▆▃▃▄▃▄▃▅▄▃█▂▄▃▅▄▃▄
wandb:      train/ensemble_f1 ▅▅▆▂▄█▅▅▄▇▄▃▄▄▃▆▄▅▁▄▃▅▃▆▅▃▂▂▃▅▄▃▃▂▄▄▆▃▅▃
wandb:         train/mil_loss ▇▄▅▃▇▃▆▇▆▆▄▄▆▂▅▆▇█▅▆▅▇▆▆▄▅▅▄▄▆▄▃▃▄▃▅▅▃▁▃
wandb:      train/policy_loss █████████▁██████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁▁█▁██▅█▁▅█▅▁█▅██▁▅█▁█▅▅▁▅▁▅█▅▁▅▅▁▁█▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9334
wandb: best/eval_avg_mil_loss 0.255
wandb:  best/eval_ensemble_f1 0.9334
wandb:            eval/avg_f1 0.89296
wandb:      eval/avg_mil_loss 0.33469
wandb:       eval/ensemble_f1 0.89296
wandb:            test/avg_f1 0.91594
wandb:      test/avg_mil_loss 0.24592
wandb:       test/ensemble_f1 0.91594
wandb:           train/avg_f1 0.89413
wandb:      train/ensemble_f1 0.89413
wandb:         train/mil_loss 0.34102
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run swift-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x6c473ej
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052552-x6c473ej/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 1cb5e482 with config:
wandb: 	actor_learning_rate: 0.0001231547552155448
wandb: 	attention_dropout_p: 0.16480834970344127
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 90
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0499544467014672
wandb: 	temperature: 4.703243829583058
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052705-1cb5e482
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1cb5e482
wandb: uploading history steps 80-91, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▁▆▄▃▂▅▃▂▅▃▂▄▇█▆▆▃▃▄▅▇▅▂▃▄▃▅▆▆▃▅▄▇▇▃▂▄▆▅▇
wandb:      eval/avg_mil_loss ▃▄▄▇▂▄▁▂▅▅▇▄▄▂▂▃▄▄▃▄▂▂▄█▃▂▃▆▅▃▄▂▃▄▇▆▃▇▃▂
wandb:       eval/ensemble_f1 ▇▅▆▂▆▆▄▅▂▇▃▆▆▃█▆▅▃▃▆█▆▄▃█▄▅▃▆▆▅▇▇▁▃▂▅▆▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▂▇▃▅▂▃▅▆▃▃▅█▄▆▄█▆▄▄▆▁▂█▅▇▆▅▅▅▆▄▅▃▃▃▆▆▄▃
wandb:      train/ensemble_f1 ▃▇▆▅▄▄▆▆▇▆▆▅▅█▄█▄▃▇▇█▆█▄▆▄▆▅▇▅▁▄▆▇▇▄▃▆▅▆
wandb:         train/mil_loss ▄▄▇▅▇▆▄▆▇▄▆▄▇▆▇▅▅▅▃▃██▃▅▄▃▆▆▆▆▆▃▃▆▃▄▃▁▅▆
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▆▄█▄▄▄▄▄▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁█▁▁▁▁▁▁▇▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.24398
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.92223
wandb:      eval/avg_mil_loss 0.23268
wandb:       eval/ensemble_f1 0.92223
wandb:            test/avg_f1 0.90521
wandb:      test/avg_mil_loss 0.25045
wandb:       test/ensemble_f1 0.90521
wandb:           train/avg_f1 0.89523
wandb:      train/ensemble_f1 0.89523
wandb:         train/mil_loss 0.46668
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run winter-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1cb5e482
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052705-1cb5e482/logs
wandb: Agent Starting Run: d756c8nk with config:
wandb: 	actor_learning_rate: 0.0001370036787979542
wandb: 	attention_dropout_p: 0.0998023843991172
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 61
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.021378549717097628
wandb: 	temperature: 5.715601727035301
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052839-d756c8nk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d756c8nk
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss ▆█▇▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▇▇▄▂▄▆▃▁▄▅▇▂▆▅▆▇▃▆▂▄▄▅▇▆▆▃▅▃█▇▇▇▅▄▅▄▆▂▄▃
wandb:      eval/avg_mil_loss ▃▃▄▅▃▄▅▅▂▂▆▄▅▄▂▂▁▅▆▄▅▆▃▆▄▁▄▅▃▃▁▂▂▄▅▁▃█▂▃
wandb:       eval/ensemble_f1 ▅▇▅▇▄▆▆▃▂▄▃▄▆▇▂▆▁▇▄▆▄▃▄▅▇▄▅▃█▇▄▇▇▅▄▃▄▄▆▃
wandb:           train/avg_f1 ▄▅▃▄▂▅▃▂▆▆█▄▄▆▄▇▁▆▃▅▃▅▅▁▄█▃▅▅▃▇▄▄▃▄▆▄▄▅▇
wandb:      train/ensemble_f1 ▅▄▅▄▅▂▅▆▅▃▆▆▆█▄▅▃▆▁▁▅▆▃▅▅▄▂▄▅▂▃▄▄▃▃▄▅▄▅▆
wandb:         train/mil_loss ▅▇▇▅▄▇█▃█▃▄▄▆▂▃▃▃▇▆▅█▂▄▄▅▂▄▅▄▃▄▂▂▄▁▅▅▂▄▁
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.1979
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.89354
wandb:      eval/avg_mil_loss 0.3017
wandb:       eval/ensemble_f1 0.89354
wandb:           train/avg_f1 0.90447
wandb:      train/ensemble_f1 0.90447
wandb:         train/mil_loss 0.27491
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run tough-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d756c8nk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052839-d756c8nk/logs
wandb: ERROR Run d756c8nk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: f08s7ws4 with config:
wandb: 	actor_learning_rate: 0.00022177634746077825
wandb: 	attention_dropout_p: 0.1519385247506671
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 111
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.32512200955055437
wandb: 	temperature: 3.5047643171822407
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052946-f08s7ws4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f08s7ws4
wandb: uploading wandb-summary.json
wandb: uploading history steps 101-112, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss ██▁▁▆
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▆▂▁▅▁▅▂▄▄▄▄▅▆▃▄▄▂▅▅▆▃▃▄█▅▃▄▆▃▂▄▃▄▃▅▅▁▃▃▃
wandb:      eval/avg_mil_loss ▅▆▃▂▂▄▃▄▅▇▂▄▃▄▅▇▄▇▅▄▂▃▇▅▄▃▃▆▆▅▅▄▁▆█▄▆▄▂▃
wandb:       eval/ensemble_f1 █▆▅▃▂▇▆▅▆▅▇▆█▄▆▅▄▆▇▆▆▅▇▆█▄▆▄▅▆▁▄▅▄▇▅▅▄▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▇▁▇▇▆▃▇▂▃▇▆▆▆▇▆▇▇▆▆▆▃▃█▆▃▆▄▅▆▆▄▇▇▆▆▆▆▄▃
wandb:      train/ensemble_f1 ▅▃▁▇▅▆▃▄▃▇▆▆▇▆▆▅▇▆▅▃▅▆▅▇▆▅▆▅▅▆▄▇▄▅█▄▆▅▆▆
wandb:         train/mil_loss ▃▆▃▇█▃▅▇▆▅▆▇▆▆▄▆▆▇█▄▃▃▃▇▅▅▆▅▃▆▄▃▆▄▆▄▁▅▅█
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94099
wandb: best/eval_avg_mil_loss 0.32567
wandb:  best/eval_ensemble_f1 0.94099
wandb:            eval/avg_f1 0.89341
wandb:      eval/avg_mil_loss 0.44362
wandb:       eval/ensemble_f1 0.89341
wandb:            test/avg_f1 0.89036
wandb:      test/avg_mil_loss 0.41521
wandb:       test/ensemble_f1 0.89036
wandb:           train/avg_f1 0.88399
wandb:      train/ensemble_f1 0.88399
wandb:         train/mil_loss 0.30707
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fearless-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f08s7ws4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052946-f08s7ws4/logs
wandb: Agent Starting Run: 1cg2gcjq with config:
wandb: 	actor_learning_rate: 7.877087014915887e-05
wandb: 	attention_dropout_p: 0.2510988949124105
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1839484235918325
wandb: 	temperature: 3.567379783551312
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053135-1cg2gcjq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1cg2gcjq
wandb: uploading history steps 80-91, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▅█
wandb: best/eval_avg_mil_loss ▅█▁▃▃
wandb:  best/eval_ensemble_f1 ▁▁▅▅█
wandb:            eval/avg_f1 ▆▅▄▃▅▇▆▁▆▅▅▇▅▆▄█▅▄▃▄▅▆▇▄▅▆▇▅▅▇▅▅▅▄▅▁▇█▃▇
wandb:      eval/avg_mil_loss ▃▄▄█▅▇▅▇▅▄▃▃▃▇▄▂▇▃▅▄▄▂▄▆▆▄▁▃▆▃▂▅▄▃▄▃▂▃▆▂
wandb:       eval/ensemble_f1 ▆▅▄▄▆▆▄▇▅█▇▄▆▄▃▄▁▄▇▅▅▄▇▄▄▅▄▄▅▄▆▅▆▅▅▂▄▆▅▇
wandb:           train/avg_f1 ▁▇▅▆▄▄▄▅▄▄▂▃█▅▅▅▆▆▄▅▅▃▄▅▄▄▇▃▇▆▅▄▂▇▅▆▅▅▃▆
wandb:      train/ensemble_f1 ▁▁▄▇▃▅▃▂█▅▇▆▄▇▆▄▅▄▄▇▆▄▆▃▇▄▅▂▅▇▅▄▃▇▇▅▆▅▇▆
wandb:         train/mil_loss ▄▅▃▆▅▅▆▅▄▆▂▆▂▄▄▅█▄▆▄▅▃▆▂▄▅▅▆▅▅▁▄▅▃▃▅▄▅▄▅
wandb:      train/policy_loss ▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▂▅▅▅▅█▅▆▃▁▅▅▅▅▅▃▅▅▃▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▂██████████████████▂██████████▁██████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90116
wandb: best/eval_avg_mil_loss 0.30391
wandb:  best/eval_ensemble_f1 0.90116
wandb:            eval/avg_f1 0.88642
wandb:      eval/avg_mil_loss 0.29877
wandb:       eval/ensemble_f1 0.88642
wandb:           train/avg_f1 0.86985
wandb:      train/ensemble_f1 0.86985
wandb:         train/mil_loss 3.26744
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run ruby-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1cg2gcjq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053135-1cg2gcjq/logs
wandb: ERROR Run 1cg2gcjq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jzzk8cf6 with config:
wandb: 	actor_learning_rate: 4.867921162845706e-06
wandb: 	attention_dropout_p: 0.1318030472286474
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.08190278584262511
wandb: 	temperature: 4.940519414777996
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053314-jzzk8cf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jzzk8cf6
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 85-92, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▄█▇▄▄▆▄▃▁▅▅▃▆▅▄▅▄▃▄▄▇▆▅▇▅█▆▁▃▄▅▅▆▅▅▄▄▄▄
wandb:      eval/avg_mil_loss ▂▂█▄▅▂▁▂▃▂▂▂▂▅▅▂▂▂▃▂▂▄▃▃▄▃▃▂▂▂▃▃▅▂▂▂▃▃▃▄
wandb:       eval/ensemble_f1 ▄▆▅▅▅▅▆▄▄▄▅▄▆▅▄▇▄▄▅▅▆▇▆▆▇▁▅▄█▆▇▄▆▆▃▅▅▅▄▅
wandb:           train/avg_f1 ▁▂▄▂▃▄▃▆▁▅▅▃▄▄▅▂▅▂▃▅▅▃▄▂▃▃▅▄▄▆▁█▃▅▃▃▂▅▃▅
wandb:      train/ensemble_f1 ▂▅▃▂▄▃▃▆▂▄▇▆▅▅▆▇▄▅▄▄▂▅▃▅▄▃▅▁▅▇▁▆▅▃█▆▃▂▅▂
wandb:         train/mil_loss ▂▂▄▂▅▅▅▃▃▆▅▃▂▄▇▃▃▄▃▂▃▅▄▄▆▃▆▅▃▁▄▆▄▄▃▃▆▄█▇
wandb:      train/policy_loss ▄█▄█▄▄▄█▄█▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄█▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁█▄▄▄▄▄▄▄▄▄▄█▄█▄▄██▄▄▄▄▄▄▄▄▄▁█▄▁▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91928
wandb: best/eval_avg_mil_loss 0.29144
wandb:  best/eval_ensemble_f1 0.91928
wandb:            eval/avg_f1 0.88198
wandb:      eval/avg_mil_loss 0.33896
wandb:       eval/ensemble_f1 0.88198
wandb:           train/avg_f1 0.88995
wandb:      train/ensemble_f1 0.88995
wandb:         train/mil_loss 1.05433
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bumbling-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jzzk8cf6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053314-jzzk8cf6/logs
wandb: ERROR Run jzzk8cf6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: tvwvxold with config:
wandb: 	actor_learning_rate: 3.926197083652118e-05
wandb: 	attention_dropout_p: 0.27897180394775445
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 183
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.14982054380568144
wandb: 	temperature: 9.410199434831776
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053442-tvwvxold
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tvwvxold
wandb: uploading history steps 148-152, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅▅█
wandb: best/eval_avg_mil_loss ▄█▄▁▇▂
wandb:  best/eval_ensemble_f1 ▁▂▄▅▅█
wandb:            eval/avg_f1 ▆▇█▅▇▃▅▅▅█▄▆▅▄▆▆▄▄▇█▅▃▆▄▅▃▅▃▄▅▆▅▃▅▁▄▃▆█▁
wandb:      eval/avg_mil_loss ▂▃▂▃▁▂▅▂▄▂▄▅▃▃▄▃▄▅▃▄▅▄▅▄▂▄▅▃▇▆▆▅▂█▄▅▅▄▇▅
wandb:       eval/ensemble_f1 ▇█▆▆▅▅█▅▆▆▆▄█▇▅▄▄▅▅▄▄▇▂▆▄▄▆▅▅▂▆▆▇▆▆▃▃▆▁▅
wandb:           train/avg_f1 ▆▆▄▅▇▇▅▄▇▅█▆▄▅▃▃▇▆▆▅▄▅▄▃▅▆▆▅▄▂▄▃▂▃▁▄▄▁▁▂
wandb:      train/ensemble_f1 ▇▆▆▆▇▅▅▆▆█▇▅▃▆▄▄▇▆▅▅▄▅▃▅▅▃▃▅▄▃▂▁▁▂▂▂▃▂▁▂
wandb:         train/mil_loss ▆▅▇▇█▆▆▇▇█▇▇▇▄▇█▅▄▇▃▇▄▄▃▄▆▅▄▂▅▄▂▃▁▁▃▃▂▄▁
wandb:      train/policy_loss ▅▅▅▅▅▅▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▅▇█▇▇▇▇▇▇▇▇▇▇▇▅▇▇▇▆▇▇▇▇▁█▇▇▇▇▇▇▅▇▇▇▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93351
wandb: best/eval_avg_mil_loss 0.21811
wandb:  best/eval_ensemble_f1 0.93351
wandb:            eval/avg_f1 0.86125
wandb:      eval/avg_mil_loss 0.33871
wandb:       eval/ensemble_f1 0.86125
wandb:           train/avg_f1 0.87329
wandb:      train/ensemble_f1 0.87329
wandb:         train/mil_loss 0.27948
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sweepy-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tvwvxold
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053442-tvwvxold/logs
wandb: ERROR Run tvwvxold errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: uvu8xczv with config:
wandb: 	actor_learning_rate: 0.0009944188009506194
wandb: 	attention_dropout_p: 0.26717824861108885
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 155
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3725142555045408
wandb: 	temperature: 7.468942614011382
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053739-uvu8xczv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uvu8xczv
wandb: uploading history steps 127-140, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss █▄▂▁
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 ▄▅▅▅▅▁▇▅▇▅█▅▅▄▇▃▃▅▇█▆▆▆▅▅▅▁▆▅▃▃▂██▄▅▅▇▆▅
wandb:      eval/avg_mil_loss ▃▂▆▅▂▅█▂▃▄▂▂▂▃▅▆▃▄▂▃▆▁▆▁▂▅▇▁▃▂▄▃▆▅▄▁▆▂▄▃
wandb:       eval/ensemble_f1 ▃▃▃▆▇▅▆▃▁▁▁█▇▄▃▁▂▇▅▃▆▅▆█▇▄▆▅▄▄▃▃▂▃▁▃▄▂▃▇
wandb:           train/avg_f1 ▃▃▄▅▁▂▄▅▂▆▃▅▅▄▆▃▆▆▇▄▆▅▄▅▅█▄▁▃▃▅▁▆▄▄▄▄▃▄▆
wandb:      train/ensemble_f1 ▃▅▂▂▅▄▄▅▃▃▃▂▂▅▄▄▄▃▅▅▃█▅▇▄▃▄▁▃▅▅▄▆▄▆▅▄▅▆▄
wandb:         train/mil_loss ▇▆█▇█▆▃▇▆▄▆▆▅▆▅▆▅▅▄▄▄▅▄▄▄▆▄▃▃▅▃▁▃▄▂▃▃▂▁▂
wandb:      train/policy_loss ████████████████████████████████▁███████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▁▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93007
wandb: best/eval_avg_mil_loss 0.23037
wandb:  best/eval_ensemble_f1 0.93007
wandb:            eval/avg_f1 0.91886
wandb:      eval/avg_mil_loss 0.30126
wandb:       eval/ensemble_f1 0.91886
wandb:           train/avg_f1 0.89159
wandb:      train/ensemble_f1 0.89159
wandb:         train/mil_loss 0.26489
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pretty-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uvu8xczv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053739-uvu8xczv/logs
wandb: ERROR Run uvu8xczv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: pfcy10i5 with config:
wandb: 	actor_learning_rate: 4.869107112794649e-06
wandb: 	attention_dropout_p: 0.09597888403813289
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.09298410657013868
wandb: 	temperature: 6.47386709449575
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053957-pfcy10i5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pfcy10i5
wandb: uploading history steps 161-171, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▅▇█
wandb: best/eval_avg_mil_loss █▄▅▇▁▄
wandb:  best/eval_ensemble_f1 ▁▄▅▅▇█
wandb:            eval/avg_f1 ▅▆█▅▃▅▆▅▄▆▆▂▆▆▄▆▄▃▅▅▃▆▃▁▆▃▆▂▃▃▂▄▄▄▄▃▂▇▂▁
wandb:      eval/avg_mil_loss ▃▁▃▂▄▅▄▃▅▁▃▁▂▃▄▃▂▄▄▄▅▄▆▂▄▆▂█▃▅▂▃▃▅▅▃▂▆▅▅
wandb:       eval/ensemble_f1 ▅▆▇▅▇█▃▅▄▄▃▅▄▄▆▃▄▄▆▃▅▆▄▆▃▃▃▄▃▅▄▂▄▂▁▂▇▄▁▁
wandb:           train/avg_f1 ▅█▇▇▆▅▅▆▆▆▇▇▅▆▇▄▅▄▅▃▆▄▄▄▁▄▄▅▄▅▃▄▃▂▅▃▃▂▂▃
wandb:      train/ensemble_f1 ▅▇▇██▆▆▅▅▆█▆▅▅▆█▇▅▇▅▅▅▆▆▄▅▅▆▆▃▄▂▆▄▄▁▅▄▃▂
wandb:         train/mil_loss ▆█▆▄▆▄▅▇▃▅▃█▃▄▆▅▂▅▄▄▂▅▃▄▃▂▁▄▃▄▃▃▃▂▃▄▂▂▃▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93759
wandb: best/eval_avg_mil_loss 0.24212
wandb:  best/eval_ensemble_f1 0.93759
wandb:            eval/avg_f1 0.85751
wandb:      eval/avg_mil_loss 0.39001
wandb:       eval/ensemble_f1 0.85751
wandb:           train/avg_f1 0.88368
wandb:      train/ensemble_f1 0.88368
wandb:         train/mil_loss 0.21066
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pfcy10i5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053957-pfcy10i5/logs
wandb: ERROR Run pfcy10i5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: lxsw02mz with config:
wandb: 	actor_learning_rate: 3.761697161044507e-05
wandb: 	attention_dropout_p: 0.06434157693962017
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 131
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.15822994779921495
wandb: 	temperature: 0.9298262633577592
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054314-lxsw02mz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lxsw02mz
wandb: uploading history steps 122-131, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▄▅▄▇▅▅▃▅▄▆▅▄█▆▅▅▂▅▅▁▄▇▅▃▃▅▆▃▃▃▂▃▄▄▂▆▄▃▄
wandb:      eval/avg_mil_loss ▄▅▂▁▁▅▄▁▃▃▄▄▂▁▃▄▃▂█▆▃▅▃▃▂▅▂▄▅▃▇▇▇▂▅▄▆▇▆▇
wandb:       eval/ensemble_f1 █▇▆▆█▇▆▄▅▇▆▄▆▄▅▆▅▁▄▇▄▃▄█▆▃▂▆▆▂▃▆▃▄▁▄▁▂▆▂
wandb:           train/avg_f1 █▆▆██▇▆▆▅▅▆▆▅▅▆▆▆▆▅▅▄▅▅▅▆▄▅▆▄▅▅▄▃▅▄▄▄▃▅▁
wandb:      train/ensemble_f1 ▆█▇▇█▇▇▆▇▆▆▅▇▅▇▆▆▇▅▅▅▅▆▅▅▄▇▅▆▅▄▄▆▄▄▄▃▄▃▁
wandb:         train/mil_loss ▇█▆█▆▆▆▇▇█▄▆▅▆▆▆▄▄▅▂▃▁▄▃▅▃▄▁▂▄▂▂▃▃▁▁▁▃▃▁
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▅▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92987
wandb: best/eval_avg_mil_loss 0.21361
wandb:  best/eval_ensemble_f1 0.92987
wandb:            eval/avg_f1 0.88271
wandb:      eval/avg_mil_loss 0.36079
wandb:       eval/ensemble_f1 0.88271
wandb:           train/avg_f1 0.87724
wandb:      train/ensemble_f1 0.87724
wandb:         train/mil_loss 0.21324
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run gallant-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lxsw02mz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054314-lxsw02mz/logs
wandb: ERROR Run lxsw02mz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: znp4l1e2 with config:
wandb: 	actor_learning_rate: 1.1212091900057902e-05
wandb: 	attention_dropout_p: 0.017087322651275327
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 81
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.14752848194246548
wandb: 	temperature: 4.384300567935725
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054544-znp4l1e2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/znp4l1e2
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄█
wandb: best/eval_avg_mil_loss ▅█▆▁
wandb:  best/eval_ensemble_f1 ▁▂▄█
wandb:            eval/avg_f1 ▃▅▆▃▃▅▃▅▃▄▅▆▄▂▂▄█▂▆▆▅▆█▁▆▃▄▂▄▅▁▂▁▆▇▅▅▄▃▂
wandb:      eval/avg_mil_loss ▃▄▄▃▄▃▃▄█▄▆▄▂▂█▄▆▃▇▁▅▁▃▄▆█▄▄▄▃▁▅▄▃▂▃▆▃▄▄
wandb:       eval/ensemble_f1 ▃▅▆▅▄▄▃▄▅▄▅▄▁▅▆▃▆▄▄█▃▅▆█▂▄▃▆▅▇▆▃▂▆▆▃▅▆▄▃
wandb:           train/avg_f1 ▅▅▇▅▃▆▆▄▄▅▃▆▁▆▃▆▄▆▅▄▃▅▅▆▄▄▅▆▆█▃▃▃█▇▆▆▂▆▇
wandb:      train/ensemble_f1 ▄▄▆▆▇▄▃▂▆▁▁▆▃▇▆▆▆▆▃▆█▅▅▅▅▄▆▆▄▆██▃▃▅▅█▅▆▅
wandb:         train/mil_loss ▆▅▆▅█▇▅█▇▅▇▅▇▅▅█▇▆▇▆▃▆▂▆▃▁▅▂▇▄▅▂▅▅▅▃▄▃▆▅
wandb:      train/policy_loss █████████▆▁█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████▇██▁████████████████▁████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.23443
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.87511
wandb:      eval/avg_mil_loss 0.32942
wandb:       eval/ensemble_f1 0.87511
wandb:           train/avg_f1 0.90034
wandb:      train/ensemble_f1 0.90034
wandb:         train/mil_loss 0.44203
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run warm-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/znp4l1e2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054544-znp4l1e2/logs
wandb: ERROR Run znp4l1e2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: kpf4c8y8 with config:
wandb: 	actor_learning_rate: 0.00030811795174924566
wandb: 	attention_dropout_p: 0.0685024825061693
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 98
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6130961715833939
wandb: 	temperature: 6.163497515172841
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054722-kpf4c8y8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kpf4c8y8
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 83-98, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄▄▅▁▇▄▂▅▃▆▅▃█▅▅▅▅▄▆▇▅▅▅▇▆▅▄▂▅▄▅▅▄▁▆▆▄▆▆▄
wandb:      eval/avg_mil_loss ▄▄▅▃▆█▆▃▅▃▇▄▃▁▇▂▁▄▄▂▅▄▄▄▄▃▂▁▆▂▅█▃▄▂▄▃▆▂▂
wandb:       eval/ensemble_f1 ▄▄▂▁▅▄▆▅▇▅▆▃▄▅▅▃▆▅▄▅▅▂▇▇▇▆▄█▃▄▃▅▅▄▄▆▄▄▆▆
wandb:           train/avg_f1 ▅▆▃▄▄▅▅▅▁▁▂▅▆▄▅▂▇▃▂▆▃▂▇▆▅▃▄▂▃▁▇▅▆▂▆▄█▇▆█
wandb:      train/ensemble_f1 ▆▅▃▁▁▄▅▄▄▄▆▂▃▃▆▆▅▄▂▇▆▃▄▃▇▅▄▃▂▃▆▂▇▃▃▆▄███
wandb:         train/mil_loss ▁█▅▅▂▂▅▂▆▆▄▃▁▄▂▃▂▄▃▆▆▄▁▁▅▅▃▄▄▃▁▄▄▂▆▂▂▅▃▂
wandb:      train/policy_loss ███▁████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄█▄▄▁▄▄▄▁▄▄▄▄▄█▄▄▁▄█▄█▄▄▄█▄█▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92644
wandb: best/eval_avg_mil_loss 0.34772
wandb:  best/eval_ensemble_f1 0.92644
wandb:            eval/avg_f1 0.88606
wandb:      eval/avg_mil_loss 0.27945
wandb:       eval/ensemble_f1 0.88606
wandb:           train/avg_f1 0.89588
wandb:      train/ensemble_f1 0.89588
wandb:         train/mil_loss 3.08648
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run decent-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kpf4c8y8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054722-kpf4c8y8/logs
wandb: ERROR Run kpf4c8y8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ra5h8x4r with config:
wandb: 	actor_learning_rate: 7.009706718699551e-06
wandb: 	attention_dropout_p: 0.2174677553244423
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 143
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5296094231144065
wandb: 	temperature: 6.05482035670607
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054906-ra5h8x4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ra5h8x4r
wandb: uploading history steps 119-131, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▄█
wandb: best/eval_avg_mil_loss █▅▁▂▆
wandb:  best/eval_ensemble_f1 ▁▄▄▄█
wandb:            eval/avg_f1 ▃▄▆▄▆▂▃▃▄▄▁█▄▄▃▄▃▆▅▄▂▆▇▅▂▄▇▅▃▇▆▇▆▆▃▄▃▄▄▆
wandb:      eval/avg_mil_loss ▇▂▄▃▆▅▅█▁▆▆▆▄▃▅▅▇▄▇▂▂▅▄▇▁▆▄▄█▁▂▇▁▇▁▃▄▃▆▇
wandb:       eval/ensemble_f1 ▄▄▃▅▃▄▁█▄▁▄▄▄▄▄▄▃▇▃▅▇▅▃▂▄▄▅▃▆▆▄▄▄▄▆▅▃▂▄▆
wandb:           train/avg_f1 ▆▄▃▁█▂▃▆▄▆▂▄▃▄▃▄▂▄▇▂▅▅▃▄▆▂▃▃▂▁▅▇▃▅▆▂▅▄▂▅
wandb:      train/ensemble_f1 ▇▆▂▃▆█▂▅▃▃▄▇▄▄▂▄▃▄▃█▄▃▃▄▆▄▂▅▃▂▅▆▇▃▅▆▄▁▂▄
wandb:         train/mil_loss ▃▅▄█▇▆▄█▅▆▅▂▅▆▇▇▃▇▅█▇▆▇▅▅▄▇▅▅▅▅▅▅▆▇▆▅▆▁█
wandb:      train/policy_loss ██████████████████████████▁█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▁▅▁▁█▁▅▁▁█▁█▁▁█▅▅▅█▅▁██▅██▅█▁▁██▁▁██▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93371
wandb: best/eval_avg_mil_loss 0.26397
wandb:  best/eval_ensemble_f1 0.93371
wandb:            eval/avg_f1 0.91184
wandb:      eval/avg_mil_loss 0.24521
wandb:       eval/ensemble_f1 0.91184
wandb:           train/avg_f1 0.89086
wandb:      train/ensemble_f1 0.89086
wandb:         train/mil_loss 0.23462
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run gallant-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ra5h8x4r
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054906-ra5h8x4r/logs
wandb: ERROR Run ra5h8x4r errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 8cbkhgag with config:
wandb: 	actor_learning_rate: 3.1535021326194034e-05
wandb: 	attention_dropout_p: 0.21775071580558455
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 65
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5613615652929883
wandb: 	temperature: 7.781156430299342
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055140-8cbkhgag
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/mjcbrctw
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8cbkhgag
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅█
wandb: best/eval_avg_mil_loss ▅█▅▃▁
wandb:  best/eval_ensemble_f1 ▁▂▄▅█
wandb:            eval/avg_f1 ▆▇▃▄▃▅▁▇▄█▆▅▆▄▇▅▇▆▄▇▅▆▆▇▆▅▄▃▇▃█▅▇▃▄▅▇▅▇▄
wandb:      eval/avg_mil_loss ▂▄▄▃▂▂▃▃▂▂▂▂▂▄▂▂▄▅▃▁▂▅▃▄▃█▄▃▅▄▂▄▅▂▄▁▄▃▂▃
wandb:       eval/ensemble_f1 ▅▅▃▃▂▁▄▁▅▄▄▄▃▆▅▅▄▅▅▃▆▃▆▇▄▅▅▆▄▆▄▄▄▆▂▅█▃▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▅▁▄▄▇▆▅▅▇▆▇▆▄▅▅▇▅█▆█▄▆▄▄▅▇▇▄▅▆▄▅▄▆▅▆▆▅
wandb:      train/ensemble_f1 ▃▄▁▁▃▁▄▆█▅▁▆▄▆▁▁▃▃▃▃▃▇▅▂▂▅▂▃▄▆▃▅▃▄▄▃▂▃▄▅
wandb:         train/mil_loss ▃▇▆▆▅▅▅▆▄▄▄▆▃▅▆▆▅▄▆▃▅▃▅▆▇▄▇█▅▅▁▅▃▅▅▆▄▇▅▂
wandb:      train/policy_loss ▄▄▁▁▄▁█▄▁▁▄▄▁▄▄█▁▁▄▄▄▄█▄▁▄▄▄█▄█▁▄▄▄█▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▁▁▄██▄▁▄█▄▁▄▄▄█▄▁▄▄▁▁▄▁▄▄█▄█▁▄▁▄▄▄█▁▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94472
wandb: best/eval_avg_mil_loss 0.2219
wandb:  best/eval_ensemble_f1 0.94472
wandb:            eval/avg_f1 0.8964
wandb:      eval/avg_mil_loss 0.316
wandb:       eval/ensemble_f1 0.8964
wandb:            test/avg_f1 0.92251
wandb:      test/avg_mil_loss 0.15153
wandb:       test/ensemble_f1 0.92251
wandb:           train/avg_f1 0.90028
wandb:      train/ensemble_f1 0.90028
wandb:         train/mil_loss 0.29625
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sleek-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8cbkhgag
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055140-8cbkhgag/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 24ted5qj with config:
wandb: 	actor_learning_rate: 0.00012451551491104133
wandb: 	attention_dropout_p: 0.35853697238704224
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 137
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2847082627819584
wandb: 	temperature: 2.908259957815178
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055302-24ted5qj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/24ted5qj
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json
wandb: uploading history steps 129-138, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▅▆▇██
wandb: best/eval_avg_mil_loss █▅█▅▇▄▃▁
wandb:  best/eval_ensemble_f1 ▁▂▂▅▆▇██
wandb:            eval/avg_f1 ▅▃▃▄▃▅▅▂▄▇▂▄▅▅▃▂▄▂▄█▄▅▆▅▄▅▄▃▅▄▄▂▄▆▁▅▄▅▄▄
wandb:      eval/avg_mil_loss ▃▂▃▃▅▄▄█▄▅▃▃▃▄▇▃▄▄▄▄▂▃▃▄▃▄▃▃▅▃▂▃▃▄▂▁▅▄▃▄
wandb:       eval/ensemble_f1 ▂▃▃▃▇▅▄▁▅▂▃▂▂▄▂▅▅▄▃▂▄▂▂▄▄█▇█▆▄▂▅▃▄▅█▅▄▅▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▂▂▂▅▃▅▄▅▆▂▅▄▆▅▃▆▆█▄▄▃▅▅▃▂▁▇▅▅▃▃▆▄▆▅▂▄▇▃
wandb:      train/ensemble_f1 ▂▅▃▃▃▂▃▃▄▇▆▄▄▆▃▅▄▆▁█▆▆▇█▃▄▇▄▂▇▆▆▄▅▅▃▃▄▅▄
wandb:         train/mil_loss █▅▆▆▆▆▄▅▆▇▄▄▆▅▅▆▄▅▅▃▄▄▅▃▄▃▅▃▃▃▃▃▃▄▃▃▂▂▂▁
wandb:      train/policy_loss ▃▃▃▂▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▄▃▃▃▃▄▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93351
wandb: best/eval_avg_mil_loss 0.15597
wandb:  best/eval_ensemble_f1 0.93351
wandb:            eval/avg_f1 0.88918
wandb:      eval/avg_mil_loss 0.4299
wandb:       eval/ensemble_f1 0.88918
wandb:            test/avg_f1 0.87198
wandb:      test/avg_mil_loss 0.21136
wandb:       test/ensemble_f1 0.87198
wandb:           train/avg_f1 0.88887
wandb:      train/ensemble_f1 0.88887
wandb:         train/mil_loss 1.00101
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run elated-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/24ted5qj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055302-24ted5qj/logs
wandb: Agent Starting Run: zzm1jntq with config:
wandb: 	actor_learning_rate: 0.000736419853994619
wandb: 	attention_dropout_p: 0.3860111006837825
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 101
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5971474253857701
wandb: 	temperature: 8.717998535247341
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055501-zzm1jntq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zzm1jntq
wandb: uploading history steps 92-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss █▂▁▂
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▆▆▅▅▄▆▇▅▅▇▇▆▄▇▆▇▇▇▆▆▅▆▆▄▅▆▄▃█▆▄▄▅▄▄▆▅▄▄▁
wandb:      eval/avg_mil_loss ▇▂▅▂▅▅▃▃▃▃▂▂▄▂▅▂▁▂▆▄▃▃▇▆▃▄▃▅▂▅▄▂█▂▄▂▃▃▄▁
wandb:       eval/ensemble_f1 █▆▇▇▆██▆▇▅▅▇▆▆▆▆▆▅▇█▅▇▇▇▆▆▆▃▇▁▆▄▄▅▆▅▄▅▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▇▅▅█▇▇▆▇▅▆█▃▆▅▅▅▅▆▅▄▃▅▃▄▃▃▂▄▄▂▆▂▂▃▁▄▂▁▁
wandb:      train/ensemble_f1 ▆█▇█▇▇▇▇▇█▅▅▇▆▅▆▆▆▆▆▅▅▆▄▅▅▄▅▄▄▃▄▃▆▃▄▅▄▃▁
wandb:         train/mil_loss █▅█▆▆▄▆▃▅▇▇▅▄▇▄▆▇▄▃▅▄▂▃▅▃▃▃▂▄▃▅▃▄▅▂▄▁▃▄▃
wandb:      train/policy_loss ▄▃▁▄▄▄▄▄▄▄▂▄▄▄█▅▄▄▄▄▆▄█▄▄▄▄▂▅█▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅█▅▆▅▇▅▅▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.27394
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.82473
wandb:      eval/avg_mil_loss 0.45564
wandb:       eval/ensemble_f1 0.82473
wandb:            test/avg_f1 0.87951
wandb:      test/avg_mil_loss 0.25051
wandb:       test/ensemble_f1 0.87951
wandb:           train/avg_f1 0.85923
wandb:      train/ensemble_f1 0.85923
wandb:         train/mil_loss 0.21013
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sunny-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zzm1jntq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055501-zzm1jntq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ffbrp5y4 with config:
wandb: 	actor_learning_rate: 0.00043945480539598513
wandb: 	attention_dropout_p: 0.2667316442555101
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 116
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8744467897700423
wandb: 	temperature: 8.433027460377696
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055658-ffbrp5y4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ffbrp5y4
wandb: uploading history steps 105-116, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂█
wandb: best/eval_avg_mil_loss ▁▂█
wandb:  best/eval_ensemble_f1 ▁▂█
wandb:            eval/avg_f1 ▇▇▆▅▆▆█▇█▂▇▄▆▆▅▅▅▂▆▄▅▄▄▁▅▄▅▂▄▄▃▆▇▃▆▅▃▄▄▆
wandb:      eval/avg_mil_loss ▁▂▅▃▃▂▄▃▁▂▄▂▅▂▂▄▄▃▄▇▄▆▃▆▄▅▅▇▇▄▇▅▅▆▄▅▇█▅▂
wandb:       eval/ensemble_f1 ▇██▄▃▆▅▄█▂▇▇▄▆▇▆▂▅▆▅▇▅▄▇▁▅▆▅▃▄▁▁▃▇▄▃▆▆▅▆
wandb:           train/avg_f1 ▅▄▇▃▅▆▅▅▅▅▇▄▃█▇▄▂▄▅▁▅▄▅▄▅▆▃▄▂▃▃▂▂▃▁▂▁▄▂▇
wandb:      train/ensemble_f1 ▅▆▆▅▅▃▅▆▄▅▅▄█▅▅▆▆▄▆▁▅▂▄▆▅▆▆▅▄▅▂▃▂▃▄▄▂▂▂▅
wandb:         train/mil_loss █▇▆▆▆▅▄▄▄▃▄▃▂▂▃▃▂▂▂▂▂▂▂▂▃▂▂▁▂▁▁▁▁▂▂▁▁▂▂▁
wandb:      train/policy_loss ▂▂▁▄▂▂█▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▂▁▁▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.25443
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.90667
wandb:      eval/avg_mil_loss 0.24326
wandb:       eval/ensemble_f1 0.90667
wandb:           train/avg_f1 0.89372
wandb:      train/ensemble_f1 0.89372
wandb:         train/mil_loss 0.2471
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ffbrp5y4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055658-ffbrp5y4/logs
wandb: ERROR Run ffbrp5y4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qdvumye3 with config:
wandb: 	actor_learning_rate: 3.818099804278545e-05
wandb: 	attention_dropout_p: 0.4402402566584476
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 110
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.15643831380972628
wandb: 	temperature: 3.358905246451087
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055901-qdvumye3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qdvumye3
wandb: uploading wandb-summary.json
wandb: uploading history steps 98-110, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃██
wandb: best/eval_avg_mil_loss ▃▄▁█
wandb:  best/eval_ensemble_f1 ▁▃██
wandb:            eval/avg_f1 ▃█▇▃█▂▄▃▄▅▅▆▇▅▆▃▄▄▁▆▂▃▃▅▁▄▄▃▄▂▅▆▁▄▅▃▄▄▅▆
wandb:      eval/avg_mil_loss ▂█▅▂▁▃▂▃▂▃▂▃▄▃▂▃▃▄▂▆▆▂▅▄▂▃▅▂▃▃▇▄▄▆▃▃▄▂▂▃
wandb:       eval/ensemble_f1 ▅▃▆▂▃▂▃▅▃█▂▅▄▃▄▅▆▃▁▃▃▄▃▂▁▅▄▇▁▅▄▂▄▆▇▃▅▄▄▄
wandb:           train/avg_f1 ▇▆▄▅▆▂▅▇▄▁▆▄▂▂▄▇▂▆▆▆▂▆▇▃▆▄▇█▂▂▅▅▇▄▇█▅▄▅▄
wandb:      train/ensemble_f1 ▄▇▄█▇▄▂▄▄▄▃▁▅▂▂▇▇▂▆▂▄▆▃▆▅▆▆▅▇▄▆▅▄▆▄▄▄▂▄▄
wandb:         train/mil_loss █▇█▇█▆▅▇▅▆▆▅▆▅▆▄▅▃▅▄▃▄▅▄▃▄▄▃▄▂▂▂▃▂▃▃▁▂▂▄
wandb:      train/policy_loss ▄█▄▄▄▄▄▄▄▅▄▄▄▄▄▇▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▄█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92653
wandb: best/eval_avg_mil_loss 0.35046
wandb:  best/eval_ensemble_f1 0.92653
wandb:            eval/avg_f1 0.90706
wandb:      eval/avg_mil_loss 0.32754
wandb:       eval/ensemble_f1 0.90706
wandb:           train/avg_f1 0.8913
wandb:      train/ensemble_f1 0.8913
wandb:         train/mil_loss 0.27026
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devout-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qdvumye3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055901-qdvumye3/logs
wandb: ERROR Run qdvumye3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: rshhenzm with config:
wandb: 	actor_learning_rate: 2.7828467301200997e-06
wandb: 	attention_dropout_p: 0.29359073144135434
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 157
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3276791561718161
wandb: 	temperature: 5.543333841133704
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060034-rshhenzm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rshhenzm
wandb: uploading history steps 94-113; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 114-114, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▂▂▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▆▅▅█▅▃▄▃▄▅▅▄▃▆▆▅▃▆▃▆▅▅▆▅▃▄▄▇▄▅▄▄▄▅▄▃▄▂▁▁
wandb:      eval/avg_mil_loss █▄▅▆▆▃▄▁▃▃▂▄▃▅▂▃▆▇▄▃▄▄▁▄▃▅▅▂▄▅▇▃▂▂▄▅▄▁▄▃
wandb:       eval/ensemble_f1 ▆▅█▆█▆▄▄▄▅▅▃▄▄▆▇▅▅▆▅▄▆▄▄▄▇▄▄▅▅▅▃▅▄▄▂▅▆▁▁
wandb:           train/avg_f1 ▆▃▆▄▄▅█▄▆▆▅▄▆▃▃▅▃▃▅▂▃▃▁▅▅▅▁▃▃▂▅▃▁▄▃▅▅▃▄▂
wandb:      train/ensemble_f1 ▃▄▆▇▆▆▆▃▄▇▁▂▄█▅▄▆▃▅▂▃▃▆▄▄▇▃▂▅▂█▃▅▄▄▃▅▃▃▃
wandb:         train/mil_loss █▃▇▄▂▄▄▇▄▃▄▄▃▄▄▁▄▄▅▆▃▆▆▅▃▃▅▃▃▅▅▄▅▆▂▅▄▄▄▆
wandb:      train/policy_loss ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.22979
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.86113
wandb:      eval/avg_mil_loss 0.29386
wandb:       eval/ensemble_f1 0.86113
wandb:           train/avg_f1 0.88402
wandb:      train/ensemble_f1 0.88402
wandb:         train/mil_loss 0.19007
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run deep-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rshhenzm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060034-rshhenzm/logs
wandb: ERROR Run rshhenzm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jxtooaxo with config:
wandb: 	actor_learning_rate: 6.234510388092679e-06
wandb: 	attention_dropout_p: 0.2799895282235829
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6839511607912218
wandb: 	temperature: 1.7535948249969568
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060212-jxtooaxo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jxtooaxo
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃██
wandb: best/eval_avg_mil_loss ▄▁█▅▁
wandb:  best/eval_ensemble_f1 ▁▁▃██
wandb:            eval/avg_f1 ▅▆▄▆▆▃▁▂▃▂▅▄▅▄▇▄▄▃▄▄▄▆▆▅▃▅▃▇█▄▃▂▃▆▃▆▄▄▃▂
wandb:      eval/avg_mil_loss ▃▄▅▄▃▆▇▄▄▃▅▄▃▃▃▃▂▆▇▁▂▅▂▅▄▃▅█▆▆▃▄▅▄▃▇▃▃▆▆
wandb:       eval/ensemble_f1 ▅▆▇▅▆▃▄▁▅▅▅▇▅▅▄▄█▆▅▄▅▇▆▃█▄▆▄▆▄█▄▄▆▅▅▁▅▆▅
wandb:           train/avg_f1 ▅▂▃▅▆▃▄▆▅▆▄▆▅▄▅▄▆█▆▇▆█▇▇▆▆▆▅▂▁▃▄▅▄▄▃▆▃▃▃
wandb:      train/ensemble_f1 ▅▄▄▇█▂▄▅▅▄▅▅▇▆▆▇▆▆█▆▄█▁▇▂▆▆▆▄▃▅▇▄▆▅▆▆█▅▄
wandb:         train/mil_loss ▇█▇▄▆▆█▅▅█▆▇▇▅▅█▃▇▂▆▆▅▃▅▄▅█▅▄▁▄▁▄▃▃▆▅▄▄▂
wandb:      train/policy_loss ▆▆▆▆▆▁█▆▆▆▆▆▆▆▆▆▇▇▆▆▆▆▃▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▅▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▁▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92281
wandb: best/eval_avg_mil_loss 0.26948
wandb:  best/eval_ensemble_f1 0.92281
wandb:            eval/avg_f1 0.88139
wandb:      eval/avg_mil_loss 0.40153
wandb:       eval/ensemble_f1 0.88139
wandb:           train/avg_f1 0.88902
wandb:      train/ensemble_f1 0.88902
wandb:         train/mil_loss 3.36955
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run royal-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jxtooaxo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060212-jxtooaxo/logs
wandb: ERROR Run jxtooaxo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xv0675s1 with config:
wandb: 	actor_learning_rate: 2.3283845674668228e-06
wandb: 	attention_dropout_p: 0.23459343870485244
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 66
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17529272373906302
wandb: 	temperature: 9.521663949149463
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060355-xv0675s1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xv0675s1
wandb: uploading history steps 54-67, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅▅▆▇██
wandb: best/eval_avg_mil_loss █▄▁▄▃▄▂▁▁
wandb:  best/eval_ensemble_f1 ▁▃▄▅▅▆▇██
wandb:            eval/avg_f1 ▆▄▇▇▇▁█▄▇█▇▃██▇▅▄▄▇▄▃▃▅▆▆▆▃▅▇▃▁▄▄▅▃▄▅▃▃▇
wandb:      eval/avg_mil_loss ▇▄▅▅▁▇▄▄▂▂▄▆▂▅▂▂▄▂▂▃▃▂▇▁▅▃▄▆▂▇▅▄█▆▅▃▆▄▆▃
wandb:       eval/ensemble_f1 ▄▇▇▂▇█▄▇█▄▃██▇▇▁▅▄▇█▆▂▆▃▄▇▃▃▁▄▆▂▄▅▄▃▃▃▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▇▆▅▃▄█▄▆▅▅▇▅▅▆▅▅▇▅▇▅▅▆▃▇▃▅▃▁▃▂▃▁▅▅▇▅▅▄▅
wandb:      train/ensemble_f1 ▆▆▃▆▇▅▄▅▅▆▅▇▆▅▅▅▇▅▆▆█▃▄▆▃▅▁▃▃▆▄▁▇▅▄▆▅▄▅▅
wandb:         train/mil_loss ▆▆█▅▅▃▆▃▇▅▅█▅▃▅▅▅▅▁▃▂▆▇▅▆▄▅▇▃▄▅▄▄▇▂▃▅▇▆▅
wandb:      train/policy_loss █▁▄▄▄▄▄▄▄▄▄▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃█▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92281
wandb: best/eval_avg_mil_loss 0.24669
wandb:  best/eval_ensemble_f1 0.92281
wandb:            eval/avg_f1 0.9047
wandb:      eval/avg_mil_loss 0.27554
wandb:       eval/ensemble_f1 0.9047
wandb:            test/avg_f1 0.92817
wandb:      test/avg_mil_loss 0.1899
wandb:       test/ensemble_f1 0.92817
wandb:           train/avg_f1 0.89314
wandb:      train/ensemble_f1 0.89314
wandb:         train/mil_loss 0.19999
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run twilight-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xv0675s1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060355-xv0675s1/logs
wandb: Agent Starting Run: cwd0wmfr with config:
wandb: 	actor_learning_rate: 1.4073661490018532e-05
wandb: 	attention_dropout_p: 0.3252236525462876
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0240859698138316
wandb: 	temperature: 8.02074493211462
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060458-cwd0wmfr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cwd0wmfr
wandb: uploading wandb-summary.json
wandb: uploading history steps 58-68, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇█
wandb: best/eval_avg_mil_loss ▁█▃▁
wandb:  best/eval_ensemble_f1 ▁▄▇█
wandb:            eval/avg_f1 ▄▇▅▆▆▆▅▅▆▄▅▅▅▄▅▄▆▃▂▆▄▄▁▅▄▇▆▅▅█▄▃▅▆▅▅▅▅▄▄
wandb:      eval/avg_mil_loss ▂▅▃▄▂▄▃▄▄▂█▃▆▄▂▃▃▂▄▂▇▃▅▄▃▂▃▁▂▂▂▂▃▄▃▃▃▄▅▃
wandb:       eval/ensemble_f1 ▆▅▃▆▆▄▅▆▇▅▄▃▅▆▄▃▇▄▃▇▃▆▄▁▅▆▃▇▁▅█▃▅▆▆▅▆▆▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▆▅▅▁▄▇▂▅▇▄▅▅▅█▃▄▃▄▅▆▄▃▄▆▃▆▂▄▄▂▄▄▂▂▃▁▁▄
wandb:      train/ensemble_f1 ▅▄▄▅█▄▁▅▄▆▅▂▅▄▇▅▅▅▃▃▄▄▅▄▄▁▄▃▃▃▄▂▄▃▃▃▁▂▅▄
wandb:         train/mil_loss ██▄▅▄█▆▆▅▅▇▇▄▅▆▄▅▄▅▃▆▆▃▆▁▃█▆▅▂▆▃▃▆▇▅▁▆▅▅
wandb:      train/policy_loss ███████████████▁████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅█▅▅▇▅▅▅▄▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92669
wandb: best/eval_avg_mil_loss 0.25591
wandb:  best/eval_ensemble_f1 0.92669
wandb:            eval/avg_f1 0.8903
wandb:      eval/avg_mil_loss 0.31638
wandb:       eval/ensemble_f1 0.8903
wandb:            test/avg_f1 0.90623
wandb:      test/avg_mil_loss 0.18866
wandb:       test/ensemble_f1 0.90623
wandb:           train/avg_f1 0.89178
wandb:      train/ensemble_f1 0.89178
wandb:         train/mil_loss 0.24407
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run true-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cwd0wmfr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060458-cwd0wmfr/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rgzyr15t with config:
wandb: 	actor_learning_rate: 5.3826085339959755e-06
wandb: 	attention_dropout_p: 0.10761054846192908
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 103
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.01049306592225696
wandb: 	temperature: 7.592425393578916
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060624-rgzyr15t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rgzyr15t
wandb: uploading history steps 89-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss ▆▆█▁
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ▅▄▃▁▃▄▅▆▆▇▅▇▆▄▆▄█▃▄▅▃▆▄▇▅██▃▅▄▁▅▄█▅▆▃▂▆▅
wandb:      eval/avg_mil_loss ▆▃▃▂▃▆▄▆▃▃▄▃▂▅▃▆▃▃▃▂▃▃▂▁▂▅▃▅▃▄▇▅▂▅█▅▆▄▃▅
wandb:       eval/ensemble_f1 ▇▅▆▆█▅▅▆▆▆█▃▇▅▇▅▆▆▄▇▆▅▆█▇▇█▄▇▂▄▆▆▁▄▃▃▄▆▅
wandb:           train/avg_f1 ▇█▄▅▅▆▄▅▅▇▅▆▂▄▆▅▆▆█▄▄▆▁▃▃▄▆█▃▅▆█▅▄▅▅▄█▄▁
wandb:      train/ensemble_f1 ▄▄▅▇▇▄▂▅▆▄▄▇▇▄▇▄█▄▅▅▅▆▆▂▄▆▃▄▅▅▅▃▄▂▅▅▄▁▅▄
wandb:         train/mil_loss ▅█▇█▅█▆▆▆▅▆▅▆▇▇▃▅▆▇▇▅▅▅▆▇▆▆▅▅▆▄▅▅▆█▇▅▃▅▁
wandb:      train/policy_loss ▆█▆▆█▆▆▂▆▂▆▆▆▆▅▃▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▂▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▇▁█▄▄▄▄▄▄▄▁▄▄▄▄▃▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91135
wandb: best/eval_avg_mil_loss 0.22373
wandb:  best/eval_ensemble_f1 0.91135
wandb:            eval/avg_f1 0.88836
wandb:      eval/avg_mil_loss 0.36578
wandb:       eval/ensemble_f1 0.88836
wandb:           train/avg_f1 0.88928
wandb:      train/ensemble_f1 0.88928
wandb:         train/mil_loss 3.42136
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run major-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rgzyr15t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060624-rgzyr15t/logs
wandb: ERROR Run rgzyr15t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: bx9v55e9 with config:
wandb: 	actor_learning_rate: 1.2681738073363836e-05
wandb: 	attention_dropout_p: 0.3802311426951991
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 55
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05004118530171264
wandb: 	temperature: 9.173863498277168
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060757-bx9v55e9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bx9v55e9
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄█
wandb: best/eval_avg_mil_loss ▆█▆▃▁
wandb:  best/eval_ensemble_f1 ▁▃▃▄█
wandb:            eval/avg_f1 ▁▃▃▂▄▅▃▁▂▃▄▄▅▄▁▁▆▆▃▇▅▄▄█▄▃▆▃▆▄▄▅▂▆▄▇▅▄▃▂
wandb:      eval/avg_mil_loss ▅▇▅▆▅▃▇▅▃▄▃▃▅▅▃▂█▂▁▄▅▃▄▁▄▁▃▃▃▄▄▅▄▅▄▃▃▅▂▂
wandb:       eval/ensemble_f1 ▁▃▃▂▄█▅▃▁▂▄▄▅▁▆▁▆▆▃▇▄▄█▄▄▆▃▆▄▃▅▂▆▄▂▅▄▃▃▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▅▄▅▃▄█▄▁▅▅▅▅▄▃▁▂▆▄▇▅▃▄▇▅▅▅▅▆▅▆▅▃▄▅▅▅▄▁▅
wandb:      train/ensemble_f1 ▂▅▄▅▃▄█▆▁▅▅▅▅▃▄▁▂▆▄▇▃▅▃▄▇▅▅▅▅▄▅▄▃▄▅▅▅▁▃▅
wandb:         train/mil_loss ▆█▇▆▆▆▆▆▇▆▆▅▆▇▇▅▅▅▄▅▆▄▆▆▃▅▄▄▄▃▅▁▄▃▃▂▆▄▄▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9229
wandb: best/eval_avg_mil_loss 0.226
wandb:  best/eval_ensemble_f1 0.9229
wandb:            eval/avg_f1 0.87838
wandb:      eval/avg_mil_loss 0.27901
wandb:       eval/ensemble_f1 0.87838
wandb:            test/avg_f1 0.89399
wandb:      test/avg_mil_loss 0.23769
wandb:       test/ensemble_f1 0.89399
wandb:           train/avg_f1 0.88986
wandb:      train/ensemble_f1 0.88986
wandb:         train/mil_loss 1.68196
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run different-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bx9v55e9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060757-bx9v55e9/logs
wandb: Agent Starting Run: iyrctola with config:
wandb: 	actor_learning_rate: 2.16009541842927e-06
wandb: 	attention_dropout_p: 0.48086895253034007
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.015300339289217146
wandb: 	temperature: 8.221115039506921
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060848-iyrctola
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iyrctola
wandb: uploading wandb-summary.json
wandb: uploading history steps 56-67, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄▁▆▅▂▃▅▅▂▅▄▇▅▁▂█▇▅▃▂▆▃▆▇▅▅▇▆▂▆▃▇▁▅▃█▅█▆▆
wandb:      eval/avg_mil_loss ▄▄▆▃▃▂▇▄▄▃▃▁▃▃▁▂▃▅▃▃▄▇▄▃█▂▅▅▅▃▂▅▄▄▄▇▂▅▃▂
wandb:       eval/ensemble_f1 ▇▄▂▄▆▂▃▅▅▁▄▄▅▁█▇▇▂▂▃▇▅▄▅▇▆▂▆▅▄▃▅▅▃▄▄▅█▆▆
wandb:           train/avg_f1 ▃▆▃▅▇▄▅▄▆▆▆▄▄▆▅▇▆▃▄▁▂▆█▆▄▇▆▇▆▃▅▄▅▅▄▂▄▆▄▃
wandb:      train/ensemble_f1 ▅▂▇▂▇▃▄▄▆▆▆▆▃▃▅▇▄▆▂▆▁▄▆▅▅▅▆▅▂▆▄▅▅▁▄▅▃█▂▃
wandb:         train/mil_loss ▇▇█▆▅▇▇▆▅█▇▆▆▆▇▅▃▅▅▄▄▆▅▇▇▅▅▂▄▅▄▄▅▂▆▂▂▁▁▃
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91556
wandb: best/eval_avg_mil_loss 0.30025
wandb:  best/eval_ensemble_f1 0.91556
wandb:            eval/avg_f1 0.90396
wandb:      eval/avg_mil_loss 0.2619
wandb:       eval/ensemble_f1 0.90396
wandb:           train/avg_f1 0.89904
wandb:      train/ensemble_f1 0.89904
wandb:         train/mil_loss 0.81899
wandb:      train/policy_loss -0.52927
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.52927
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run feasible-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iyrctola
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060848-iyrctola/logs
wandb: ERROR Run iyrctola errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: pl9b98ay with config:
wandb: 	actor_learning_rate: 3.0781798477728526e-06
wandb: 	attention_dropout_p: 0.271549565709879
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 55
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.09390568320411909
wandb: 	temperature: 8.61919883576473
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060951-pl9b98ay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pl9b98ay
wandb: uploading history steps 55-56, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃█
wandb: best/eval_avg_mil_loss █▁▁
wandb:  best/eval_ensemble_f1 ▁▃█
wandb:            eval/avg_f1 ▅▄▃▆▆▄▅▂▅▃▃▅▅▆▆█▅█▇▅▃▃▃▃▃▇▄▃▅▇▄▇▃▅▄▆▃▁▇▇
wandb:      eval/avg_mil_loss ▇▄▂▃▅▃▅▅▃▄▅▄▄▃▃▅█▃▇▄▁▆▅▇▅▅▅▅▅▇▅▃▇▄▆▃▅▇▅▅
wandb:       eval/ensemble_f1 ▅▄▃▆▆▄▃▂▅▄▅▅▆▆▅▅█▇▅▄█▂▃▃▃▂▇▄▃▅▅▆▄▃▅▄▆▃▁▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▅▁▅▄▁▄▇▃▆▆▅▂▂▄▄▇▆▅▅█▃▆▅█▄▂▃▇▃▃▃▃▃▂▄▅▃▅▁
wandb:      train/ensemble_f1 ▂▅▁▅▄▁▅▇▃▆▅▂▂▄▅▃▇▆▅▅█▃▆▅▅▆▇▃▄▃▃▆▃▃▂▄▅▃▅▅
wandb:         train/mil_loss ▅▃▆▄▄▄▆▆▄▅▄▄▄▂▁▃█▄▅▄▁▅▂▂▃▅▃▃█▃▃▂▃▂▂▆▅▆▅▅
wandb:      train/policy_loss ▄▄▄▆▂▄▃▄▄▄▄▄▇▄▃▃▄▁▄▄▄▄▄▄█▄▄▂▄▄▄▄▄▄▄▄▃▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▆▃▁▄▄▄▄▄▄▄▄▄▆▄▄▄▄▄█▄▄▄▇▄▄▄▃▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91886
wandb: best/eval_avg_mil_loss 0.25488
wandb:  best/eval_ensemble_f1 0.91886
wandb:            eval/avg_f1 0.91087
wandb:      eval/avg_mil_loss 0.31406
wandb:       eval/ensemble_f1 0.91087
wandb:            test/avg_f1 0.90493
wandb:      test/avg_mil_loss 0.21875
wandb:       test/ensemble_f1 0.90493
wandb:           train/avg_f1 0.87993
wandb:      train/ensemble_f1 0.87993
wandb:         train/mil_loss 0.81005
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pl9b98ay
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060951-pl9b98ay/logs
wandb: Agent Starting Run: caag0wgo with config:
wandb: 	actor_learning_rate: 3.804410492561592e-05
wandb: 	attention_dropout_p: 0.38568371260851864
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3541108608978766
wandb: 	temperature: 9.959081282302996
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061042-caag0wgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/caag0wgo
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▄▅▅▆█
wandb: best/eval_avg_mil_loss ▅█▅▃▁▅▁▁
wandb:  best/eval_ensemble_f1 ▁▃▄▄▅▅▆█
wandb:            eval/avg_f1 ▂▃▄▃▅▃▃▇█▅▄▃▃▄▁▄▄▅▇▄▃▁█▆▂▄▄▂▃▂▅▁▅▅▄▂▃▅▄▃
wandb:      eval/avg_mil_loss ▃▃▄▄▁▃▂▄▄▃▂▃▄▃▂▃▃▂▄▃▃█▂▄▁▃▄▃▆▄▃▃▃▃▂▆▄▂▂▃
wandb:       eval/ensemble_f1 ▂▂▅▅▃▄▅▅▅▃▄▆▃▆▄█▅▁▂▇▅▅▅▂▂▂▄▆▆▂▄▆▆▅▅▄▆▄▃▃
wandb:           train/avg_f1 ▁▆█▅▄▄▅▅▇▅▅▆▆▇▅▃▆▄▄▅▆▄▄▂█▄▃▃▃▅▄▃▅▆▃▂▅▁▄▂
wandb:      train/ensemble_f1 ▇▂▆▂▄▇▇▆▇▆▇▇▆▅▅▄▆█▅▅▃▄█▃▆▅▆▆▆▅▁▂▅▄▄▄▅▃▂▃
wandb:         train/mil_loss ▄▇▄▃▃▂▆▃▅▅▄▇▄▅▇▆█▇▄▅▅▄▇▃▆▃▅▅▆▅▅▄▇▁▅▃▂▄▅▄
wandb:      train/policy_loss ▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂█▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▃▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▄▃▃▃▃▃▃▁▃█▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92236
wandb: best/eval_avg_mil_loss 0.25743
wandb:  best/eval_ensemble_f1 0.92236
wandb:            eval/avg_f1 0.88788
wandb:      eval/avg_mil_loss 0.33604
wandb:       eval/ensemble_f1 0.88788
wandb:           train/avg_f1 0.88006
wandb:      train/ensemble_f1 0.88006
wandb:         train/mil_loss 2.71191
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worthy-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/caag0wgo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061042-caag0wgo/logs
wandb: ERROR Run caag0wgo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: l6hrp8wb with config:
wandb: 	actor_learning_rate: 5.703528428177775e-06
wandb: 	attention_dropout_p: 0.2716316121996064
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 62
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4076803798939728
wandb: 	temperature: 8.465599282254292
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061205-l6hrp8wb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l6hrp8wb
wandb: uploading history steps 54-62, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▇█
wandb: best/eval_avg_mil_loss █▄▃▁▁
wandb:  best/eval_ensemble_f1 ▁▅▅▇█
wandb:            eval/avg_f1 ▄▇▆▆▇▅▄▅▇▇▅▆▄▇▆█▂▇▇▆▇▃▄▄▆▇▃▂▁▆▇▆▃▅▆▆▇▁▆▁
wandb:      eval/avg_mil_loss ▃▂▂▁▁▄▂▂▄▃▁▁▃▃▆▁▃▁▂▁▃▂▇▂▄▅▅▂▃▃▃▅▂▄▄▁█▃▄▄
wandb:       eval/ensemble_f1 ▅▄▄▆▅▄▆▅▁▃▄▃█▆▅▅▅▅▂▆█▃▃▄▃▆▂▁▅▄▅▂▄▄▅▅▅▅▅▂
wandb:           train/avg_f1 ▆▅▅▂▅▅▇▃█▄▆▃█▇▄▄▃▁▂▄▆▂▇▆▁▄▂▆▃▅▅▅▆▆█▅▄▃▅▄
wandb:      train/ensemble_f1 ▆▅▅▄▂▆▆▇▂▃▆▃▇▃▆▄▄▆▅▂▆▁▄▂▆▅▄▅▅▆█▄▅▅▆▆▄▃▆▅
wandb:         train/mil_loss ▅▅▄▅▄▄▅▅█▆▅▄▄▇▄▃▆▃▄▂▅▄▃▃▄▅▄▆▄▄▃█▆▆▄▄▄▅▁▃
wandb:      train/policy_loss ▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93388
wandb: best/eval_avg_mil_loss 0.23119
wandb:  best/eval_ensemble_f1 0.93388
wandb:            eval/avg_f1 0.87511
wandb:      eval/avg_mil_loss 0.3189
wandb:       eval/ensemble_f1 0.87511
wandb:           train/avg_f1 0.89357
wandb:      train/ensemble_f1 0.89357
wandb:         train/mil_loss 0.23969
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run usual-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l6hrp8wb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061205-l6hrp8wb/logs
wandb: ERROR Run l6hrp8wb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xo4eqz5z with config:
wandb: 	actor_learning_rate: 7.737873763380963e-05
wandb: 	attention_dropout_p: 0.3729862805926481
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.00493919803346865
wandb: 	temperature: 7.765473844394954
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061302-xo4eqz5z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xo4eqz5z
wandb: uploading history steps 38-55, summary; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▆▄▅▆▄▇▅▆█▅▆▆▇▅▅▆▆▅█▆▇▅▇▆▆▄▆▁▅▇▆▆▇▅▇▅█▅▆▆
wandb:      eval/avg_mil_loss ▅▁▅▂▃▅▇▃▂▃▂▇▃▃▃█▃▂▃▅▅▃▂▄▃█▅▂▄▆▄▃▄▃▁▄▆▂▇▃
wandb:       eval/ensemble_f1 ▇▆▄▅▆▇▂▅▆█▅▇▆▅▅▆▅▆█▆▅▇▆▆▅▆▁▅▇▆▄▇▅▇▂█▅▇▆▆
wandb:           train/avg_f1 ▂▄▅▅▄▆▅▅▂▇▅▆▄▄▅▂▄▅▃▅▆▁▂█▇▇▅▂▃▄▄▂▄▃▃▄▂▅▄▁
wandb:      train/ensemble_f1 ▂▄▅▅▄▅▅▅▂▇▅▆▄▄▅▄▅▃▇▆▂█▇▃▇▇▅▂▄▂▄▃▃▄▂▂▅▂▄▁
wandb:         train/mil_loss ▄▄▃▃▇█▅▆▂▃▄▂▄▅▅█▅▅▃▁▅▃▂▅▄▂▄▁▄▄▂▄▃▄▁▄▃▁▁▅
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▂▄▃█▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▇▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄█▄▄▃▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▇▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92261
wandb: best/eval_avg_mil_loss 0.25898
wandb:  best/eval_ensemble_f1 0.92261
wandb:            eval/avg_f1 0.89726
wandb:      eval/avg_mil_loss 0.2928
wandb:       eval/ensemble_f1 0.89726
wandb:           train/avg_f1 0.88488
wandb:      train/ensemble_f1 0.88488
wandb:         train/mil_loss 0.2055
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run zesty-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xo4eqz5z
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061302-xo4eqz5z/logs
wandb: ERROR Run xo4eqz5z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: p3220v7j with config:
wandb: 	actor_learning_rate: 1.805799040518776e-06
wandb: 	attention_dropout_p: 0.24329596990504387
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 53
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2541320444467001
wandb: 	temperature: 9.560884516916303
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061354-p3220v7j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p3220v7j
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 37-54, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▄█
wandb: best/eval_avg_mil_loss ▃▂▁▃█▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄▄█
wandb:            eval/avg_f1 ▄▅▄▅▁▄▅▃▁▄▂▆▄▄▅▁▄▃▃▄▂▃▆▃▁▆▂█▅▃▃▇▃▅▅▅▃▄▃▅
wandb:      eval/avg_mil_loss ▃▂▃▁▂▄▃▂▂▂▂▅█▁▁▂▃▅▆▃▃▅▅▅▆▂▁▁▇▁▃▃▄▅▃▄▄▂▅▅
wandb:       eval/ensemble_f1 ▅▅▅▆▅▄▆▃▃▄▃▆▄▄▅▃▃▃▄▄▃▆▃▂▃█▅▄▃▄▄▅▅▆▅▁▅▃▄▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▄▅▅▃▅▄▅▄▅▆▅▁▆▄▂▄▄▃▄▅▅▆▅▁▄▆▂▁▄█▅▄▃▂▅▃▃▅
wandb:      train/ensemble_f1 ▄▃▄▅▃▃▅▄▅▃▅▆▅▆▄▂▄▄▃▆▅▅▅▄▇▄▆▂▁▄█▃▅▄▃▂▂▅▃▄
wandb:         train/mil_loss ▆▄▅▆▆▄▆▇▆▆▅▂▆▅▂▄▅▆▆▆▅▂█▆▃▂▅▂▂▆▁▄▃▆▇▂▄▃▃▁
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▂▂█▅▅▅▅▇▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▇▅▅▅▅▅▅▂▅█▅▅▅▇▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93395
wandb: best/eval_avg_mil_loss 0.2338
wandb:  best/eval_ensemble_f1 0.93395
wandb:            eval/avg_f1 0.87871
wandb:      eval/avg_mil_loss 0.36854
wandb:       eval/ensemble_f1 0.87871
wandb:            test/avg_f1 0.92428
wandb:      test/avg_mil_loss 0.23182
wandb:       test/ensemble_f1 0.92428
wandb:           train/avg_f1 0.89581
wandb:      train/ensemble_f1 0.89581
wandb:         train/mil_loss 0.19303
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run honest-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p3220v7j
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061354-p3220v7j/logs
wandb: Agent Starting Run: sp2l1ka6 with config:
wandb: 	actor_learning_rate: 1.0154531705018548e-05
wandb: 	attention_dropout_p: 0.3499210349961917
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 63
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0028666418258028736
wandb: 	temperature: 9.053604802699144
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061446-sp2l1ka6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sp2l1ka6
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▆█
wandb: best/eval_avg_mil_loss ▁█▃▂▂
wandb:  best/eval_ensemble_f1 ▁▁▅▆█
wandb:            eval/avg_f1 ▄▅▃▄▇▃▅▆▆▇▄▆▆▄▃▄▆▄▅▅▃▃█▅▄▅▆▆▆█▆▇▆█▅▅▄▅▁▅
wandb:      eval/avg_mil_loss ▂▃▃▂▅▃▃▃▂▂▁▃▄▂▃▃▄▃▃▂▃▂▂▃▄▃▃▂▃▂▄▁▅▄▂█▂▂▂▂
wandb:       eval/ensemble_f1 ▅▄▅▅▇▄▃▃▅▆▇▆▄▆▄▃▄▄▃▅▅▃▃█▂▄▃▅▆█▇▆█▄▅▄▅▁▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▄▄█▅▅▆▅▅▃▄▄▄▃█▅▁█▆▅▃▂▂▃▄▅▄▂▂▆▂▃▄▄▆▅▂▃▂
wandb:      train/ensemble_f1 ▃▅▇▇▄▅▅▄▆▅▃▄▄▄▃▃▅▂█▆▅▃▂▁▃▅▅▄▂▂▆▃▂▃▃▁▅▃▃▂
wandb:         train/mil_loss ▄▆▆▅█▂▄▃▁▄█▄▃▄▄▆▄▅▅▂▂▃▃▂▄▂▃▃▅▂▅▄▄▁▅▅▁▁▂▄
wandb:      train/policy_loss ▃█▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▇▃▃▃▃▁▃▃▃█▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▁▄▄▂▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▂▄▄▄▄▄█▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.27008
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.90822
wandb:      eval/avg_mil_loss 0.24322
wandb:       eval/ensemble_f1 0.90822
wandb:            test/avg_f1 0.92061
wandb:      test/avg_mil_loss 0.17584
wandb:       test/ensemble_f1 0.92061
wandb:           train/avg_f1 0.8863
wandb:      train/ensemble_f1 0.8863
wandb:         train/mil_loss 0.2043
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smart-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sp2l1ka6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061446-sp2l1ka6/logs
wandb: Agent Starting Run: 1r7as9oj with config:
wandb: 	actor_learning_rate: 1.1519261567354776e-05
wandb: 	attention_dropout_p: 0.2850953035243475
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 50
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17502422730078304
wandb: 	temperature: 9.088902891215564
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061543-1r7as9oj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1r7as9oj
wandb: uploading wandb-summary.json
wandb: uploading history steps 38-51, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▇█
wandb: best/eval_avg_mil_loss ▄▇▄▁▁█
wandb:  best/eval_ensemble_f1 ▁▂▃▄▇█
wandb:            eval/avg_f1 ▅▄▆▃▇▆▇▂▆▆▇▆▄▃▆▃▄▃▅▆▆▆▄▁▇█▇▅▄▃▄▆▆▆▅▆▃▆▆▇
wandb:      eval/avg_mil_loss ▃▂▆▄▄▂▁▄▂▅▁█▂▁▂▆▅▄▆▃▄▂▃▄▆▅▁▃▁▅▃▇▇▄▃▂▅▄▃▃
wandb:       eval/ensemble_f1 ▅▄▅▃▆▆▆▂▅▁▅▃▃▅█▄▃▅▅▅▅▃▁▇▅▆▅▃▆▃▄▅▅▆▅▅▂▅▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▃▇▆▃█▅▇▆▂▆▇▂▄▆▇█▅█▆▇▇▅▃▆▅▁▄▄▄▇▄▅▆▃▆▆▄▆
wandb:      train/ensemble_f1 ▄▅▃▇▆▃█▅▇▆▆▇▂▄▂▆▇█▅█▆▇▇▅▃▆▅▁▄▄▄▄▅▆█▃▆▆▄▆
wandb:         train/mil_loss █▅▁▅▄▁▄▂▆▅▄▆▅▄▇▄▅▄▄▆▇▃▄▄▇▅▂▃▃▄▂▃▅▃▆▆▄▂▄▃
wandb:      train/policy_loss ▂▂▂█▂▂▂▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂█▂▂▂▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.38034
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.90478
wandb:      eval/avg_mil_loss 0.29232
wandb:       eval/ensemble_f1 0.90478
wandb:            test/avg_f1 0.89732
wandb:      test/avg_mil_loss 0.2173
wandb:       test/ensemble_f1 0.89732
wandb:           train/avg_f1 0.89635
wandb:      train/ensemble_f1 0.89635
wandb:         train/mil_loss 0.21273
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run visionary-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1r7as9oj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061543-1r7as9oj/logs
wandb: Agent Starting Run: aw2o6wyf with config:
wandb: 	actor_learning_rate: 2.0159503585359495e-06
wandb: 	attention_dropout_p: 0.2700853310851547
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.12315227998136412
wandb: 	temperature: 5.957903042044581
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061630-aw2o6wyf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aw2o6wyf
wandb: uploading wandb-summary.json
wandb: uploading history steps 73-88, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▆█
wandb: best/eval_avg_mil_loss ▄▁▇█
wandb:  best/eval_ensemble_f1 ▁▁▆█
wandb:            eval/avg_f1 ▄▄▃▆▆▄▄▇▃▆▇▇▆▆▄▃▅█▆▅▂▇▄▇▄▅▃▅▅▄▄▄▁▆▂▆▃▂▃▄
wandb:      eval/avg_mil_loss ▂▄▃▃▁▃▃▂▂▃▄▃▄▅▁▅▂▃▄▃▃▅▃▂▃▅▃▃█▄▃▂▄▂▂▂▇▁▃▁
wandb:       eval/ensemble_f1 ▇▅▆▃▂▅▄▄▇▂█▃▃▆▄▄▆▅▅▂▇▇▃▁▆▃▃▄▃▅▃▅▅▃▅▂▃▆▁▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄█▆▆▃▂█▄▅▃▅▃▅▃▅▁▅▄▃▆▅▅▃▂▃▆▄▇▇▂▆▂▃▃▃▆▁▆▃
wandb:      train/ensemble_f1 ▄▆▅▆▅▅▆▇▄▆▅▄▂▅▄▄▃█▂▄▅▅▄▃▅▄▄▅▂▅▂▄▁▃▄▅▁▁▆▃
wandb:         train/mil_loss █▅▅▇▇▅▆▆▆▇▆▅▆▇▆▅▄▇▆▄▄█▂▅█▄▃▁▅▅▃▄▄▅▄▃▂▃▂▁
wandb:      train/policy_loss ▅▅▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▅▄▄▄▄▄▄▅▄▄▆▄▄▄▄▄▄▄▂▄█▄▄▁▄▄▄▄▄▆▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92669
wandb: best/eval_avg_mil_loss 0.26389
wandb:  best/eval_ensemble_f1 0.92669
wandb:            eval/avg_f1 0.90098
wandb:      eval/avg_mil_loss 0.21614
wandb:       eval/ensemble_f1 0.90098
wandb:            test/avg_f1 0.90623
wandb:      test/avg_mil_loss 0.20256
wandb:       test/ensemble_f1 0.90623
wandb:           train/avg_f1 0.88402
wandb:      train/ensemble_f1 0.88402
wandb:         train/mil_loss 0.18463
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run desert-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aw2o6wyf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061630-aw2o6wyf/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: tpmh8wyj with config:
wandb: 	actor_learning_rate: 2.6449851989575936e-06
wandb: 	attention_dropout_p: 0.34523222295663786
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 55
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.082574979520052
wandb: 	temperature: 9.762945871317047
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061757-tpmh8wyj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tpmh8wyj
wandb: uploading history steps 38-55, summary; uploading wandb-summary.json
wandb: uploading history steps 38-55, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▇█
wandb: best/eval_avg_mil_loss ▅▃█▁▂
wandb:  best/eval_ensemble_f1 ▁▂▄▇█
wandb:            eval/avg_f1 ▅▅▃▆▂▅▄▆▆▃▅█▅▃█▄▃▅▃▅▂▆▂▃▆▃▄▂▂▂▄▂▅▄▃▂▃▁▂▅
wandb:      eval/avg_mil_loss ▃▃▂▂▅▂▃▃▃▂▃▄▃▁▃▂█▂▂▃▂▂▃▇▂▂▂▃▅▄▄▃▃▃▆▂▃▅▃▁
wandb:       eval/ensemble_f1 ▅▃▆▂▅▄▆▅▅▅▅█▅▃█▃▅▃▅▆▂▆▂▆▃▄▂▂▁▂▄▂▅▄▃▂▃▁▂▅
wandb:           train/avg_f1 ▄▆▃▅▁▆▅▄▄▄▆▂▅█▄▃▄▄▂▅▅▄█▄▂▆▆▂▆▄▃▆▃▂▃▅▅▅▅▂
wandb:      train/ensemble_f1 ▆▄▅▅▂▅▅▅▄▄▇▂▁▄█▇▂▃▃▂▅▃█▅▂▅▄▁▆▃▂▆▃▂▂▇▅▅▄▂
wandb:         train/mil_loss ▂▅▃▅▄▄▁▃▄▃▂▃▄▄▄▃▃█▃▅▃▆▇▄▁▅▃▄▂▂▄▄▂▃▅▆▄▄▃▃
wandb:      train/policy_loss ▅▅▅▂▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▆▅▅▅▇▅▅▅▅▅▇▅▅▅▅▅▅█▅▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▂▅▅▅▅▅▅▁▅▅▅▆▅▅▅▅▄▆▅▆▅▅▄▅▅▅▅▅▅▅▅▅▅▅█▅▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.25462
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.90034
wandb:      eval/avg_mil_loss 0.20521
wandb:       eval/ensemble_f1 0.90034
wandb:           train/avg_f1 0.88125
wandb:      train/ensemble_f1 0.88125
wandb:         train/mil_loss 3.92125
wandb:      train/policy_loss -0.1003
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.1003
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run eternal-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tpmh8wyj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061757-tpmh8wyj/logs
wandb: ERROR Run tpmh8wyj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: h4y64r2h with config:
wandb: 	actor_learning_rate: 1.402868033822093e-06
wandb: 	attention_dropout_p: 0.11829577224427644
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 75
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5775944145838223
wandb: 	temperature: 2.9432986469056397
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061849-h4y64r2h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/h4y64r2h
wandb: uploading history steps 71-75, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▃▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▅█▇▇▆▇▇▇▇▆▆▆▇▆▆▆▆▅▅▅▄▄▆▃▅▄▅▅▄▃▄▃▆▃▃▃▁▄▂▄
wandb:      eval/avg_mil_loss ▂▃▂▃▁▃▃▃▃▃▄▅▃▃▂▂▄▄▅▆▄▆▃▅▆▄▄▆▆▆▅▅▇▅▆▄▇▆█▄
wandb:       eval/ensemble_f1 ▅█▇▆█▆▆▇▇▆▅▇▇▇▆▇▄▅▆▅▅▆▄▃▅▄▃▅▅▅▅▃▃▄▄▁▄▄▂▄
wandb:           train/avg_f1 ▇█▇▇▅█▇▇▇▆▆▇▇▆▅▇▅▆▅▅▆▅▅▅▅▄▅▅▅▃▃▄▄▄▃▂▂▃▂▁
wandb:      train/ensemble_f1 ▇█▇▇▇▇█▇▇▆▇▆▆▇▇▅▆▅▅▆▅▅▅▆▅▄▅▅▃▃▃▄▄▃▃▂▂▂▃▁
wandb:         train/mil_loss █▇▆▅▆▆▅▇▅▅▅▇▃▆▃▄▅▅▃▄▅▄▅▆▄▅▃▅▄▄▄▄▃▃▄▄▁▃▅▃
wandb:      train/policy_loss ▄▆▅▆▄▆▇▂▄█▅▄▃▆▆▄▄▄▁▄▄▄▂▄▄▅▆▆▇▆▆▄▄▆▄▄▄▄▂▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▄▄▃▃▃▃▅▃▃▄▃▃▃▃▄▅▃▃▁▄▃▆▃▃▃▃▃▄▅▃▄▄▃▄▄█▃▃▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91898
wandb: best/eval_avg_mil_loss 0.23577
wandb:  best/eval_ensemble_f1 0.91898
wandb:            eval/avg_f1 0.86492
wandb:      eval/avg_mil_loss 0.32829
wandb:       eval/ensemble_f1 0.86492
wandb:           train/avg_f1 0.82045
wandb:      train/ensemble_f1 0.82045
wandb:         train/mil_loss 0.19562
wandb:      train/policy_loss 0.0805
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0805
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run frosty-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/h4y64r2h
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061849-h4y64r2h/logs
wandb: ERROR Run h4y64r2h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ypo8ue52 with config:
wandb: 	actor_learning_rate: 2.0879682607719107e-06
wandb: 	attention_dropout_p: 0.15488476303322968
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 75
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8786041470517116
wandb: 	temperature: 9.70239165321868
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062012-ypo8ue52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ypo8ue52
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▆▇▇█
wandb: best/eval_avg_mil_loss ▄▃▁▂▂█
wandb:  best/eval_ensemble_f1 ▁▂▆▇▇█
wandb:            eval/avg_f1 ▄▅▂▃▇▇█▄▂▄▃▃▄▄▆▃▄▆▄▇▆▃▄▃▃▃▃▃▃▅▅▆▃▃▃█▃▁▅▃
wandb:      eval/avg_mil_loss ▃▂▃▂▄▁▅▂▂▄▅▇▂▆▆▆▃▄▄▄▃▄▃▃▄▇▅█▅▄▄▅▅▆▅▇█▆▃▄
wandb:       eval/ensemble_f1 ▄▇▅▂▂▇▃█▄▂▃▄▅▄▃▆▃▂▆▃▃▃▂▄▃▃▂▅▃▅▃▃█▃▁▃▃▆▃▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▄▅█▇▅▇▆▆▅▇▆▅▆▄▇▇▅▄▃▃▄▃▃▄▆▃▅▁▆▄▂▂▃▄▄▁▄▂▄
wandb:      train/ensemble_f1 █▅▅▇▇▇▅▆▅▇▅▅▆▅▆▅▆▇▆█▅▄▃▃▄▄▅▄▂▅▇▂▄▃▂▃▄▅▁▂
wandb:         train/mil_loss █▆▇▅▇▆▇▅▄▃▄▆▆▄▃▅▆▄█▆▃▂▅▁▄▂▃▇▅▇▄▅▄▄▄▆▄▆▆▆
wandb:      train/policy_loss ▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄█▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92323
wandb: best/eval_avg_mil_loss 0.35125
wandb:  best/eval_ensemble_f1 0.92323
wandb:            eval/avg_f1 0.88316
wandb:      eval/avg_mil_loss 0.27271
wandb:       eval/ensemble_f1 0.88316
wandb:            test/avg_f1 0.85084
wandb:      test/avg_mil_loss 0.33
wandb:       test/ensemble_f1 0.85084
wandb:           train/avg_f1 0.88432
wandb:      train/ensemble_f1 0.88432
wandb:         train/mil_loss 0.18097
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ypo8ue52
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062012-ypo8ue52/logs
wandb: Agent Starting Run: 3a4wigp1 with config:
wandb: 	actor_learning_rate: 1.0297001819174946e-05
wandb: 	attention_dropout_p: 0.391762503467407
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 168
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.31888789177917176
wandb: 	temperature: 2.2590410174163567
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062140-3a4wigp1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3a4wigp1
wandb: uploading history steps 128-139, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▅█
wandb: best/eval_avg_mil_loss ▇█▆▁▆
wandb:  best/eval_ensemble_f1 ▁▃▃▅█
wandb:            eval/avg_f1 ▇█▄█▇▆█▅▅▆▆▆▆▇▅▇▆▅█▅▄▇▆▇▄▅▄▅▆▃▄▂▄▄▃▂▂▁▁▂
wandb:      eval/avg_mil_loss ▂▂▃▂▃▂▂▂▁▁▂▄▂▃▂▁▅▄▄▃▃▄▅▂▃▅▃▄▆▅▅▇▆▇▇▆█▆▇█
wandb:       eval/ensemble_f1 ▇▆▇▇▇▇▇▆▇▆▆▆▅█▆▅▇▆▆▅▅▅▇▄▆▄▅▄▄▄▄▄▃▃▃▃▃▃▁▂
wandb:           train/avg_f1 ▇▆▇█▆█▇▇▆▇▆▆▆▅▅▇▄▅▅▅▄▅▅▃▅▃▄▄▃▃▃▃▃▂▂▄▁▁▃▂
wandb:      train/ensemble_f1 ▇▇█████▇▇▇█▆▇▆▆▆▆▆▆▅▅▅▆▆▃▅▄▃▅▄▄▃▄▃▃▃▂▁▂▂
wandb:         train/mil_loss ██▇▇▆▆▇▇▆▆▅▆▄▅▄▄▄▄▄▄▃▃▄▃▃▄▃▃▃▂▂▂▂▂▂▂▂▁▁▂
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃█▃▃▃▃▃▃▆▃▃▆▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▇▅▅▅▅▄▅▇▅▅▅█▅▅▅▅▇▅▅▅▅▅▅▅▅▅▅▆▅█▆▅▁▅▅▅▅▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9186
wandb: best/eval_avg_mil_loss 0.27224
wandb:  best/eval_ensemble_f1 0.9186
wandb:            eval/avg_f1 0.83952
wandb:      eval/avg_mil_loss 0.45458
wandb:       eval/ensemble_f1 0.83952
wandb:           train/avg_f1 0.82869
wandb:      train/ensemble_f1 0.82869
wandb:         train/mil_loss 1.60323
wandb:      train/policy_loss 0.1365
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.1365
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run clear-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3a4wigp1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062140-3a4wigp1/logs
wandb: ERROR Run 3a4wigp1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 68p233j1 with config:
wandb: 	actor_learning_rate: 0.0003689158634652431
wandb: 	attention_dropout_p: 0.4876330717928533
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 53
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05214692715107838
wandb: 	temperature: 3.386479136088658
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062450-68p233j1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/68p233j1
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆█
wandb: best/eval_avg_mil_loss ▇▂█▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▆█
wandb:            eval/avg_f1 ▃▄▄▅▅▆▄▃▅▅▄▁▅▄▄▂▃▃█▅▅▃▂▄▅▅▇▅▂▄▆▄▂▅▄▄▃▂▃▅
wandb:      eval/avg_mil_loss ▄▂▅▄█▂▂▄▂▂▄▃▃▂▃▂▃▂▃▆▃▇▃▂▃▂▂▃▃▂▆▄▁▄▅▃▄▄▅▅
wandb:       eval/ensemble_f1 ▃▄▅▂▄▄▃▅▆▄▁▅▄▄▅▃█▅▂▅▂▄▅▆▅▅▂▅▆▄▅▄▇▄▅▃▂▃▄▅
wandb:           train/avg_f1 ▄▄█▆▅▆▇█▃▆▅▇▄▆▅▇▆▆▃▄▇▅▄▄▄▆▆█▅▆▃▃▄▃▇▃▄▃▄▁
wandb:      train/ensemble_f1 ▄▄█▆▅▆█▄▆▇▇▅▆▆▇▆▄█▄▄▅▄▄▅▄▆▆█▃▅▇▁▃▄▅▂▄▄▄▂
wandb:         train/mil_loss █▄▅▆▅▄▃▆▄▅▆▄▅▇▅▃▃▄▅▆▅▃▄▄▅▄▂▂▄▂▃▂▂▂▃▂▁▄▅▇
wandb:      train/policy_loss ▅▅▆▅▅▅▅▅▅▄▅▅▅▅▁▃▅▅▅▃▅▆▅▅▅▅▅▅▅█▅▅▅▅▅▄▅▅▆█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▆▅▅▅▅▅▅▄▅▅▅▆▅▁▆▃▅▃▅▅▆▅▅▅▅▅█▅▅▅▅▅▄▅▅▆█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93402
wandb: best/eval_avg_mil_loss 0.24659
wandb:  best/eval_ensemble_f1 0.93402
wandb:            eval/avg_f1 0.90498
wandb:      eval/avg_mil_loss 0.29008
wandb:       eval/ensemble_f1 0.90498
wandb:           train/avg_f1 0.876
wandb:      train/ensemble_f1 0.876
wandb:         train/mil_loss 0.23077
wandb:      train/policy_loss 0.77152
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.77152
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dashing-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/68p233j1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062450-68p233j1/logs
wandb: ERROR Run 68p233j1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 0yl04xv8 with config:
wandb: 	actor_learning_rate: 3.705344422532654e-05
wandb: 	attention_dropout_p: 0.04011388817524275
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.20357229942733104
wandb: 	temperature: 7.172225215947245
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062542-0yl04xv8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0yl04xv8
wandb: uploading history steps 101-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 █▆▆▆▇▆▆▆▇▅▄▇▆▇▅▅▆▃▅▄▆▃▅▃▆▆▅▃▅▇▃▃▄▁▅▃▃▅▅▄
wandb:      eval/avg_mil_loss ▃▅▄▃▃▁▆▃▂▃▄▃▆▄▃▄█▄▄▅▄▃▄▄▂▃▄▄▃▅▇▅▃▅▃▅█▅▃▆
wandb:       eval/ensemble_f1 ▄▆█▆▆█▆▇▇▇▆▆▇▆▃█▇▅▅▆▁▆▅▄▆▃▅▅▇▇▃▃▆▅▄▄▂▅▄▂
wandb:           train/avg_f1 █▆█▇▇▆▇▆▅▇█▆▅▆█▆▇▆▃▇▆▇▇▄▆▆▇▄▅▅▅▄▅▅▄▂▄▆▄▁
wandb:      train/ensemble_f1 ▇▆█▆▆▇▅▇▆▄▆▄▅▅▅▅▄▅▅▆▆▄▆▃▃▄▄▄▄▄▄▄▃▂▂▁▂▂▄▃
wandb:         train/mil_loss ▃██▃▆▆▂▄▄▃▇▅▄▄▃▆▄▄▃▃▂▅▄▂▂▂▂▂▄▃▂▂▂▂▂▂▁▁▂▁
wandb:      train/policy_loss ▄█▄▄▄▄▂█▄▅▄▃▄▄▄▄▄▄▄▄▄▁▄▆▄▄▄▄▇▄▆▄▆▄▄▄▄▄▄▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄█▄▄▄▄▄▄▂▄▄▅▄▄▄▁▄▄▄▄▄▄▄▄▅▄▄▄▄▆▄▄▄▄▄▆▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.25719
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.85765
wandb:      eval/avg_mil_loss 0.31736
wandb:       eval/ensemble_f1 0.85765
wandb:           train/avg_f1 0.8603
wandb:      train/ensemble_f1 0.8603
wandb:         train/mil_loss 0.21501
wandb:      train/policy_loss -0.11703
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.11703
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rare-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0yl04xv8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062542-0yl04xv8/logs
wandb: ERROR Run 0yl04xv8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: w8r1k87f with config:
wandb: 	actor_learning_rate: 1.389017796574796e-06
wandb: 	attention_dropout_p: 0.2656203457080973
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.799406975804411
wandb: 	temperature: 9.689374787847475
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062720-w8r1k87f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/w8r1k87f
wandb: uploading wandb-summary.json
wandb: uploading history steps 84-94, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▅▇▇█
wandb: best/eval_avg_mil_loss █▁█▇▅▁▆
wandb:  best/eval_ensemble_f1 ▁▁▄▅▇▇█
wandb:            eval/avg_f1 ▆▆▅▇▆▇▆▅▅▆▃▆▆▄▄▇▄█▄▄▇█▅▅▅▅▅▄▁▅▂▄▄▅▁▅▅▅▂▂
wandb:      eval/avg_mil_loss ▄▂▆▄▁▃▃▁▄▃▆▂▃▅▅▄▃▆▆▁▅▃▁▅▆█▇▆▄▆▃▄▄▃▅▁▄▅▅▇
wandb:       eval/ensemble_f1 ▇▆█▆█▅▆▇▇▅█▆▅▄▄▃▃▆▄▄▆▅▄▄▂▅▄▄█▄▄▂▃▁▆▆▇▆▅▂
wandb:           train/avg_f1 ▄▆▆▅▇▅▄▅▅▅█▄▃▅▂▃▃▄▄▄▂▂▄▄▃▄▃▃▄▂▁▃▂▃▃▁▁▂▁▂
wandb:      train/ensemble_f1 ▄▆▅▆▇▇▆▄▅▅▅▄▆▅▆▅█▅▂▅▃▅▃▄▁▄▄▁▃▄▃▂▄▁▃▁▃▁▁▂
wandb:         train/mil_loss ▅▄▆▆▇▅▃▅█▆▃▆▄▃▅▃▄▂▁▅▆▂▃▅▇▃▅▃▃▄▃▆▄▂▂▅▅▁▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄█▄▄▄▄▄█▄▄▄▄▄▄▄▄▆▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91943
wandb: best/eval_avg_mil_loss 0.27593
wandb:  best/eval_ensemble_f1 0.91943
wandb:            eval/avg_f1 0.84668
wandb:      eval/avg_mil_loss 0.44682
wandb:       eval/ensemble_f1 0.84668
wandb:           train/avg_f1 0.86685
wandb:      train/ensemble_f1 0.86685
wandb:         train/mil_loss 0.24124
wandb:      train/policy_loss 0.77073
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.77073
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run robust-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/w8r1k87f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062720-w8r1k87f/logs
wandb: ERROR Run w8r1k87f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6pflodzi with config:
wandb: 	actor_learning_rate: 3.223058242438322e-05
wandb: 	attention_dropout_p: 0.4039506484151494
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 58
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.38631857919048695
wandb: 	temperature: 0.8751047632704234
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062918-6pflodzi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6pflodzi
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▂▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▄▂▆▅▆▇▆▅▆▃▄▆▆▆▁▇▆▅▃▅▄▆▇▆▇▅▅▄▆▆█▃▄▅▅▇▅▅▄▄
wandb:      eval/avg_mil_loss ▅▇▆▂▅▆▃▇▃▄▃▄▄▃▄▇▅▆▅▃▄▆▅▆▂▅▄▁▃█▂▆▇▆▃▄▄▂▄▆
wandb:       eval/ensemble_f1 ▄▂█▇▇▆▅▇▄▆▇▇▁▅█▆▆▃▅▇▇▇▇█▇▅▇▆▄▅▃▅▅▅▅▇▅▅▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▇▅▇▅█▇▇▅▄▅▅▅▇▄▄▃▆▆▃▄▄▂▃▃▂▄▅▄▃▂▄▂▄▁▂▁▄▂
wandb:      train/ensemble_f1 ▄▇▅▇▆█▇▇▆▅▃▄▄▇▆▄▄▆▃▆▂▄▃▂▂▄▂▁▃▄▂▃▂▄▃▃▃▁▃▁
wandb:         train/mil_loss █▃▅▂▆▆▄▃▄▂▃▄▅▁▅▆▃▅▃▃▆▃▅▄▄▄▆▄▂▄▂▁▁▆▆▂▅▃▃▂
wandb:      train/policy_loss ▂▂▃▂▂▄▆▃▂▄▆▁▂▂▂▂▂▂▂▂▂▂▂▄▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▃▂▂▄▆▂▃▂▄▂▆▁▂▂▂▂▂▂▄▂▂▄█▂▂▂▂▂▂▂▂▂▂▂▂▂▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91573
wandb: best/eval_avg_mil_loss 0.25875
wandb:  best/eval_ensemble_f1 0.91573
wandb:            eval/avg_f1 0.87206
wandb:      eval/avg_mil_loss 0.33053
wandb:       eval/ensemble_f1 0.87206
wandb:            test/avg_f1 0.88012
wandb:      test/avg_mil_loss 0.26131
wandb:       test/ensemble_f1 0.88012
wandb:           train/avg_f1 0.87254
wandb:      train/ensemble_f1 0.87254
wandb:         train/mil_loss 0.19484
wandb:      train/policy_loss 0.14099
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.14099
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run legendary-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6pflodzi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062918-6pflodzi/logs
wandb: Agent Starting Run: uub846qa with config:
wandb: 	actor_learning_rate: 0.00016033649991883963
wandb: 	attention_dropout_p: 0.39836079674220287
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 155
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.28449930297579085
wandb: 	temperature: 2.240531648500763
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063028-uub846qa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uub846qa
wandb: uploading history steps 112-118, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▁▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▄▄▄▆▅▆▄█▆▆▄▄▆▅▄▆▄▅▃▆▄▆▆▅▄▅▅▄▂▄▁▅▂▁▂▃▂▁▁▂
wandb:      eval/avg_mil_loss ▅▃▁▁▁▂▃▃▃▂▃▅▃▂▅▂▄▆▅▃▂▄▄▂▃▅▅▄▆▇▆▅▅▅▄█▆▅▇▇
wandb:       eval/ensemble_f1 █▆▄▅▇▆▄▇▆▄▄▆▆▇▃▆▅▃▄▆▆▆▆▆▃▂▄▂▆▂▄▁▂▂▃▂▂▁▁▁
wandb:           train/avg_f1 ▅▆█▇▇▇▇▆▇▆▆▄▆▇█▇█▆▆▅▆▇▆▅▅▄▅▆▆▆▄▃▂▃▃▄▂▃▂▁
wandb:      train/ensemble_f1 ▅▆▇▆▇▇▆█▇▇▆█▆▇▄█▇▇▇█▅▅▆▅▅▄▄▅▆▆▆▃▅▃▅▂▄▂▃▁
wandb:         train/mil_loss █▇▇█▆▆█▆▇▇█▄▆▇▄▇▃▄▆▄▃▁▅▄▃▄▄▃▆▄▂▂▃▄▃▁▁▂▁▁
wandb:      train/policy_loss ▄▄█▄▃▄▄▄▄▄▄▄▅▄▄▄▄▁▄▃▄▄▄▄▄▄▄▅▂▄▄▄▄▄▄▄▄▄▆▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▄▄▃▃▃▅▃▃▃▃▃▃▅▁▃▃▃▄▃▃▃▃▃█▃▃▃▃▂▅▃▃▃▃▅▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92236
wandb: best/eval_avg_mil_loss 0.22773
wandb:  best/eval_ensemble_f1 0.92236
wandb:            eval/avg_f1 0.85296
wandb:      eval/avg_mil_loss 0.37278
wandb:       eval/ensemble_f1 0.85296
wandb:           train/avg_f1 0.85516
wandb:      train/ensemble_f1 0.85516
wandb:         train/mil_loss 2.87711
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run ruby-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uub846qa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063028-uub846qa/logs
wandb: ERROR Run uub846qa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: v4rfvm8q with config:
wandb: 	actor_learning_rate: 0.0001482107590594552
wandb: 	attention_dropout_p: 0.0735707576507032
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 188
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6771944340111713
wandb: 	temperature: 4.4469625409765134
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063243-v4rfvm8q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v4rfvm8q
wandb: uploading history steps 186-188, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▅▇██
wandb: best/eval_avg_mil_loss ▅▇█▇▄▁▄
wandb:  best/eval_ensemble_f1 ▁▃▅▅▇██
wandb:            eval/avg_f1 ▄▆▅▆█▂▅▂▅▆▅▅▅▄▆▆▄▆▇▅▄▃▃▅▆▄▆▅▁▆▆▇▄▆▃▆▆▅▄▄
wandb:      eval/avg_mil_loss █▄▆▃▄▃▄▂▁▃▂▆▆▅▅▅▆▂▄▄▄▄▄▂▅▃▄▄▅▅▂▄▅▅▄▅▄▅▄▄
wandb:       eval/ensemble_f1 ▂▄▅▇▇▇▂▄▅▄▄▅▄▆▄▅▃▄▅▄▃▄▁▃▆▆▃█▅▆▄▅▂▂▅▄▄▅▅▅
wandb:           train/avg_f1 ▆▇▆▂█▆▅▅▇▄▃▆▄▄▆▅▆▇▃▆▅▃▅▁▂▂▂▇▃▄▅▇▁▅▄▆█▆▅▅
wandb:      train/ensemble_f1 ▅▂█▃▄▄▃▄▃▃▅▅▅▅▂▇▃▃▅▅▅▂▆▃▂▂▅▆▅▄▂▄▆▆▁▃▄▅▄▅
wandb:         train/mil_loss █▅▄▇▄▅▅▇▅▅▃▅▃▂▄▃▃▄▆▄▄▄▃▅▃▅▃▄▄▂▅▅▁▆▄▆▄▃▅▂
wandb:      train/policy_loss ▁██▄████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.24138
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.87373
wandb:      eval/avg_mil_loss 0.29551
wandb:       eval/ensemble_f1 0.87373
wandb:           train/avg_f1 0.89405
wandb:      train/ensemble_f1 0.89405
wandb:         train/mil_loss 0.2407
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vague-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v4rfvm8q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063243-v4rfvm8q/logs
wandb: ERROR Run v4rfvm8q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kblduoza with config:
wandb: 	actor_learning_rate: 1.8336827046139213e-06
wandb: 	attention_dropout_p: 0.33144553973648905
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.11464995548577615
wandb: 	temperature: 6.203726555885444
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063614-kblduoza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kblduoza
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 54-57, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▆▇█
wandb: best/eval_avg_mil_loss ▅▆▆█▆▁
wandb:  best/eval_ensemble_f1 ▁▂▅▆▇█
wandb:            eval/avg_f1 ▄▄▆▃▅▇▄▃▆▇▄▅▇▅▄▃▆▅▅▆▂█▃▃▂▇▁▇▁▅▆▄█▅▇▅▂▆▁▄
wandb:      eval/avg_mil_loss ▃▃▃▄▄▃▄▄▄▃▅▄▃▄▄▃▃▂▃▂▃▃▄▄▃▅█▄▄▃▂▄▁▆▃▂▅▂▃▄
wandb:       eval/ensemble_f1 ▄▆▅▂▅▇▄▃▆▂▄▅▇▅▄▃▆▅▅▆▂█▃▃▆▁▇▁█▆█▅▇▅▆▂▇▆▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▅▂▆▄▂▃▆▄▂▄▆▄▄▂▁▅▃▂▄▄▂▄▄▃▃▅▆▅▆▄▄▄▃▃▄█▅▃
wandb:      train/ensemble_f1 ▇▄▆▃▇▆▄█▂▄▅▅▄▃▅▅▃▁▂▄▆▆▅▆▃▅▆█▆▆▇▅▅▆▆▄▇▅▇▄
wandb:         train/mil_loss ▇▅▇▄▄█▅▆▅▇▆▆▅▅▆▄▆▆▄▄▅▅▅▃▂▅▄▄▄▃▃▃▄▃▅▄▁▃▁▄
wandb:      train/policy_loss ▆▆▁▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▁▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91886
wandb: best/eval_avg_mil_loss 0.15201
wandb:  best/eval_ensemble_f1 0.91886
wandb:            eval/avg_f1 0.88606
wandb:      eval/avg_mil_loss 0.37743
wandb:       eval/ensemble_f1 0.88606
wandb:            test/avg_f1 0.90887
wandb:      test/avg_mil_loss 0.18743
wandb:       test/ensemble_f1 0.90887
wandb:           train/avg_f1 0.88834
wandb:      train/ensemble_f1 0.88834
wandb:         train/mil_loss 0.93315
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run swift-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kblduoza
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063614-kblduoza/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 25q49u9p with config:
wandb: 	actor_learning_rate: 0.0005665959013635104
wandb: 	attention_dropout_p: 0.10895751177542212
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 176
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9824228429058788
wandb: 	temperature: 7.987302139037187
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063716-25q49u9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/25q49u9p
wandb: uploading history steps 167-176, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss █▂▃▁
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▄▆▂▅▄▃▅█▄▅█▄▃▄▆▅▃▄▅▅▃▅▃▄▂▆▂▂▃▂▃▄▂▃▄▃▅▃▃▁
wandb:      eval/avg_mil_loss ▂▃▅▄▄▃▃▃▅▆▃▇▅▆▃▃█▄▄▄▅▃▃▆▄▄▄▄▁▄▅▆▄▄▃▅▄▄▅▅
wandb:       eval/ensemble_f1 ▆▄▂▅▃█▅▄▃▇▆▄▆▄▄▇▆▅▄▅▄▃▆▄▅▅▇▄▄▅▂▇▂▅▄▁▃▄▅▁
wandb:           train/avg_f1 ▅▅▇▆▆▄▆▄▇▅▆▅▇▄▄▃▄▄▂▄▅▅█▄▄▆▅▄▆▄▆▅▃▄▂▅▂▁▂▂
wandb:      train/ensemble_f1 ▇▆▅▆█▇█▅▆▆▄▅▇▆▅▇▆▇▅▆▇▄▇▆▄▃▄▆▅▄▄▃▃▅▅▄▅▃▁▅
wandb:         train/mil_loss ███▇█▆▇▇▇▆▆▇▆▆▅▆▇▆▆▅▅▅▄▃▃▃▃▃▃▄▂▂▃▂▁▂▃▁▂▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁█▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.236
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.8648
wandb:      eval/avg_mil_loss 0.36222
wandb:       eval/ensemble_f1 0.8648
wandb:           train/avg_f1 0.87236
wandb:      train/ensemble_f1 0.87236
wandb:         train/mil_loss 1.01195
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run youthful-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/25q49u9p
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063716-25q49u9p/logs
wandb: ERROR Run 25q49u9p errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: lqft7gq8 with config:
wandb: 	actor_learning_rate: 5.405036647935141e-05
wandb: 	attention_dropout_p: 0.4386715922750977
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 74
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3037816290396088
wandb: 	temperature: 1.4224872674587652
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064002-lqft7gq8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lqft7gq8
wandb: uploading history steps 68-75, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▂▄▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▃█▅▆▅▅▄▆█▇▄▅▂▆█▅▅▁▄▁▅▆▄▅▃▆▆▅▃▅▄▃▅▅▄▃▁▄▂▆
wandb:      eval/avg_mil_loss ▃▂▂▄▆▃▁▃▂▄▃▁▃▂█▆▁▃▃▁▂█▄▇▅▂▅▃▄▅▃▅▅▄█▄▅▄▄▆
wandb:       eval/ensemble_f1 █▄▆▄▇▅▄▅▅▄▆▇▃▆▄▆█▇▆▁▃▁▄▅▆▄▅▃▆▃▃▅▄▃▃▃▂▁▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▇█▇█▆▆▅█▇▇█▇▇▆▆▆▅▆▆▅▆▄▅▃▅▅▅▆▃▂▁▁▅▂▁▂▄▂▃
wandb:      train/ensemble_f1 ▇▆▇▇▆▆█▅▆▄▇▆▆▅▅▅▅▅▅▄▆▅▃▃▄▄▄▂▄▄▄▄▅▄▄▄▁▁▁▂
wandb:         train/mil_loss ▇▄▃▅▅█▅▆▇▅▅▅▆▇▄▃▅▅▄▅▅▃▅▂▂▂▁▃▃▃▄▃▂▂▅▅▂▂▁▂
wandb:      train/policy_loss ▃▃▂▄▃▁▃▃▃▃▃▁▃▃█▃▃▃▄▃▄▃▃▃▃▃▃▃▃▄▃▃▃▄▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▂█▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▆▄▄▄▄▄▄▆▄▄█▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92669
wandb: best/eval_avg_mil_loss 0.22159
wandb:  best/eval_ensemble_f1 0.92669
wandb:            eval/avg_f1 0.90861
wandb:      eval/avg_mil_loss 0.32444
wandb:       eval/ensemble_f1 0.90861
wandb:            test/avg_f1 0.90964
wandb:      test/avg_mil_loss 0.22899
wandb:       test/ensemble_f1 0.90964
wandb:           train/avg_f1 0.88171
wandb:      train/ensemble_f1 0.88171
wandb:         train/mil_loss 0.20976
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run trim-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lqft7gq8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064002-lqft7gq8/logs
wandb: Agent Starting Run: xn8fk95k with config:
wandb: 	actor_learning_rate: 4.9817694310635225e-06
wandb: 	attention_dropout_p: 0.26796709246993006
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.29480004537870375
wandb: 	temperature: 6.077747928495553
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064114-xn8fk95k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xn8fk95k
wandb: uploading wandb-summary.json; uploading history steps 50-67, summary
wandb: uploading history steps 50-67, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss █▂▁
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 ▃██▄▄▇▄▅▃▃▆▆▅▆▆▆▁▆▆▆▃▇▆▆▆▆▅▇▇▅▅▆▄▄▆▇▆█▇▇
wandb:      eval/avg_mil_loss ▅▃▅▃▆▄▃▃▄▆▄▄▂▁▂▄▂▅▄▄▆▅█▄▃▃▃▃▃▃▆▃▁▇▅▄▂▄▃▄
wandb:       eval/ensemble_f1 ▄▃▇▄▅▃▅█▅▇▅▅▄▄▆▅▁▅▅▅▆▅▅▄▅▆▆▆▄▄▄▅▄▅▆▅▅▇▅▆
wandb:           train/avg_f1 ▅▄▁▃▂▃▅▃▄▃▅▆▅█▆▂▂▄▆▄▅█▅▅▄▂▁▄▅▃▅▅▂▆▃█▆█▆▅
wandb:      train/ensemble_f1 ▄▂▄▆▂▄▅▅▅▄▅▅▆▅█▃▆▅▁▆▆▆▅▃▆▄▆▃▅▆▄▇▇▅▅▄█▇█▆
wandb:         train/mil_loss ▇▃▅▆▆▅▄▄▄▃▅▄▅▃▄▄▁▄▄▄▃▄▃▅▃▂▃▄▆▄▅▃▃▄█▃▃▁▂▃
wandb:      train/policy_loss ██████████████████████▆███▁█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▁██████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94099
wandb: best/eval_avg_mil_loss 0.24717
wandb:  best/eval_ensemble_f1 0.94099
wandb:            eval/avg_f1 0.90758
wandb:      eval/avg_mil_loss 0.32984
wandb:       eval/ensemble_f1 0.90758
wandb:           train/avg_f1 0.89637
wandb:      train/ensemble_f1 0.89637
wandb:         train/mil_loss 0.32625
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xn8fk95k
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064114-xn8fk95k/logs
wandb: ERROR Run xn8fk95k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9mb067wy with config:
wandb: 	actor_learning_rate: 2.133198186858992e-05
wandb: 	attention_dropout_p: 0.3319575869697212
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 139
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.43106988272520175
wandb: 	temperature: 4.7328829330492015
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064221-9mb067wy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9mb067wy
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss █▁▄
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▃▂▅▅▇▁▆▅▄▄█▃▇▃▄▃▄▄▅▆▄▄▆▄▄▄▃▅▅▃▂▅▁▄▂▃▂▂▅▁
wandb:      eval/avg_mil_loss ▅▁▂▃▃▃█▇▂▄▁▄▃▅▂▂▃▃▅▄▆▁▄▂▁▄▄▄▃▅▅▅▃▁▆▄▅▃▄▅
wandb:       eval/ensemble_f1 ▇▄█▅▆▅▄▆▄▂▇▃▇▃▇█▄▂▅▃▁▅▄▄▃▃▄▅▃▆▄▅▁▃▂▂▄▃▅▂
wandb:           train/avg_f1 ▄▆▆▆█▅█▆▆▅▆▆▆█▆▆▅▆▅▆▄▄▅▄▃▅▄▅▄▆▄▇▂▃▅▃▃▁▄▃
wandb:      train/ensemble_f1 ▄▅▆▆▅▆▅▅█▆▆▆█▅▆▆▇▅▅▆▅▆▄▄▅▇▄▅▄▅▅▄▆▄▆▄▃▄▁▂
wandb:         train/mil_loss ▅▅▇█▄▅▅▅▅▄▆▅▆▇▃▅▅▅▅▁▃▃▇▅▆▃▃▆▄▂▆▂▅▄▅▁▅▃▁▅
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▃▄▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92249
wandb: best/eval_avg_mil_loss 0.28349
wandb:  best/eval_ensemble_f1 0.92249
wandb:            eval/avg_f1 0.87591
wandb:      eval/avg_mil_loss 0.38873
wandb:       eval/ensemble_f1 0.87591
wandb:           train/avg_f1 0.88184
wandb:      train/ensemble_f1 0.88184
wandb:         train/mil_loss 2.73971
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run decent-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9mb067wy
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064221-9mb067wy/logs
wandb: ERROR Run 9mb067wy errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4ryj6co7 with config:
wandb: 	actor_learning_rate: 0.00018874105033241852
wandb: 	attention_dropout_p: 0.002527855623222608
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 185
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4981255301454741
wandb: 	temperature: 3.700377669722511
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064410-4ryj6co7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4ryj6co7
wandb: uploading history steps 105-109, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆█
wandb: best/eval_avg_mil_loss ██▄▁
wandb:  best/eval_ensemble_f1 ▁▃▆█
wandb:            eval/avg_f1 ▂▃▃▆▃█▅▄█▅▃▁▃▁▅▄▅▇▃▇▄▄▇▆▇▇▂▅▆▆▃▅▄▄▇▄▆▂▄▆
wandb:      eval/avg_mil_loss ▄▄▃▃▁▄▂▃█▄█▂▆▃▆▂▃▃▃▂▂▃▃▃▃▃▃▄▃▃▂▄▄▂▅▂▃▂▄▃
wandb:       eval/ensemble_f1 ▁▃▂▃█▄▅▄▂█▂▄▄▅▇▆▄▆▅▄▂▆▅▃▅▁▄▅▃▄▄▇▄▅▆▅▂▃▂▇
wandb:           train/avg_f1 ▂▄▄▃▃▅▁▅▄▃▄▅▄█▄▄▅▃▃▅▂█▂▃▄▂▇█▅▄█▄▅▅▄▇▂▁▆▆
wandb:      train/ensemble_f1 █▄▇▃▆▅▄▃▆█▄▄▄▁▃▅▅▃▄▄▃▄▂▃▇▄▃▅▅▂▄█▄▄▆▆▃▆▂▇
wandb:         train/mil_loss █▄▄▃▂▅▁▅▁▃▄▁▆▅▅▅▅▃▂▆▄▆▆▂▄▆▅▆▇▃▄▄▆▅▃▂▅▃▅▅
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91919
wandb: best/eval_avg_mil_loss 0.19711
wandb:  best/eval_ensemble_f1 0.91919
wandb:            eval/avg_f1 0.90832
wandb:      eval/avg_mil_loss 0.29371
wandb:       eval/ensemble_f1 0.90832
wandb:           train/avg_f1 0.89543
wandb:      train/ensemble_f1 0.89543
wandb:         train/mil_loss 0.2848
wandb:      train/policy_loss 0.53082
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.53082
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worldly-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4ryj6co7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064410-4ryj6co7/logs
wandb: ERROR Run 4ryj6co7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mx5zsppc with config:
wandb: 	actor_learning_rate: 8.061241658030274e-05
wandb: 	attention_dropout_p: 0.3963362061086197
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 186
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6184658237400195
wandb: 	temperature: 4.759491038392765
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064548-mx5zsppc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mx5zsppc
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂█
wandb: best/eval_avg_mil_loss ▆▁█
wandb:  best/eval_ensemble_f1 ▁▂█
wandb:            eval/avg_f1 ▆▅▆▆▇█▅▇█▆█▇▆▅▇▇█▇▇▇█▆█▅▆▆▄▅▄▄▃▃▃▄▄▄▃▃▂▁
wandb:      eval/avg_mil_loss ▂▄▁▄▃▃▆▂▄▄▄▄▂▃▄▆▃▂▅▃▃▄▃▄▄▃▃▅▄▆▃▆▄▆▆█▆▇▇▆
wandb:       eval/ensemble_f1 █▅▆▇▆▅▆█▆▆▆▅▃▆█▆▄▆▆▇█▆▇▆▇▃▄▆▃▅▅▆▂▂▂▁▃▂▁▂
wandb:           train/avg_f1 ▇██▇▆▆▇▇▅█▅▇██▅▇▆▇▇▆▇▅▄▆▆▅▃▃▅▃▃▄▂▅▃▄▃▃▁▂
wandb:      train/ensemble_f1 ▇▅█▇▇▇▆█▇▇▆▅▆▆▇▆▇▅▇▆▅▅▅▅▅▅▄▄▅▂▄▄▃▃▄▂▂▃▁▂
wandb:         train/mil_loss ▇▆██▇▇▇▇▅▅▇▆▅▆▅▅▅▆▄▄▆▃▇▄▄▄▄▅▄▄▄▄▃▂▄▃▂▂▂▁
wandb:      train/policy_loss ▄▄▄█▂▄▄▄▄▄▄▄▄▄▅▄▄▄▄▁▅▄▄▄▄▅▄▄▃▄▄▄▆▄▄▄▄▆▅▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆█▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▅▆▆▆▅▆▆▆▇▇▆▇▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91148
wandb: best/eval_avg_mil_loss 0.26779
wandb:  best/eval_ensemble_f1 0.91148
wandb:            eval/avg_f1 0.85169
wandb:      eval/avg_mil_loss 0.38187
wandb:       eval/ensemble_f1 0.85169
wandb:           train/avg_f1 0.85362
wandb:      train/ensemble_f1 0.85362
wandb:         train/mil_loss 3.16924
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run giddy-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mx5zsppc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064548-mx5zsppc/logs
wandb: ERROR Run mx5zsppc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1yndyaua with config:
wandb: 	actor_learning_rate: 0.0007189808380906439
wandb: 	attention_dropout_p: 0.4333918711707093
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22092929969272057
wandb: 	temperature: 4.49227324130658
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064803-1yndyaua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1yndyaua
wandb: uploading history steps 66-68, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss ▄▁▄█
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▅▂▂▇▃▆▄▄▃█▄▅▆▄▅▂▅▂▂█▅▃▃▅▃▃▄▁▂▄▇▄▅▄▅▅▃▂▂▄
wandb:      eval/avg_mil_loss ▂▄▁▁▂▃▃▃▁▁▂▃▄▂█▆▅▃▃▃▂▃▃▅▃▂▃▄▂▄▃▄▂▃▂▂▃▃▂▂
wandb:       eval/ensemble_f1 ▅▅▅▁█▃▅▇▄▄▂▄▆▆▄▅▂▅▂▂▃▅▅▃▃▆▃▄▂▄▄▅▃▄▅▃▇▆▄▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▄▂▅▃▃▃▂▄▄▅▅▂▃█▅▃▆▅▄▃▇▅▆▅▁▁▅▅▄▆▆▄▅▆▅▂▆▃
wandb:      train/ensemble_f1 ▅▄▅▃▆▄▄▄▅▃▆▃▅▅▅▃▅▄▃█▆▇▄▅▇▆▆▅▅▂▄▅▁▅▅▃▆▂▄▃
wandb:         train/mil_loss █▇▆▇▇▇▇▆▇▅▇▅▅▅▄▆▆▄▆▄▄▅▂▅▃▄▄▃▃▃▃▃▃▃▂▃▃▂▂▁
wandb:      train/policy_loss ▆█▆▆▇▆▆▆▄▆▆▁▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆█▆▆▆▆▆▄▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.26426
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.90363
wandb:      eval/avg_mil_loss 0.27714
wandb:       eval/ensemble_f1 0.90363
wandb:            test/avg_f1 0.92406
wandb:      test/avg_mil_loss 0.15175
wandb:       test/ensemble_f1 0.92406
wandb:           train/avg_f1 0.88959
wandb:      train/ensemble_f1 0.88959
wandb:         train/mil_loss 0.29091
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run feasible-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1yndyaua
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064803-1yndyaua/logs
wandb: Agent Starting Run: r3uy9i6l with config:
wandb: 	actor_learning_rate: 0.00087479822948516
wandb: 	attention_dropout_p: 0.28756628827081737
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 71
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6124750029193072
wandb: 	temperature: 0.5315052789032026
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064911-r3uy9i6l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r3uy9i6l
wandb: uploading history steps 66-71, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▅▆█
wandb: best/eval_avg_mil_loss █▄▂▅▅▃▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄▅▆█
wandb:            eval/avg_f1 ▅▅▆▆▇▇▅▄▄▃▇█▆▆▅▆▆▅▅▆▄▄▃██▅▆▆▅▆▆▅▄▄▅▆▅▁▆▄
wandb:      eval/avg_mil_loss ▆▂▂█▃▁▅▂▆▃▃▂▂▄▁▄▄▃▂▃▄▄▃▃▅▁▅▄▆▅▅▆▅▆▃▅▃▃▃▅
wandb:       eval/ensemble_f1 ▅▆▆▄▆▄▄▃▃▄▇▅▅▆▅█▅▇▆▅▄▃▃▆▇█▅▅▆▄▅▆▅▆▃▄▃▁▅▄
wandb:           train/avg_f1 ▃▃▅▅▄▄▆▅▇▃▄▅▇▅█▇▃▄▅▁▅▅▅▇▄▅▇▇▅▃▆▆▅█▄▄▆▄▅▄
wandb:      train/ensemble_f1 ▃▄▅▃▅▄▄▃▆▄▃▄▄▅▅▇▃▄▅▁▅▇▆▅▇▅▇▅▅▃▆▆▆█▅▆▅▄▅▄
wandb:         train/mil_loss ▆▆▅█▆▃▇▅▅▅▆▇▆▆▅▄▅▅▄▅▄▄▅▄▄▄▆▄▄▃▃▄▃▁▄▆▄▃▃▂
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆█▆▅▆▁▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▄▆▇▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████▇█▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.22421
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.88498
wandb:      eval/avg_mil_loss 0.34722
wandb:       eval/ensemble_f1 0.88498
wandb:           train/avg_f1 0.88908
wandb:      train/ensemble_f1 0.88908
wandb:         train/mil_loss 0.57012
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run twilight-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r3uy9i6l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064911-r3uy9i6l/logs
wandb: ERROR Run r3uy9i6l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: g4mv6rwy with config:
wandb: 	actor_learning_rate: 0.000546528659309499
wandb: 	attention_dropout_p: 0.15753569804260575
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 128
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5702817521623363
wandb: 	temperature: 3.3496803211870985
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065023-g4mv6rwy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g4mv6rwy
wandb: uploading wandb-summary.json
wandb: uploading history steps 114-127, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▆▆██
wandb: best/eval_avg_mil_loss ▅▄█▁▅▃█
wandb:  best/eval_ensemble_f1 ▁▂▂▆▆██
wandb:            eval/avg_f1 ▇▆▇▆██▇▆▇▆▇█▃▇▄▆▅█▄▇▅█▃▅▇▅▁▃▅▆▂▅▅▇▄▃▄▁▄▄
wandb:      eval/avg_mil_loss ▂▅▄▂▁▄▄▃▂▃▃▃▆▁▂▅▄▃▄▇▆▄▅▅▄▇▆▃▄▅▃▆▃▅▇▆█▃▆▆
wandb:       eval/ensemble_f1 ▆▆█▆▅▆█▄█▄█▅▅▅▅▃▂▅▆▃▅█▅▅▁▅▃▄▄▃▄▃▂▂▄▂▃▃▂▄
wandb:           train/avg_f1 ▆████▆▇▇▆▅▇▇▆▇▇▅▅▆▆▅▆▅▅▅▆▄▅▅▅▄▃▃▃▄▃▁▂▁▃▁
wandb:      train/ensemble_f1 ▆▇▇██▅▇▇█▆▆▄▆▅▆▆▇▄▄▅▄▄▄▂▃▄▂▃▂▃▂▂▃▂▁▂▂▂▁▁
wandb:         train/mil_loss █▄▆█▂▅▄▅▆▃▃▄▇▃▃▄▃▂▃▃▂▂▄▃▂▂▃▂▃▂▄▃▁▂▂▁▂▂▁▂
wandb:      train/policy_loss ▄▄▅▄▄▄▄▅▁▂▄▇▄▄▄▄▄▄▄▇▄█▄▄▄▄▅▄▄▅▄▄▁▄▄▄▄▄▄▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▄▅▃▃▆▃▃▅▁▃▃▃▃▃▃▃▃█▃▃▃▃▄▃▃▃▃▅▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91565
wandb: best/eval_avg_mil_loss 0.32098
wandb:  best/eval_ensemble_f1 0.91565
wandb:            eval/avg_f1 0.86861
wandb:      eval/avg_mil_loss 0.37256
wandb:       eval/ensemble_f1 0.86861
wandb:           train/avg_f1 0.84755
wandb:      train/ensemble_f1 0.84755
wandb:         train/mil_loss 0.21316
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run peach-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g4mv6rwy
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065023-g4mv6rwy/logs
wandb: ERROR Run g4mv6rwy errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5wh73pg5 with config:
wandb: 	actor_learning_rate: 0.0001429179496623191
wandb: 	attention_dropout_p: 0.024631935824581452
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.49897317115157025
wandb: 	temperature: 2.4021259108065185
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065227-5wh73pg5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5wh73pg5
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃██
wandb: best/eval_avg_mil_loss ▁▇▁█
wandb:  best/eval_ensemble_f1 ▁▃██
wandb:            eval/avg_f1 ▇▅█▆▇█▇▇▇▇▅█▇▅▄▅▅▅▄▅▄▆▅▆▅▃▃▂▄▃▄▂▂▄▅▄▃▄▁▂
wandb:      eval/avg_mil_loss ▂▂▁▂▃▁▁▄▁▁▂▂▂▃▃▃▃▄▄▃▃▄▃▃▃▄▄▅▃▆█▄▄▃▄▆▆▅▄▄
wandb:       eval/ensemble_f1 █▆▇▇█▇▆▆█▆▄▅▆▄▄▆▅▄▅▆▅▅▄▃▃▄▆▃▄▃▃▃▁▂▄▂▃▄▁▃
wandb:           train/avg_f1 █▇█▇▇▇▇█▇▇▇▇▆▇▇▆▅▆▅▆▄▅▄▅▅▄▄▅▃▃▃▃▄▃▂▂▃▁▁▃
wandb:      train/ensemble_f1 ▇▇▇▇▇▇▇██▆▆▆▇▅▅▆▆▅▅▄▄▃▃▄▅▃▃▂▃▂▂▂▃▂▂▂▁▂▁▂
wandb:         train/mil_loss ▇▆█▆▅▅▅▅▅▅▅▄▄▄▃▃▃▅▃▃▂▂▃▁▂▂▃▃▄▂▄▃▃▁▁▂▃▁▁▃
wandb:      train/policy_loss ▂▃▃▂▃▂▂▃▂▃▂▃▂█▂▂▂▁▂▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▃▂▂▂▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▄▄▁▃▃▃▄▃▃▃▁▃▃█▃▃▃▃▃▃▃▂▃▃▄▃▃▄▃▃▄▃▃▃▃▃▃▄▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.916
wandb: best/eval_avg_mil_loss 0.28695
wandb:  best/eval_ensemble_f1 0.916
wandb:            eval/avg_f1 0.79404
wandb:      eval/avg_mil_loss 0.38251
wandb:       eval/ensemble_f1 0.79404
wandb:           train/avg_f1 0.82368
wandb:      train/ensemble_f1 0.82368
wandb:         train/mil_loss 0.18262
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run atomic-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5wh73pg5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065227-5wh73pg5/logs
wandb: ERROR Run 5wh73pg5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 53lnl6tx with config:
wandb: 	actor_learning_rate: 4.066132030459444e-06
wandb: 	attention_dropout_p: 0.2921289056730528
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22859938486168008
wandb: 	temperature: 5.775165052685453
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065503-53lnl6tx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/53lnl6tx
wandb: uploading history steps 83-89, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▇█
wandb: best/eval_avg_mil_loss █▁▄▃▁
wandb:  best/eval_ensemble_f1 ▁▅▅▇█
wandb:            eval/avg_f1 ▄▃▄▆▄▄▅▄▅▆▄██▃█▄▁▆▄▄▅▅▄▄▅▃▃▁▄▅▂▃▂▅▅▅▃▂▄▄
wandb:      eval/avg_mil_loss ▁▄▃▃▂▃▂▃▁▅▁▄▂▃▄▃▄▃▄█▄▅▄▆▅▄▃▃▅▄▅▅▇▄▆▃▁▅▄▃
wandb:       eval/ensemble_f1 ▃▄▄█▁▃▄▅▄▂▆▄██▅▂▅▄▁▅▆▃▆▅▂▅▅▄▃▃▃▂▄▆▅▄▄▄▂█
wandb:           train/avg_f1 ▆▅▅▃▄▁▃▇▆▅▅▃▅▆█▃▃▂▂▃▅▅▂▃▄▂█▅▅▆▆▄▄▂▆▄▄▅▁▃
wandb:      train/ensemble_f1 ▅▅▂▂▅▃▇▇▆▃█▂▄▃▃▁▄▂▅▁▃▂▅▁▄▄▅▃▂▆▆▃▂▄▆▄▃▄▅▇
wandb:         train/mil_loss ▇█▇▄▅▄▃▅▃▅▄▄▄▄▁▅▂▂▄▁▄▁▅▄▄▃▅▃▅▂▁▄▂▃▃▅▄▆▂▄
wandb:      train/policy_loss ▄▄▄▄▁▄▄▄▄▄▂▄▅█▅▄▄▄▇▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▂▃▆▁▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.22457
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.89602
wandb:      eval/avg_mil_loss 0.26539
wandb:       eval/ensemble_f1 0.89602
wandb:           train/avg_f1 0.89062
wandb:      train/ensemble_f1 0.89062
wandb:         train/mil_loss 0.26819
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run confused-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/53lnl6tx
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065503-53lnl6tx/logs
wandb: ERROR Run 53lnl6tx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xnsvkeu4 with config:
wandb: 	actor_learning_rate: 3.588740512417314e-06
wandb: 	attention_dropout_p: 0.052795821044947566
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 147
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4340924974832274
wandb: 	temperature: 0.45472327442851457
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065646-xnsvkeu4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xnsvkeu4
wandb: uploading wandb-summary.json
wandb: uploading history steps 136-147, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇▇▇▇█
wandb: best/eval_avg_mil_loss █▁▁▁▂▂▅
wandb:  best/eval_ensemble_f1 ▁▅▇▇▇▇█
wandb:            eval/avg_f1 ▃▅▆▇▆▁▂█▆▇▇▅▄█▄▆█▆▅▅▅▆▆▂█▃▆▅▅▅▃▇▆▅▃▅▅▄▄▆
wandb:      eval/avg_mil_loss █▄▄▃▄▃▃▃▃▁▆▃▂▂▃▁▃▁▄▃▄▂▄▂▃▃▄▄▂█▃▂▅▃▄▂▅▄▃▅
wandb:       eval/ensemble_f1 ▄▃▅▅▆▅▄▅▆▂▆▄▇▆██▇▅▅▃▄▆▇▆▄▄▄▄▁▇▄▆▄▄▃▄▅▃▅▄
wandb:           train/avg_f1 ▅█▅▄▆▁▂▄▆█▅▄▂▄▄▆▅█▆▅▅▂█▇▆▄▄▆▄█▃▅▃▆▆▇▁█▅▁
wandb:      train/ensemble_f1 ▅▇▅▅▄▂▆▅▃▅▄▅▅▃▄▅▃▂▅▃▅▅▄▆▅▄▇▄▄▇▄█▅▁▄▆▅▄▃▄
wandb:         train/mil_loss ▅▄▆▆▇█▆▅▄▆▅▄▄▄▄▅▆▄▂▃▄▃▄▂▄▄▂▅▂▃▃▂▅▃▃▃▃▂▁▆
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄█▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄█▄▄▄▄▄▁▄█▁█▄▁▁▄▄▄▄██▁▄▄▁▄▄▄▄▁▄▄▄▄▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.40107
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.87756
wandb:      eval/avg_mil_loss 0.36202
wandb:       eval/ensemble_f1 0.87756
wandb:           train/avg_f1 0.90373
wandb:      train/ensemble_f1 0.90373
wandb:         train/mil_loss 0.26983
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devoted-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xnsvkeu4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065646-xnsvkeu4/logs
wandb: ERROR Run xnsvkeu4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: r5gxdets with config:
wandb: 	actor_learning_rate: 3.367712139951768e-05
wandb: 	attention_dropout_p: 0.07180354174379128
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 109
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.47445565430834136
wandb: 	temperature: 2.769044278259104
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065911-r5gxdets
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r5gxdets
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss █▆▁
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 ▁▅█▇▄▆▅▆▅▆▇▅▆▅▃▇▆▄▆▄▅▅▆▆▄▄▃▇▆▃▅▁▂▁▆▅▁▃▁▂
wandb:      eval/avg_mil_loss ▄▄▂▂▂▂▁▃▃▂▃▃▃▃▃▃▄▄▃▂▃▂▂▄▃▄▃▆▄▅▅▅▅▄▅▅▆▅█▆
wandb:       eval/ensemble_f1 ▂▅▇▆▇▇▄█▄▄█▄▄▆▄▅▆▆▅▆▄▅▆▃▄▇▃▃▂▅▃▆▄▂▁▅▆▃▁▁
wandb:           train/avg_f1 ▆▆▇▆▅▆▆▆█▆█▅▇▆▆▆▅▇▆▆▆▆▆▅▅▅▄▅▄▄▄▃▃▅▅▄▄▁▄▃
wandb:      train/ensemble_f1 ▇▆▆▅▆▆█▅▆▅▆█▅▆▄▇▇▇▆▅▇▄▆▅▅▃▅▄▄▂▁▃▂▄▃▃▂▃▁▂
wandb:         train/mil_loss ▆█▅█▆█▇▆▄▅▆▆▅▅▄▃▅▆▄▃▇▄▄▃▂▄▄▃▃▂▃▂▁▄▃▁▁▁▂▂
wandb:      train/policy_loss ▃▁▃▂▃▄█▃▁▃▃▃▃▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▁▁▃▃▃▃█▃▃▃▂▃▃▁▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91919
wandb: best/eval_avg_mil_loss 0.27069
wandb:  best/eval_ensemble_f1 0.91919
wandb:            eval/avg_f1 0.84899
wandb:      eval/avg_mil_loss 0.40225
wandb:       eval/ensemble_f1 0.84899
wandb:           train/avg_f1 0.85801
wandb:      train/ensemble_f1 0.85801
wandb:         train/mil_loss 2.76767
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run misty-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r5gxdets
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065911-r5gxdets/logs
wandb: ERROR Run r5gxdets errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: k0ltrhsf with config:
wandb: 	actor_learning_rate: 4.753328532884217e-05
wandb: 	attention_dropout_p: 0.0010293972987228115
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.030341784540562022
wandb: 	temperature: 8.596117233847375
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070110-k0ltrhsf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k0ltrhsf
wandb: uploading wandb-summary.json
wandb: uploading data
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▃▆▇█
wandb: best/eval_avg_mil_loss █▃▅▂▅▃▁
wandb:  best/eval_ensemble_f1 ▁▃▃▃▆▇█
wandb:            eval/avg_f1 ▄▃▇▄▇▃▇▆▄█▅▂▃▃▁▃▁▄▃▃▄▄▃▅▃▂▂▃▃▄▂▆▃▂▄▂▂▄▇▂
wandb:      eval/avg_mil_loss ▄▃▃▂▄▂▃▄▃▂▂▂▄▂▄▄▃▄▄▃▅▆▃▂▅▄▁▄▃▄█▅▄▃▄▄▆▅▄▂
wandb:       eval/ensemble_f1 ▁▇▄▄▅▇▃█▅▆▅▂▃▅▇▅▅▇▄▆▅▂▁▄▄▁▇▃█▂▃▅▂▂▄▃▅▇▂▃
wandb:           train/avg_f1 ▇▅▅█▄▄▄▅█▆▆▆▇▆▁▆▁▄▆▄▁▃▅▇▄▄▇▄▄▄▄▃▁▂▂▅▂▃▂▄
wandb:      train/ensemble_f1 ▅▅▅▇▂▃█▄▄▄▅▃▅▇▆▆▆▁▄▃▂▂▆▅▅▅▁▅▃▄▅▁▁▄▅▅▂▁▂▄
wandb:         train/mil_loss █▇▇██▅▇▆▆▅▅▅▅▆▅▅▅▄▄▅▃▄▄▄▃▃▃▄▃▄▃▃▂▃▁▂▁▁▁▁
wandb:      train/policy_loss ▅▅▅▅▅▅▅▂▅▅█▅▅▅▅▅▅▅▅▁▅▅▅▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▃▅▅▅▅▇▅▅▅▅▅▅▁▆▄▅▄▅█▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91873
wandb: best/eval_avg_mil_loss 0.25035
wandb:  best/eval_ensemble_f1 0.91873
wandb:            eval/avg_f1 0.884
wandb:      eval/avg_mil_loss 0.37904
wandb:       eval/ensemble_f1 0.884
wandb:           train/avg_f1 0.89213
wandb:      train/ensemble_f1 0.89213
wandb:         train/mil_loss 0.68481
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k0ltrhsf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070110-k0ltrhsf/logs
wandb: ERROR Run k0ltrhsf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5x93dbhi with config:
wandb: 	actor_learning_rate: 4.762961019084255e-06
wandb: 	attention_dropout_p: 0.40683865221687654
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1051359957106851
wandb: 	temperature: 6.956763474755691
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070314-5x93dbhi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5x93dbhi
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇█
wandb: best/eval_avg_mil_loss ▅█▃▁▃
wandb:  best/eval_ensemble_f1 ▁▄▅▇█
wandb:            eval/avg_f1 ▄▄█▆▅▄▅▇▅▇▆▅▆▆▁▅▄▇▃▆▁▇▆▆▅▇▄▄▆▇▄▃▄▆█▄█▆▆▃
wandb:      eval/avg_mil_loss ▆▂▂▃▁▃▄▅▄▅▅▃▄▅▄▇▄▅▄▆▆▄▄▄▃▂▄▆▆▄▄▄█▄▅▁▅▄▃▆
wandb:       eval/ensemble_f1 ▆█▆▄▅▂▃▄▄▅▄▄▄▂▄█▅▆▂▄▄▂▆▄▂▃▂▃▃▄▁▂▃▂▁▆▅▁▁▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▇▆▆▅▇▇█▅▆▇▅▆▆▇▇▆▆█▅▄▇▆▆▇▆▆▁▄▆▇▇▇▅▅▄▇▅▆
wandb:      train/ensemble_f1 ▅▆▆▅▆▄▅▆█▇█▇█▇▅▆▅▆▇▅▅▇▆▆▄▇▆▆▆▆▁▆▆▆▆▆▅▅▄▆
wandb:         train/mil_loss ▇▇▆▇██▇▇▅▆▇▇█▅▆▅▅▆▆▆▆▇█▇▄▅▄▄▅▃▅▆▃▂▁▃▂▄▄▃
wandb:      train/policy_loss █▆▆▃▆▆▆▃▆▇▁▆▆▃▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▄▄▃▄▄▂▄▃▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.25042
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.9038
wandb:      eval/avg_mil_loss 0.34724
wandb:       eval/ensemble_f1 0.9038
wandb:            test/avg_f1 0.90127
wandb:      test/avg_mil_loss 0.20421
wandb:       test/ensemble_f1 0.90127
wandb:           train/avg_f1 0.89149
wandb:      train/ensemble_f1 0.89149
wandb:         train/mil_loss 0.93293
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run golden-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5x93dbhi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070314-5x93dbhi/logs
wandb: Agent Starting Run: s9xwry1t with config:
wandb: 	actor_learning_rate: 5.22023445502798e-06
wandb: 	attention_dropout_p: 0.27384862456296094
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 186
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2539147957747846
wandb: 	temperature: 1.0565512741016547
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070442-s9xwry1t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s9xwry1t
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss ▃▂█▁▂
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▆▅▆▄▆▆█▆▆▇▇▆▆▆▆▅▅▅▆▆▄▅▄▆▅▆▄▇▃▄▃▄▄▂▂▁▂▃▃▃
wandb:      eval/avg_mil_loss ▄▇▆▂▁▁▂▂▁▂▄▆▄▄▄▄▄▂▂▃▂▇▃▅▅▃▅▅▅▄▇▃█▄█▄▅▃▇▆
wandb:       eval/ensemble_f1 ▇▅█▇▆▆▆▄▆▆█▇▅█▆▇▇▆▆▆▅▅▄▆▅▆▄▅▃▅▁▅▅▃▃▅▂▄▃▄
wandb:           train/avg_f1 ▇▇███▇▅▇▆▇▇▇▇▆▅▇▅▅▅▅▄▅▅▅▆▄▅▃▃▄▃▃▃▄▃▂▃▃▂▁
wandb:      train/ensemble_f1 ▇▇██▇▇▇▆█▅▇▇▆▇▇▆▅▇▆▇▅▇▆▅▄▅▅▅▄▆▄▆▃▃▃▃▂▃▃▁
wandb:         train/mil_loss ▄█▅▃▅▃▄▄▄▆▃▃▅▅▄▅▄▄▃▃▂▃▃▂▃▃▄▃▃▃▃▅▂▂▄▄▁▃▁▁
wandb:      train/policy_loss ▅▅▁▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▃▇▅▅▅▅▄▅▅▅▅▅▅▅█▅▅▅▆▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▅▂▂▂▂▂▂▄▂▂▂▂▃▃█▂▂▂▂▂▂▂▃▂▂▃▂▁▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92299
wandb: best/eval_avg_mil_loss 0.24564
wandb:  best/eval_ensemble_f1 0.92299
wandb:            eval/avg_f1 0.85027
wandb:      eval/avg_mil_loss 0.34595
wandb:       eval/ensemble_f1 0.85027
wandb:           train/avg_f1 0.83967
wandb:      train/ensemble_f1 0.83967
wandb:         train/mil_loss 0.20898
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run effortless-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s9xwry1t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070442-s9xwry1t/logs
wandb: ERROR Run s9xwry1t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: fdgpw3ep with config:
wandb: 	actor_learning_rate: 0.00030969644007180147
wandb: 	attention_dropout_p: 0.31071020934660754
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 72
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2232003580512516
wandb: 	temperature: 1.4329071246005587
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070648-fdgpw3ep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fdgpw3ep
wandb: uploading history steps 63-73, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▇█
wandb: best/eval_avg_mil_loss ▅█▂▂▁
wandb:  best/eval_ensemble_f1 ▁▂▄▇█
wandb:            eval/avg_f1 ▃▃▄▃▄▁▄▄█▂▂▃▄▃▄▃▇▄▂▃▃▄▃▆▆▄▄▄▂▃▅▂▅▄▅▂▄▅▃█
wandb:      eval/avg_mil_loss ▃▅▃▄▆█▂▄▄▅▄▄▃▃▃▂▃▆▅▆▃▄▂▂▃▃▆▄▅▃▄▅▄▄▄▄▃▂▃▁
wandb:       eval/ensemble_f1 ▃▃▄▆▁▄▄▃▃▆▄▃▄▃▃▇▄▂▃▃▃▃▃▇▄▅▅▃▅▂▄▅▃▃▂▅▃▃▆█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▆▆▃▆▄▅▄▁▄█▄▆▄▃▄▅▆▅▁▆▅▃▃▃▅▆▃▅█▆▃▁▃▅▅▅▃▂▂
wandb:      train/ensemble_f1 ▄▆▄▅▆▄▄▅▆▄▅▄▃█▅▆▄▃▆▅▃▂▄▆▄▅▁▅█▃▄▃▃▆▃▄▃▂▂▃
wandb:         train/mil_loss █▇▇▇▇▇▇▆▆▆▅▅▅▅▄▅▄▄▄▄▄▃▃▂▃▃▃▂▂▂▁▂▂▂▂▁▂▁▁▁
wandb:      train/policy_loss ▄▄▄▄▄▄▁▄▇▄▃▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▇▄▃▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄█▄▄▄▆▁▄▄▄▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆█▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93328
wandb: best/eval_avg_mil_loss 0.21024
wandb:  best/eval_ensemble_f1 0.93328
wandb:            eval/avg_f1 0.93328
wandb:      eval/avg_mil_loss 0.21024
wandb:       eval/ensemble_f1 0.93328
wandb:            test/avg_f1 0.93845
wandb:      test/avg_mil_loss 0.15732
wandb:       test/ensemble_f1 0.93845
wandb:           train/avg_f1 0.89029
wandb:      train/ensemble_f1 0.89029
wandb:         train/mil_loss 0.37173
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run laced-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fdgpw3ep
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070648-fdgpw3ep/logs
wandb: Agent Starting Run: f7c6v5bm with config:
wandb: 	actor_learning_rate: 0.0001338369062007317
wandb: 	attention_dropout_p: 0.2814088821392602
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 60
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.37268504429933613
wandb: 	temperature: 6.816625805592703
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070806-f7c6v5bm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f7c6v5bm
wandb: uploading history steps 54-60, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss ▁█▂
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▇▇▅▅▃▁▇▆█▆▅▄▅▄▇▄▂▄▇▆▅▆▅▆▇▆▅▅▄▄▆▆▃▆▄▅▇▇▅▆
wandb:      eval/avg_mil_loss ▂▃▄▂▇▆▃█▃▃▃▃▂▂▃▄▄▃▄▂▅▅▃▃▄▆▄▂▂▇▄▃▄▆▆▄▃▂▁▆
wandb:       eval/ensemble_f1 ▆▇▅▅▅▁▇▆█▆▅▆▆▄▅▇▆▄▅▂▆▆▅▆▅▆▆▅▆▄▅▇▄▂▆▆▅▇▇▄
wandb:           train/avg_f1 ▅▇█▇▅▅▄▄▆▃▆▅▅▅▄▅▅▃▁▃▄▃▂▃▆▆▄▂▅▆▃▅▆▂▁▅▆▄▄▁
wandb:      train/ensemble_f1 ▅▇▃█▇▄▅▄▆▂▃▅▅▅▄▆▅▅▃▂▄▂▂▂▁▇▄▂▄▅▂▁▅▄▅▅▆▃▄▅
wandb:         train/mil_loss █▇▆▆▆▅▅▅▅▅▅▅▆▄▄▄▄▄▄▄▃▃▃▄▃▃▃▂▂▃▂▂▃▂▂▃▂▁▂▁
wandb:      train/policy_loss ▄█▄▄▄█▄▃▄▄▄▄▆▄▄▄▄▄▄▄▃▄▄▅▄▅▅▄▄▄▆▁▄▄▄▄▄▄▄▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄▄▄▃▄▄▄▆▄▄▄▄▄▄▄▄▄▂▇▄▄▅▄▅▄▅▄▄▆▁▇▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.24481
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.88116
wandb:      eval/avg_mil_loss 0.33909
wandb:       eval/ensemble_f1 0.88116
wandb:           train/avg_f1 0.87963
wandb:      train/ensemble_f1 0.87963
wandb:         train/mil_loss 0.68622
wandb:      train/policy_loss 0.10942
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.10942
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cool-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f7c6v5bm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070806-f7c6v5bm/logs
wandb: ERROR Run f7c6v5bm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: eb747g57 with config:
wandb: 	actor_learning_rate: 0.0001960987734013094
wandb: 	attention_dropout_p: 0.2496334401642426
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 102
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.25749613109362857
wandb: 	temperature: 4.629180510033947
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070918-eb747g57
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eb747g57
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▆▆▇█
wandb: best/eval_avg_mil_loss ▇█▇▁▄▂▃▅
wandb:  best/eval_ensemble_f1 ▁▄▄▅▆▆▇█
wandb:            eval/avg_f1 ▁▄▆▄▄▅▅▅▅█▅▇▃▅▇▄▇▅▄▆▇▂▅▃▁▅▆▅▃▇▃▄▂▅▄▆▄█▅▄
wandb:      eval/avg_mil_loss ▅▁▃▂▄▃▅▆▆▁▂▄▂▃▂▄▄▆▆▃▃█▇▅▄▅▄▂▂▄▂▂▅▁▅▄▃▃▃▄
wandb:       eval/ensemble_f1 ▁▅▄▃▄▅▄▃▆▆▃▄▆▃▅▄▂▅▆▂▃▁▅▅▄▃▄▆▄▂▅▂▅▄▃█▄▁▆▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▅▃▅▅▂▅▅▅▂▄▅▆▁▄▆▅▆▅▇▃█▃▂▁▂▃▃▄▅▅▂▄▃▆▂▁▄▃▄
wandb:      train/ensemble_f1 ▅▄▂▆▇▄▂▇▆▄▇▆▁▅▄█▄▅▃▄▅█▆▁▅▅▄▆▃▅▄▄▇▅▃▂▁▃▃▄
wandb:         train/mil_loss ▇█▃▆▄▃▃▅▃▄▇▄▂▄▃▂▄▆▆▆▄▁▅▆▃▂▅▃▄▄▃▂▅▅▄▄▆▅▇▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅██▅▁▁█▁▁▅▁▁▅▁█▁▁█▁▅▁██▅█▅██▅█▅█▅▁▅▁▅▁█▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92662
wandb: best/eval_avg_mil_loss 0.28502
wandb:  best/eval_ensemble_f1 0.92662
wandb:            eval/avg_f1 0.88642
wandb:      eval/avg_mil_loss 0.26919
wandb:       eval/ensemble_f1 0.88642
wandb:            test/avg_f1 0.90236
wandb:      test/avg_mil_loss 0.2581
wandb:       test/ensemble_f1 0.90236
wandb:           train/avg_f1 0.88721
wandb:      train/ensemble_f1 0.88721
wandb:         train/mil_loss 0.20013
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dandy-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eb747g57
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070918-eb747g57/logs
wandb: Agent Starting Run: xixczwvx with config:
wandb: 	actor_learning_rate: 1.048378607128682e-05
wandb: 	attention_dropout_p: 0.3951600448853638
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 195
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04561006223835962
wandb: 	temperature: 2.4387372864453827
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071116-xixczwvx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/zoc1c2p7
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xixczwvx
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄█
wandb: best/eval_avg_mil_loss ▄▇▅█▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄█
wandb:            eval/avg_f1 ▆▅▆▅▄▇▆█▄▄▅▅▅▅▃▃▇▆▅▅▅▄▅▃▇▆▆▅▇▅▄▅▅▄▄▁▁▃▅█
wandb:      eval/avg_mil_loss ▄▆▂▄▂▆▂▅▆▂▃▆▂▄█▃▄▃▂▅▃█▇▇▄▁▄▄▄▁▅▃▅▅▃▆▅▃▆▂
wandb:       eval/ensemble_f1 ▇▆▅▆▆▇▇██▆▅▆▆▇▅▆▅▅▆▅▅▆█▆▆▅▅▅▆▆▆▁▅▆▅▆▅▅▄▄
wandb:           train/avg_f1 ▅▅▅█▄▆▆▆▆▄▅▆▃▄▄▄▃▅▅▂▃▄▃▃▂▄▃▃▄▃▄▁▃▂▂▁▂▂▃▁
wandb:      train/ensemble_f1 ▅▅█▇▄▇▄▆▇▆▆▆▆▆▄▅▃▄▅▆█▅▂▅▅▂▃▄▃▂▅▁▄▂▄▁▁▁▄▂
wandb:         train/mil_loss ▅▆█▆▆▆▅▆▅▆▄▄▆▅▆▄▄▅▅▄▆▄▄▃▅▅▃▄▄▅▄▄▄▅▄▄▄▄▁▄
wandb:      train/policy_loss ▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▅▇▇▇▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▅▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93032
wandb: best/eval_avg_mil_loss 0.22926
wandb:  best/eval_ensemble_f1 0.93032
wandb:            eval/avg_f1 0.87197
wandb:      eval/avg_mil_loss 0.26509
wandb:       eval/ensemble_f1 0.87197
wandb:           train/avg_f1 0.89256
wandb:      train/ensemble_f1 0.89256
wandb:         train/mil_loss 0.23834
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hearty-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xixczwvx
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071116-xixczwvx/logs
wandb: ERROR Run xixczwvx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: z0pp2097 with config:
wandb: 	actor_learning_rate: 0.00014709944718370563
wandb: 	attention_dropout_p: 0.4763025436272121
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9995385860820388
wandb: 	temperature: 2.666252642114378
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071404-z0pp2097
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z0pp2097
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json; uploading history steps 70-88, summary
wandb: uploading history steps 70-88, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆█
wandb: best/eval_avg_mil_loss █▅▁▁▅
wandb:  best/eval_ensemble_f1 ▁▄▅▆█
wandb:            eval/avg_f1 ▅▆▄▆▅▅▅▂▄▄▅▄▃▁▃▄▃██▂▄▂▅▅▄▅▅▅▅▃▆▅▆▃▃▄█▅▅▅
wandb:      eval/avg_mil_loss ▆▁▄▃▂▅▇█▃▃▆▃▁▆▄▆▅▄█▂▇▁▂▅▃▅▅▄▆▄▇▄▂▄█▆▃▅▅▅
wandb:       eval/ensemble_f1 ▇▆▃▆▃▁▃▄▅▅▄▁▄▅▄▄▂▂██▄▄▄▄▃▄▃▃▅▆▄▃▄▄▆▃▃▄▄▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃█▅▆▆▇▄▄▃▃▅▃▁▆▄▅▄▃▁▂▃▃▃▄▅▃▄▄▃▇▃▄▁▂▄▂▂▅▃▃
wandb:      train/ensemble_f1 ▄▇▂█▂▆▃▃▆▃▄▁█▄▄█▆▅▄▃▄▄▄▂▆▇▂▄▅▅▄▃▄▄▂▂▄▃▂▄
wandb:         train/mil_loss ▄▂▂▅▄▂▂▇▃▄▅▇▆▁▆▃▃█▂▇▃▅▂▄▁▂▃▅▆▃▂▄▄▃▂▄▃▆▅▃
wandb:      train/policy_loss ▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94857
wandb: best/eval_avg_mil_loss 0.29201
wandb:  best/eval_ensemble_f1 0.94857
wandb:            eval/avg_f1 0.91565
wandb:      eval/avg_mil_loss 0.32502
wandb:       eval/ensemble_f1 0.91565
wandb:            test/avg_f1 0.92061
wandb:      test/avg_mil_loss 0.17757
wandb:       test/ensemble_f1 0.92061
wandb:           train/avg_f1 0.90319
wandb:      train/ensemble_f1 0.90319
wandb:         train/mil_loss 0.16308
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vibrant-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z0pp2097
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071404-z0pp2097/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: nsxzy6me with config:
wandb: 	actor_learning_rate: 7.996887666648481e-06
wandb: 	attention_dropout_p: 0.373279429509775
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 131
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3866469187760365
wandb: 	temperature: 9.175031108681358
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071535-nsxzy6me
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nsxzy6me
wandb: uploading wandb-summary.json
wandb: uploading history steps 114-121, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇█
wandb: best/eval_avg_mil_loss ▅▄█▁▅
wandb:  best/eval_ensemble_f1 ▁▄▇▇█
wandb:            eval/avg_f1 ▄▄▇█▄▇▅▃▆▅▅▆▅▄▃▆▂▆▃▃▆▄▇▄▃▇▆▅▇▅▄▁▇▄▇▅▆▂▄▄
wandb:      eval/avg_mil_loss ▃▁▁▂▅▆▃█▃▇▇▂▄▄▆▃▂▅▆▆▅▄▃▃▃▄▃▃▂▃▄▃▄▄▂▃▂▇▄▃
wandb:       eval/ensemble_f1 ▄▄▇▆█▅▅▆▄▅▄▃▆▃▆▆▆▃▆▆▅▆▄▃█▂▇▅▄▅▁▇▄▇▄▆▅▂▄▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▁▄▄▃▂▃▆▂▄▃▁▄▄▅▄▄▄▅▄▆▃▅▅▅▅▅▃▆▆█▄▇▅▃▅▂▇▅▃
wandb:      train/ensemble_f1 ▃▃▂▅▄▅▄▅▇▆▄▅▅▆▅▅▅▅▄▁▅▄▄▇▇▇▆▅▆██▇▆█▂▆▅▄▇▆
wandb:         train/mil_loss ▆▅▇▇▅▄▇▇▃▄▅▂▅▃▅▄▅▁▅▃▂▆█▄▁▄▄▁▂▇▃▇▄▅▃▃▃▇▅▄
wandb:      train/policy_loss ▆▆▆▄▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▅▆▆▆▁▇▆▆▆▆▆▆▆▆▆▆▆▆▆▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93025
wandb: best/eval_avg_mil_loss 0.24846
wandb:  best/eval_ensemble_f1 0.93025
wandb:            eval/avg_f1 0.90411
wandb:      eval/avg_mil_loss 0.26974
wandb:       eval/ensemble_f1 0.90411
wandb:            test/avg_f1 0.89897
wandb:      test/avg_mil_loss 0.23355
wandb:       test/ensemble_f1 0.89897
wandb:           train/avg_f1 0.89787
wandb:      train/ensemble_f1 0.89787
wandb:         train/mil_loss 4.56914
wandb:      train/policy_loss 0.52291
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.52291
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run blooming-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nsxzy6me
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071535-nsxzy6me/logs
wandb: Agent Starting Run: gn1qml5d with config:
wandb: 	actor_learning_rate: 0.0005826732002706557
wandb: 	attention_dropout_p: 0.0701003429011402
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 79
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9974657760358944
wandb: 	temperature: 5.905445122576953
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071718-gn1qml5d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gn1qml5d
wandb: uploading wandb-summary.json
wandb: uploading history steps 76-80, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▆█
wandb: best/eval_avg_mil_loss █▅▁▁▂
wandb:  best/eval_ensemble_f1 ▁▃▄▆█
wandb:            eval/avg_f1 ▃▄▅▄█▅▆▆▂▆▆▂▄▇▃▅▃▅▁▅▃▄▅▂▃▄▅▅▄▂▁▆▅▄▁▁▃▂▂▄
wandb:      eval/avg_mil_loss ▅▂▃▄▆▅▂█▂▇▂▁▄▃▄▄▆▁▃▆▇▄▂▆▄▃▂▄▃▃▄▂▅▂▆▃▅▃▂▅
wandb:       eval/ensemble_f1 ▃▁▇▆▃▄▆▇▇▇▇▅▃█▆▆▃▃▆▆▅▁▃▂▆▄▆▅▆▆▇▅▆▅▃▂▄▃▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▇▇▄▆█▅▄▇▆▇▄▅▅█▇▆▇▇▅▇▇▇▆▇▆█▃▆▅▄▅▅▇▆▁▃▃▃
wandb:      train/ensemble_f1 █▅▅▅▆▃▄▅██▆▄▂▄▄▂█▆▇▄▆▄▆▅▅▆▄▃▆▃▅▄▄▆▆▁▄▃▆▁
wandb:         train/mil_loss ▄▇▄▅▅▃▃▆▄▄▅▄▄▄▅▆▄▅▅▃▁▃▄▃█▃▄▅▃▃▄▃▄▂▅▁▃▂▃▁
wandb:      train/policy_loss ▅▁▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▇▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94099
wandb: best/eval_avg_mil_loss 0.2586
wandb:  best/eval_ensemble_f1 0.94099
wandb:            eval/avg_f1 0.90486
wandb:      eval/avg_mil_loss 0.36989
wandb:       eval/ensemble_f1 0.90486
wandb:            test/avg_f1 0.91253
wandb:      test/avg_mil_loss 0.19565
wandb:       test/ensemble_f1 0.91253
wandb:           train/avg_f1 0.88812
wandb:      train/ensemble_f1 0.88812
wandb:         train/mil_loss 0.16323
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hardy-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gn1qml5d
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071718-gn1qml5d/logs
wandb: Agent Starting Run: yz4dvrc9 with config:
wandb: 	actor_learning_rate: 2.3935504134161713e-06
wandb: 	attention_dropout_p: 0.021412493081727435
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 104
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9950683016085384
wandb: 	temperature: 5.330187687051532
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071830-yz4dvrc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yz4dvrc9
wandb: uploading history steps 101-105, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▇█
wandb: best/eval_avg_mil_loss ▃█▁▂▁
wandb:  best/eval_ensemble_f1 ▁▁▅▇█
wandb:            eval/avg_f1 ▅▅▃▄▆▃▅▅▃▃▃▂▃▃▁▅▃▄▆▇▅▇▁▆▆█▅▃▅▅▄▃▇▂▄▂▂▃▇▁
wandb:      eval/avg_mil_loss ▄▄▅▃▃▄▆▂▂▄▂▄▂▂▃▄▃▅▃▃█▂▃▂▃▂▂▂▁▄▃▃▄▆▂▄▄▂▂▁
wandb:       eval/ensemble_f1 ▄▂▃▃▄▆▆▄▄▃▂▆▁▂▃▃▆▆█▃▇▁▄▃▄▄▄▃▂▇▃▃▂▄▄▂▃▅▆▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▅▃▆▇▂▃▄█▇▂▆▅▇▆█▄▃▃▅▁▅▃▄▅▂▃▄▃▃▂▄▃▄▆▆▁▆▅
wandb:      train/ensemble_f1 ▂▅▅▃▅▁▃▃▃▄██▂▂▅▇▇▆▄▅▄▆▅▅▃▃▅▅█▃▄▇▂▅▃█▆█▆▄
wandb:         train/mil_loss ▇▃▅▄▃▄▄▅▅▆▄▁▂▂▃█▄▆▃▂▁▇▃▃▄▄▄▄▄▅▃▆▄▂▂▅▅▂█▁
wandb:      train/policy_loss ▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████▆███▁██████▇█████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9409
wandb: best/eval_avg_mil_loss 0.24296
wandb:  best/eval_ensemble_f1 0.9409
wandb:            eval/avg_f1 0.91135
wandb:      eval/avg_mil_loss 0.26755
wandb:       eval/ensemble_f1 0.91135
wandb:            test/avg_f1 0.90367
wandb:      test/avg_mil_loss 0.21301
wandb:       test/ensemble_f1 0.90367
wandb:           train/avg_f1 0.90203
wandb:      train/ensemble_f1 0.90203
wandb:         train/mil_loss 0.19879
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worthy-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yz4dvrc9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071830-yz4dvrc9/logs
wandb: Agent Starting Run: anm0cqvl with config:
wandb: 	actor_learning_rate: 3.401609018248062e-06
wandb: 	attention_dropout_p: 0.036270330758697455
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 97
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7963847503429016
wandb: 	temperature: 5.871908814010201
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072024-anm0cqvl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/anm0cqvl
wandb: uploading history steps 87-98, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▇█
wandb: best/eval_avg_mil_loss ██▁▅▆▃
wandb:  best/eval_ensemble_f1 ▁▂▄▄▇█
wandb:            eval/avg_f1 ▇▅▇▅▅█▇▅▄▆█▅▅▆▇▄▆▄▄▇█▃▄▅▄▆▄▇▅▇▄▃▅▂▁▆▇▆▆▅
wandb:      eval/avg_mil_loss ▆▅▆▃▆▅▃▄▄▅▅▂▃▅▁▆▆▄▃▅▅▇█▃▆▇▆▅▅▅▄▃▄▄▆▃▃▃▆▇
wandb:       eval/ensemble_f1 ▇▅▃▆▅▅▅▅▇█▄▆▃▇▅▆▄▄▆▅▅▇▅▅▄▃▆▆▃▃▄▄▅▆▂▆▅▁▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆██▃▆▄▆▇▆▆▆▅▆▇▆█▇▄█▅▆▆▂█▁▆▆▆▆▆▆▇▇▅▄▃▆▄▄
wandb:      train/ensemble_f1 ▆▆██▃▆▄▅▇▅▆▂▄▆▆▅▆█▅▁▄▄▅▃▄▄▁██▅▄▆▆▅▇▂▄▃▄▂
wandb:         train/mil_loss █▄▆▆▆▇▆▃▅▄▅▄▂▄▂▄▂▄▅▄▄▃▅▄▂▃▃▁▃▃▄▂▄▁▂▃▁▃▃▂
wandb:      train/policy_loss ▇▇▅▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▁▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92653
wandb: best/eval_avg_mil_loss 0.22927
wandb:  best/eval_ensemble_f1 0.92653
wandb:            eval/avg_f1 0.90478
wandb:      eval/avg_mil_loss 0.26501
wandb:       eval/ensemble_f1 0.90478
wandb:            test/avg_f1 0.88761
wandb:      test/avg_mil_loss 0.22107
wandb:       test/ensemble_f1 0.88761
wandb:           train/avg_f1 0.89178
wandb:      train/ensemble_f1 0.89178
wandb:         train/mil_loss 0.39174
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run autumn-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/anm0cqvl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072024-anm0cqvl/logs
wandb: Agent Starting Run: bjy0cumv with config:
wandb: 	actor_learning_rate: 2.0715191289695443e-06
wandb: 	attention_dropout_p: 0.029034097952086668
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 76
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6656394704575374
wandb: 	temperature: 6.025742021894029
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072212-bjy0cumv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bjy0cumv
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃█
wandb: best/eval_avg_mil_loss █▁▅▇▅
wandb:  best/eval_ensemble_f1 ▁▂▂▃█
wandb:            eval/avg_f1 ▆▅▆▆▆▄▆▆▅▆▅▄▃▄▂▅▅▅▄▇▃▄▆▄▅▅▄▅█▂▄▅▂▁▂▃▄▄▂▃
wandb:      eval/avg_mil_loss ▄▄▁▄▃▂▃▄▃▃▅▅▄▆▅▄▇▄▆▄▅▆▄▅▃▄▃▅▅▅█▆▅▅▃▆▅▇▇▅
wandb:       eval/ensemble_f1 ▆▅▇▇▆▃▆▆▅▅▄▅▆▅▄▄▃▄▆▄▅▅█▂▃▂▆▃▃▅▃▇▃▂▄▃▁▃▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▇▆▄█▅█▅▆▇▆▆▆▆▆▆▄▄▇▆▆▅▃▄▃▄▄▂▂▆▂▅▄▄▂▄▄▁▃▁
wandb:      train/ensemble_f1 ▇▇▇▇█▄█▇▆▇▆▇▅▆▆▆▆▅▅▇▇▄▅▄▆▃▅▅▅▄▇▄▃▂▅▄▅▄▃▁
wandb:         train/mil_loss ▅█▆▆█▄▅▅▅▄▅▅▃▄▅▅▃▄▄▃▃▄▄▅▄▅▅▄▄▄▅▂▃▅▄▄▄▁▂▃
wandb:      train/policy_loss ▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▅▃▁▁▁▁▁▁▅▁▁▁▁▁▁▁▂▁▁▁▁▁▁▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▆▄▄▄▆▅▄▄▄▄▄▄▄▄▄▄▄▄▅▆▄▄▄▄▁▄▄▄▄▄▄▆▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93408
wandb: best/eval_avg_mil_loss 0.24765
wandb:  best/eval_ensemble_f1 0.93408
wandb:            eval/avg_f1 0.89416
wandb:      eval/avg_mil_loss 0.32156
wandb:       eval/ensemble_f1 0.89416
wandb:            test/avg_f1 0.90623
wandb:      test/avg_mil_loss 0.21833
wandb:       test/ensemble_f1 0.90623
wandb:           train/avg_f1 0.87253
wandb:      train/ensemble_f1 0.87253
wandb:         train/mil_loss 0.27261
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dry-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bjy0cumv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072212-bjy0cumv/logs
wandb: Agent Starting Run: n3b8maqg with config:
wandb: 	actor_learning_rate: 3.064554738933209e-06
wandb: 	attention_dropout_p: 0.4677817603077958
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 198
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6030123586305005
wandb: 	temperature: 6.709232425838955
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072335-n3b8maqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n3b8maqg
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▅▆▄▄▄▅▂▄▄▇▆▃▆▆▇▅▇▅▅▄▅▃▃▇▆▅▃▇▆▆▆▅▄▆▆▅▄▁▇
wandb:      eval/avg_mil_loss ▃▃▃▃▃▃▅▄▁▃▂▃▆▂▃▃▄▁▃▃█▃▄▃▃▂▄▃▂▄▄▂▃▂▃▄▄▂▃▃
wandb:       eval/ensemble_f1 ▃▅▅▄▆▄▅▆▄▆█▃▆█▅▅▄▅▁▆▅▂▄▃▃▃▄▂▅▄▂▄▄▄▄▇▆▇▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▇▆▄▆▇█▄▄▆▅▅▅▇▃▆▅▅▁▅▅▆█▄▃▃▃▄▄▅▅▄▆▇▆▂▅▄▆▃
wandb:      train/ensemble_f1 ▆█▅▇▅▅█▄▇█▇▅▆▆▁▅▅▅▅▅▆▅▆▃▆▃▃▁█▆▄▁▇▇▅▂▁▆▄▃
wandb:         train/mil_loss ▅▆▃▄▄▆▅▇▄▅▆▁▄▁▃▁▃▆▂▃▅▅▃▄▄▃▄▂▅▃▃▆▃█▅▅▅▃▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▇▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94108
wandb: best/eval_avg_mil_loss 0.21439
wandb:  best/eval_ensemble_f1 0.94108
wandb:            eval/avg_f1 0.92261
wandb:      eval/avg_mil_loss 0.29129
wandb:       eval/ensemble_f1 0.92261
wandb:            test/avg_f1 0.91594
wandb:      test/avg_mil_loss 0.23596
wandb:       test/ensemble_f1 0.91594
wandb:           train/avg_f1 0.89451
wandb:      train/ensemble_f1 0.89451
wandb:         train/mil_loss 0.19553
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run apricot-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n3b8maqg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072335-n3b8maqg/logs
wandb: Agent Starting Run: csg8k02t with config:
wandb: 	actor_learning_rate: 3.5423595654807686e-06
wandb: 	attention_dropout_p: 0.27056259481906814
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 175
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7975058940204185
wandb: 	temperature: 4.918914306057315
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072534-csg8k02t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/csg8k02t
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▄▆█
wandb: best/eval_avg_mil_loss ▃▁█▅▂▂▄
wandb:  best/eval_ensemble_f1 ▁▂▂▃▄▆█
wandb:            eval/avg_f1 ▃▂▂▅▅▄▃█▆▅▅▃▆▂▃▂▄▄▄█▃▃▃▄▂▃▅▃▃▃▄▃▄▇▃▁▃▂▁▂
wandb:      eval/avg_mil_loss ▄▄▃▄▄▂▄▃▂▃▂▅▄▆▄▄▄▄▂▃▁▆▅▁▃▃▆▄▁▁▃▅▅▄▅▆▁▃▄█
wandb:       eval/ensemble_f1 ▄▃▃▃▄▃▄▇▅▅▆▅▆▆▄▅▃█▂█▂▆▇▃▅▅▆▄▅▁▆▆▃▄▂▂▄▄▁▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▆▇▅▆▇▅▆▆▆██▆█▁▇▅▅▆▄▄▇▅▄▇▅▄▆▄▆▂▃▄▅▆▃▅▃▄
wandb:      train/ensemble_f1 ▅█▆▆▃▃▅▅▇▆▆▇▅▄▆▅▆▇▄▅▅▆▇▄▆▄▅▄▅▄▂▂▃▃▅▃▃▂▂▁
wandb:         train/mil_loss ▆█▆▆▆▅▆▆▅▅▅▇▆▅▃▄▄▃▃▆▄▅▃▄▃▂▁▃▂▃▁▂▁▂▂▃▃▅▁▄
wandb:      train/policy_loss ▅▂▅▅▅▄▅▅▂▇▄▁▅▅▅▅▇▅▆▅▆▆▅▅▆▅▇▅█▅▅▅▅▅▅▅▅▅▅▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▅▅▅▅▅▅▅▅▅▃▅▅▃▁▅▇▅▆▅▅▆▅▅▅▅▅▆▅▅▇▅▇█▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93361
wandb: best/eval_avg_mil_loss 0.30098
wandb:  best/eval_ensemble_f1 0.93361
wandb:            eval/avg_f1 0.8648
wandb:      eval/avg_mil_loss 0.37671
wandb:       eval/ensemble_f1 0.8648
wandb:            test/avg_f1 0.90236
wandb:      test/avg_mil_loss 0.20684
wandb:       test/ensemble_f1 0.90236
wandb:           train/avg_f1 0.87518
wandb:      train/ensemble_f1 0.87518
wandb:         train/mil_loss 2.95605
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sandy-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/csg8k02t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072534-csg8k02t/logs
wandb: Agent Starting Run: ruh5ai7o with config:
wandb: 	actor_learning_rate: 2.3212829916331103e-05
wandb: 	attention_dropout_p: 0.3651875631884795
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 122
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22899316549973925
wandb: 	temperature: 9.167577384109316
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072734-ruh5ai7o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ruh5ai7o
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇█
wandb: best/eval_avg_mil_loss ▇▃▆▁█
wandb:  best/eval_ensemble_f1 ▁▄▇▇█
wandb:            eval/avg_f1 ▄▃▆▄▃▆█▆▁▅▂▇▇▂▄▆▅▂▅▃▆▅▄▇▇▅▂▄▆▁▆▄▇▅▄▂▄▆▅▅
wandb:      eval/avg_mil_loss ▂▂▄▄▄▅▃█▄▂▂▃▄▃▄▂▄▁▂▃▅▂▂▅▃▄▃▃▂▃▄▅▂▃▃▂▃▄▁▂
wandb:       eval/ensemble_f1 ▅▃█▃▂█▆▄▁▆▄▂▆▅▃▆▆▃▂▆▆▂▃▇▄▂▂▄▄▇▅▆▄▄▆▁▆▄▄▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▅▅█▄▄▄▄▁▅▅▆▂▅▆▄▄▅▇▅▄▅▅▅▅▄▆▇▄▅▄▃▅█▄▆▄▄▄
wandb:      train/ensemble_f1 ▅▆▅▄█▄▃█▃▁▅▄▃▄▄▄▆▄▅▄▅▅▄▅▇▄▄▃▆▃▅▅▆▄▅▆▆▃▄▄
wandb:         train/mil_loss ▅▅▇█▇▅▆▄▇▅▄▆▆█▃▃▄▄▄▄▄▄▄▃▄▄▄▂▃▄▅▂▃▃▃▃▅▁▂▂
wandb:      train/policy_loss █████████▆█████████████████▁████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▁███████████████████████████████████▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93395
wandb: best/eval_avg_mil_loss 0.39334
wandb:  best/eval_ensemble_f1 0.93395
wandb:            eval/avg_f1 0.91482
wandb:      eval/avg_mil_loss 0.25808
wandb:       eval/ensemble_f1 0.91482
wandb:            test/avg_f1 0.86217
wandb:      test/avg_mil_loss 0.39351
wandb:       test/ensemble_f1 0.86217
wandb:           train/avg_f1 0.90395
wandb:      train/ensemble_f1 0.90395
wandb:         train/mil_loss 1.35269
wandb:      train/policy_loss 0.13173
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.13173
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run icy-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ruh5ai7o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072734-ruh5ai7o/logs
wandb: Agent Starting Run: 2h24vwf7 with config:
wandb: 	actor_learning_rate: 8.057233903035425e-06
wandb: 	attention_dropout_p: 0.27771671124411607
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 96
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1656610480195886
wandb: 	temperature: 8.750327063431987
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072917-2h24vwf7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2h24vwf7
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▆▇████
wandb: best/eval_avg_mil_loss █▂▅▄▄▁▂▁▁
wandb:  best/eval_ensemble_f1 ▁▃▃▆▇████
wandb:            eval/avg_f1 ▅▄▆▁▄▃▅▄▅▅▅▅▅▅▄▅▄▆▂▆▂▃▄▄▄▃█▃▆▃▆▇▄▃▆█▄▇▇▂
wandb:      eval/avg_mil_loss █▄▄▃▅▃▆▃▅▅▆▅▂▄▁▄▆▄▃▂▃▃▆▂▃▂▁▄▆▃▁▂▄▄▁▂▃▄▄▅
wandb:       eval/ensemble_f1 ▄▄▆▁▃▆▅▄▅▇▅▅▂▃▆▄▆▂▇▂▂▅▃▅█▃▃▅▆▇▁▅▄▃▃▄█▄▂▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▇▄▅▅▃▇█▄▂▇▃▅▆▄▅▂▂▇▆▆▇▅▄▃▇█▂██▅▅▃▆▅▄█▇▇▄
wandb:      train/ensemble_f1 ▅▁▅▃▆▃▇▂▆▂▅▇▃▅▅▇▇▇▆▆▇▇▆▆▄▂▃█▅▇▂▃▅▄▃▁▄█▇▂
wandb:         train/mil_loss ▇▆▇▇▆▅██▆▅▅▅▅▆█▅▂▃▆▂▅▅▄▆▅▄▅▁▆▅▅▆▄▅▄▄▅▄▄▆
wandb:      train/policy_loss ████████████████████▁███████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅█▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▁█▁▁▅▅▅▅▅█▅█▁█▁█▁▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92669
wandb: best/eval_avg_mil_loss 0.26015
wandb:  best/eval_ensemble_f1 0.92669
wandb:            eval/avg_f1 0.8852
wandb:      eval/avg_mil_loss 0.40158
wandb:       eval/ensemble_f1 0.8852
wandb:            test/avg_f1 0.89763
wandb:      test/avg_mil_loss 0.16591
wandb:       test/ensemble_f1 0.89763
wandb:           train/avg_f1 0.89534
wandb:      train/ensemble_f1 0.89534
wandb:         train/mil_loss 0.46415
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2h24vwf7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072917-2h24vwf7/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: t1emiarz with config:
wandb: 	actor_learning_rate: 1.1046241984893558e-06
wandb: 	attention_dropout_p: 0.1414142898749236
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 130
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7864688694591203
wandb: 	temperature: 8.971906720616264
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073048-t1emiarz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t1emiarz
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 105-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▇▄█▇▆▆▇▇▆▆▆▅▆▆▅▇▅▆▃▄▄▅▆▅▄▄▅▇▅▄▂▄▄▂▅▅▄▃▁
wandb:      eval/avg_mil_loss ▇▄▇▂▅▁▁▂▂▂▂▃▂▃▂▂▄▆▇▃▆▄▆▅▁▅▅▆▁▆▃▄▄▅█▄▄▅▇▇
wandb:       eval/ensemble_f1 ▇▆▄▆█▇▅▆█▆▂▅▄▆▆▄▅▇▄▆▂▄▅▅▂▄▆▅▁▅▆▃▂▄▂▁▄▂▃▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇█▇▆█▅▅▆▄▆▇▆▅▇▇▆▇▆▃▅▆▆▄▅▆▂▃▄▅▂▂▃▄▃▃▂▄▂▂▁
wandb:      train/ensemble_f1 ▇▇▆▇▅▄▇▆▆▆█▅▄▅▅▄▆▅▃▆▆▂▄▅▇▄▄▄▂▄▂▅▄▄▄▄▂▂▁▁
wandb:         train/mil_loss ▆▅▅█▅▆▄▄▄▅▄▄▄▄▃▃▂▅▃▃▅▂▄▃▃▂▃▄▅▄▁▃▁▃▄▂▄▂▃▁
wandb:      train/policy_loss ▃▃▁▃▅▃▃▆▃▃▁▃▃▃▃█▃▃▃▃▃▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▄▂▂▁▂▂▄▂▂▂▂▂▅▆▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂█▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.267
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.87218
wandb:      eval/avg_mil_loss 0.36734
wandb:       eval/ensemble_f1 0.87218
wandb:            test/avg_f1 0.91963
wandb:      test/avg_mil_loss 0.15967
wandb:       test/ensemble_f1 0.91963
wandb:           train/avg_f1 0.87366
wandb:      train/ensemble_f1 0.87366
wandb:         train/mil_loss 0.16596
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run treasured-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t1emiarz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073048-t1emiarz/logs
wandb: Agent Starting Run: aft2y3pe with config:
wandb: 	actor_learning_rate: 0.0002725864690436605
wandb: 	attention_dropout_p: 0.3386660368023196
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 128
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1959691456466244
wandb: 	temperature: 7.636904664866885
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073226-aft2y3pe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aft2y3pe
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 112-129, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▇█
wandb: best/eval_avg_mil_loss ██▃▁▄▆
wandb:  best/eval_ensemble_f1 ▁▂▄▄▇█
wandb:            eval/avg_f1 ▄▃▄▆▆▅▇▄▅█▆▄▂▃▄▄▆▆▅▆▅▂▁▃▅▅▃▃▅▃▃▆▆▅▂▅▁▄▃▄
wandb:      eval/avg_mil_loss ▅▅▄▆▁▄▄▅▃▅▃▄▆▃▄▃▄▃▂▇█▃▅▄▇▇▆▅▆▅▃▄▄▄▅▆▄█▆▅
wandb:       eval/ensemble_f1 ▃▅▄▃▅▄█▆▃▃▃▁▃▇▄▅▅▃▆▅▅▃▃▃▄▄▃▅▃▂▄▅▃▂▅▃▅▂▃▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▄▂▆█▅▄▆▇▂▇▃▅▃▂▁▆▇█▂▁▂▄▄▄▇▇▃▆▄▃▁▂▂▂▄▆▂▄▅
wandb:      train/ensemble_f1 ▆▆▇▇▆█▆▇▄██▇▂▅▆▅▃▆▆▆▅▅▇█▇▅▅▃▅▇▇▄▅▁▅▃▄▄▅▃
wandb:         train/mil_loss █▇▆▄▃▇▆▆▄▃▃▆▂▂▅▄▅▅▃▄▄▄▃▃▁▄▂▂▂▂▁▃▄▃▃▂▂▃▂▂
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▆▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄█▄█▄█▄▁▄▄▄█▄▄▁▁▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.32285
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.86514
wandb:      eval/avg_mil_loss 0.3678
wandb:       eval/ensemble_f1 0.86514
wandb:            test/avg_f1 0.89191
wandb:      test/avg_mil_loss 0.28248
wandb:       test/ensemble_f1 0.89191
wandb:           train/avg_f1 0.87154
wandb:      train/ensemble_f1 0.87154
wandb:         train/mil_loss 1.99975
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fiery-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aft2y3pe
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073226-aft2y3pe/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8s7wfsc0 with config:
wandb: 	actor_learning_rate: 9.665470361171922e-06
wandb: 	attention_dropout_p: 0.4126594969457582
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4168660638842648
wandb: 	temperature: 8.578492133144838
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073426-8s7wfsc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8s7wfsc0
wandb: uploading wandb-summary.json
wandb: uploading history steps 113-121, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss ▇▂█▁
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▁▆▆▅▃▆▅▆▂█▃▆▄▅▄▅▆▄▄▅▆▆▅█▇▅▇▆▆▇▅▆█▅▆▄▆▇▄▅
wandb:      eval/avg_mil_loss ▆▆▅▃▄▃▁▂▁▄▅▄▃▁▃▃▃▄▄▃▃▄█▃▂▃▂▆▅▆▇▆▃▄▆▆▅▅▇▆
wandb:       eval/ensemble_f1 ▂▆▁█▁▆▃▅▄▆▄▆▅▆▃▆▇▅▄▆▇▇▆▅▆▆▆▄▅█▆▅▆▅▅▆▆▆▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▅▅▅▆▃▃▅▅▆▅▃▅▃▃▅▄▂▂▅▄▄▆▅▃▄▄▂▆▄▅▅▃▁▁▃▄▄█
wandb:      train/ensemble_f1 ▃▂▂▄▃▅▂▅▅▄▅▄▆▅▃▂▆▃▁▁▄▂▃▆▅▄▂█▂▄▃▄▂▃▃▃▄▅▃█
wandb:         train/mil_loss ██▇█▇▇▇▆▅▄▅▅▅▄▄▄▄▄▄▄▃▄▃▃▃▃▃▃▃▃▃▃▃▂▂▃▁▂▂▂
wandb:      train/policy_loss ▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▆▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.18643
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.89197
wandb:      eval/avg_mil_loss 0.3901
wandb:       eval/ensemble_f1 0.89197
wandb:            test/avg_f1 0.90493
wandb:      test/avg_mil_loss 0.18433
wandb:       test/ensemble_f1 0.90493
wandb:           train/avg_f1 0.88385
wandb:      train/ensemble_f1 0.88385
wandb:         train/mil_loss 0.56012
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run atomic-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8s7wfsc0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073426-8s7wfsc0/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: zx4sise4 with config:
wandb: 	actor_learning_rate: 0.0003812795773607847
wandb: 	attention_dropout_p: 0.3082131093470254
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 135
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.585515869782965
wandb: 	temperature: 4.649321121715929
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073645-zx4sise4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zx4sise4
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅█
wandb: best/eval_avg_mil_loss █▆▁▇
wandb:  best/eval_ensemble_f1 ▁▂▅█
wandb:            eval/avg_f1 █▆█▆▄█▇▇▇▆▇▅▆▆▇▇▆▇▅▅▃▅▅▁▄▄▅▄▃▂▂▄▃▂▃▁▂▂▄▁
wandb:      eval/avg_mil_loss ▄▁▁▅▁▂▁▂▂▁▃▂▁▄▃▄▃▃▄▃▄▃▃▅▄▆▃▆▆▇▅▆▇▇██▇▆▇▆
wandb:       eval/ensemble_f1 ▄▆▅▆▃▄▄▅█▇▅▅▅▅▅▅▅▅▅▅▃▄▅▄▂▃▄▂▃▄▄▂▂▂▂▁▂▂▃▁
wandb:           train/avg_f1 ▅▆▇▇▆▆█▆▆▆▇▆█▇▇▆▇▆▅▅▃▃▄▅▃▄▃▆▃▃▂▄▂▂▂▂▂▂▁▃
wandb:      train/ensemble_f1 ▆▆▇▇▇▆▇█▅▇▇▇▅▅▇▇▆▆▅▇▆▅▃▄▅▅▃▄▃▄▄▄▂▂▂▃▂▂▁▄
wandb:         train/mil_loss ▆▆█▆▆▅▇████▄▅▆▅▅▆▅▅▅▄▄▄▄▅▅▅▄▇▇▆▅▅▂▄▃▃▃▄▁
wandb:      train/policy_loss ▅▃▅▃▃▃▃▃▃▃▃▃▆▃▃▄▃▇▃▃▃▃▃▃▆▃▁▃▃▃█▃▃▃▁▃▄▃▃▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅█▃▃▃▃▃▂▃▃▃▅▃▃▃▃▆▃▃▆▃▃▃▃▃▃▅▃▃▅▃▃▂▃▁▃▃▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93715
wandb: best/eval_avg_mil_loss 0.25596
wandb:  best/eval_ensemble_f1 0.93715
wandb:            eval/avg_f1 0.84309
wandb:      eval/avg_mil_loss 0.38586
wandb:       eval/ensemble_f1 0.84309
wandb:           train/avg_f1 0.86844
wandb:      train/ensemble_f1 0.86844
wandb:         train/mil_loss 3.39551
wandb:      train/policy_loss 0.10606
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.10606
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wild-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zx4sise4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073645-zx4sise4/logs
wandb: ERROR Run zx4sise4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: une9qnob with config:
wandb: 	actor_learning_rate: 0.0005822823892568788
wandb: 	attention_dropout_p: 0.2139476862207863
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0938699251046038
wandb: 	temperature: 1.59265877903082
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073854-une9qnob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/une9qnob
wandb: uploading wandb-summary.json
wandb: uploading history steps 76-91, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃█
wandb: best/eval_avg_mil_loss █▃▇▁
wandb:  best/eval_ensemble_f1 ▁▃▃█
wandb:            eval/avg_f1 ▂▆▇▇▆▅▃▄▄█▅▄▆▆▇▅▆▇▃▂▅▇▂█▄▅▁▆▆▄▅▅▅▁▃▆▆▂▅▂
wandb:      eval/avg_mil_loss ▇▅▄▄▇▄▄▆▄▃▇▆▃█▆▁▅▃▅▆█▅▄▄▄▂▅▆▅▆▇▆▆▁▅▇▄▄█▄
wandb:       eval/ensemble_f1 ▁▃▃█▅▂▄▄▃▄▂▄▄▆▅▆▄▂▅▇▃▅▇▇▆▄▄▃▅▅▅▇▄▃▄▅▃▅▆▁
wandb:           train/avg_f1 ▆▄▄▇▄▃▃▅▄▄▅▆█▆▄▄▄▃▆▆▃█▃▄▃▇▆▅▆▄▇▇▄▆▇█▄▆▅▁
wandb:      train/ensemble_f1 ▆▃▁▂▂▄▃▃▅▇▃▅▃▄▄▆▄▂▇▃▄▅▂█▆▁▅▅▅▄▄▇▇▃▅▇▃▆▄▄
wandb:         train/mil_loss ▆▇▇█▃▅▄▆▇▂█▆▃▅▆▅▆▃▄▅▃▁▃▄▅▂▅▅▂▄▄▂▁▅▄▅▃▄▂▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▃▆█▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.29657
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.91482
wandb:      eval/avg_mil_loss 0.27085
wandb:       eval/ensemble_f1 0.91482
wandb:           train/avg_f1 0.89111
wandb:      train/ensemble_f1 0.89111
wandb:         train/mil_loss 2.87322
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run radiant-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/une9qnob
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073854-une9qnob/logs
wandb: ERROR Run une9qnob errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9ny5ckme with config:
wandb: 	actor_learning_rate: 5.4915481843704854e-05
wandb: 	attention_dropout_p: 0.20016079465637215
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 162
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9226202545168436
wandb: 	temperature: 7.239556365785123
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074012-9ny5ckme
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9ny5ckme
wandb: uploading history steps 123-130, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss █▁▄▄
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 █▆▃▇▇▇▅▇▄▂▇▆█▄█▄▃▆▆▄▄▅▄▂▆▁▄▄▄▄▅█▃▄▅▇▄▅▃▂
wandb:      eval/avg_mil_loss ▂▃▃▃▃▁▂▂▆▆█▆▄▄▆▁▅▂▃▁▃▄▆▅▄▂▄▆▄▅▄▄▄▂▂▃▄█▃▃
wandb:       eval/ensemble_f1 ▇▅█▆▃▆█▅▅▂▄▇▅▅▆▄▆▆▇▃▄▅▅▄▂▁▄▅▄▂▅▇▄▄▄▅▃▇▃▂
wandb:           train/avg_f1 █▆█▆▆▂▇▅▆█▆▅▆▅▇▅▇▂▅▄▅▆▅▄▄▄▂▆▂▂▁▃▆▁▄▄▄▃▂▃
wandb:      train/ensemble_f1 █▆▇▇▇▇▆▆▇▅▅▆▅▆▆▆▄▄▅▅▄▄▄▂▄▅▂▃▂█▄▅▆▅▄▄▃▃▁▃
wandb:         train/mil_loss ▆█▂▂▅▅▅█▄▄▄▄▄▄▄▃▅▅▄▃▄▅▄▃▁▂▄▄▆▂▄▃▆▅▆▂▃▅▅▆
wandb:      train/policy_loss ▄▄▄█▆▆▄▄▄▄▄▄▁▄▄▄▄▄▄▃▄▅▄▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▅▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▃▃▃█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃▃▃▃▃▃▁▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92669
wandb: best/eval_avg_mil_loss 0.26788
wandb:  best/eval_ensemble_f1 0.92669
wandb:            eval/avg_f1 0.85762
wandb:      eval/avg_mil_loss 0.40765
wandb:       eval/ensemble_f1 0.85762
wandb:           train/avg_f1 0.87439
wandb:      train/ensemble_f1 0.87439
wandb:         train/mil_loss 0.22613
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dainty-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9ny5ckme
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074012-9ny5ckme/logs
wandb: ERROR Run 9ny5ckme errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9mv58pyv with config:
wandb: 	actor_learning_rate: 3.4223711445639496e-05
wandb: 	attention_dropout_p: 0.2707302691629202
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7428851403894099
wandb: 	temperature: 0.8909713393323426
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074211-9mv58pyv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9mv58pyv
wandb: uploading wandb-summary.json
wandb: uploading history steps 75-91, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅█
wandb: best/eval_avg_mil_loss ▄█▂▁
wandb:  best/eval_ensemble_f1 ▁▃▅█
wandb:            eval/avg_f1 ▅▄▄▄▁▄▆▂▄▂▄▁▃▅▄▃█▂▅▄▄▅▃▂▁▃▅▃▃▂▄▃▂▃▄▃▆▃▂▁
wandb:      eval/avg_mil_loss ▂▂▁▁▂▂▃▁▁▂▂▃▄▁▁▂▃▂▃▁▄▁▂▁▂▂▂▂▂█▃▁▄▂▃▁▁▅▂▂
wandb:       eval/ensemble_f1 ▄▄▅▄▆▇▅▆▃▂▄▅▄▄▂▁▂▄▄▄█▅▃▂▂▂▁▅▄▃▆▁▄▄▄▃▄▆▄▂
wandb:           train/avg_f1 ▄▄▃▄▇▅▄▆▅▅▅▃▃▂▁▃▄▅▆▃▃▄▄▂▇▃▃▃▆▅█▆▅▆▅▅▆▆▅▅
wandb:      train/ensemble_f1 ▆▇█▄▅█▅▆▆▆▆▇▆▃▆▆▃▅▆▄▇▆▆▅▆▅▇▆▄▆▇▆▅▄▆▅▆▁▇▄
wandb:         train/mil_loss ██▆▇▆▇▅▅▆▅▅▅▅▅▄▂▄▅▅▅▄▂▄▃▅▂▂▄▂▄▂▃▁▂▃▃▄▁▃▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▄▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91135
wandb: best/eval_avg_mil_loss 0.30801
wandb:  best/eval_ensemble_f1 0.91135
wandb:            eval/avg_f1 0.85582
wandb:      eval/avg_mil_loss 0.43071
wandb:       eval/ensemble_f1 0.85582
wandb:           train/avg_f1 0.86527
wandb:      train/ensemble_f1 0.86527
wandb:         train/mil_loss 0.24551
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9mv58pyv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074211-9mv58pyv/logs
wandb: ERROR Run 9mv58pyv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: b1wrpl9k with config:
wandb: 	actor_learning_rate: 2.5105516646672105e-06
wandb: 	attention_dropout_p: 0.0773249199725522
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 108
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3398966477176455
wandb: 	temperature: 8.662717657235309
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074333-b1wrpl9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/b1wrpl9k
wandb: uploading wandb-summary.json
wandb: uploading history steps 94-109, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇█
wandb: best/eval_avg_mil_loss █▆▂▄▁
wandb:  best/eval_ensemble_f1 ▁▄▇▇█
wandb:            eval/avg_f1 ▇▇▅▅▆▂▅▅█▄▅▁▂█▅▄█▄▅▄▃▅▇▇▇▃▃▄▇▄▆▃▆▄▄▂▇▂▃▅
wandb:      eval/avg_mil_loss ▆▄▃█▄▂▄▄▃▃▅▃▄▃▇▄▃▃▃█▁▅▆▂▄▄▄▆▄▆▃▆▃▁▃▇▅▅▅▃
wandb:       eval/ensemble_f1 ▆▆▄█▅▆▅▁▆▃▄▇▄▃▇▄▅▅▅▄▃▇▆▆▃▃▁▄▅▅▃▆▇▅█▂▅▃▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▅▄▄▇▅▃▃▁▆▁▂▄▅▅▂▅▃▅▃▄▃▅▁▂▅▅▅▃▄▄▃▂▄▃▆█▄▄
wandb:      train/ensemble_f1 ▃▅▆▅▇▄▃▄▄▅▅▃▃▄▂▆▄▅▃▆▄▄▄▅▄▄▅▃▄▅▃▄▁▆▂▄▃█▅▅
wandb:         train/mil_loss ▆▇▂▁▆▄▄▅█▅▁▃▄▃▅█▇▁▂▄▃▅▄▅▄▃▄▂▄▃▅▅▆▄▆▂▅▅▃▁
wandb:      train/policy_loss ██████████████████████▁█████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████▁█████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92644
wandb: best/eval_avg_mil_loss 0.20072
wandb:  best/eval_ensemble_f1 0.92644
wandb:            eval/avg_f1 0.89354
wandb:      eval/avg_mil_loss 0.29762
wandb:       eval/ensemble_f1 0.89354
wandb:            test/avg_f1 0.88704
wandb:      test/avg_mil_loss 0.18762
wandb:       test/ensemble_f1 0.88704
wandb:           train/avg_f1 0.88493
wandb:      train/ensemble_f1 0.88493
wandb:         train/mil_loss 1.99606
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run twilight-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/b1wrpl9k
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074333-b1wrpl9k/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ma49iy7w with config:
wandb: 	actor_learning_rate: 1.0129717410828038e-06
wandb: 	attention_dropout_p: 0.22233680520384613
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 154
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5216350944642097
wandb: 	temperature: 5.49691210691188
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074528-ma49iy7w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ma49iy7w
wandb: uploading history steps 113-122, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▆█
wandb: best/eval_avg_mil_loss █▃▄▁▆
wandb:  best/eval_ensemble_f1 ▁▃▅▆█
wandb:            eval/avg_f1 █▅▅▅█▆▇▄▃▅▅▇▅▄▃▃▄▃▃▄▃▂▅▄▂▃▂▃▂▄▂▂▃▄▄▂▂▂▃▁
wandb:      eval/avg_mil_loss ▄▄▃█▃▁▃▃▄▅▄▄▅▇▄▆▄█▇▇▅▇▇▆▆▆▆█▅▅▅█▇▆▅▅▄▆▇▇
wandb:       eval/ensemble_f1 ▇█▅▆▇▇▅▆▇█▆▇▇▄▆▅▃▄▆▄▆▃▄▂▄▄▄▁▅▄▂▁▁▄▂▃▄▁▃▂
wandb:           train/avg_f1 █▇▇▇▇▇▇▅▇▆▅▅▅▅▅▄▄▅▅▄▅▃▄▃▄▃▃▂▂▃▃▂▃▁▂▂▁▂▁▁
wandb:      train/ensemble_f1 ▇████▆▇▇▇▇▇▅▅▆▅▄▄▃▄▄▄▂▃▂▂▂▂▁▃▂▃▁▃▂▁▂▂▂▃▁
wandb:         train/mil_loss █▇▆▅▆▆▇▄▆▅▄▃▃▃▃▄▃▄▂▄▃▂▃▃▁▃▂▃▄▂▁▂▄▃▂▁▂▂▃▂
wandb:      train/policy_loss ▄▄▄▄▇▆▁▄▆▂▄▃▄██▅▄▃▆▄▄▂▄▄▄▄▄▄▄▄▄▅▄▆▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▁▃▃▃▅▆▃▃▆▃▃▃▅█▄▃▃▃▃▃▃▄█▃▃▃▃▃▃▄▃▃▅▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91909
wandb: best/eval_avg_mil_loss 0.29756
wandb:  best/eval_ensemble_f1 0.91909
wandb:            eval/avg_f1 0.82473
wandb:      eval/avg_mil_loss 0.42887
wandb:       eval/ensemble_f1 0.82473
wandb:           train/avg_f1 0.81369
wandb:      train/ensemble_f1 0.81369
wandb:         train/mil_loss 0.17194
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run winter-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ma49iy7w
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074528-ma49iy7w/logs
wandb: ERROR Run ma49iy7w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: zjni8k2u with config:
wandb: 	actor_learning_rate: 1.8360880672692e-05
wandb: 	attention_dropout_p: 0.3841703588112641
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 133
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3559233546546765
wandb: 	temperature: 9.214791006264974
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074742-zjni8k2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zjni8k2u
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆██
wandb: best/eval_avg_mil_loss █▃▁▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆██
wandb:            eval/avg_f1 ▂▅▁█▆▆▅▅▄█▅▇▆▅▂▅▇▄▆▅▅▃▆▄▅▃▄▅▅▅▄▅▂▅▂▆▄▇▂▅
wandb:      eval/avg_mil_loss █▁▂▂▃▂▂▃▂▁▃▄▁▃▂▂▁▅▂▁▂▂▂▃▂▁▃▆▂▂▁▂▂▂▂▄▄▇▄▃
wandb:       eval/ensemble_f1 ▂▆▇▆▇▅▅█▄▆▅▅▂▅▄▃▄▆▆▅▆▂▅▆▇▅▅▆▁▅▁▃▅▆▄▂█▃▁▂
wandb:           train/avg_f1 ▅▄█▇▆▆▃▆▆▆▆▆▅▄▄▃▄▄▅▆▅▄▄▆▄▃▇▄▅▁▄▄▄▄▄▄▂▄▃▄
wandb:      train/ensemble_f1 ▆▅▅▇▄▆▆▆▇▆▄▆▆▇▆▅▅▄█▆▅▇▅▆▅▂▃▅▁▄▄▆▅▄▆▄▃▃▆▄
wandb:         train/mil_loss ▆█▆▅▄▅▅▃▂▄▄▆▄▄▅▅▄▃▄▃▄▄▃▅▅▂▃▂▄▄▂▁▂▁▂▃▁▂▂▂
wandb:      train/policy_loss ▅▅▅▅▅▃▅▅▅▅▅▅▁▅▅▅▅▅▄▅▅▅▅▅▅▅▅▇▇▅▅▅▅▅▅█▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████▁█████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90742
wandb: best/eval_avg_mil_loss 0.32951
wandb:  best/eval_ensemble_f1 0.90742
wandb:            eval/avg_f1 0.86836
wandb:      eval/avg_mil_loss 0.50663
wandb:       eval/ensemble_f1 0.86836
wandb:           train/avg_f1 0.85571
wandb:      train/ensemble_f1 0.85571
wandb:         train/mil_loss 0.30158
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run likely-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zjni8k2u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074742-zjni8k2u/logs
wandb: ERROR Run zjni8k2u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1rnxe956 with config:
wandb: 	actor_learning_rate: 1.0265087510324368e-06
wandb: 	attention_dropout_p: 0.1515235263317119
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 63
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.45166847152659995
wandb: 	temperature: 5.418478471437767
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074936-1rnxe956
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1rnxe956
wandb: uploading history steps 52-63, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅█
wandb: best/eval_avg_mil_loss ▄▁▁█
wandb:  best/eval_ensemble_f1 ▁▃▅█
wandb:            eval/avg_f1 ▆▄▆▆▃█▃█▅▄▄▂▃▃▃▂▄▂▇▅▃▂▃▅▅▃▅▄▇█▆▅▅█▆▂▄▁▆▅
wandb:      eval/avg_mil_loss ▄▂▄▄▄▃█▅▅▅▄▄▄▆▄▃▅▄▃▄▄▄▅▂▅▂▄▃▃▃▃▄▅▅▂▄▄▅▁▄
wandb:       eval/ensemble_f1 ▅▄▆▄▂▆▃█▄▃▅▅▄▃▅▄▂▇▅▂▂▃▅▅▅▅▄▄▇▅▆▄▅▅█▁▅▄▇▄
wandb:           train/avg_f1 ▄▁▃▃▄▃▅▆▄▆▅▃▁▆▅▆▃▇▆█▃▅▅▇▇▅▄▇▇▄▅▅▆▆▆▃██▄▆
wandb:      train/ensemble_f1 ▄▃▄▄▄▆▁▆▅▆▅▃▁▆▅▆▄▆█▅▅▅▅▃▇▅▅▄▇▇▅▅▅▅▇▆▇▃█▅
wandb:         train/mil_loss ▃▅▃▆▆▃▄█▃▅▄▆▄▁▄▅▅▃▅▃▄▃▆▂▆▆▃▄▅▄▆▃▃▂▃▄▃▃▇▁
wandb:      train/policy_loss ▅▅▅▅▂▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▅▄▅▁▅▅▅▅█▅▅▅▅▄▅▅▅▅▅▅▅▄▅▅▅▆▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91556
wandb: best/eval_avg_mil_loss 0.35772
wandb:  best/eval_ensemble_f1 0.91556
wandb:            eval/avg_f1 0.8888
wandb:      eval/avg_mil_loss 0.29413
wandb:       eval/ensemble_f1 0.8888
wandb:           train/avg_f1 0.89041
wandb:      train/ensemble_f1 0.89041
wandb:         train/mil_loss 3.93205
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cool-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1rnxe956
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074936-1rnxe956/logs
wandb: ERROR Run 1rnxe956 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 6hrdjcq0 with config:
wandb: 	actor_learning_rate: 0.00024159212154294333
wandb: 	attention_dropout_p: 0.3105003500734307
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 62
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7347681583429799
wandb: 	temperature: 9.049495612147725
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075038-6hrdjcq0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6hrdjcq0
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆█
wandb: best/eval_avg_mil_loss ▄▄█▁▅
wandb:  best/eval_ensemble_f1 ▁▂▃▆█
wandb:            eval/avg_f1 ▄▃▅▇▃█▆▂▅▅▅█▃▅▅▆▆▃▅▅▆▃▄▃▂▁▂▆▄▅█▆▅▆▂▃▃▃▅▃
wandb:      eval/avg_mil_loss ▂▂▂▁▂▁▂▂▂▁▂▁▁▂▂▁▂▂▂▂▃▁▂▁▁▃▂▁▂▂▁▁▂▁█▁▃▂▃▂
wandb:       eval/ensemble_f1 ▄▃▅▇▃▆▂▅▅▆▇▆█▃▅▆▇▆▆▆▅▅▃▄▃▆▃▁▂▅▄█▆▅▂▃▆▅▃▂
wandb:           train/avg_f1 █▄▅▅▅▅▃▃▅▂▆▅▄▅▄▁▅▂▅▅▂▅▄▁▄▄▃▂▂▃▄▁▃▂▂▄▄▂▄▅
wandb:      train/ensemble_f1 █▄▅▆▄▆▅▆▅▆▆▆▃▃▆▆▃▄▆▆▆▃▅▄▄▄▄▃▄▅▅▅▃▅▄▃▅▄▅▁
wandb:         train/mil_loss ▆▇█▅▇▆▇▆█▄▄▄▆▄▆▂▄▄▅▇▅▅▇▃▄▄▇▆▃▃▂▄▄▇▁▄▅▅▃▄
wandb:      train/policy_loss ▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▁▃▃▃▃▃▃▃▃▅▃█▃█▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9338
wandb: best/eval_avg_mil_loss 0.28648
wandb:  best/eval_ensemble_f1 0.9338
wandb:            eval/avg_f1 0.87898
wandb:      eval/avg_mil_loss 0.3112
wandb:       eval/ensemble_f1 0.87898
wandb:           train/avg_f1 0.86711
wandb:      train/ensemble_f1 0.86711
wandb:         train/mil_loss 0.22222
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run comfy-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6hrdjcq0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075038-6hrdjcq0/logs
wandb: ERROR Run 6hrdjcq0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: c49m8h3g with config:
wandb: 	actor_learning_rate: 5.163446152079088e-06
wandb: 	attention_dropout_p: 0.14364625633252937
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 96
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9105618952559122
wandb: 	temperature: 6.182958862244904
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075135-c49m8h3g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/c49m8h3g
wandb: uploading history steps 87-97, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▆▇█
wandb: best/eval_avg_mil_loss ▇█▅▅▄▄▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆▆▇█
wandb:            eval/avg_f1 ▄▆█▇▆█▇▆▅▇▅▃▇▄▄▂▆▇█▄▄▇▂▅▆▆▂▄▄▇▂▁▅▇▇▃▅▆█▄
wandb:      eval/avg_mil_loss ▅▄▅▅▁▅▅▂▄▆▅█▅▆▄▄▄▆▅▄▆▆▆▄█▄▅▄▅▄▆▂▅▆█▅▆▇▄▄
wandb:       eval/ensemble_f1 ▄▁▄▇▆▆█▃▄▇▆▅▇▅▅▆▄▄▂▄▅▄▄▃█▆▆▅▅▂█▆▆▆▂▆▅▆▄▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅█▆▃▅█▄▄▄▄▄▅▆▁▇▁▄▆▆▄▄▄▁▄▅▅▃▆▅▄▄▃▅▆▂▄▂▆▅▁
wandb:      train/ensemble_f1 ▄▄██▂▃▆▄▄▆▂▄▃▅▆▇▆▅▆▄▄▆▃▂▄▄▃▄▅▃▆▂▃▁▂▇▆▅▄▄
wandb:         train/mil_loss ▄▄▇▅▅▅▅▄▅▇▆█▅▅▆▆▇▇▃▆▇▄▃▄█▄▇▄▅▇▆▆█▁▅▅▆▇▅▄
wandb:      train/policy_loss ▃▃▃▃▃▃▃▅▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁█▁▁▄▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93388
wandb: best/eval_avg_mil_loss 0.20448
wandb:  best/eval_ensemble_f1 0.93388
wandb:            eval/avg_f1 0.89726
wandb:      eval/avg_mil_loss 0.25635
wandb:       eval/ensemble_f1 0.89726
wandb:            test/avg_f1 0.87982
wandb:      test/avg_mil_loss 0.23937
wandb:       test/ensemble_f1 0.87982
wandb:           train/avg_f1 0.89355
wandb:      train/ensemble_f1 0.89355
wandb:         train/mil_loss 0.21925
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run expert-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/c49m8h3g
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075135-c49m8h3g/logs
wandb: Agent Starting Run: u7agdamg with config:
wandb: 	actor_learning_rate: 1.1959183244652544e-05
wandb: 	attention_dropout_p: 0.04780036694713197
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 171
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7595650062012105
wandb: 	temperature: 7.8717581850388765
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075309-u7agdamg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/u7agdamg
wandb: uploading wandb-summary.json
wandb: uploading history steps 164-171, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▆█
wandb: best/eval_avg_mil_loss █▂▁▃
wandb:  best/eval_ensemble_f1 ▁▂▆█
wandb:            eval/avg_f1 ▆▃▆▅▃▄▄▄▄▇▃▆▅█▃▆▆▄▅▆▆▅▄▄▆▄▄█▆▅▁▄▇▃▆▁▄▃▄▅
wandb:      eval/avg_mil_loss ▄▂▃▂▂▄▃▂▃▄▂▂▄▄▁▄▃▃▃▆▄▂▂▂▃▂▂▂▂▁▄▄▃█▃▃▅▂▂▂
wandb:       eval/ensemble_f1 ▅▅▁▃▃▆▂▅▃▃▆▂▄▃▄▂▄▃▄▃▄▂▆▂▆▃▂▄▆▃▅▁█▅▃▁▅▂▃▃
wandb:           train/avg_f1 ▁█▄▂▄▄▃▆▄▄▄▇▄▇▆▆▅▁▆▇▇▆▃▃▇▆▄▄█▇▇▆▆▆▅▅▆▇▄▃
wandb:      train/ensemble_f1 ▅▅▇▅▂▆▂▆▂▂▅▄▄▃▅▁▄▆▄▅▆▇▄▄▃▄▅▃▄▆▁▃▂▄▆▅█▄▇▆
wandb:         train/mil_loss ▁▂▄██▄▅▆▃▅▄▂▃▄▄▂▅▅▃▅▅▇▂▄▃▅▆▅▆▂▆▆▃▅▄▄▃▃▅▃
wandb:      train/policy_loss ▄▄▁▄▄█▄▄▁▁▄▄▄▄█▄▄█▄▄▄▄▄▄▄▁▄█▄▄▄▄▄▄▄▄▄▄▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄█▄██▄▄▄█▄█▄▄▄█▄██▄▄▄█▄██▁▁▄█▁▄▄▄█▄▄▄▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93371
wandb: best/eval_avg_mil_loss 0.30572
wandb:  best/eval_ensemble_f1 0.93371
wandb:            eval/avg_f1 0.88619
wandb:      eval/avg_mil_loss 0.255
wandb:       eval/ensemble_f1 0.88619
wandb:           train/avg_f1 0.88389
wandb:      train/ensemble_f1 0.88389
wandb:         train/mil_loss 0.23624
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run unique-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/u7agdamg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075309-u7agdamg/logs
wandb: ERROR Run u7agdamg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: l327x5nz with config:
wandb: 	actor_learning_rate: 3.4889928334881583e-06
wandb: 	attention_dropout_p: 0.08102055437813377
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6942688807539016
wandb: 	temperature: 5.8690513955521
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075539-l327x5nz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l327x5nz
wandb: uploading history steps 99-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▅▄▇▆▆▇▂▄▇█▇▅▆▅▆▅▆▄▄▄█▂▄▅▆▃▃▂▄▃▄▅▁▅▂▅▃▄▂▄
wandb:      eval/avg_mil_loss ▃▃█▃▁▃▄▃▂▁▃▄▅▂▃▂▄▆▃▄▂▅▂▃▆▄▄▃▃▆▃▅█▁▄▃▃▄▃▃
wandb:       eval/ensemble_f1 ▇▄▆▆▂▄▆▇█▇▄▆▆▄▂▆▂▆▄▆▇█▄▆▅▃▇▁▄▃▄▆▃▄▅▁▆▅▃▃
wandb:           train/avg_f1 ▅▄▇▇▇█▆▆▆█▆▆▄▄▄▅▆▃▂▅▃▄▄▄▃▂▆▅▃▃▂▃▃▅▄▅▄▃▃▁
wandb:      train/ensemble_f1 ▅▅██▆▆██▇▅▆▇▅▆▆▆▇█▇▇▃▅▅▄▆▆▅▃▁▄▅▄▅▆▂▅▁▂▄▃
wandb:         train/mil_loss ▆▅▅▇▅▆▆▅▅█▆▄▅▆▅▅▆▅▅▇▄▅▅▃▄▆▅▄▅▃▄▄▂▄▄▄▁▃▂▃
wandb:      train/policy_loss ▁█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▇▄▄▄▄▄▄▃▅▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.89602
wandb: best/eval_avg_mil_loss 0.32868
wandb:  best/eval_ensemble_f1 0.89602
wandb:            eval/avg_f1 0.84765
wandb:      eval/avg_mil_loss 0.39467
wandb:       eval/ensemble_f1 0.84765
wandb:           train/avg_f1 0.8349
wandb:      train/ensemble_f1 0.8349
wandb:         train/mil_loss 1.75371
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run absurd-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l327x5nz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075539-l327x5nz/logs
wandb: ERROR Run l327x5nz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: csckbe4v with config:
wandb: 	actor_learning_rate: 1.0231043846545432e-05
wandb: 	attention_dropout_p: 0.1375362705760509
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 104
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.03699301814173939
wandb: 	temperature: 2.3634283025282965
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075748-csckbe4v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/csckbe4v
wandb: uploading history steps 100-104, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▆█
wandb: best/eval_avg_mil_loss █▅▂▅▁▂
wandb:  best/eval_ensemble_f1 ▁▄▄▅▆█
wandb:            eval/avg_f1 ▃▄▇▇▄█▇█▆▅▅▇█▄▆▃▅▇▅▃▄▄▆▂▂▂▇▅▁▃▄█▇▂▁▂▅▅▄▅
wandb:      eval/avg_mil_loss ▃▂█▂▁▂▂▃▁▂▂▃▂▂▂▃▃▃▂▃▃▄▃▄▃▂▃▃▁▂▅▄▂▄▂▃▃▂▃▃
wandb:       eval/ensemble_f1 ▃▅▇▃▆▇▂▆▅▆▃▆▄▆▃▂▄▇▆▄▁▅▂▂▆▃▆▅█▃▃▃▁▄▁▅▄▅▃▅
wandb:           train/avg_f1 ▅▅▅▆▇▇▃█▆▄▃▄▆▅▅▂▃▃▃▅▄▄▄▂▄▄▅▁▁▁▄▃▃▄▃▅▅▃▁▂
wandb:      train/ensemble_f1 ▆▅▆▇▆▆▆▆▅▃▆█▄▅▄▇▃▂▄▃▆▃▆▂▄▅▅▅▃▁▂▃▂▃▄▅▂▁▄▂
wandb:         train/mil_loss ▆▆▆▆▅▇▅▄▇▆▇▆▆▆▆▅▄▇▁▄▆▅▅▅▄▅▇▄█▃▅█▅▅▄▃▅▄▆▇
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▅▃▄▄▄▃▄▆▄▄▄▄▄▁▄▄▄▆▄▄▄▄▄▄▄█▄▃▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂█▂▂▂▂▂▁▂▅▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93025
wandb: best/eval_avg_mil_loss 0.21926
wandb:  best/eval_ensemble_f1 0.93025
wandb:            eval/avg_f1 0.89392
wandb:      eval/avg_mil_loss 0.36661
wandb:       eval/ensemble_f1 0.89392
wandb:           train/avg_f1 0.87333
wandb:      train/ensemble_f1 0.87333
wandb:         train/mil_loss 0.29462
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wobbly-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/csckbe4v
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075748-csckbe4v/logs
wandb: ERROR Run csckbe4v errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9mslo9yi with config:
wandb: 	actor_learning_rate: 4.769084477296948e-06
wandb: 	attention_dropout_p: 0.4277505181782555
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6234425573274751
wandb: 	temperature: 5.663839814677289
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075936-9mslo9yi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9mslo9yi
wandb: uploading history steps 101-115, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss ▃▂█▁
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ▆▅▆▆█▂▇█▅█▆▇▆▆▅▇▄▄▇▅█▄▆▇▇▇▅▅▃▆▅▄▅▃▆▂▄▁▇▁
wandb:      eval/avg_mil_loss ▂▃▄▁▄▂▃▂▄▁▃▄▃▄▂▁▂▄▃▄▃▄▄▂▂▃▃▃▄▇▆▄▃▂▆▄▅█▄▂
wandb:       eval/ensemble_f1 ▇▅▆▄█▆▇▅▃▆▆▅▇▅▆▃▅▇▅▂▇▃▇▁▅▅▄▅▂▄▂▆▃▂▁▃▆▆▅▃
wandb:           train/avg_f1 ▅▆█▆▄▅▅▃▄▄▇█▇▅▆▆▆▄▆▂▄▆▄▆▄▄▂▄▃▅▃▂▄▄▅▄▁▃▃▄
wandb:      train/ensemble_f1 ▆▆██▆█▅▆▅▆▆▄▅▅▅▅▆▄▃▄▄▆▅▅▄▄▃▆▂▄▅▁▃▃▄▄▄▄▅▄
wandb:         train/mil_loss ▆▄▆▆▃▆▅▆▆▇▆▅██▆▄▅▇▆▇▃▆▂▅▄▄▇▄▄▄▆▅▃▄▆▃▃▅▁▅
wandb:      train/policy_loss ▂▂▂▂▂▂▄▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▅▄▃▂▂▂▂▂▂▂▄█▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.83342
wandb: best/eval_avg_mil_loss 0.66588
wandb:  best/eval_ensemble_f1 0.83342
wandb:            eval/avg_f1 0.71405
wandb:      eval/avg_mil_loss 1.21735
wandb:       eval/ensemble_f1 0.71405
wandb:           train/avg_f1 0.76438
wandb:      train/ensemble_f1 0.76438
wandb:         train/mil_loss 2.71517
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sweepy-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9mslo9yi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075936-9mslo9yi/logs
wandb: ERROR Run 9mslo9yi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: a1uuqwkr with config:
wandb: 	actor_learning_rate: 2.241385889543854e-06
wandb: 	attention_dropout_p: 0.18330529962029157
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 80
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8448712984326538
wandb: 	temperature: 5.712022849702002
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080126-a1uuqwkr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a1uuqwkr
wandb: uploading wandb-summary.json; uploading config.yaml; uploading history steps 65-80, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▄▅▅▇▇▇▇█
wandb: best/eval_avg_mil_loss ▅▆▅▂▁▁▁█▃▂▃
wandb:  best/eval_ensemble_f1 ▁▂▃▄▅▅▇▇▇▇█
wandb:            eval/avg_f1 ▁▅▅▁▆▆▄▇▄█▇▅▇██▅▅▅▅▇▆▄▅▇█▅▇▇▅▇▆▅▆▅▇▄▇▅▇▇
wandb:      eval/avg_mil_loss ▄▅▄▅▁▃▁▅▄▃▃▂▃▂▄▄▇▂▃▄▂▃▂▄▇▁█▆▅▄▃▂▂▄▁▆▃▃▃▂
wandb:       eval/ensemble_f1 ▄▁▅▅▁▄▆▄▇█▄▅▆▅▅▇▃▇▅▄▇█▄▅▆▇▅▇▇▅▅▇▇▇▅▇▇▆▄▇
wandb:           train/avg_f1 ▅▃▃▂▆▁▃▄▆▄▃▅▃▆▅▅▄▃▂▅▃▂▁▃▄▄▁▄▆▃▇▂▇▃▃▃▅▃█▄
wandb:      train/ensemble_f1 ▅▆▄▇▃▃▂▄▅▇▆▄▇▆▅▆▅▃▄▃▂▄▅▁▅▇▄▄▄██▇▄▄▅▄▄▅▆▅
wandb:         train/mil_loss █▇▃█▄▂▂▃▃▃▅▃▆▃▄▁▁▂▄▂▄▆▅▄▁▄▄▁▄▁▂▂▃▂▄▂▃▂▂▁
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92236
wandb: best/eval_avg_mil_loss 0.26182
wandb:  best/eval_ensemble_f1 0.92236
wandb:            eval/avg_f1 0.91161
wandb:      eval/avg_mil_loss 0.23895
wandb:       eval/ensemble_f1 0.91161
wandb:           train/avg_f1 0.88902
wandb:      train/ensemble_f1 0.88902
wandb:         train/mil_loss 0.53096
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run crimson-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a1uuqwkr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080126-a1uuqwkr/logs
wandb: ERROR Run a1uuqwkr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4lj4vnol with config:
wandb: 	actor_learning_rate: 0.00011288051337670984
wandb: 	attention_dropout_p: 0.4399087243074741
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9488872545836176
wandb: 	temperature: 7.1835467324155475
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080257-4lj4vnol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4lj4vnol
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 105-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅█
wandb: best/eval_avg_mil_loss █▄▄▁
wandb:  best/eval_ensemble_f1 ▁▄▅█
wandb:            eval/avg_f1 █▄▅▆▇▆▄▄▅▄▆▆▃▅▄▅▅▃▅▃▄▆▃▆▆▅▆▄▄▄▃▅▆▄▅▅▆▃▁▇
wandb:      eval/avg_mil_loss ▂▂▂▁▃▁▄▃▄▃▅▃▃▂█▃▃▆▄▃▅▄▅▆▄▂▂▄▄▃▃▃▄▃▄▃▅▄▅▃
wandb:       eval/ensemble_f1 ▅▄█▆▆▆▆█▄▄▅▄▄▄▃▃▄▄▄▄▃▆▅▅▄▅▄▄▅▃▄▃▄▄▅▄▃▁▁▆
wandb:           train/avg_f1 █▇██▄█▅▇▅▅▅▆▆▇▆▆▆▃▅▅▄▅▄▃▅▁▇▃▄▅▄▃▄▅▃▃▄▃▅▆
wandb:      train/ensemble_f1 ▆█▇████▄▇▅▅▇▅▇▆▄▄▅▄▅▅▄▇▅▁▄▇▅▄▅▄▅▆▄▅▄▄▅▅▃
wandb:         train/mil_loss ▅██▇▂▆▄▅▃▄▅▃▄▃▂▄▃▆▄▄▃▄▅▅▅▄▄▃▁▅▂▃▄▃▃▃▂▅▄▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▃█▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████▁████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9409
wandb: best/eval_avg_mil_loss 0.19598
wandb:  best/eval_ensemble_f1 0.9409
wandb:            eval/avg_f1 0.91565
wandb:      eval/avg_mil_loss 0.27917
wandb:       eval/ensemble_f1 0.91565
wandb:           train/avg_f1 0.88032
wandb:      train/ensemble_f1 0.88032
wandb:         train/mil_loss 0.20693
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run stellar-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4lj4vnol
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080257-4lj4vnol/logs
wandb: ERROR Run 4lj4vnol errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: duymtic8 with config:
wandb: 	actor_learning_rate: 1.057110108488449e-06
wandb: 	attention_dropout_p: 0.11353752592398592
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 118
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2448840851612648
wandb: 	temperature: 9.835440827727195
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080435-duymtic8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/duymtic8
wandb: uploading wandb-summary.json
wandb: uploading history steps 107-119, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅██
wandb: best/eval_avg_mil_loss █▃▁▂▄
wandb:  best/eval_ensemble_f1 ▁▁▅██
wandb:            eval/avg_f1 ▄▅▅▅▅▆▇▇▆▅▆▄▆▇▃▇▇▄▅▅▆▂▄▇▄▃▆▂▇▆█▃▃▇▄█▄▂▁▄
wandb:      eval/avg_mil_loss ▆▃█▅▅▃▃▅▅▄▄▂▆▂▅▇▃▃▆▆▆▃▃▁▄▃▃▄▂▃▄▃▄▁▃█▃▅▃▅
wandb:       eval/ensemble_f1 ▇▆▅▃▆▇▆▆▄▅▄▆█▅▇█▄▄▇▅▄▄▄▅▇▅▆▁▆▂▄▅▄▅█▇▆▅▂▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆▄▇▇▅▆▃▆▆▆▆▆▅▆▄▆▆▇▆█▅▅▆▅▃▁▁▄▄▃▃▂▃▄▅▅▂▃▃
wandb:      train/ensemble_f1 ▇▇▅▇▆▅▆▇▆▇▆▆▆▇█▆▅▇▇▇▄▄▂▅▄▅▄▄▂▄▄▄▄▅▄▆▁▅▄▃
wandb:         train/mil_loss ▄█▄▃▅▆▂▅▄▆▆▄▂▆▄▃▅▃▃▃▂▂▂▄▂▃▄▃▄▄▁▂▃▄▄▃▃▁▆▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆█▆▆▆▄▆▆▆▆▆▆▆▆█▃▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▅▁▁▁▁▁▁▁▁▁▅▁▆▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93007
wandb: best/eval_avg_mil_loss 0.26254
wandb:  best/eval_ensemble_f1 0.93007
wandb:            eval/avg_f1 0.90088
wandb:      eval/avg_mil_loss 0.22565
wandb:       eval/ensemble_f1 0.90088
wandb:            test/avg_f1 0.88373
wandb:      test/avg_mil_loss 0.20386
wandb:       test/ensemble_f1 0.88373
wandb:           train/avg_f1 0.89314
wandb:      train/ensemble_f1 0.89314
wandb:         train/mil_loss 0.19955
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run spring-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/duymtic8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080435-duymtic8/logs
wandb: Agent Starting Run: 3qe72njo with config:
wandb: 	actor_learning_rate: 2.3722380584229744e-05
wandb: 	attention_dropout_p: 0.3979634680462585
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 122
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.40900176991949544
wandb: 	temperature: 9.007204900136744
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080623-3qe72njo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3qe72njo
wandb: uploading wandb-summary.json
wandb: uploading history steps 106-123, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▅▆██
wandb: best/eval_avg_mil_loss ▄█▂▂▂▁▁
wandb:  best/eval_ensemble_f1 ▁▂▅▅▆██
wandb:            eval/avg_f1 ▃▂▂▁▁▄▃▂▂▆▄▃▂▂▄▆▃▆▅▃▄▃▄▅▄▄▇▄▄▆▅▄▅▅█▅▄▅▅▃
wandb:      eval/avg_mil_loss ▄▄▃▂▂▇▆▂█▂▂▃▅▄▃▂▆▂▂▂▃▄▂▅▂▃▃▃▂▂▃▂▁▆▂▄▃█▄▂
wandb:       eval/ensemble_f1 ▅▃▅▄▂▆▁▅▄▃▆▃█▅▄▄▅▆▅▅▆█▆▄▆▅▇▇▇▆▅▅▇▆█▇▇▇▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▃▄▁▃▄▃▃▃▃▅▄▂▄▃▄▄▄▄▄▅▄▄▆▄▇▄▅▆▅▃▅▆▇█▆▇▇▆█
wandb:      train/ensemble_f1 ▄▃▁▄▁▃▁▃▃▃▃▆▅▃▄▅▄▆▄▅▅▅▅▅▆▆▅▅▅▇▅▅▅█▅██▇▅▆
wandb:         train/mil_loss ▅▄▆▅▆▆▅▇▄▅▄▅▆█▆▄▇▇▇▆▄▆▆▃▆▇▄▄█▃▄▃▁▄▆▄▃▆▆▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▃▆▆▆▆▆▄▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▁▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90116
wandb: best/eval_avg_mil_loss 0.35313
wandb:  best/eval_ensemble_f1 0.90116
wandb:            eval/avg_f1 0.83576
wandb:      eval/avg_mil_loss 0.50874
wandb:       eval/ensemble_f1 0.83576
wandb:            test/avg_f1 0.85472
wandb:      test/avg_mil_loss 0.3201
wandb:       test/ensemble_f1 0.85472
wandb:           train/avg_f1 0.84579
wandb:      train/ensemble_f1 0.84579
wandb:         train/mil_loss 5.43115
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run iconic-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3qe72njo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080623-3qe72njo/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: w8iigotb with config:
wandb: 	actor_learning_rate: 1.547796455533775e-05
wandb: 	attention_dropout_p: 0.29830784362446794
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 105
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.28029420299295227
wandb: 	temperature: 9.44676458713263
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080823-w8iigotb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/w8iigotb
wandb: uploading wandb-summary.json
wandb: uploading history steps 89-105, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▅▇█
wandb: best/eval_avg_mil_loss █▇█▆▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃▅▇█
wandb:            eval/avg_f1 ▄▄▅▄▆▅▅▂▅█▃▅▆▄▅▁▂▃▂▂▆▃▃▃▆▅▁▄▂▄▁▄▅█▃▆▄▃▇▅
wandb:      eval/avg_mil_loss ▄▄▄▅▆▃▃▅▂▇▄▇▁▃▄▅▅█▆█▄▆▃▄▅▂▇▂▄▇▃▄▄▂▅▆▄▁▆▃
wandb:       eval/ensemble_f1 ▄▆▅▇▆▂▆▁▇▅▆▂▁▅█▅▅▃▄▇▇▇▂▂▁▄▄▂▄▅▅▆▇▆▅▃█▁▃▆
wandb:           train/avg_f1 ▇▅▅▅▅▅▆▃▅█▃▄▆▃▄▄▄▆▆▅▄▅▃▄▅▅▄▅▃▆▅▂▅▃▃▄▁▂▃▁
wandb:      train/ensemble_f1 █▆▆▆▆▅▅█▇▃▇▃▄▄▅▆▄▅▅▅▃▇▇▆▄▅▆▃▅▂▄▃▃▃▃▄▂▄▃▁
wandb:         train/mil_loss ▅▇▇▆▇▅█▇▅▄▄▆▅▅▅▆▅▄▃▅▇▄▅▅▅▂▂▅▆▄▆▂▃▅▄▄▄▅▅▁
wandb:      train/policy_loss ▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▃▃█▃▃▃▃▃█▃▃▁▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▄▇▅▁▅▂▅▅▅▆▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅█▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94115
wandb: best/eval_avg_mil_loss 0.18787
wandb:  best/eval_ensemble_f1 0.94115
wandb:            eval/avg_f1 0.91928
wandb:      eval/avg_mil_loss 0.24987
wandb:       eval/ensemble_f1 0.91928
wandb:           train/avg_f1 0.87943
wandb:      train/ensemble_f1 0.87943
wandb:         train/mil_loss 0.19083
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run radiant-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/w8iigotb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080823-w8iigotb/logs
wandb: ERROR Run w8iigotb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 15f4wj3l with config:
wandb: 	actor_learning_rate: 2.9231655203881635e-06
wandb: 	attention_dropout_p: 0.2037673010988607
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 118
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.35656090810204233
wandb: 	temperature: 9.998191227050764
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080956-15f4wj3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/15f4wj3l
wandb: uploading wandb-summary.json
wandb: uploading history steps 107-118, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅▆▇█
wandb: best/eval_avg_mil_loss ▄▂▆▆█▁
wandb:  best/eval_ensemble_f1 ▁▁▅▆▇█
wandb:            eval/avg_f1 ▄▆█▂▅▃▆▅█▃▅▄▄▃▆▅▄▄▂▃▁▂▃▆▃▂▃▅▇▃▇▄▄▁▇▅▅▄▄▆
wandb:      eval/avg_mil_loss ▃▁▆▄█▃▄▁▄▆▆▂▃▃▇▂▅▂▃▁█▆▄▄▁▄▃▂▂▃▄▃▅▄▃▇█▃▂▇
wandb:       eval/ensemble_f1 ▅▄█▆▅▅▇▆█▆▅▄▅▆▄▄▂▆▂▅▃▂▆▇▂▅▅▇▃▃▃▄▄▃▅▄█▄▁▄
wandb:           train/avg_f1 ▆▅▅▆█▅▆▅▁▆▄▃▄▄▄▄▄▄▃▃▅▄▄▄▇▆▃▄▇▂▅▄▄▅▄▂▅▅▄▁
wandb:      train/ensemble_f1 ▇▄▅▄█▅▅▇▇▁▄▆▅▆▃▄▆▇▄▂▅▅▆▅█▅▂▄▃▅▆▆▄▆▅▂▃▆▃▄
wandb:         train/mil_loss ▄▆▆▃▆▇▄▄█▆▄▆▆▄▂▆▇▃▅▇▄▄▄▇▄▂▂▆▆▅▃▁▅▅█▅▃▄▃▁
wandb:      train/policy_loss ▁▅▅▁▁▅▁▅▁▁▅▅▁▅▅█▅▅▅█▅▅▅▁▁▅▅█▅█▅▅▅▅▁▅█▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅█▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93032
wandb: best/eval_avg_mil_loss 0.19229
wandb:  best/eval_ensemble_f1 0.93032
wandb:            eval/avg_f1 0.89384
wandb:      eval/avg_mil_loss 0.37782
wandb:       eval/ensemble_f1 0.89384
wandb:           train/avg_f1 0.88493
wandb:      train/ensemble_f1 0.88493
wandb:         train/mil_loss 0.23017
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run earthy-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/15f4wj3l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080956-15f4wj3l/logs
wandb: ERROR Run 15f4wj3l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xl43sblu with config:
wandb: 	actor_learning_rate: 0.00012154699673827924
wandb: 	attention_dropout_p: 0.34762685568222085
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6993289399847991
wandb: 	temperature: 0.4495953267505514
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081145-xl43sblu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xl43sblu
wandb: uploading history steps 117-130, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇█
wandb: best/eval_avg_mil_loss █▁▂▁▅
wandb:  best/eval_ensemble_f1 ▁▅▆▇█
wandb:            eval/avg_f1 ▅▆▆█▄▄▅▄▅▄▅▆▁▆▂▄▄▆▆▅▅▂▆▅▄▃▆▅▄▃▂▃▇▄▄▅▅▆▆▅
wandb:      eval/avg_mil_loss ▅▂▂▂▃▂▂█▆▂▇▃▂▄▂▁▃▂▃▄▃▃▂▂▂▃▂▁▅▁▄▂▄▃▄▃▄▅▄▄
wandb:       eval/ensemble_f1 ▁▇▃▅▅▄▆▄▅▆▅▄▃▆▅▆▂▃▆█▅▆▅▆▅▅▂▅▆▃▅▃▆▃▄▆▇▆▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▄▆▅▄▄▃▅▃▆▅▅█▄▆▅█▅▅▇▂▅▆▅▇▁▅▅▆▄▄▄▇▅▂▄▅▄▆
wandb:      train/ensemble_f1 ▄▃▄▄▃▆▄▆▃▆▄▆▆▆▄▆▇▇▅▇▆▁▄▅▆▅▄▅▆▄▄▃▃▇█▆▁▄▃▆
wandb:         train/mil_loss ▆▂▇▅▁▃▅▃▄▄▄▅▃▃▄▃▃▂▂▆▅▄▃▂▅▄▃▄██▃▆▅▃▄▄▆▃▃▅
wandb:      train/policy_loss ▃▅▃▃▁▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93351
wandb: best/eval_avg_mil_loss 0.3078
wandb:  best/eval_ensemble_f1 0.93351
wandb:            eval/avg_f1 0.90742
wandb:      eval/avg_mil_loss 0.25013
wandb:       eval/ensemble_f1 0.90742
wandb:            test/avg_f1 0.88211
wandb:      test/avg_mil_loss 0.2288
wandb:       test/ensemble_f1 0.88211
wandb:           train/avg_f1 0.89744
wandb:      train/ensemble_f1 0.89744
wandb:         train/mil_loss 0.2669
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vocal-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xl43sblu
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081145-xl43sblu/logs
wandb: Agent Starting Run: vigpsszk with config:
wandb: 	actor_learning_rate: 3.675573215134836e-05
wandb: 	attention_dropout_p: 0.4824389073847605
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8770409373545949
wandb: 	temperature: 3.725094176454
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081349-vigpsszk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vigpsszk
wandb: uploading history steps 85-94, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄██
wandb: best/eval_avg_mil_loss ▅▁▄█▅
wandb:  best/eval_ensemble_f1 ▁▃▄██
wandb:            eval/avg_f1 ▇▅▆▇▅▆▅▅█▅▅▇▇▇▇▅▆▇▄▄▁▄▆▂▆▆▄▄▄▄▆▄▂▅▄▅▄█▆▂
wandb:      eval/avg_mil_loss ▄▄▄▅▃▄▄▂▄▄▃▃▁▁▄▃▄▁▂▄▃█▅▃▃▄▃▅▂▂▃▂▃▃▄▅▅▄▅▃
wandb:       eval/ensemble_f1 ▆▇▄▆▄▅▄▅▇█▄▅▆▆▃▄▅▆█▃▅▄▃▂▅▆▃▇▆▆▂▄▄▁▂▅▃▆▃▇
wandb:           train/avg_f1 █▆▇▆▅█▅▄▄▆▆▆▃▄▄█▆▆▄▆▄▄▂▅▅▅▁▅▅▄▆▄▁▄▃▃▃▄▃▄
wandb:      train/ensemble_f1 ▄▆▇▅▅▆█▄▅▄▄▆▆▄█▄▂▃▆▆▃▅▂▅▅▆▁▅▂▄▃▆▄▄▄▂▂▃▃▃
wandb:         train/mil_loss █▄▆▅▇▄▄▄▆▅▆▃▂▃▆▅▁▃▅▃▃▄▂▂▂▃▄▅▂▁▃▄▂▃▂▂▁▃▂▁
wandb:      train/policy_loss ▃▄▄▄▄▄▄▄▄▄▄▄▄▂▄▄█▄▂▄▄▄▄▄▁▆▄▄▄▄▃▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▅▇▅▅▅▅▅▅▅▁▅▁▅▅▅▁▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93371
wandb: best/eval_avg_mil_loss 0.26347
wandb:  best/eval_ensemble_f1 0.93371
wandb:            eval/avg_f1 0.87948
wandb:      eval/avg_mil_loss 0.26423
wandb:       eval/ensemble_f1 0.87948
wandb:           train/avg_f1 0.88766
wandb:      train/ensemble_f1 0.88766
wandb:         train/mil_loss 0.21799
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run playful-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vigpsszk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081349-vigpsszk/logs
wandb: ERROR Run vigpsszk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: uyiqmqcs with config:
wandb: 	actor_learning_rate: 6.070773767510106e-06
wandb: 	attention_dropout_p: 0.4326555870067055
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7283024637918067
wandb: 	temperature: 5.745938963917164
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081517-uyiqmqcs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uyiqmqcs
wandb: uploading history steps 100-107, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅▆█
wandb: best/eval_avg_mil_loss █▃▁▂▁
wandb:  best/eval_ensemble_f1 ▁▅▅▆█
wandb:            eval/avg_f1 ▅▇▆▆▅▆▆▇▇▇▇▇▇▃▇▅█▇▇█▇█▂▆▅▆▂▅▅▅▄▅▅▆▅▄▁▄▂▅
wandb:      eval/avg_mil_loss ▆▃▂▃▄▄▄▃▃▂▄▂▃▃▅▄▃▂▃▄▂▃▃▁▄▅▃▃▃▃▄▄▄█▆▄▃▅▅▂
wandb:       eval/ensemble_f1 ▄▆▆▆▆▇▆█▆▅▅▇▆▇▃▆▆▆▆▆▆▅▇▄▅▅▁▄▆▄▂▆▅▆▅▅▅▃▅▄
wandb:           train/avg_f1 ▆▆▅▆▆▆▆▆▅▇▅▄▅▇█▄▆▃▆▄▆▃▆▃▄▂▄▄▅▃▂▃▁▂▃▂▄▄▁▄
wandb:      train/ensemble_f1 ▄▅▅▆▆▇▅▅▆▅▄▅▅▆█▅▅▅▄▄▄▃▄▅▁▄▂▂▂▃▃▁▂▂▃▃▂▂▃▃
wandb:         train/mil_loss ██▄█▇▃▄▆▅▃▆▃▅▁▆▃▃▄▅▄▅▃▃▃▄▃▅▄▆▄▂▂▂▂▃▄▄▅▅▄
wandb:      train/policy_loss █▆▆▆▆▆▃▁▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇█▆▆▇▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▂▃▃▃▁▃▃▃▃▃▁▃█▃▅▃▇▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94108
wandb: best/eval_avg_mil_loss 0.18596
wandb:  best/eval_ensemble_f1 0.94108
wandb:            eval/avg_f1 0.9047
wandb:      eval/avg_mil_loss 0.24855
wandb:       eval/ensemble_f1 0.9047
wandb:           train/avg_f1 0.89132
wandb:      train/ensemble_f1 0.89132
wandb:         train/mil_loss 0.18372
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run firm-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uyiqmqcs
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081517-uyiqmqcs/logs
wandb: ERROR Run uyiqmqcs errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 2s16bi48 with config:
wandb: 	actor_learning_rate: 0.0004009646338173207
wandb: 	attention_dropout_p: 0.4574122657774131
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 163
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.870867891735003
wandb: 	temperature: 8.724381357253563
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081701-2s16bi48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2s16bi48
wandb: uploading history steps 137-153, summary; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▇▇█
wandb: best/eval_avg_mil_loss ▆▄▇▅█▁
wandb:  best/eval_ensemble_f1 ▁▁▄▇▇█
wandb:            eval/avg_f1 ▄▂▃▃▃▅▆▅▅▅█▄▄▆▇▆▅▅▃▅▅▄▃█▅▁▃▃▄▃▁▃▄▃▃▅▅▅▃▃
wandb:      eval/avg_mil_loss ▃█▁▃▅▄▂▇▅▄▅▇▅▇▃▃▃▆▃▆▁▆▄▄▂▆▅▅▃▄▄▃▅▆▃▆▅▂▄▁
wandb:       eval/ensemble_f1 ▄▄▄▅▇█▆▅▇▇▇▅▇▆▇▆▆▆▄▆▆▆▂▆▅▅▃▇▃▆▆▅▁▅▆▆▆▄▅▄
wandb:           train/avg_f1 █▆▇▇▇▇▅▄▆▆▅▆█▇▅▆▆▅▇▆▄▆▆▆▄▆▃▄▅▄▄▆▁▄▄▄▄▅▃▁
wandb:      train/ensemble_f1 █▇▇▅▅▆▅▅▆▇▅▆▇▇▄█▆▃▅▆▃▅▆▅▄▄▆▅▄▃▃▆▄▅▅▄▁▄▅▃
wandb:         train/mil_loss ██▆█▅▆▆▇▇▅▃▅▄▄█▄▆▄▆▆▄▂▄▄▄▁▄▂▂▅▂▄▄▄▄▃▂▂▅▃
wandb:      train/policy_loss ▄▄▄█▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94135
wandb: best/eval_avg_mil_loss 0.14486
wandb:  best/eval_ensemble_f1 0.94135
wandb:            eval/avg_f1 0.91194
wandb:      eval/avg_mil_loss 0.28146
wandb:       eval/ensemble_f1 0.91194
wandb:           train/avg_f1 0.89036
wandb:      train/ensemble_f1 0.89036
wandb:         train/mil_loss 0.18102
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vivid-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2s16bi48
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081701-2s16bi48/logs
wandb: ERROR Run 2s16bi48 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ri2n0kmc with config:
wandb: 	actor_learning_rate: 0.00017161307485062078
wandb: 	attention_dropout_p: 0.14507275646655865
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 199
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.371320916915179
wandb: 	temperature: 2.558158629149474
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081931-ri2n0kmc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ri2n0kmc
wandb: uploading history steps 148-157, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▆▇▇██
wandb: best/eval_avg_mil_loss █▇▆▁▂▁▃▂
wandb:  best/eval_ensemble_f1 ▁▁▄▆▇▇██
wandb:            eval/avg_f1 ▇▆█▆▆█▇▆▆▆▆▇▇█▇▇▇▆▆▇▅▆▆▆▆▆▄▄▅▅▄▄▂▃▄▁▃▃▁▁
wandb:      eval/avg_mil_loss ▁▂▃▁▁▁▁▂▁▁▂▁▂▂▃▃▂▂▃▄▄▄▃▃▃▅▄▃▅▄▅▆▆▅▆▇▆▇█▇
wandb:       eval/ensemble_f1 ▆▇██▆▆▇█▇▆▆▇▇▆█▆▅▆▆▆▄▅▄▆▅▆▅▃▄▅▄▃▂▄▂▂▁▁▁▂
wandb:           train/avg_f1 ▇█▇▇▇▇▇▇██▇▇▇█▇██▇▇▇▇▆▇▇▇▆▅▆▅▅▄▃▄▄▄▂▂▃▁▂
wandb:      train/ensemble_f1 ▇▆▇▇█▆█▇█▇▇▆▆▇▇▇▆▇▆▇▆▇▇▆▆▅▅▅▅▅▄▄▃▃▂▃▄▃▁▁
wandb:         train/mil_loss █▆▆▆▇▇▆▇▆▆▇▅▄▄▅▆▄▄▅▅▄▄▅▃▄▃▄▃▄▃▃▅▃▃▄▄▂▄▁▃
wandb:      train/policy_loss ▅▅▁▅▅▆▅▅▅▅█▅▅▅▅▄█▅▆▅▄▅▅▅▅▅▅▅▅▁▅▅█▅▅▅▅▅▅▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁▅▅▅▃▅▅▇▅▄▅▄▅▅▆▅▅▅▅▅▇▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.25356
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.81332
wandb:      eval/avg_mil_loss 0.56654
wandb:       eval/ensemble_f1 0.81332
wandb:           train/avg_f1 0.79322
wandb:      train/ensemble_f1 0.79322
wandb:         train/mil_loss 1.34021
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run true-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ri2n0kmc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081931-ri2n0kmc/logs
wandb: ERROR Run ri2n0kmc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mvkuwn6d with config:
wandb: 	actor_learning_rate: 2.19970728181698e-05
wandb: 	attention_dropout_p: 0.32400383081889816
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 143
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.43461978262161527
wandb: 	temperature: 6.927469694618679
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082232-mvkuwn6d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mvkuwn6d
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆█
wandb: best/eval_avg_mil_loss ▆▁█▅
wandb:  best/eval_ensemble_f1 ▁▃▆█
wandb:            eval/avg_f1 ▄▆▄█▁▄▅▁▅▅▅▁▄▂▂▄▃▄▆▇▂▁▆▃▇▅▄▃▄▅▆▄▂▅▂▂▃▆▂▃
wandb:      eval/avg_mil_loss ▃▂▃▃▂▃▃▂▁▂▂▅█▄▂▃▃▂▄▂▄▃▄▃▅▆▇▃▃▅▁▂▃▄▄▃▂▃▃▂
wandb:       eval/ensemble_f1 ▄▄▁▄▄██▄▃▅▄▄▁▂▅▄▃▆▆▄▆▃▄▅▇▄▁▃▃▅▄▄▆▃▅▄▄▃▄▄
wandb:           train/avg_f1 ▆▇▅█▃▄▄▅█▆▄▃▆▄▆▃▅▅▄▄▇▆▄▃▂▂▁▂▇▅▁▃▆▄▄▃▆▇▅▆
wandb:      train/ensemble_f1 ▆▅▅█▅▅▆▇▄▅▃▄▆▃▅▅▇▁▄▄▆▆▅▆▇▅▃▆▆▇█▆▄▇▅▄▄▆▇█
wandb:         train/mil_loss ▃▄▆▅▅▂▂▅▆▃▄▂▅▃█▅▂▁▃▁▃▂▄▄▄▃▃▄▃▆▂▅▅▇▃▄▆▅▃▂
wandb:      train/policy_loss ▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▃▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93388
wandb: best/eval_avg_mil_loss 0.26445
wandb:  best/eval_ensemble_f1 0.93388
wandb:            eval/avg_f1 0.90088
wandb:      eval/avg_mil_loss 0.29608
wandb:       eval/ensemble_f1 0.90088
wandb:           train/avg_f1 0.89771
wandb:      train/ensemble_f1 0.89771
wandb:         train/mil_loss 0.2256
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run avid-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mvkuwn6d
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082232-mvkuwn6d/logs
wandb: ERROR Run mvkuwn6d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wtkleszt with config:
wandb: 	actor_learning_rate: 0.00018945463118131363
wandb: 	attention_dropout_p: 0.008273676461495183
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 159
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5854272204874191
wandb: 	temperature: 4.816739250978327
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082415-wtkleszt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wtkleszt
wandb: uploading history steps 111-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂██
wandb: best/eval_avg_mil_loss █▇▁▃▅
wandb:  best/eval_ensemble_f1 ▁▁▂██
wandb:            eval/avg_f1 ▆▆▁▃█▇▅▆▆▇▂▇▅▄▅▅▇▅▆▅▅▇▅▇▄▆▄▅▂▅▄▅▄▅▃▆▄▂▁▅
wandb:      eval/avg_mil_loss ▃▁▄▂▅▄▄▆▇▅▄▅▇▅▃▅▆▃▃▅▃▃▆▁▄▆▂▃▄▅▄▄█▄▄▄▅▅▆▄
wandb:       eval/ensemble_f1 ▇▇▁▄▆▅█▇▆▄▂▃█▅█▆▇▇▅▆▅▃▇▆▇▅▆█▆▅▃▅▃▅▅▄▅▇▄▄
wandb:           train/avg_f1 ▇▅▃▆▄▆█▅▅▅▄▆██▅▆▅▇▇█▄█▇▇▃▆▄▅▅▅▆▅▃▄▄▄▃▃▃▁
wandb:      train/ensemble_f1 ▄▂▆█▄▅▅█▇▆▄▅▅▇▇▅▇▇▇▆▄▆▆▄█▄▃▄▄▅▆▂▁▂▃▄▃▃▂▁
wandb:         train/mil_loss ▇█▇█▆█▄▅▇▆▆▅▄▄▇▅▃▄▄▅▃▅▄▆▅▆▂▄▆▅▆▅▅▃▄▃▁▃▃▂
wandb:      train/policy_loss ▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▇▄▄▄▄▄▄▄▄▄▄▄▄█▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▇▅▃▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92644
wandb: best/eval_avg_mil_loss 0.24032
wandb:  best/eval_ensemble_f1 0.92644
wandb:            eval/avg_f1 0.884
wandb:      eval/avg_mil_loss 0.26756
wandb:       eval/ensemble_f1 0.884
wandb:           train/avg_f1 0.89167
wandb:      train/ensemble_f1 0.89167
wandb:         train/mil_loss 0.82709
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run silver-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wtkleszt
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082415-wtkleszt/logs
wandb: ERROR Run wtkleszt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: q6iuix74 with config:
wandb: 	actor_learning_rate: 3.2856041629421323e-06
wandb: 	attention_dropout_p: 0.4294270113235974
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 177
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3238248168278983
wandb: 	temperature: 7.739655734321127
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082610-q6iuix74
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/q6iuix74
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▇██
wandb: best/eval_avg_mil_loss ▅▆▃█▂▁
wandb:  best/eval_ensemble_f1 ▁▄▆▇██
wandb:            eval/avg_f1 ▁▄▂▂▃▄▅▃▅▄▅▄▅▄▅▅▅▄▅▅▇▅▅▄▃▅█▄▄▆▆▄▅▄▅▄▃▁▅▆
wandb:      eval/avg_mil_loss ▄▅▂▃▃▂▃▃▃▂▃▂█▃▄▂▃▃▁▃▂▂▂▃▃▂▃▂▂▃▂▂▃▃▃▃▅▄▂▂
wandb:       eval/ensemble_f1 ▅▄▅▃▃▆█▅█▄▄▆▅▃▂▄▅▄▆▄▅█▄▄▃▄▃▅▄▆▅▅▆█▄▄▆▆▁▄
wandb:           train/avg_f1 ▃▃▁▃▃▁▄▂▂▃▃▃▂▆▄▅▃▄▃▄▇▅▆██▃▃▂▆▆▇▅▄▅▆▅▅█▇▅
wandb:      train/ensemble_f1 ▄▂▁▄▂▄▂▅▃▃▆▃▄▄▃▅▄▅▆▅▄▄▅▄▇▇▆█▆▇▆▅▆▇▅▅▆▅▇▆
wandb:         train/mil_loss ▅▅▅▆▆▅▆▆▆▆▄▆▇▇▃█▄▅▅▆▅▅▆▄▄▆▆▃▅▃▄▅▅▅▃▃▃▁▆▃
wandb:      train/policy_loss ▂▂▂▂█▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂█▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9338
wandb: best/eval_avg_mil_loss 0.15119
wandb:  best/eval_ensemble_f1 0.9338
wandb:            eval/avg_f1 0.89688
wandb:      eval/avg_mil_loss 0.38522
wandb:       eval/ensemble_f1 0.89688
wandb:           train/avg_f1 0.86506
wandb:      train/ensemble_f1 0.86506
wandb:         train/mil_loss 5.4967
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rose-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/q6iuix74
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082610-q6iuix74/logs
wandb: ERROR Run q6iuix74 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ws7h4u56 with config:
wandb: 	actor_learning_rate: 4.1357020070638704e-05
wandb: 	attention_dropout_p: 0.46268007051195553
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 142
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5564148424729671
wandb: 	temperature: 0.6923481930531261
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082931-ws7h4u56
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ws7h4u56
wandb: uploading summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▄▅▆█
wandb: best/eval_avg_mil_loss █▄▃▅▄▁▂
wandb:  best/eval_ensemble_f1 ▁▃▄▄▅▆█
wandb:            eval/avg_f1 ▆▆▆▃▆▆▇█▄▆▂█▇▅▇▃▆▁▆▇▅▃▅▅▇▅▅▃▅▆▁▄▆▁▄▅▃▂▁▄
wandb:      eval/avg_mil_loss ▅▂▃▂▃▃▂▅▁▃▃▂▆▃▃▄▅▆▄▂▃▃▅▅▅▇▄▄▅▄▄▄▂▄▅▄▃█▅▄
wandb:       eval/ensemble_f1 ▇▇▆▆▇▇▅▇█▆█▆▇▄▅█▅▃▄▆▅▆▅▄▄▅▄▁▄▂▆▃▂▇▅▄▄▃▆▄
wandb:           train/avg_f1 ▆▇█▆█▅▆▇▇▆▇▆▅▅▅▆▅▅▆▆▄▅▄▄▃▄▃▄▄▃▂▄▄▄▄▄▄▁▃▃
wandb:      train/ensemble_f1 ▆█▆▅▇▆█▇▅▅▆▅▄▄▅▆▄▄▃▃▅▄▃▂▄▁▂▃▃▂▃▃▃▃▂▁▂▂▁▂
wandb:         train/mil_loss ▆█▃█▃▅▇▅▃▅▃▂▄▄▃▃▃▄▄▃▅▆▁▄▄▂▄▃▅▂▄▃▅▅▅▁▄▄▄▂
wandb:      train/policy_loss ▁▁▁▁▁▁▃▁▁█▁▃▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▇▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93766
wandb: best/eval_avg_mil_loss 0.20674
wandb:  best/eval_ensemble_f1 0.93766
wandb:            eval/avg_f1 0.88319
wandb:      eval/avg_mil_loss 0.26442
wandb:       eval/ensemble_f1 0.88319
wandb:           train/avg_f1 0.85913
wandb:      train/ensemble_f1 0.85913
wandb:         train/mil_loss 0.18014
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dainty-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ws7h4u56
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082931-ws7h4u56/logs
wandb: ERROR Run ws7h4u56 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: udgd795t with config:
wandb: 	actor_learning_rate: 0.0003740428953581395
wandb: 	attention_dropout_p: 0.2845875317531739
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6184849480694462
wandb: 	temperature: 3.605764603608703
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083219-udgd795t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/udgd795t
wandb: uploading history steps 80-90, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▇█
wandb: best/eval_avg_mil_loss ▇▆▂█▁▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆▇█
wandb:            eval/avg_f1 ▅▇▃▇▅▆▅▄▇█▄▅▃▄▆▇▇▅▄▅▆▂▅▃▄▁█▅▅▅▅▄▃▄▆▄▅▅▃▅
wandb:      eval/avg_mil_loss ▅▁▆▆▄▆▃▂▇▆▄▂▅█▄▇▅▅▂▆▂▆▁▅▅▅█▆▄▆▃▅▇▄▇▂▇▇▄▅
wandb:       eval/ensemble_f1 ▆▇▇▇▆▆▆▅▅▇▁█▅▆█▅▆▅▄▅▆▅▃▅▄▇▁▄▅▆▃▅▆▅▅▄▄▄▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▇▆▆▆▆▆▄▅▃▆▅▅▅▅▅▅▆▇▃▃▁▃▅▅▃▄▅▂▄▄▅▅▅▂▄▅▃▄▆
wandb:      train/ensemble_f1 ▆█▇█▅█▅▆▄▄▆▅▅▅▂▃▄▃▄▄▄▆▃▁▅▄▆▆▄▃▂▄▄▃▃▃▃▅▂▂
wandb:         train/mil_loss ▃▂▇▆▃▆▅▆▄▄▄▄▄▄▅▄▅▅▂▂▃▄▄▅▅▁█▂▃▅█▃▄▄▂▄▄▅▄▅
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅█▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▅▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.20308
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.88291
wandb:      eval/avg_mil_loss 0.27185
wandb:       eval/ensemble_f1 0.88291
wandb:            test/avg_f1 0.86934
wandb:      test/avg_mil_loss 0.32508
wandb:       test/ensemble_f1 0.86934
wandb:           train/avg_f1 0.87893
wandb:      train/ensemble_f1 0.87893
wandb:         train/mil_loss 0.21826
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sparkling-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/udgd795t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083219-udgd795t/logs
wandb: Agent Starting Run: zfulfikr with config:
wandb: 	actor_learning_rate: 0.0002289132262966363
wandb: 	attention_dropout_p: 0.23974022029334735
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 102
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6663332790689431
wandb: 	temperature: 9.055084947121134
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083406-zfulfikr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zfulfikr
wandb: uploading history steps 93-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇█
wandb: best/eval_avg_mil_loss ▃▁█▄
wandb:  best/eval_ensemble_f1 ▁▄▇█
wandb:            eval/avg_f1 ▆▄▄▅▅▅▁█▂▁▅▄▃▄▅▄▇▃▅▂▅▄▄▄▃▅▄▄▄▅▄▃▇▅▄▄▃▃▄▅
wandb:      eval/avg_mil_loss ▁▅▄▃▇▅▄▆▁█▃▄▇▃▂▂▃▂▄▄▂▄▇▆▃▅▅▄▃▆▆█▆▆▆▅▂▄▅▂
wandb:       eval/ensemble_f1 ▆▁▄▅▄▅▂▅▄▆▄▅▄▅▃▅▇▆▄▃▅█▆▄▇▅█▅▅▄▅▅▄▅▇▃▇▅▃▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▂▅▃▂▄▄▄▄▄▅▅▆▆▆▅▆▃▆▃▄▆▅▅▅▇▆▅▆▇▇▅▄▇▆▃▅█▅
wandb:      train/ensemble_f1 ▃▁▂▃▅▂▄▅▃▄█▃▆▇▄▆▅▃▆▆▄▄▅▅▅▄▆▇▅▇█▅▆▄█▅▄▃▆▆
wandb:         train/mil_loss █▆▆▅▄▅▅▄▄▄▄▅▄▄▃▃▃▁▃▃▄▃▄▃▁▁▄▃▂▃▁▃▃▃▂▂▃▃▂▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▂▄▄▄█▄▂▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▄▄▅▅▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄█▇▄▄▄▄▄▂▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.75636
wandb: best/eval_avg_mil_loss 4.71175
wandb:  best/eval_ensemble_f1 0.75636
wandb:            eval/avg_f1 0.71479
wandb:      eval/avg_mil_loss 7.46318
wandb:       eval/ensemble_f1 0.71479
wandb:            test/avg_f1 0.68422
wandb:      test/avg_mil_loss 7.84697
wandb:       test/ensemble_f1 0.68422
wandb:           train/avg_f1 0.67889
wandb:      train/ensemble_f1 0.67889
wandb:         train/mil_loss 0.18473
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run revived-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zfulfikr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083406-zfulfikr/logs
wandb: Agent Starting Run: 8jkac1yo with config:
wandb: 	actor_learning_rate: 4.993398397454436e-05
wandb: 	attention_dropout_p: 0.07784604236474318
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 132
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17820934899036056
wandb: 	temperature: 6.061028238031211
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083611-8jkac1yo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8jkac1yo
wandb: uploading wandb-summary.json; uploading history steps 117-132, summary
wandb: uploading history steps 117-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃█
wandb: best/eval_avg_mil_loss ▂▁█
wandb:  best/eval_ensemble_f1 ▁▃█
wandb:            eval/avg_f1 ▃▄▂▃▂▁▅▃▃▂▄▃▃▃▂▂▄▂▄▁▄▄█▃▅▃▂▄▁▆▂▃▄▂▄▄▃▂▃▄
wandb:      eval/avg_mil_loss ▁▄▄▂▃▄█▅▆▃▂▃▃▆▆▅▁▄▄▄▇▂▂▄▂▅▂▂▇▅▃▄▆▅▃▅▂▂▂▄
wandb:       eval/ensemble_f1 ▆▄▄▂▆▆▆▄▅▂▃▄▂▄▄▃▂▂▅▃▁▅▆▅▄▅▄▃█▃▅▅▄▅▂▂▅▃▅▂
wandb:           train/avg_f1 ▄▃▄▇▆▄▄▅▄▃▄▅▁▆▅▄▂▆▄▅▅▄▇▅▇▅▄▄▅▃▄▄▄█▆▄▆▃▅▄
wandb:      train/ensemble_f1 ▅▅▅▃█▅▂▅▂▅▅▄█▄▄▄▆▅▄▃▇▄▅▁▂▄█▁▄▄▃▄▄▂▃▃▄▇▂▅
wandb:         train/mil_loss ▅▆█▆▄▂█▅▅▅▅▃▄▆▆▃▄▆▄▄▆▅▄▃▇▁▆▆▆▃▄▅▇▄▄▄▅▄▅▆
wandb:      train/policy_loss █▁█▅█████████████████▅██████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▁███████████████▁████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.94099
wandb: best/eval_avg_mil_loss 0.22138
wandb:  best/eval_ensemble_f1 0.94099
wandb:            eval/avg_f1 0.91467
wandb:      eval/avg_mil_loss 0.29999
wandb:       eval/ensemble_f1 0.91467
wandb:           train/avg_f1 0.8909
wandb:      train/ensemble_f1 0.8909
wandb:         train/mil_loss 0.21406
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run spring-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8jkac1yo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083611-8jkac1yo/logs
wandb: ERROR Run 8jkac1yo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: mkskb4ln with config:
wandb: 	actor_learning_rate: 2.9629724987512494e-06
wandb: 	attention_dropout_p: 0.35352481075782144
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 197
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.093362053141671
wandb: 	temperature: 8.159943598280453
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083825-mkskb4ln
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mkskb4ln
wandb: uploading history steps 108-112, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▅█
wandb: best/eval_avg_mil_loss ▅▆▁█▃
wandb:  best/eval_ensemble_f1 ▁▂▅▅█
wandb:            eval/avg_f1 ▅▃▃▆▇▆▇▇▄█▄▄▅▆▃▂▆▅▄▆▃▃▆▄▅▇▁▅▆▇▂▂▁▄▃▃▂▅▄▆
wandb:      eval/avg_mil_loss ▅▄█▁▅▂▃▄▄▃▃▂▄▃▅▃▄█▅▄▃▆▄▂▄▆▅█▆▅█▅▃▃▅▅▆▅▃▇
wandb:       eval/ensemble_f1 ▆▅█▇▅█▇█▄▁▅▃▅▆▅▄▇▇▄█▇▇▆▃▄▃▅▅▇▃▃▅▃▅▅▂▃▄▅▇
wandb:           train/avg_f1 ▆▆▆▇▆▆▄▅▇▆▆▆▇▇█▇▆▄▆▅▆▅▂▅▇▅▄▃▃▄▁▄▆▅▄▃▅▂▆▃
wandb:      train/ensemble_f1 ▂▆█▆▃▆▄▆▇█▃▄██▃▃▃▆▄▃▄▄▆▄▂▂▆▄▃▅▇▅▄▃▄▂▆▅▄▁
wandb:         train/mil_loss █▇█▇▆▄▆█▇▆▄▆▅▅▆▅▄▆▄▇▆▄▃▄▂▅▄▃▄▃▂▂▃▄▂▂▂▁▂▃
wandb:      train/policy_loss ▃▂▂▁▂▂▂█▂▁▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▄▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▂▃▃▃▃▃▃█▃▃▃█▃▃▅▃▃▂▃▃▃▁▃▃▃▃▃▃▃▄▃▃▃▃▄▃▅▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.23043
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.91009
wandb:      eval/avg_mil_loss 0.31978
wandb:       eval/ensemble_f1 0.91009
wandb:           train/avg_f1 0.88323
wandb:      train/ensemble_f1 0.88323
wandb:         train/mil_loss 1.28405
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run electric-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mkskb4ln
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083825-mkskb4ln/logs
wandb: ERROR Run mkskb4ln errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: g1arsdhi with config:
wandb: 	actor_learning_rate: 8.878027019592529e-06
wandb: 	attention_dropout_p: 0.16891267802323157
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4969738682370925
wandb: 	temperature: 2.343022613069542
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084019-g1arsdhi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g1arsdhi
wandb: uploading history steps 78-89, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▁█
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▆▅▇▅▆▆▅▄▅▄▆▁▆▇▇▅▆▄▇█▇▆▅▇▆▇▆█▄▅█▅▅▆▄▄▅▅▃█
wandb:      eval/avg_mil_loss ▆▃▂▁▆▅▃▂▃▃▄▄▄▃▃▂▃▆▅▅▄▃▄▄▅▄▄▁▇▅▅▄▅▇█▄█▅▆▃
wandb:       eval/ensemble_f1 ▄█▆▃▃▄▄▂▆▅▆▄▆▄▁▃▃▂▆▇▆▆▆▃▄▅▆▆▅▄▅█▅▃▃▃▃▄▄▇
wandb:           train/avg_f1 ▆▆▅▅▃▅▄▆▆▅▄▆▆▃▂▄▄▄▄▅▅▄█▅▂▃▇▄▆▅▆▆▆▄▅▁▄▂▃▄
wandb:      train/ensemble_f1 ▆▆▄▆▄▆▅▄▇▅▆▁▆▆▆▅▅▄▄▅▄█▅▄▆▆▃▇▄▃▄▅▅▃▄▄▃▄▅▅
wandb:         train/mil_loss ▇▃▇▃▄█▃▅▃▄▄▄▅▅▂▁▅▄▅▆▆▇▇▄▆▃▃▃█▂▄▆▃▃▄▅▅▃▅▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▇▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92644
wandb: best/eval_avg_mil_loss 0.32503
wandb:  best/eval_ensemble_f1 0.92644
wandb:            eval/avg_f1 0.91194
wandb:      eval/avg_mil_loss 0.25929
wandb:       eval/ensemble_f1 0.91194
wandb:           train/avg_f1 0.88304
wandb:      train/ensemble_f1 0.88304
wandb:         train/mil_loss 1.053
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run stellar-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g1arsdhi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084019-g1arsdhi/logs
wandb: ERROR Run g1arsdhi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 8mw97bl0 with config:
wandb: 	actor_learning_rate: 1.1306961116786264e-05
wandb: 	attention_dropout_p: 0.0355848255168415
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 134
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7989734056789406
wandb: 	temperature: 5.337237410467854
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084152-8mw97bl0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8mw97bl0
wandb: uploading history steps 109-112, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss ▁▂▃█
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▄█▆▆▇▆▄█▇▇▄▄▆▇▆▃▅▆▃▄▃▃▃▆▆▆▃▃▃▆▆▂▁▂▆▁▁▅▃▄
wandb:      eval/avg_mil_loss ▁▂▂▂▂▂▂▁▂▂▅▂▂▄▃▃▂▂▂▂▃▃▃▂█▃▃▃▂▃▃▅▃▃▃▃▄▄▃▃
wandb:       eval/ensemble_f1 ▄▇▃▇▄█▅▇▅▆▇▇▃▆▅▅▇▆▅▃▁▃▅▄▃▇▂▂▂▂▄▆▅▂▃▂▂▃▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▄▇▅▅▅▆▆▆▅▅▄▅▆▆▄▅▅█▄▄▃▄▃▃▄▄▄▄▅▂▅▂▁▄▁▄▄▅
wandb:      train/ensemble_f1 ▅▄▅▅▇▇▄█▄▆▆▆▄▇▇█▆▆▇▄▅▄▇▅▄▇▆▅▄▄▆▃▂▇▆▄▆▁▅▂
wandb:         train/mil_loss ▅▆▇▆▃▃█▆▅▅▇▆▆▃▆▄▅▃▁▄▆▄▆▃▂█▅▃▅▇▃▄▂▃▅▂▁▂▁▃
wandb:      train/policy_loss ▆▆▆▅▆▆▆▆▆▆▁█▆▆▃▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▇██████▁█████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91511
wandb: best/eval_avg_mil_loss 0.32258
wandb:  best/eval_ensemble_f1 0.91511
wandb:            eval/avg_f1 0.88067
wandb:      eval/avg_mil_loss 0.32755
wandb:       eval/ensemble_f1 0.88067
wandb:            test/avg_f1 0.90521
wandb:      test/avg_mil_loss 0.26965
wandb:       test/ensemble_f1 0.90521
wandb:           train/avg_f1 0.86592
wandb:      train/ensemble_f1 0.86592
wandb:         train/mil_loss 2.73619
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run blooming-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8mw97bl0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084152-8mw97bl0/logs
wandb: Agent Starting Run: xc0sfxqf with config:
wandb: 	actor_learning_rate: 2.821941710592629e-06
wandb: 	attention_dropout_p: 0.32442747839589575
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 88
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7404102943617946
wandb: 	temperature: 4.685355198840197
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084346-xc0sfxqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xc0sfxqf
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▇█
wandb: best/eval_avg_mil_loss █▅▅▁▁
wandb:  best/eval_ensemble_f1 ▁▄▆▇█
wandb:            eval/avg_f1 ▄▂▂▄▇▃▁▄▄▄▃▅█▅▃▄▃▂▃█▇▅▇▂▃▃▄▄▂▄▇▂▄▄▄▆▃▆▄▁
wandb:      eval/avg_mil_loss ▅▄█▄▃▅▁▄▄▆▆▇▁▃▃▅▆▂▆▂▄▃▁▂▃▆▁▃▆▅▁▂▄▄▁▅▂▂▄▃
wandb:       eval/ensemble_f1 ▃▅▃▄▅▆▁▅▇▄▅▅▃▃█▃▆▅▆▅▄▃▆▅▄▄▅▁▅▄▆▆▅▄▅▅▄▆▇▅
wandb:           train/avg_f1 ▂▂▁▂▃▄▄▄▅▅▄▄▃▃▅▄▆▃▆▅▆▄▄▄▆▅▃▄▅▄▄▄▇█▃▅▆▄▄▅
wandb:      train/ensemble_f1 ▄▃▁▂▂▄▄▃▅▅▅▆▄▆▆▄▅█▅▇█▇▅▄▆▅▆▅▆▅▅▅▄▅▄▇▆▅▅▅
wandb:         train/mil_loss ▅█▇▄▆▆▄▆▇▅▆▅▄▆▆▃▄▅▆▄▄▄▅▅▆▅▆▃▃▄▁▃▄▄▃▃▆▄▂▃
wandb:      train/policy_loss ▅▅▅▂▅▅▅▄▆▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅█▅▅▅▅▅▅▄▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▅▃▄▇▃▇▆▇▇▇▇▇▇▇▇▄▇▇▇▇▁▇▇▇▅▇▇▇▇▆▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.24573
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.87733
wandb:      eval/avg_mil_loss 0.30043
wandb:       eval/ensemble_f1 0.87733
wandb:           train/avg_f1 0.88491
wandb:      train/ensemble_f1 0.88491
wandb:         train/mil_loss 4.17975
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glorious-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xc0sfxqf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084346-xc0sfxqf/logs
wandb: ERROR Run xc0sfxqf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: aw9qgbus with config:
wandb: 	actor_learning_rate: 7.996425503513453e-05
wandb: 	attention_dropout_p: 0.43465316110385194
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8333094883995185
wandb: 	temperature: 1.0463941706854396
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084534-aw9qgbus
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/b1jkj33p
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aw9qgbus
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss █▁▃
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▇▆▆██▇█▇▆▆▇▇▇▆▆▆▅▆▆▆▆▆▄▅▅▆▄▄▄▄▅▄▅▃▄▄▁▃▄▂
wandb:      eval/avg_mil_loss ▂▁▃▁▂▂▄▂█▅▁▂▃▂▂▃▄▄▂▅▂▂▃▃▃▄▃▂▃▄▃▄▅▄▂▅▃▄▄▃
wandb:       eval/ensemble_f1 ▅█▇▇▇▆▆▆█▆▃▆▆▅▅▄▅▄▄▇▅▃▃▅▃▄▃▃▃▄▁▆▃▁▁▂▃▃▄▂
wandb:           train/avg_f1 ▆▇▇▇█▆▅▇▆▆▇▇▆▅▆▅▅▄▅▆▄▅▄▄▆▄▄▃▄▃▄▂▃▂▃▁▁▁▂▁
wandb:      train/ensemble_f1 ▇▇▇▇█▅▇▇▇▇▆▇▆▆▅▅▅▆▆▆▅▃▅▄▄▃▅▃▃▄▃▂▃▃▃▂▂▂▂▁
wandb:         train/mil_loss ▆▆▆▄▅▅▅▇▄▇▅▆▅▅█▅▃▄▃▅▅▅▃▄▃▄▃▃▄▃▃▂▁▃▂▃▁▂▃▁
wandb:      train/policy_loss ▃▇▃██▁▃▄▃▃▃▃▃▃▃▅▃▃▄▃▃▃▃▃▅▄▃▃▃▄▆▃▃▄▇▅▃▅▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▃▃▁▁▁▁▁▁▁▁▁▃▁▁▁▁▂▁▁▂▃▁▇█▁▁▁▁▁▃▆▁▁▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.85692
wandb: best/eval_avg_mil_loss 0.5353
wandb:  best/eval_ensemble_f1 0.85692
wandb:            eval/avg_f1 0.73077
wandb:      eval/avg_mil_loss 0.85436
wandb:       eval/ensemble_f1 0.73077
wandb:           train/avg_f1 0.74243
wandb:      train/ensemble_f1 0.74243
wandb:         train/mil_loss 1.37123
wandb:      train/policy_loss 0.04538
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.04538
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run stilted-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aw9qgbus
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084534-aw9qgbus/logs
wandb: ERROR Run aw9qgbus errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 9gh6edjb with config:
wandb: 	actor_learning_rate: 6.738074223709388e-06
wandb: 	attention_dropout_p: 0.2335523720652571
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1806922984980801
wandb: 	temperature: 3.616592685696535
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084757-9gh6edjb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9gh6edjb
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▆▇█
wandb: best/eval_avg_mil_loss █▁▂▂▅▂
wandb:  best/eval_ensemble_f1 ▁▆▆▆▇█
wandb:            eval/avg_f1 ▅█▆▂▇▆▅▆▆▅▃▅▄▆▂▆▃▆▆▄▃▂▄▁▃▄▅█▇▃▃▂▄▂▃▂▂▄▅▃
wandb:      eval/avg_mil_loss ▂▄▂▃▁▃▅▂▄▂▁▅▆▄▃▃▄▇▄█▃▄▂▄▄▄▃▄█▅▅▄▄▇▇▅▅█▅▅
wandb:       eval/ensemble_f1 ▄▅▅▇▁▅▃█▆▇▇█▅▅▄▃▄▅▆▂▅▅▅▅▃▂▄▄▂▄▃▃▄▇▄▄▂▂▃▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇██▇▆▇█▆▇▆▆▆▄▆▆▆▂▅▄▄▃▆▄▄▄▃▂▄▄▅▅▁▃▃▃▂▂▁▂▃
wandb:      train/ensemble_f1 ▇▇▇▆▇▆▆▆█▅▆█▅▅▄▅▅▅▅▅▄▅▆▃▅▅▄▄▃▄▄▄▄▅▃▃▃▃▁▄
wandb:         train/mil_loss ▆▆█▅▇▇▄▃▆▅▅▆▁▆█▄▄▃▄▂▃▄▆▅▆▄▆▃▄▆▅▃▆█▅▅▅▃▄▄
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▆▃▄▁▄▄▄▄▄▄▄▄▅▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▅▇▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93025
wandb: best/eval_avg_mil_loss 0.23193
wandb:  best/eval_ensemble_f1 0.93025
wandb:            eval/avg_f1 0.86844
wandb:      eval/avg_mil_loss 0.31072
wandb:       eval/ensemble_f1 0.86844
wandb:            test/avg_f1 0.89511
wandb:      test/avg_mil_loss 0.27247
wandb:       test/ensemble_f1 0.89511
wandb:           train/avg_f1 0.8703
wandb:      train/ensemble_f1 0.8703
wandb:         train/mil_loss 0.21924
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run serene-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9gh6edjb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084757-9gh6edjb/logs
wandb: Agent Starting Run: e9z8w1w2 with config:
wandb: 	actor_learning_rate: 3.2071719043385185e-06
wandb: 	attention_dropout_p: 0.35518519870270554
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 148
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2262265476274462
wandb: 	temperature: 2.1698187055813403
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085043-e9z8w1w2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/e9z8w1w2
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇▇██
wandb: best/eval_avg_mil_loss ▆▃▁▆█▃▅
wandb:  best/eval_ensemble_f1 ▁▄▅▇▇██
wandb:            eval/avg_f1 ▇█▄█▄▇▄▅▄▅▅▅▄█▆▃▃▆▇▅▂▄▄▃▄▃▄▂▅▄▂▃▃▂▅▄▅▅▂▁
wandb:      eval/avg_mil_loss ▄▄▂▃▃▁▄▄▄▆▃█▅▃▃▃▅█▂▂▃▂▂▃▄▃▄▃▆▄▄▄▂▇▅▄▄▅▃▄
wandb:       eval/ensemble_f1 ▄▆▆█▆▄█▇▄▆▅▅▇▆▄▅▇▅▃▆▁▄▄▄▆▅▃▄▃█▆▇▅▄▃▆▂█▅▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▆▆▆▆▇▇▅▇█▆▅▆▄▆▅▅▆▄▅▆▃▅▆▆▆▄▅▄▄▅▃▁▂▂▅▅▂▂▄
wandb:      train/ensemble_f1 █▇▆▇▆▆▅█▆▇▆▆▆▇▆▅▆▆▃▆▆▅▅▆▄▄▃▄▆▄▁▄▄▄▆▃▅▅▁▃
wandb:         train/mil_loss ▅▅▅▅▆▆▇▅▆▆█▆▅▄▅▅▄▆▅▆▆▆▅▄▅▃▆▅▅▆▄▅▅▅▁▄▂▃▃▄
wandb:      train/policy_loss ▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆▆▆▆▆▆▆▆▆▃▅▇▆█▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▂▆▆▆▆▆▆▆▆▆▆▆▆█▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92662
wandb: best/eval_avg_mil_loss 0.27632
wandb:  best/eval_ensemble_f1 0.92662
wandb:            eval/avg_f1 0.85398
wandb:      eval/avg_mil_loss 0.3057
wandb:       eval/ensemble_f1 0.85398
wandb:            test/avg_f1 0.8982
wandb:      test/avg_mil_loss 0.23893
wandb:       test/ensemble_f1 0.8982
wandb:           train/avg_f1 0.86784
wandb:      train/ensemble_f1 0.86784
wandb:         train/mil_loss 0.26309
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run generous-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/e9z8w1w2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085043-e9z8w1w2/logs
wandb: Agent Starting Run: 12jdhpqz with config:
wandb: 	actor_learning_rate: 9.261268695245258e-05
wandb: 	attention_dropout_p: 0.21526992787432464
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 77
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2788757446740633
wandb: 	temperature: 3.8131075961843344
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085241-12jdhpqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/12jdhpqz
wandb: uploading history steps 75-78, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇██
wandb: best/eval_avg_mil_loss █▄▃▄▇▁
wandb:  best/eval_ensemble_f1 ▁▄▇▇██
wandb:            eval/avg_f1 ▅▆▄█▆▅▆▁▃▄▆▃▅▆▂▆▅▅▆▇▅▅▅█▆▄▅▆▅▅▇▄▂▅█▄▄▆▅▄
wandb:      eval/avg_mil_loss ▄▂▂▃▇▁▄▃▇█▂▅▃▅▃▂▅▂▂▄▅▁▃▂▂▅▃▃▃▁▄▃▇▇▆▁▃▃▂▅
wandb:       eval/ensemble_f1 ▆▄▄▆▆▁▅▅▄▆▄█▆▆▄▅▃▃▄▁▄█▆▅▂▅▆▁▅▇▃█▅▆▃▆▆▄▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▃▅▇▅▆▆▄▅▇▇▅▃▁▃▅█▃▄▄▅▃▂▂█▁▅▄▅▄▁▄█▆▂▂▆▂▅▄
wandb:      train/ensemble_f1 ▁▄▆▅▄▆▆▅▅▇▅▇▂▅▅█▃▄▄▇▃▃▃▆▅▄▅▅▄▅▃▄█▆▂▇▃▃▅▄
wandb:         train/mil_loss ▇▆▆█▄▇▆▆▅▅▅▅▄▆▃▄▄▄▄▅▄▄▄▄▄▄▄▄▄▃▄▂▅▃▂▄▁▂▁▃
wandb:      train/policy_loss ▅▅█▁█▁▅▁▅█▁▅▁██▁█▅█▁▅▅█▁█▁▁▅█▁▁██████▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅█▁██▁█▁█▅█▁▅█▁█▁▁▅▁▅█▁█▁███▁██▁▁█▁▁█▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.926
wandb: best/eval_avg_mil_loss 0.20837
wandb:  best/eval_ensemble_f1 0.926
wandb:            eval/avg_f1 0.88936
wandb:      eval/avg_mil_loss 0.29316
wandb:       eval/ensemble_f1 0.88936
wandb:            test/avg_f1 0.88674
wandb:      test/avg_mil_loss 0.27349
wandb:       test/ensemble_f1 0.88674
wandb:           train/avg_f1 0.89108
wandb:      train/ensemble_f1 0.89108
wandb:         train/mil_loss 0.30123
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worthy-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/12jdhpqz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085241-12jdhpqz/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rhd36yqi with config:
wandb: 	actor_learning_rate: 3.3441679334188214e-05
wandb: 	attention_dropout_p: 0.30968516603012675
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 63
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.25454986828201065
wandb: 	temperature: 4.563841751940197
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085417-rhd36yqi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rhd36yqi
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▄▇▁▄▂▅▄▅▇▆▅▇▄▅▄█▆▆▂▂▄▃▆▃▄█▆▇▆▅▃▅▄▇▂▇▃▆▆
wandb:      eval/avg_mil_loss ▃▆▄▃▆▁▃▅▇▇▆▄▂▂▅█▂▆▅▁▂▇▄▆▄▄▄█▄▆▃▄▅▄▃█▆▆▇█
wandb:       eval/ensemble_f1 █▄▇▄▄▅▄▇▅▃▅▇▄▅▅▆▆▃▇▃▆▅▄█▆▇▆▅▃▅▄▇▇▁▄▇▄▆▆▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▄▄█▄▃█▁▂▄█▁▃▂▅▅▅▂▃▄▂▃▆▅▃▃▄▁▆▄█▆█▄▆▁▆▆▅
wandb:      train/ensemble_f1 ▁▄▄▃▆▃▄█▁▂▄█▇▁▃▅▅▅▂▃▅▃▃▅▄▃▁▆▆▄▂█▅▆▆▇▁▆▆▅
wandb:         train/mil_loss ▆▇▅█▅▃█▆▄▄▄▂▇▂▅▄▅▅▄▄▇▃▂▅▃▂▄▅▂▄▄▄▆▄▂▂▁▁▄▁
wandb:      train/policy_loss ▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▁▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▅▆▆▆▆▆▁▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▅▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.24196
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.90396
wandb:      eval/avg_mil_loss 0.34518
wandb:       eval/ensemble_f1 0.90396
wandb:            test/avg_f1 0.91989
wandb:      test/avg_mil_loss 0.18906
wandb:       test/ensemble_f1 0.91989
wandb:           train/avg_f1 0.89268
wandb:      train/ensemble_f1 0.89268
wandb:         train/mil_loss 0.5361
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run daily-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rhd36yqi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085417-rhd36yqi/logs
wandb: Agent Starting Run: ty6yrols with config:
wandb: 	actor_learning_rate: 9.507365999903295e-05
wandb: 	attention_dropout_p: 0.08234530115348332
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 102
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6836761632283365
wandb: 	temperature: 4.437124754832941
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085520-ty6yrols
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ty6yrols
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▆█
wandb: best/eval_avg_mil_loss █▅▃▃▁
wandb:  best/eval_ensemble_f1 ▁▁▄▆█
wandb:            eval/avg_f1 ▅▄▇▇▅█▇▅▂▃▄▆▃▂▅▆▄▆▆▆▄▅█▅▄▆▅▆▇▅▁▃▄▆▅▆▃▃▂▄
wandb:      eval/avg_mil_loss ▃▂▄▄▃▃▃▁▄▅▂▅▆▅▃▆▄▁▃▄▄▅▅▄▁▆▄▂▁▃▄█▂▅▆▆▃▆▇▅
wandb:       eval/ensemble_f1 ▅▅▆▇▆▇▆▆▅▅█▅▅▃▄▃▅▁▅▄▅▆▅▄▅▅▄▄▅▁▃▃▃▆▂▆▆▂▃█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅▃▅▃▅▆▄▂▅▆▂▄▅▄▄▃▆▄▅▅▅▄▇▂▄▄▆▂▁▅▅▄▃▃▅▃▁▃▄
wandb:      train/ensemble_f1 ▆█▅▇▅▄▆▆▆▅▃▅▆▇▆▂▅▄▄▅▅▆▄▂▃▂▆▅▁▅▂▃▄▄▄▄▅▅▃▃
wandb:         train/mil_loss ▇▄▅▅▅▅▄▅▅▆█▂▅▄▅▃▅▄▃▄▆▅▅▂▄▄▃▂▄▆▄▁▄▄▃▃▂▃▄▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅█▅▅▅▅▁▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆█▃▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91919
wandb: best/eval_avg_mil_loss 0.21957
wandb:  best/eval_ensemble_f1 0.91919
wandb:            eval/avg_f1 0.88642
wandb:      eval/avg_mil_loss 0.30116
wandb:       eval/ensemble_f1 0.88642
wandb:            test/avg_f1 0.89036
wandb:      test/avg_mil_loss 0.2516
wandb:       test/ensemble_f1 0.89036
wandb:           train/avg_f1 0.8863
wandb:      train/ensemble_f1 0.8863
wandb:         train/mil_loss 0.27573
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lively-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ty6yrols
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085520-ty6yrols/logs
wandb: Agent Starting Run: 3qhdn1om with config:
wandb: 	actor_learning_rate: 2.8382278530505597e-06
wandb: 	attention_dropout_p: 0.3611621594454816
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 62
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.34224244626058253
wandb: 	temperature: 0.37461401223347623
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085652-3qhdn1om
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3qhdn1om
wandb: uploading history steps 60-63, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss █▁▅
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 ▆▄▃▇▂▆▅█▅▆▅▆▆▅█▇█▆▁▇▅▅▅▅▃▆▇▆▃▇▇▂▆▄▅▆▄▄▆▅
wandb:      eval/avg_mil_loss ▄▄▂█▂▅▄▃▁▁▄▅▅▄▂▃▅▃▄▂▂▃▆▃▅▃▂▅▃▂▃▃▃▄▅▅▄▅▂▄
wandb:       eval/ensemble_f1 ▆▅▄▆▂▆▄▅▆▃▃▃▅▆█▆▇▇▆██▁▅▅▃▆▇▆▃▇▂▃▆▄▅▆▆▃▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▅▃▃▄▅▅▅▆▄▄▁▆▅▄▆█▅▆▅▅▆▅█▄▄▃▄▅▅▆▇▄▄▆▃▆▆█
wandb:      train/ensemble_f1 ▅▅▅▄▁▅▂▃▃▆▅▅▁▅▃▄▆▄▃▃▆█▄▅▄▅▄▃▂▃▃▄▄▅▇▅▆▅▅▄
wandb:         train/mil_loss ▇▂▆▅█▅▄▇▆▃▃▃▇▇▅▅▅▂▅▂▅▄▃▃▃▄▂▁▂▄▁▃▂▁▃▂▂▄▃▃
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆▄▄▁▄▄▄█▄▁▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▄▁▇▇▇▇▅▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92223
wandb: best/eval_avg_mil_loss 0.25045
wandb:  best/eval_ensemble_f1 0.92223
wandb:            eval/avg_f1 0.89558
wandb:      eval/avg_mil_loss 0.26813
wandb:       eval/ensemble_f1 0.89558
wandb:            test/avg_f1 0.87885
wandb:      test/avg_mil_loss 0.27584
wandb:       test/ensemble_f1 0.87885
wandb:           train/avg_f1 0.90439
wandb:      train/ensemble_f1 0.90439
wandb:         train/mil_loss 1.65041
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vague-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3qhdn1om
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085652-3qhdn1om/logs
wandb: Agent Starting Run: m36t75rs with config:
wandb: 	actor_learning_rate: 0.0006966217285848561
wandb: 	attention_dropout_p: 0.401821066104465
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 167
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.13481215575768724
wandb: 	temperature: 0.7009405400996183
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085800-m36t75rs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/m36t75rs
wandb: uploading history steps 149-158, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▆▇█
wandb: best/eval_avg_mil_loss ▇█▇▁▆▅
wandb:  best/eval_ensemble_f1 ▁▂▄▆▇█
wandb:            eval/avg_f1 ▂▆▃▆▅▄▅▄▁▆▅▄▆▄▃▅▆█▆▅▅▃▆▄▄▃▂▃▆▂▇▅▂▅▃▃▅▄▁▅
wandb:      eval/avg_mil_loss ▆▁▄▃▄▃▁▄▆▂▅▆▄▆▆▆▃▅▆▆▄▄▃▆▃▆▅▇▅▃▄▄▇▅▆█▅▇▅▅
wandb:       eval/ensemble_f1 ▅▃▅▄▅▄▄▄▆▁▅▆▄█▂▃▆▂▄▅▅▄▄▄▅▄▂▃▆▅▂▂▃▅▃▄▃▃▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▅▆▄▆▅▃▅▅▇▆▇▆▅▄▅▅▅▄▇▆▆▁▅▂▃▅▂▆▅▃▅█▄▄▂▃▄▂
wandb:      train/ensemble_f1 ▄▃▄▅▃▅▃▆▅▄▅▄▆▆▅▄▅▆█▃▄▃▅▇▄▃█▅▅▄▂▃▅▃▅▃▂▃▁▂
wandb:         train/mil_loss ▆▆▆▆▅██▅▄▅█▇▅▄▇▅▂▆▅▅▆▄▄▅▃▅▆▃▅▁▄▃▅▅▃▃▂▅▄▃
wandb:      train/policy_loss ▃▃▃█▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▆▁▁▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.24392
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.88761
wandb:      eval/avg_mil_loss 0.29639
wandb:       eval/ensemble_f1 0.88761
wandb:            test/avg_f1 0.89302
wandb:      test/avg_mil_loss 0.25497
wandb:       test/ensemble_f1 0.89302
wandb:           train/avg_f1 0.8769
wandb:      train/ensemble_f1 0.8769
wandb:         train/mil_loss 2.35702
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bumbling-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/m36t75rs
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085800-m36t75rs/logs
wandb: Agent Starting Run: dgcl5el3 with config:
wandb: 	actor_learning_rate: 0.000954244673722494
wandb: 	attention_dropout_p: 0.3924130277644958
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 167
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5015351233716016
wandb: 	temperature: 9.85061924287767
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090045-dgcl5el3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dgcl5el3
wandb: uploading wandb-summary.json
wandb: uploading history steps 153-168, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇▇▇█
wandb: best/eval_avg_mil_loss ▇▁█▃▂▆
wandb:  best/eval_ensemble_f1 ▁▅▇▇▇█
wandb:            eval/avg_f1 ▄▆▃▅▁▅▄▃▅▆▅▄▇▅▄▆█▃▄▅█▆▄▅▅▄█▁▂▆▅▆▅▂▅▃▆▅▇▇
wandb:      eval/avg_mil_loss ▆▄▇▄▅▅▅▄▃▆▆▃▁▅▃▄▆▄▄▃▄▃▃▃█▄▅▁▅▆▅▂▆▄▆▃▂▂▆▅
wandb:       eval/ensemble_f1 ▃▅▇▁▅▅▅▃▂▄▄▅▆▆▇▄▅▅▂█▂▇▄▃▃▃▅▃▄▁▄▄▄▂▄█▅▂▅▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▁▂▃▂▅▅▄▅▁▂▄▂▃▇▃▄▁▄▃▆▃█▅▂▂▅▆▅▆▄▁▆▄▆▂▃▆▃▂
wandb:      train/ensemble_f1 ▁▃▄▃▃▄▅▁▃▂▄▂▁▃▄▃█▂▁▂▁▇▄▃▃▁▆▁▁▃▂▄▁▃▃▄▄▃▃▅
wandb:         train/mil_loss ▇▆▅█▂██▇▅▄▆██▃▅▄▆▂▅▅▂▇▁▇▃▃▂▆▄▂▄▄▄▄▁▅▄▇▄▂
wandb:      train/policy_loss ▄▄▄█▄█▁▄█▄▄▄▄▄▄▄▄▄▁▄▁█▄▄█▄▄▄▁█▄▄█▁██▁█▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92964
wandb: best/eval_avg_mil_loss 0.33178
wandb:  best/eval_ensemble_f1 0.92964
wandb:            eval/avg_f1 0.91467
wandb:      eval/avg_mil_loss 0.34287
wandb:       eval/ensemble_f1 0.91467
wandb:            test/avg_f1 0.90914
wandb:      test/avg_mil_loss 0.17628
wandb:       test/ensemble_f1 0.90914
wandb:           train/avg_f1 0.88213
wandb:      train/ensemble_f1 0.88213
wandb:         train/mil_loss 0.32925
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run exalted-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dgcl5el3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090045-dgcl5el3/logs
wandb: Agent Starting Run: 4xg6nml1 with config:
wandb: 	actor_learning_rate: 2.7062131077268756e-06
wandb: 	attention_dropout_p: 0.10895399675951406
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 136
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.34550794102155413
wandb: 	temperature: 2.9181217446083583
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090305-4xg6nml1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4xg6nml1
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆██
wandb: best/eval_avg_mil_loss ▇█▅▃▁
wandb:  best/eval_ensemble_f1 ▁▆▆██
wandb:            eval/avg_f1 ▆█▅▅▅▄▆▄▄▆▆▆▆▅▃▄▃▂▃▄▅▆▄▇▃▁▃▂▃▂▇▄▃▃▄▄▇▅▃▂
wandb:      eval/avg_mil_loss ▄▅▂▃▄▅▅▄▃▂▆▅▅▅▁▅▇▅▄▃▅▄▃▆▅▅▄▃▃▅▄▂▃▄▅▅▂▃█▅
wandb:       eval/ensemble_f1 ▇█▇▄▄▄▄▆▅▆▅▆▆▇▃▅▆▄▃▅▆▃▆▆▇▁▃▅▃▇▃▃▄▅▅▄▁▄▁▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆█▄▅▇▄▄▂▄▅█▆▂█▄▄▆█▅█▅▂▄▄▆▅▆▆▃▁▄▄▃▅▇▅▆▅▃
wandb:      train/ensemble_f1 ▂▆▃▃▃▇▂█▅▂▄▃▄▂▃▃▂▆▄▅▆▂▃▅▅▄▄▃▄▂▂▂▁▂▅▄▄▁▁▄
wandb:         train/mil_loss ▇▃▆▆█▅▆▇▄▆▆▄▅▇▅▄▅▅▃▄▆▆▃▅▃▄▆▂▅▄▅▄▅▃▄▃▄▁▃▃
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.21866
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.8791
wandb:      eval/avg_mil_loss 0.33789
wandb:       eval/ensemble_f1 0.8791
wandb:            test/avg_f1 0.87622
wandb:      test/avg_mil_loss 0.26483
wandb:       test/ensemble_f1 0.87622
wandb:           train/avg_f1 0.88264
wandb:      train/ensemble_f1 0.88264
wandb:         train/mil_loss 0.29251
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run floral-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4xg6nml1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090305-4xg6nml1/logs
wandb: Agent Starting Run: pz3zgku0 with config:
wandb: 	actor_learning_rate: 1.067033222529124e-06
wandb: 	attention_dropout_p: 0.28209629581312534
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7880506457036284
wandb: 	temperature: 4.563275023111952
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090458-pz3zgku0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pz3zgku0
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▇█
wandb: best/eval_avg_mil_loss █▁▃▃▅▃
wandb:  best/eval_ensemble_f1 ▁▅▆▆▇█
wandb:            eval/avg_f1 ▂▄▅▆▄▄▅▄▇▇▇▃▃▅▆▁▄▅▄▇▃▁▁▅▃▅▅▃█▇▆█▃▆▃▅▅▆▅█
wandb:      eval/avg_mil_loss ▁▅▄▄▇▃▃▃▅▂▄▃▄▄▆▂▃▃▂▃▄▅▄▃▃▄▃▂▅▅▅▂▄▃█▄▅▁▄▃
wandb:       eval/ensemble_f1 ▇▅▂▅▂▃▇█▇▁▅▄▆▆▇▃▄▆▇▅▇▄▄▆▃▄▃▅▆▇▆█▆▃▆▆▂▃▃▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▂▂▆▆▃▆█▅▃▅▃▆▂▁▃▆▅▅▃▂▃▄▃▃▇▄▅▄▂▃▅█▁▃▄▄▅▂▄
wandb:      train/ensemble_f1 ▃▂▃▆▆▂▄▅▃▂▃▃▃▂▅▂▄▂▃▂▃▃▂▃█▃▄▂▂▁▅▃▅▃▄▂▃▄▅▄
wandb:         train/mil_loss ▃▆▆▅▆▄▅▄▂▃▇▅▄█▅▄▄█▅▅██▅▆▆█▁▄▆▅▃▄█▃▂▄▄▇▄▆
wandb:      train/policy_loss ▅▅▅█▁█▅▅▅▅█▅██▅▅▅▅▅█▁▅▁▅▅▁▅▅▅▅▁▁▅▁▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅█▁█▅▅▅▅█▅▅█▁█▅▅▅▁█▅▅█▅▅▁▅▅▁▅▅██▅▅▁▅▁▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92223
wandb: best/eval_avg_mil_loss 0.26023
wandb:  best/eval_ensemble_f1 0.92223
wandb:            eval/avg_f1 0.91497
wandb:      eval/avg_mil_loss 0.29028
wandb:       eval/ensemble_f1 0.91497
wandb:            test/avg_f1 0.90493
wandb:      test/avg_mil_loss 0.16802
wandb:       test/ensemble_f1 0.90493
wandb:           train/avg_f1 0.88144
wandb:      train/ensemble_f1 0.88144
wandb:         train/mil_loss 3.17736
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run celestial-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pz3zgku0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090458-pz3zgku0/logs
wandb: Agent Starting Run: 4tok3its with config:
wandb: 	actor_learning_rate: 0.00020509092686755615
wandb: 	attention_dropout_p: 0.44377512460631446
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8804864672395011
wandb: 	temperature: 4.0300282289356755
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090637-4tok3its
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4tok3its
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss ▅█▆▂▁
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▂▄█▅▆▅▃▄▂▅▂▂▄▅▅▆▇▃▄▆▁▄▅▆▆▇▄▄▆▆▇▆▃▂▄▅▆▇▇▇
wandb:      eval/avg_mil_loss ▄▆▆▇▅▃▅▄▄▄▁▄▃▇▄▃▇▄▆▆▄▆▇▅▂▃▃▅▆█▂▃▆▅▇█▅▄▇▃
wandb:       eval/ensemble_f1 ▁▆▃▅▃▂▄▅▇▄▄█▅▃▃▂▃▁▃▃▆▆▄▂▄▃▃▄▇▃▅▂▁▃▃▇▅▅▂▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▃▁▂▁▄▃▆▆▃▁▄▅▇▄▄▂▅▇▅▆▅▃▂▆▄▅▄█▂▆▅▄▄▅▆█▃▃
wandb:      train/ensemble_f1 ▅▁▅▃▅▄▄▃▅▆▆▇▆▇▃▄▂▅▆▅█▆▇▂▄█▄▄▄▄▂▆▅▆▄▃▅█▃▃
wandb:         train/mil_loss ▄▅▄█▅▆▄▃▃▅▄▇▃▆▅▇█▂▄▅▃▄▆▇▆▅▆▁▅▄▃▇▃▆▅▆▅▃█▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.21195
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.88859
wandb:      eval/avg_mil_loss 0.36023
wandb:       eval/ensemble_f1 0.88859
wandb:            test/avg_f1 0.90033
wandb:      test/avg_mil_loss 0.22386
wandb:       test/ensemble_f1 0.90033
wandb:           train/avg_f1 0.89428
wandb:      train/ensemble_f1 0.89428
wandb:         train/mil_loss 2.04797
wandb:      train/policy_loss 0.13384
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.13384
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run frosty-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4tok3its
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090637-4tok3its/logs
wandb: Agent Starting Run: 6nfmdl37 with config:
wandb: 	actor_learning_rate: 2.9393663714404093e-05
wandb: 	attention_dropout_p: 0.14720751443549646
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 106
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04294698175966771
wandb: 	temperature: 2.5393470314735933
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090825-6nfmdl37
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6nfmdl37
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▇█
wandb: best/eval_avg_mil_loss ▆██▇▁▂
wandb:  best/eval_ensemble_f1 ▁▂▃▆▇█
wandb:            eval/avg_f1 ▅█▅▄█▃▅▆▇▇▁▄▃▅▅▄▄▇▆▂▇▇▄▄▇▃▄▆▅▅▇▄█▃▅▅▄▁▆▅
wandb:      eval/avg_mil_loss ▅▄▅▄▁▃▂▄▄▆▄█▄▂▄▄▄▂▆▆▂▃▅▅▄▆▇▅▃▂▆▄▆▆▅▁▆▄▃█
wandb:       eval/ensemble_f1 ▁▅▂▄▇▄▅█▅▃▅▅▁▄▅▇▃▂▅▄▄▇▆▆▁▇▁▄▂▆▄▆▄▆▃▃▄▄▅▁
wandb:           train/avg_f1 ▄▄▂▄▁▆▃▆▆▃▂▁▂▅▆▃▄▅▃▅▅▃▃█▃▂▃▅▁▆▇▄▃▅▅▅▅▄▄▂
wandb:      train/ensemble_f1 ▄▂▅▃▂▆▅▃▅▅▂▃▅▆▆▄▅█▅▄▄▅▄▃█▃▂▄▄▅▅▁▃▄▆▅▃▄▇▂
wandb:         train/mil_loss █▆▇▆▆█▇▆██▆▇▆▂▄▅▄▅▆▅▅▆▄▄▅▄▅▅▆▄▄▅▃▄▅▃▃▁▄▂
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▇▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▇▆▆▆▆█▆▆▆▆▆▇▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.20226
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.87756
wandb:      eval/avg_mil_loss 0.34717
wandb:       eval/ensemble_f1 0.87756
wandb:           train/avg_f1 0.88284
wandb:      train/ensemble_f1 0.88284
wandb:         train/mil_loss 2.92706
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run quiet-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6nfmdl37
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090825-6nfmdl37/logs
wandb: ERROR Run 6nfmdl37 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: no2gsiad with config:
wandb: 	actor_learning_rate: 1.6799781682845855e-05
wandb: 	attention_dropout_p: 0.12846526794216784
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 119
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6401182717254932
wandb: 	temperature: 2.598803451956968
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091018-no2gsiad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/no2gsiad
wandb: uploading history steps 103-109, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss ▃█▁
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 ▄▆▃█▆▅▂▆▇▃▇▇▆▆▅▆▅▆▄▅▃▅▆▄▆▃▁▆▆▂▂▄▅▅▆▇▆▅▄▆
wandb:      eval/avg_mil_loss ▁▃▄█▄▃▄▂▂▂▄▄▄▃▃▅▂▄▂▄▅▃▄▄▄▃▃▃▆▄▅▄▃▄▃▂▃▄▄▁
wandb:       eval/ensemble_f1 ▄▅▆▆▃▆▃▅▁▃▇▃▆▄▃▆▆▇▆▄▇▅▆▇▆▆▃▄▄▂▄▁▁▁▅▁█▄▅▆
wandb:           train/avg_f1 ▆▆▅▂▄▃▃▆▆▄▁▄██▄▆▇▄▄▇▄▆▄▃▃▅▄▇▄▃▄▄▃▄▃▃▃▂▅▂
wandb:      train/ensemble_f1 ▇▄▆▅█▄▅▄▃▃▆▄▂▆█▅▇▄▄▄▄▆▂▅▇▁▄▁▃▁▄▃▃▃▄▃▄▇▂▂
wandb:         train/mil_loss ▇██▇█▇▅▇▇▇▅█▇▇▆▇▅▇▅▆▄▅▅▃▄▄▄▄▃▄▂▃▃▃▄▂▁▂▁▂
wandb:      train/policy_loss ▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▅▄▅▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.20443
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.90344
wandb:      eval/avg_mil_loss 0.24292
wandb:       eval/ensemble_f1 0.90344
wandb:           train/avg_f1 0.88268
wandb:      train/ensemble_f1 0.88268
wandb:         train/mil_loss 1.47124
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run northern-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/no2gsiad
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091018-no2gsiad/logs
wandb: ERROR Run no2gsiad errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4f3vto14 with config:
wandb: 	actor_learning_rate: 0.0008405909740341981
wandb: 	attention_dropout_p: 0.23083621438179447
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 112
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.31013403459854016
wandb: 	temperature: 8.700578546305163
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091248-4f3vto14
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4f3vto14
wandb: uploading history steps 106-112, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss ▄▅▁█
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▅▇█▅▃▃▅▅█▇▇▇▆▂▇▇▄▅▂▁▂▂▇▅▅▂▂▂▆▆▃▅▆▄▄▅▄▅▇▁
wandb:      eval/avg_mil_loss ▃▄▂▄▂▄▄▄▄▄▂▂▁▁▄█▂▃▆▄▃▃▅▃▂▄▅▅▂▄▂▃▄▄▅▅▂▄▃▃
wandb:       eval/ensemble_f1 ▇▄▅▇▂█▄▂▆▆▄▃▅▅▇▃▁▅▅▃▅▂▄▂▆▄▅▂▄▅▅▄▄▅▃▅▄▂▃▄
wandb:           train/avg_f1 ▇██▇▆▅▇▆▃▆▆▆▇▅▄▄▅▇▅▅▃▆▅▄▆▅▅▆▄▄▃▂▃▃▄▄▅▅▁▄
wandb:      train/ensemble_f1 ▇█▆▇▆▆▆█▇▄▆▄▅▃▆▇▇▅▃▃▅▅▄▅▃▄▇▄▆▄▄▅▆▄▄▄▃▁▆▅
wandb:         train/mil_loss ▄▅▅█▇▃▄▄▅▅▃▅▄▅▄▄▆▅▆▄▅▆▄▂▆▃▂▃▆▂▅▄▄▃▄▁▃▂▆▂
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▄▃▃▃▃▃▃▃▂▃▃▃▃▃▃▁▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▃▂▂▂▂▂▂▂▂▁▂▂▂▃▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.32031
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.89022
wandb:      eval/avg_mil_loss 0.28747
wandb:       eval/ensemble_f1 0.89022
wandb:           train/avg_f1 0.88151
wandb:      train/ensemble_f1 0.88151
wandb:         train/mil_loss 0.25638
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run flowing-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4f3vto14
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091248-4f3vto14/logs
wandb: ERROR Run 4f3vto14 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: dw1wk5tl with config:
wandb: 	actor_learning_rate: 0.0001286751247064528
wandb: 	attention_dropout_p: 0.4851546242099795
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 101
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7853571460594675
wandb: 	temperature: 8.639300942551007
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091431-dw1wk5tl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dw1wk5tl
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▆▆█
wandb: best/eval_avg_mil_loss ▆█▁▁▂
wandb:  best/eval_ensemble_f1 ▁▁▆▆█
wandb:            eval/avg_f1 ▅▅▇▅▃▇▅▆▅▆▆▅▂▄█▄▄▄▆▃▆▁▆▅▃▂▄▃█▅▄▅▆▅▄▅▅▇▃▆
wandb:      eval/avg_mil_loss ▇▄▅▆▅▄▂▅▄▅▆▃▅▅▄▅█▄▇▃▃▅▅▆▃▇▂█▂▃▃▃▅▄▁▄▆▃▆█
wandb:       eval/ensemble_f1 ▅▇▅▆▅▅▃▅▇▄█▅▃▄▃▄▃▅▄▃▄▂▅▇▃▃▃▃▃▅▃█▅▅▇▁▄▃▄▅
wandb:           train/avg_f1 █▇▄▃▁▄▁▄▆▅▄▆▇▄▄▄▃▄▆▆▅▇▃▆▅▄▄▂▅▂▂▆▁▇▇▄▃▃▄▃
wandb:      train/ensemble_f1 █▂▅▃▆▇▅▇▅▁▇▇▅▃▃▆▅▃▄▇▄▃▅▄▆▅▂▄▇▇▆▄▃▄█▅▂▂▄▆
wandb:         train/mil_loss ▇██▇██▆▄▆▇▅▆█▅▆▆▅▄▄▅▅▄▄▂▁▅▅▅▄▅▄▃▅▃▃▄▄▄▄▅
wandb:      train/policy_loss ▁▁▅▁▁▁▁▁▁▁▁▁▇▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▅▅▅▅▅▅▅▅▅▅▅▅▅▇▅▇▅▁▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92236
wandb: best/eval_avg_mil_loss 0.23903
wandb:  best/eval_ensemble_f1 0.92236
wandb:            eval/avg_f1 0.90344
wandb:      eval/avg_mil_loss 0.33218
wandb:       eval/ensemble_f1 0.90344
wandb:           train/avg_f1 0.89515
wandb:      train/ensemble_f1 0.89515
wandb:         train/mil_loss 2.94178
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run copper-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dw1wk5tl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091431-dw1wk5tl/logs
wandb: ERROR Run dw1wk5tl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: b5aksgk7 with config:
wandb: 	actor_learning_rate: 1.4902843790329453e-06
wandb: 	attention_dropout_p: 0.01991734248864724
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 55
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5605509078394622
wandb: 	temperature: 4.880498263353966
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091624-b5aksgk7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/b5aksgk7
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 54-55, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▄▆█
wandb: best/eval_avg_mil_loss ▂▃▄▁▃▄█
wandb:  best/eval_ensemble_f1 ▁▂▃▃▄▆█
wandb:            eval/avg_f1 ▅▆▆▆▅▅▆▇▇▃▅▇▃▆█▅▆▆▅▇▅▁█▄▆▅▄█▄▄▅▅▃▅▅▅▇▆▅▇
wandb:      eval/avg_mil_loss ▃▃▅▃▄▃▂▄▃▂▅▇▃▅▆▃▃▅▃▃█▅▇▄▁▄▃▅▂▃▁▃▁▄▅▄▄▂▆▅
wandb:       eval/ensemble_f1 ▅▆▆▄▅▇▃▆▅▇▆██▅▆▇▅▅▁█▆█▅▅▄█▄▂▄▅▅▃▅▅█▅▇▆▅▇
wandb:           train/avg_f1 ▅▅▇▇▄▃▄▅▅▂▄▄▂█▆▅▅▄▁▅▆▆▄▇▆▇▄█▃▇▃▅▇▄▆▇▆▄▃▃
wandb:      train/ensemble_f1 ▅▇▆▃▅▄▅▅▂▄▂▇▃▆▃▅▄▁▄█▅▄▄▆▅▂▃▇▃▇▂▄▆▄▅▆▃▃▄▃
wandb:         train/mil_loss ▄▇▇▆▄▅▇▇▆▄█▇▆▆▄▆▆▅▃█▆▆▆▅▆▄▄▆█▃▆▆▄▅▅▅▆▃▁▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91898
wandb: best/eval_avg_mil_loss 0.43674
wandb:  best/eval_ensemble_f1 0.91898
wandb:            eval/avg_f1 0.90758
wandb:      eval/avg_mil_loss 0.36547
wandb:       eval/ensemble_f1 0.90758
wandb:           train/avg_f1 0.88126
wandb:      train/ensemble_f1 0.88126
wandb:         train/mil_loss 0.67477
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run major-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/b5aksgk7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091624-b5aksgk7/logs
wandb: ERROR Run b5aksgk7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: blmcd3yw with config:
wandb: 	actor_learning_rate: 0.0003903865430568406
wandb: 	attention_dropout_p: 0.06112355961884691
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 73
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8045847346198504
wandb: 	temperature: 3.768092050892319
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091734-blmcd3yw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/blmcd3yw
wandb: uploading wandb-summary.json
wandb: uploading history steps 58-74, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆█
wandb: best/eval_avg_mil_loss █▇▁▄
wandb:  best/eval_ensemble_f1 ▁▃▆█
wandb:            eval/avg_f1 ▄▁█▇▁▃▁▅▅▃▃▂▂▆▆▄▂▄▂▅▂▇▄▄▂▃▆▅▆▅▅▇▆▁▂▄▄▆▄▅
wandb:      eval/avg_mil_loss ▅▅▅▁▂▃▄▇▃█▂▁▆▂▅▁▂▃▆▅▄█▃▆▄▃▁▅▂▄▄▁▂▇▄▅▅▁▅▂
wandb:       eval/ensemble_f1 ▄▆▃▂█▇█▆▆▄▂▅▃▅▄▅▃▇▅▃▃▃▃▃▇▁▄▆▃▆▄▆▂▃▅▆▆▅▃▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▄▄▄▆▆▅▄▃▅▅▂▃▄▃▁▁▄▅█▄▅▁▅▃▂▆▁▁▅▃▅▄▄▃▄▆▆▂
wandb:      train/ensemble_f1 ▃▅▃▅▄▇▅▅█▇▂▃▆▅▂▃▁▁▇▅▄▃▁█▂▃▇▂▄▇▄▂▂▅▅▂▅▃▇▂
wandb:         train/mil_loss ██▄▄▇█▄▅▃▆▇▅▅▆▄▃▆▂▆▄▂▄▅▁▃▄▆▃▅▂▃▂▆▂▃▃▂▁▂▁
wandb:      train/policy_loss ▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.26793
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.90411
wandb:      eval/avg_mil_loss 0.25837
wandb:       eval/ensemble_f1 0.90411
wandb:            test/avg_f1 0.87951
wandb:      test/avg_mil_loss 0.17971
wandb:       test/ensemble_f1 0.87951
wandb:           train/avg_f1 0.88035
wandb:      train/ensemble_f1 0.88035
wandb:         train/mil_loss 0.49629
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fiery-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/blmcd3yw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091734-blmcd3yw/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4ajhaado with config:
wandb: 	actor_learning_rate: 4.383997085946187e-06
wandb: 	attention_dropout_p: 0.4092617513705835
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 85
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2878829305806857
wandb: 	temperature: 0.9316573352213632
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091847-4ajhaado
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4ajhaado
wandb: uploading history steps 72-85, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇█
wandb: best/eval_avg_mil_loss ▇▆▃█▁
wandb:  best/eval_ensemble_f1 ▁▅▆▇█
wandb:            eval/avg_f1 ▇▄▄▇▄▃▄▁▇▄█▆▃▇▅▂▅▄▅▆▃▇▅▃▃▃▃█▆▂▆▆▆▃▅▄▄▃▃▃
wandb:      eval/avg_mil_loss ▃▁▄▄▃▃▄▄▃▄▂▄▄▄▂▃▄▄▄▂▃▄▃▄▃▄█▃▄▆▃▅▅▄▂▄▄▃▅▄
wandb:       eval/ensemble_f1 ▁▄▄▄▂▇▆▅▆█▅▄▅▄▃▅▅▄▅▆▃▃▃▆▄▁▃▂▆▆▆▃▄▄▄▄▃▆▂▄
wandb:           train/avg_f1 ▁▇▂▃▆▅▃▄▅▃▅▃█▅▅▅▅▆▆▆▄▅▅▇▇▆▄▆▄▄█▄▄▇▅▄▇▇▃▃
wandb:      train/ensemble_f1 ▄▁▅▇▂▆▄█▃▄▃▃▆▄▃▃▅▄▆▄▆▅▇▅▄▆▆▄▆▅▄▆▃▂▄▆▂▇▅▃
wandb:         train/mil_loss ▆▆▅███▆▅▄▅▅▆▆▆▂▃▅▆▃▆▆▂▅▆▆▄▄▄▄▄▁▄▂▂▁▂▃▃▁▃
wandb:      train/policy_loss ▅▆▆▆▆▆▆▆▆▆▆▆▁▂▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▇█▇▇▇▇▇▇▇▄▇▇▇▁█▇▇▇▆▇▆█▇▅▇▇▇▇▇▇█▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92249
wandb: best/eval_avg_mil_loss 0.19497
wandb:  best/eval_ensemble_f1 0.92249
wandb:            eval/avg_f1 0.88427
wandb:      eval/avg_mil_loss 0.30358
wandb:       eval/ensemble_f1 0.88427
wandb:           train/avg_f1 0.8871
wandb:      train/ensemble_f1 0.8871
wandb:         train/mil_loss 2.33412
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run honest-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4ajhaado
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091847-4ajhaado/logs
wandb: ERROR Run 4ajhaado errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ehvpjz0f with config:
wandb: 	actor_learning_rate: 4.140538420721073e-05
wandb: 	attention_dropout_p: 0.12415884580866764
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 146
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7358252008576243
wandb: 	temperature: 7.864444147582948
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092030-ehvpjz0f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ehvpjz0f
wandb: uploading history steps 141-146, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss ▃█▄▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▂▃▅▄▄▅▄▃▅▃▆▃▄▄▃▅█▂▅▇▄▇▃▆▄▄▃▄▅▅▇▆▄▅▃▆▁▅▄▃
wandb:      eval/avg_mil_loss ▃▄▄▇▅▅▃▆▅▃▃▆▅▁▅▄▄▄▃▃▄▇▅▇▃▃▄▄▃▃▅▃█▃▃▆▅▃▆▃
wandb:       eval/ensemble_f1 █▃▆▂▄▆▆▃▄▂▁▅▇█▄▄▅▇▃▃▆▅▇▃▆▄▅▇▅█▄▅▇▇▄▆▄▄▁▃
wandb:           train/avg_f1 ▆▅▄▇▆▄▄█▆▅▅▆▅▂▆▄▇▄▅▂▅▅▃▇▄▄▃▆▄▄▃▁▄▁▁▂▅▅▃▆
wandb:      train/ensemble_f1 ▅▅▄▇▄▃▆▅▅▅▃▂▃▅▄▄▄▆▃▃▃▅▅█▇▅▃▄█▆▁▁▅▂▄▄▄▃▄▅
wandb:         train/mil_loss ▁▅▇▅▅▄▇▄▅█▄▄███▄▆▆▄▅▆▃▄▆▂▆▃▄▃▅▃█▅▃▆▆▂▄▁▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄█▁▄▄██▁▄▄███▄█▄▁▄▁▄▁▁▄▄▁█▄██▄▄█▄▄█▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.19559
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.91556
wandb:      eval/avg_mil_loss 0.33489
wandb:       eval/ensemble_f1 0.91556
wandb:           train/avg_f1 0.88811
wandb:      train/ensemble_f1 0.88811
wandb:         train/mil_loss 0.24413
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run effortless-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ehvpjz0f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092030-ehvpjz0f/logs
wandb: ERROR Run ehvpjz0f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jn8hsclw with config:
wandb: 	actor_learning_rate: 7.26140785775574e-05
wandb: 	attention_dropout_p: 0.42297937089607063
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 131
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3752589736756293
wandb: 	temperature: 1.8328069056734464
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092249-jn8hsclw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jn8hsclw
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅▆▇▇█
wandb: best/eval_avg_mil_loss █▅▃▃▃▁▂▂
wandb:  best/eval_ensemble_f1 ▁▃▄▅▆▇▇█
wandb:            eval/avg_f1 ▃▃▁▃▅▄▄▂▆▁▃▄▃▄▄▄▃▂▅▄▇▄▄▄▃▄▅▄▃▄▆▆▁▂▅▃▂▃█▂
wandb:      eval/avg_mil_loss ▃▇▃▆▄▃▆▁▂▅▂▂▇▃▂▅▇▃▃▄▄▄▄▁▃▃▄▃▃▃▃▄▂▂▆▂▅█▄▃
wandb:       eval/ensemble_f1 ▄▃▆▄▂▇▆▁▄▆▁▄▄▄▄▃▁█▄▅▃▆▅▄▆▃▃█▇▇▇▄▃▄▇▆▆▃▃▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▄▁▂▃▅▂▆▆▅▄▄▄▄▃▆▆▇▄▅▄▁▆▅▆▃▇▃▃▅▇█▅▄▅▇▇▆▅
wandb:      train/ensemble_f1 ▅▁▃▃▄▆▆▄▆▄▂▆▅▆▇▆▆▆▃▇█▄▄▆▅▅▅▄▆▅▄█▆▄▆▇▅▇▆▆
wandb:         train/mil_loss ▅▆█▅▄▃▅▅▆▄▄▃▃▃▄▃▅▄▅▄▂▄▃▅▆▂▅▁▃▄▄▂▃▄▄▃▄▃▂▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▆▁█▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93328
wandb: best/eval_avg_mil_loss 0.26365
wandb:  best/eval_ensemble_f1 0.93328
wandb:            eval/avg_f1 0.88139
wandb:      eval/avg_mil_loss 0.39444
wandb:       eval/ensemble_f1 0.88139
wandb:            test/avg_f1 0.90701
wandb:      test/avg_mil_loss 0.22
wandb:       test/ensemble_f1 0.90701
wandb:           train/avg_f1 0.88892
wandb:      train/ensemble_f1 0.88892
wandb:         train/mil_loss 3.18156
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sparkling-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jn8hsclw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092249-jn8hsclw/logs
wandb: Agent Starting Run: axj0jn0q with config:
wandb: 	actor_learning_rate: 2.9162321491308614e-05
wandb: 	attention_dropout_p: 0.38382678634483497
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.185390923373442
wandb: 	temperature: 6.0270761854058925
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092449-axj0jn0q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/axj0jn0q
wandb: uploading history steps 91-108; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄█
wandb: best/eval_avg_mil_loss ▃██▇▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄█
wandb:            eval/avg_f1 ▅▅▇▄▂▆█▅▄▄▁▁▅▆▄▇▇▃▄▄▃▇▄▄▇▄▆▅▆▃▆▆▇▃▆▄▄█▅█
wandb:      eval/avg_mil_loss ▆▄▂▅▃▄▅▅▂▂▃▅▂▄█▄▃▇▁▅▃▂▄▅▄▂▂▅▄▃▂▅▄▅▃▆▅▅▄▅
wandb:       eval/ensemble_f1 ▄▄▃▄▆▂▃▅▃█▃▃▄▅▃▁▅▃▃▆▃▂▅▅▃▆▄▆▃▄▂▄▆▄▅▅▄▆▅▆
wandb:           train/avg_f1 ▆▂▃▄▄▇▄█▅▇▆▇▃▆▃▅▄▅▃▄▆▆▆▆▅▆▄▂▆▄▇▆▅▂▆▅▃▄▅▁
wandb:      train/ensemble_f1 ▅▅▃▇▅▆▅▇█▅▆▃▇▄▇█▅▅██▅▆▅▆▅▅▇▆▅▇▃▆▅▃▄▁▄▅▅▅
wandb:         train/mil_loss ▇▄▆▄▇▂▅▅▆▃▄▆▅▅▇██▄▂▃▅▃▄▃▆▃▄▂▆▄▄▄▄▄▁▁▄▄▂▂
wandb:      train/policy_loss ▁▁▁▁▁▄▁▁▁█▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▄▄▄█▄▄▄▄▄▄█▄█▄▄▄▁█▄▁█▄▄▁▄▄▄▄█▄▁▄▄▁▄▁▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9338
wandb: best/eval_avg_mil_loss 0.23012
wandb:  best/eval_ensemble_f1 0.9338
wandb:            eval/avg_f1 0.91873
wandb:      eval/avg_mil_loss 0.23325
wandb:       eval/ensemble_f1 0.91873
wandb:           train/avg_f1 0.87945
wandb:      train/ensemble_f1 0.87945
wandb:         train/mil_loss 0.4956
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run prime-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/axj0jn0q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092449-axj0jn0q/logs
wandb: ERROR Run axj0jn0q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1m9oaueq with config:
wandb: 	actor_learning_rate: 1.557551255055008e-06
wandb: 	attention_dropout_p: 0.2317454274845523
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 198
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.38471189197669864
wandb: 	temperature: 9.670072342475317
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092627-1m9oaueq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1m9oaueq
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▇▇█
wandb: best/eval_avg_mil_loss █▄▅▆▁▄▆
wandb:  best/eval_ensemble_f1 ▁▄▄▅▇▇█
wandb:            eval/avg_f1 ▄▄▂▄▃▂▅▄▃▄▃█▅▆▆▂▄▁▇▅▇▄▃▆▂▅▅▆▇▅▆▅▇▂▇█▅▃▄▅
wandb:      eval/avg_mil_loss ▄▅▇▄▅▆▃▄▃▄▃▅▇▂▅▃█▆▇▄▃▁▅▅▄▅▆▆▅▇▅▄▄▃▃▄▂▇▇▄
wandb:       eval/ensemble_f1 ▄▅▃▆▅▅▆▁▄▇▄▃▃▂▆▅▄▆▇▅▆▇▄█▄▂█▅▇▄▅▆▆▆▂▅▄▅▇▅
wandb:           train/avg_f1 ▄▁▄▂▃▄▃▃▄▅▅▅█▄▃▅▄▄▃▄▅▄▃▅▅▅▂▅▂▄▅▃▄▄▃▃▇▆▄▄
wandb:      train/ensemble_f1 ▆▃█▇▅▅█▂▆▇█▄▇▅▄▆▇▄▆▅▄▇▇▇▆▇▅▁▇▄▅█▇▆▆▅▅█▆▅
wandb:         train/mil_loss ▅▄▅▅▇▇▆█▃▃▅▄▃▂▅▇▂▃▅▂▃▄▃▄▃▄▂▃▄▃▃▄▄▁▄▃▃▂▂▃
wandb:      train/policy_loss ███▂█▁██████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████▁█████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93351
wandb: best/eval_avg_mil_loss 0.28092
wandb:  best/eval_ensemble_f1 0.93351
wandb:            eval/avg_f1 0.90773
wandb:      eval/avg_mil_loss 0.32093
wandb:       eval/ensemble_f1 0.90773
wandb:           train/avg_f1 0.89642
wandb:      train/ensemble_f1 0.89642
wandb:         train/mil_loss 0.29772
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run eager-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1m9oaueq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092627-1m9oaueq/logs
wandb: ERROR Run 1m9oaueq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 41fjjjsv with config:
wandb: 	actor_learning_rate: 1.3418209215853745e-06
wandb: 	attention_dropout_p: 0.04700592707233564
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 177
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4435283289952415
wandb: 	temperature: 0.09217539717247702
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092928-41fjjjsv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/41fjjjsv
wandb: uploading wandb-summary.json
wandb: uploading history steps 109-120, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss ▁▇█
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▆█▄▃▂▁▄▇▁▂▃▅▆▂▇▇█▅▁▇▂▅▅▃▂▄▁▇▇▁▆▅▅▅██▅▂▅▄
wandb:      eval/avg_mil_loss ▂▃▃▆▃█▄▃▃▃█▂▁▅▄▁▃▅▄▂▂▂▆▄▃▇▁▃▁▇▄▇▅▂▂▄▄▅▂▃
wandb:       eval/ensemble_f1 ▆█▆▇▇▄█▄▆▄▇▅▅▄▆▇█▄▄▅▇▆▇▁▅▅▅▅▅▅▇▄▄▅▆▆▄▆▅▆
wandb:           train/avg_f1 ▇▄▅▇▆▄▅▆▁▄▅▆▅▄▆▄█▅▆▅▅▇▆▅▄▅▆▆▅▃▇▆▅▅▅▅▁▃▆█
wandb:      train/ensemble_f1 ▄▄█▃▇▄▃▅▄▃▄▄▂▆▃▃▂▃▄▃▄▆▄▁▄▅▂▆▅▅▃▄▅▂▂▄▆▄▆▅
wandb:         train/mil_loss ▃▇▆█▅▆▅▄▆▄▂▄▃▆▅▂█▃▅▄▄▃▅▃▂▄▄▃▃▁▄▂▅▅▁▂▄▃▂▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄███▄█▄███▄█▄▄▄▄▄▄▄▄▁▄▄▄▄█▄█▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92249
wandb: best/eval_avg_mil_loss 0.30547
wandb:  best/eval_ensemble_f1 0.92249
wandb:            eval/avg_f1 0.87119
wandb:      eval/avg_mil_loss 0.28017
wandb:       eval/ensemble_f1 0.87119
wandb:           train/avg_f1 0.89167
wandb:      train/ensemble_f1 0.89167
wandb:         train/mil_loss 0.43224
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fine-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/41fjjjsv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092928-41fjjjsv/logs
wandb: ERROR Run 41fjjjsv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: y5qfcih2 with config:
wandb: 	actor_learning_rate: 1.6671210501819318e-05
wandb: 	attention_dropout_p: 0.2063881796837808
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 121
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17909303301924884
wandb: 	temperature: 8.243444383948614
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093122-y5qfcih2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y5qfcih2
wandb: uploading history steps 100-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▃▅▅▅▃▅▄▁▂▅▄▄▃▆▃▅▄▇▅▅▂▃▆▄▆▄▃▃▄▁▄▃▇▇▅▆▄▆█▇
wandb:      eval/avg_mil_loss ▃▄▃▂▇▃█▇▄▆▆▂▅▃▄▇▅▅▃▄█▄▅▂▂▃▃▄█▅▇▃▅▆▅▁▆▅▄▅
wandb:       eval/ensemble_f1 ▃█▇▅▅▇▂▂▂▄▃▇▅▆▅▃▅▄▅▆▄▅▃▅▄▄▆▃▇▃▄▅▃▇▁▄▅▇▆▂
wandb:           train/avg_f1 ▇▅▅█▅▄▅▄▇▇▆▄█▅▆▃▂▅▅▄▅▅▄▃▄▅▄▃▃▄▄▂▄▄▃▄▂▂▁▁
wandb:      train/ensemble_f1 ▅▅█▅▅▅▅▃▇▇▆▃▆▆█▅▆▄▂▅▄▄▅▄▁▃▁▅▄▄▄▃▃▄▅▄▄▃▂▁
wandb:         train/mil_loss ▆▄▆▄▅▆▇█▅█▅▅▆▄▃▇▆▄▅▅▆▄▅▄▇▆▄▃▄▄▁▄▄▄▄▃▁▅▂▄
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▃▃█▃▃█▃▃▃▃▃▃▃▁▃▃▃▃▃▇▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▁▃▃▃▃▃▃▃▃▃▃▃▃█▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92653
wandb: best/eval_avg_mil_loss 0.19078
wandb:  best/eval_ensemble_f1 0.92653
wandb:            eval/avg_f1 0.90848
wandb:      eval/avg_mil_loss 0.28632
wandb:       eval/ensemble_f1 0.90848
wandb:           train/avg_f1 0.8739
wandb:      train/ensemble_f1 0.8739
wandb:         train/mil_loss 0.28533
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/y5qfcih2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093122-y5qfcih2/logs
wandb: ERROR Run y5qfcih2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: vmnz162i with config:
wandb: 	actor_learning_rate: 0.0002825017095972234
wandb: 	attention_dropout_p: 0.2445670347166075
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 169
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6539501940117618
wandb: 	temperature: 4.932040976913273
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093300-vmnz162i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vmnz162i
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂█
wandb: best/eval_avg_mil_loss █▁▆
wandb:  best/eval_ensemble_f1 ▁▂█
wandb:            eval/avg_f1 █▆▆▄▂▄▅▅▃▄▆▅▆▁▃▄▆▆▄▇▇▃▇▃▃▅▅██▇▆▆▄▄▇▆▄▇▅▄
wandb:      eval/avg_mil_loss ▇▂▃▅▄▂▂▃▅▃▅▃▂▄▃▄▄▅▃▅▅▁▄▆▂▆▄█▄▄▃▅▅▄▅▃▆▅▅▃
wandb:       eval/ensemble_f1 ▅▃▁▅▆▃▅▃▅▆▇▆▄▃▆▆▃▃▅▆▂▃▄█▆▂▅▃▂▁▄▃▅▅▄▅▄▄▄▅
wandb:           train/avg_f1 ▂▅▂▄▅▂▄▅▅▃▃▇▅▅▅▄▆▃▁▁▅▂▄▃▆▇█▂▅▄▃▆▅▁▅▅▅▄▅▄
wandb:      train/ensemble_f1 ▅▅▆▃▇▆▂▆▆▂▄▄▁▇▅▅▆▅▅▄▅▅▇▅▆▆▇▆▅▄▇█▄▄▇▃▃▇▆▅
wandb:         train/mil_loss █▆▄▆▅▄▅▆▆▄▅▄▄▅▄▄▅▄▄▆▃▃▇▃▄▂▂▃▂▃▅▄▂▂▂▃▁▂▃▂
wandb:      train/policy_loss ▅▅█▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▃▅▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁████████████▄█████▇████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92261
wandb: best/eval_avg_mil_loss 0.30452
wandb:  best/eval_ensemble_f1 0.92261
wandb:            eval/avg_f1 0.9038
wandb:      eval/avg_mil_loss 0.2636
wandb:       eval/ensemble_f1 0.9038
wandb:           train/avg_f1 0.8913
wandb:      train/ensemble_f1 0.8913
wandb:         train/mil_loss 0.27217
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sage-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vmnz162i
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093300-vmnz162i/logs
wandb: ERROR Run vmnz162i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6ax2l9u1 with config:
wandb: 	actor_learning_rate: 3.4502969025460243e-06
wandb: 	attention_dropout_p: 0.007571551089922912
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 134
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6150355562289465
wandb: 	temperature: 2.942430979142101
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093449-6ax2l9u1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6ax2l9u1
wandb: uploading wandb-summary.json
wandb: uploading history steps 91-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ██▇▇▃▅▄▇▅█▄▅▅█▄▁▅▇▅▄▄▆▂▅▅▇▅▄▃▅▅▃▄▂▄▄▅▃▄▇
wandb:      eval/avg_mil_loss ▄▃▅▄▃▄▃▆▅▃▆▆▄▁▅▂▃▆▃█▃▃▅▄▆▃▅▂▃▄█▆█▆▆█▃▄▇▂
wandb:       eval/ensemble_f1 █▁▅▅▄▃▇█▁▅▃▁▄▆▅▄▅▃▄▂▃▂▄█▃▄▆▆▂▆▁▃▃▄▂█▃▂▂▅
wandb:           train/avg_f1 ▇▅▅▆▅▅▅█▇▅▆▆▆▆▄▅▆█▆▆█▇▆▆▇▇▇▇▅▁▆▄▆▆▇▅▆▆▅▅
wandb:      train/ensemble_f1 ▇▅▅▆▅▅▇▄▆▄▆▆▄▃▄▄▅█▆▇▄▄▅▇▇▅▁▇▄▂▆▆▆▇▄█▆▇▇▅
wandb:         train/mil_loss ▅▅▆▆▆▆▇▆▄█▆▇▆▇▅▅▇▃▆▇█▄▅▃▅█▄▄▅▄▁▁▃▅▅▅▅█▆▁
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.20738
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.878
wandb:      eval/avg_mil_loss 0.32053
wandb:       eval/ensemble_f1 0.878
wandb:           train/avg_f1 0.8871
wandb:      train/ensemble_f1 0.8871
wandb:         train/mil_loss 0.60271
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run silver-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6ax2l9u1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093449-6ax2l9u1/logs
wandb: ERROR Run 6ax2l9u1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 3iiqvzd1 with config:
wandb: 	actor_learning_rate: 6.0336193319395275e-05
wandb: 	attention_dropout_p: 0.33294575265865406
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9546260289224212
wandb: 	temperature: 0.0978221277707214
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093621-3iiqvzd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3iiqvzd1
wandb: uploading history steps 52-68, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▆▆█
wandb: best/eval_avg_mil_loss █▁▄▂▁▁
wandb:  best/eval_ensemble_f1 ▁▃▄▆▆█
wandb:            eval/avg_f1 ▄▅▃▆▂▇▇▂▄▁▂▆▃▇▄▅▂▂▄▃▃▂▅▄▄▃▃▃▆▁▄▆▄▃▄▅▄▆█▂
wandb:      eval/avg_mil_loss ▃▇▅▅█▃▆▃▃▃▅▄▄▄▂▇▂▅▂▁▅▇▃▆▃▄▃▄▅▄▄▅▄▁▂▂▄▃▃▃
wandb:       eval/ensemble_f1 ▅▅▄▃▅▃▆▇▂▅▄▇▅▅▆▃▅▇▆▇▃▇▅▁▄▄▄▄▅▆▃▆▄▆▅▆▅▆█▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▄▅▃▁▄▂▁▂▇▄▅▆▄▃▃▂▆▅▄██▁▅▃▇▅▄▄█▄▄▅▆▃▄▄▄▂▅
wandb:      train/ensemble_f1 ▃▆▄▅▅▃▁▃▄▂▄▇▄▃▄▅▆▃▅▆▆█▁▅▃▅▅▄▅█▄▄▄▆▃▄▄▂▆▅
wandb:         train/mil_loss ▆▇▅▆▄▃▁▆▄▅▆▅▂▁▄▄▅▅▃▅▃▃▅▂▄▂▂▇▂▂▃▄▁▃▃█▂▄▆▃
wandb:      train/policy_loss ██████▁█████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▁████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92587
wandb: best/eval_avg_mil_loss 0.27556
wandb:  best/eval_ensemble_f1 0.92587
wandb:            eval/avg_f1 0.88591
wandb:      eval/avg_mil_loss 0.25664
wandb:       eval/ensemble_f1 0.88591
wandb:            test/avg_f1 0.91989
wandb:      test/avg_mil_loss 0.1849
wandb:       test/ensemble_f1 0.91989
wandb:           train/avg_f1 0.89262
wandb:      train/ensemble_f1 0.89262
wandb:         train/mil_loss 0.24937
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run quiet-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3iiqvzd1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093621-3iiqvzd1/logs
wandb: Agent Starting Run: zwwd2g0t with config:
wandb: 	actor_learning_rate: 8.172950888168546e-05
wandb: 	attention_dropout_p: 0.2451022783524028
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 159
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.005497120722998905
wandb: 	temperature: 7.815955992879937
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093729-zwwd2g0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zwwd2g0t
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁███
wandb: best/eval_avg_mil_loss ▄▁█▅
wandb:  best/eval_ensemble_f1 ▁███
wandb:            eval/avg_f1 ▇█▆▆▅▆█▇▆█▆▇▇▆▆▇▆▆▆▇▆▆▆▆▅▅▆▄▇▅▆▆▄▃▃▄▆▁▅▂
wandb:      eval/avg_mil_loss ▂▃▁▃▄▄▄▂▃▄▃▂▄▃▂▃▅▄▄▃▄▄▄▄▃▄▄▅▇▆▄▆▅▆▇▆▇█▇█
wandb:       eval/ensemble_f1 ▇█▅█▅▆▅▆▅█▇▅▅▄▆▅▆▃▅▄▃▅▅▃▆▅▃▂▂▃▃▃▃▄▄▂▁▂▁▁
wandb:           train/avg_f1 ▇█▇▇▇▇▇█▇▆▆▆▅▆▆▅▅▆▆▆▄▄▅▅▅▄▅▄▄▂▄▄▂▃▃▃▁▂▁▁
wandb:      train/ensemble_f1 █▇▇█▇█▇█▇▇▆▇▇▆▇▆▆▇▇▆▅▅▆▄▅▅▄▄▅▄▃▄▄▃▂▃▂▁▁▁
wandb:         train/mil_loss ▆▇█▆▆▅▆▅▅▆▄▃▅▄▄▄▃▃▁▃▂▂▄▃▃▂▄▃▃▃▃▃▂▁▁▂▂▂▂▁
wandb:      train/policy_loss ▁▄▄▄█▄▂▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▄▄▅▄▄▄▄▄▄▆▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▁▁▁█▁▁▁▁▅▃▁▁▁▁▁▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▄▆▄▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91928
wandb: best/eval_avg_mil_loss 0.23701
wandb:  best/eval_ensemble_f1 0.91928
wandb:            eval/avg_f1 0.8354
wandb:      eval/avg_mil_loss 0.45513
wandb:       eval/ensemble_f1 0.8354
wandb:           train/avg_f1 0.81104
wandb:      train/ensemble_f1 0.81104
wandb:         train/mil_loss 0.20818
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fluent-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zwwd2g0t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093729-zwwd2g0t/logs
wandb: ERROR Run zwwd2g0t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 7dsal07f with config:
wandb: 	actor_learning_rate: 0.0003263904793994719
wandb: 	attention_dropout_p: 0.4557638876767941
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 173
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6019343527860107
wandb: 	temperature: 6.481720202521149
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094014-7dsal07f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7dsal07f
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss ▅▁▄█
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▇▆▆▅▇▂▁▄▆▇▅▄▇▂▃█▆▆▅▄▃▅▅▅▂▅▃▅▁▆▆▄▆▅▄▂▃▅▅▃
wandb:      eval/avg_mil_loss ▃▅▄▄▁▃▄▄▄▃▅▆▂▂▄▆▄▄▄▄▅▂▆▃▃▂▃▁▃▃█▃▄▂▂▃▄▆▃▄
wandb:       eval/ensemble_f1 █▅▄▆▅▇▄▇▄▅▃▅█▁▆▄▄▅▄▁▄▃▁▄▃▄▃▆▄▃▅▄▅▃▁▃▄▅▄▅
wandb:           train/avg_f1 ▇▃▆▂▄▃▄▆▅▂█▄▆▅▃▄▅▆▃▆▁▂▃▃▄▄▃▃▄▄▃▅▄▁▂▃▃▄▁▂
wandb:      train/ensemble_f1 ▅▅▆▅▃▆▄▃▇▅▃█▄▃▆▇▅▆▄▆▄▅▃▆▆▃▄▃▇▇▂▁▂▃▄▄▃▅▄▃
wandb:         train/mil_loss ▆▇▅█▆▄▆▅▃▆█▆▅▇█▄▄▃▇▆▄▅▄▄▇▄▅▅▁▃▂▄▄▄▄▂▂▂▂▄
wandb:      train/policy_loss ▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▅▂▁▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.30122
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.87885
wandb:      eval/avg_mil_loss 0.28784
wandb:       eval/ensemble_f1 0.87885
wandb:           train/avg_f1 0.88493
wandb:      train/ensemble_f1 0.88493
wandb:         train/mil_loss 0.30667
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run neat-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7dsal07f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094014-7dsal07f/logs
wandb: ERROR Run 7dsal07f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qbqc0825 with config:
wandb: 	actor_learning_rate: 0.00020587923653031675
wandb: 	attention_dropout_p: 0.28783984161047044
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 196
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8509422790063299
wandb: 	temperature: 6.239407227940756
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094239-qbqc0825
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qbqc0825
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▅█
wandb: best/eval_avg_mil_loss ▅█▂▅▁
wandb:  best/eval_ensemble_f1 ▁▁▄▅█
wandb:            eval/avg_f1 ▁▂▆▂▇█▄▁▃▂▃▅▁▃▃▃▂▅▃▃▃▅▂▂▄▂▅▄▆▅▃▃▂▆▇▃▂▅▆▅
wandb:      eval/avg_mil_loss ▃▄▄▄▃▁▆▄▂▂▅▅▂▆▃▄▃▅▄▅▃▆▃▅▂▆▅▅▂▅▄▂▆█▅▄▄▆▅▂
wandb:       eval/ensemble_f1 ▅▆▃▃▂▂▅▅▄▂▅▇▄▁▆▆▆▅▅▃▄▅▂▃▆▄▅▃█▃▄▆▄▇▅▅▄▇▃▇
wandb:           train/avg_f1 ▇▇▅▅▇▄▇▇▄▅▃▇▅▂▆█▄▆▃▄█▄▂▇▇▅▆█▄▂▆▄▇▁▆▅█▅▆▇
wandb:      train/ensemble_f1 ▄▆▃▅▃▆▂▆▃▄▁▄▁▅▃▃▃█▅▆▃█▂▅▄▇▇▄▄▆▇▄█▁▆▆▃▄█▅
wandb:         train/mil_loss ▅▄▆▃▃▅▃▄▆▄▃▄▆▆█▃▅▆▄▃▇▆▆▇▂▂▅▃▁▃▄▆▃▄▂▅▁▁▄▄
wandb:      train/policy_loss ▅▅▁▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▅▁█▅▁▁█▅▁▅▅▅█▁▁██▁▅█████▅█▁███▅█▅▁▅▁▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9334
wandb: best/eval_avg_mil_loss 0.23827
wandb:  best/eval_ensemble_f1 0.9334
wandb:            eval/avg_f1 0.91087
wandb:      eval/avg_mil_loss 0.25382
wandb:       eval/ensemble_f1 0.91087
wandb:           train/avg_f1 0.89772
wandb:      train/ensemble_f1 0.89772
wandb:         train/mil_loss 0.24358
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qbqc0825
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094239-qbqc0825/logs
wandb: ERROR Run qbqc0825 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ey2f0p4o with config:
wandb: 	actor_learning_rate: 8.022046324769785e-05
wandb: 	attention_dropout_p: 0.34711351975201377
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 81
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04016020947159116
wandb: 	temperature: 5.496406569391597
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094448-ey2f0p4o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ey2f0p4o
wandb: uploading wandb-summary.json
wandb: uploading history steps 71-82, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▇▇█
wandb: best/eval_avg_mil_loss ▇▁█▆▁▄
wandb:  best/eval_ensemble_f1 ▁▄▆▇▇█
wandb:            eval/avg_f1 ▆▅▆▅▃▆▃▆▅▅▄▄▇▆▇▅▂▅▁▆▇▅▅▆▇▆▆▇▅▆▄▅▆▆▁▆▅█▆▇
wandb:      eval/avg_mil_loss ▄▅▂▄▆▄▅▄▅▃▅▅▅▁▂▁▄▆▆▄▃▁▃▅▄▅▆▃▆▆▅█▅█▆▆▇▅▆▇
wandb:       eval/ensemble_f1 ▆▆▆▅▅▇▇▇▅▅▄▃▅▇▅▆▁▇▅█▆▆▆▇█▃▅▅▆▅▄█▆▅▄▄▇█▆█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▅▆▄▄▄▆▃▆▂▅▆▅▅▃▅▂▇▂█▅▅▅▂▅▄▃▄▃▅▅▃▄▃▄▆▆▅▁
wandb:      train/ensemble_f1 ▅▂▄▅▂▄▇▆▄▅▅▅▂▅▂▂▄▃▅█▃▅▂▇▂▃▄▃▆▁▃▃▄▃▅▅▆▆▄▁
wandb:         train/mil_loss ▇▇▆▇▅▃▅▄▄▄▇▅▇▆▇▅▄▆▄▄█▃▅▁▄▂▄▇▆▅▄▄▃▄▄▄▇▄▁▇
wandb:      train/policy_loss ▁▅▁▅▁▅▅▅▁▅█▁▁█▅▅▅▅▅▅██▅▅▅█▅▅▅▁▅█▅▅▅█▅▅▅█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁▁▅▅▁▅▅▁▅▅█▅▅▁▁█▅▅█▅▅▅▅▅█▅▅▅█▅█▅▅▅██▅▅█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92249
wandb: best/eval_avg_mil_loss 0.25908
wandb:  best/eval_ensemble_f1 0.92249
wandb:            eval/avg_f1 0.91135
wandb:      eval/avg_mil_loss 0.21665
wandb:       eval/ensemble_f1 0.91135
wandb:            test/avg_f1 0.89792
wandb:      test/avg_mil_loss 0.18566
wandb:       test/ensemble_f1 0.89792
wandb:           train/avg_f1 0.87515
wandb:      train/ensemble_f1 0.87515
wandb:         train/mil_loss 0.29275
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glamorous-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ey2f0p4o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094448-ey2f0p4o/logs
wandb: Agent Starting Run: zvci1bny with config:
wandb: 	actor_learning_rate: 0.0001026209791553034
wandb: 	attention_dropout_p: 0.19534363339644423
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 115
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2846279194302449
wandb: 	temperature: 4.030196749501731
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094606-zvci1bny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zvci1bny
wandb: uploading wandb-summary.json
wandb: uploading history steps 106-115, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅▆█
wandb: best/eval_avg_mil_loss ▁█▇▁▅▅
wandb:  best/eval_ensemble_f1 ▁▂▄▅▆█
wandb:            eval/avg_f1 ▅▄▅▄▆▅▅▆▄▃▆▄▄▄▄▄▁▃▅▂▅▂▅▂▃▃▃▃▅▆▆▆▅▂▁▂█▃▁▄
wandb:      eval/avg_mil_loss ▃▃▃▁▂▃▄▃▁▁▃█▃▃▂▁▄▄▅▃▄▃▂▄▂▃▄▂▁▁▃▄▃▅▁▂▃▄▄▂
wandb:       eval/ensemble_f1 ▆▄▅▇▇▅▂▅▆▆▆▄▅▅▅▅▅▃▅▅▅▃▅▆▅▄▄▃▄▄▆▆▄▆▇▁▄█▂▄
wandb:           train/avg_f1 █▅▆▆▆▇▇█▆▆▆▆▅▇█▇▅▅▄▇▇▅▅█▇▃▆▄▅▅▆▄▅▃▇▅▅▅▁▆
wandb:      train/ensemble_f1 ▆▄▅▄█▅▃▅▃▄▅▄▃▄▅▄▄▃▄▄▄▆▆▅▇▃▅▄▂▄▄▃▅▃▄▅▅▅▁▂
wandb:         train/mil_loss ▁▅▅▅▅▁▃▇▄▇▄▅▇█▅█▆▄▇▄▅▃▃▇▅▅▂▃█▄▃▂▆▅▂▃▄▅▆▂
wandb:      train/policy_loss ████████████████▁███████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▄▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93016
wandb: best/eval_avg_mil_loss 0.28714
wandb:  best/eval_ensemble_f1 0.93016
wandb:            eval/avg_f1 0.87151
wandb:      eval/avg_mil_loss 0.41115
wandb:       eval/ensemble_f1 0.87151
wandb:           train/avg_f1 0.86803
wandb:      train/ensemble_f1 0.86803
wandb:         train/mil_loss 0.22449
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run soft-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zvci1bny
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094606-zvci1bny/logs
wandb: ERROR Run zvci1bny errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jhdnrsh7 with config:
wandb: 	actor_learning_rate: 0.00015672400664063285
wandb: 	attention_dropout_p: 0.48101881267533814
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.27538666133720613
wandb: 	temperature: 5.181600277241941
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094750-jhdnrsh7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jhdnrsh7
wandb: uploading history steps 147-152, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss ▇█▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▄█▇▅▃▇█▆▅▅▅▁▇▆▅▆▄▇▅▅█▅▄▆▆▅▅▆▅▃█▅▄▄▄▆▂▄▃▂
wandb:      eval/avg_mil_loss ▄▄▃▃▂▃▃▂▃▅▃▅▃▇▁▂▃▂▃▅▄▆▅▄▂▆▃▃▄▅▂▅▃▅▅▃█▅▁▂
wandb:       eval/ensemble_f1 ▄▄▇▅█▆█▆▅▆▅▆▃▆▄▄▆▅▆▃▇▆▆▆▅▄▅▄▃▅▁▄▄▅▆▁▅▃▄▂
wandb:           train/avg_f1 ▆▇█▇▆▆▇▇▅▆▆▆▆▆▅▇▅▅▇▅▅▅▅▆▆▅▃▅▄▃▃▃▄▃▂▂▃▁▂▁
wandb:      train/ensemble_f1 █▅▆▆▄▄▅▅▅▆▇▄▅▅▅▇▄▅▇▅▄▄▄▂▄▅▄▅▂▃▄▃▂▄▄▁▃▂▃▁
wandb:         train/mil_loss ▅█▅▅▄▅▄▄▄▆▆▅▃▄▅▅▅▄▄▄▅▄▃▅▃▅▃▄▂▂▄▃▃▂▃▁▂▃▂▂
wandb:      train/policy_loss ████████████▁███████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▆▁▁▁▁▁▁▇▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.21393
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.89042
wandb:      eval/avg_mil_loss 0.22977
wandb:       eval/ensemble_f1 0.89042
wandb:           train/avg_f1 0.86215
wandb:      train/ensemble_f1 0.86215
wandb:         train/mil_loss 0.24348
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run good-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jhdnrsh7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094750-jhdnrsh7/logs
wandb: ERROR Run jhdnrsh7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wo03ttnq with config:
wandb: 	actor_learning_rate: 1.482149595585085e-06
wandb: 	attention_dropout_p: 0.09561582200396151
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 160
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6370143107688649
wandb: 	temperature: 5.470459888133469
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095015-wo03ttnq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wo03ttnq
wandb: uploading history steps 131-146, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▃▄▇██
wandb: best/eval_avg_mil_loss ▅▅▄▁▂█▃▂▄
wandb:  best/eval_ensemble_f1 ▁▂▃▃▃▄▇██
wandb:            eval/avg_f1 ▆▆▅▆▇▇▆▇▆▅▅▆▃█▅▃▆█▃▃██▆▅▃▇▅▄▅▅▆▇▂▇▃▄▆▃▁▁
wandb:      eval/avg_mil_loss ▂▁▄▃▅▄▁▄▁▂▂▅▂▅▃▂▅▄▅▅▃▄▂▆▄▅▃▄▃▂▅▅▂▆█▄▅▂▅▅
wandb:       eval/ensemble_f1 ▆▄▆▄▅▆▇▄▄▄▄▄▆▃▄▅▇▇▃▇▄▅▅█▅▅▄▆▄▅▆▂▅▆▃▅▁▂▄▅
wandb:           train/avg_f1 ▅▆▆▇▇▅▄▆▅▄▆▆▃█▅▅▅▄▆▄▄█▅▄▃▅▇▃▅▅▄▅▄▂▃▁▂▄▄▂
wandb:      train/ensemble_f1 ▇█▄▆█▅▅▇▆▇▄▆▅▆▃▆▅▄▄▄▃█▅▆▃▆▃▄▅▃▂▃▃▂▃▃▁▁▃▂
wandb:         train/mil_loss █▆▇▅▇▅▄▅▅▆▄▅▅▂▃▅▅▄▆▄▃▃▃▃▃▁▅▁▄▃▂▂▂▃▂▃▃▄▁▂
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▅█▆▆▇▇▆▆▆▆▆▆▆▆▆▆▁▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9229
wandb: best/eval_avg_mil_loss 0.26486
wandb:  best/eval_ensemble_f1 0.9229
wandb:            eval/avg_f1 0.88668
wandb:      eval/avg_mil_loss 0.33366
wandb:       eval/ensemble_f1 0.88668
wandb:           train/avg_f1 0.8719
wandb:      train/ensemble_f1 0.8719
wandb:         train/mil_loss 0.21798
wandb:      train/policy_loss -0.11725
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.11725
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glorious-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wo03ttnq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095015-wo03ttnq/logs
wandb: ERROR Run wo03ttnq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 26u0g4cf with config:
wandb: 	actor_learning_rate: 4.893322696718856e-06
wandb: 	attention_dropout_p: 0.4227824966335229
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5007798811829925
wandb: 	temperature: 0.2327282961999233
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095234-26u0g4cf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/26u0g4cf
wandb: uploading history steps 49-52, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄█▂▂▃▃▄▁▂▂▇▇▆▁▁▆▄▄▄▇▂▄▁▂▄▁▅▁▇▇▇▅▄▅▆▃▄▂▇▂
wandb:      eval/avg_mil_loss ▁▂▃▂▃▂▃▃▂█▂▂▂▂▂▃▁▄▄▂▁▁▁▃▃▂▂▂▂▁▂▂▃▃▂▂▄▃▁▂
wandb:       eval/ensemble_f1 ▇▄█▂▃▃▄▄▂▂▇▇▆▁▁▆▆▄▄▇▂▄▁▂▄▁▅▁▇▇▇▅▄▄▅▃▃▂▇▂
wandb:           train/avg_f1 ▂▄▂▃▄▃▄▇▃▆▁▄▅▁▆▆▂▄▄▆▂▅▅▇▆▂▃▅▄▆▇▆▆▅▃▅▆▇▆█
wandb:      train/ensemble_f1 ▂▄▂▃▅▅▃▄▃▆▁▄▅▁▅█▆▂▄▄▅▇▆▃▂▅▅▄▆▆▆▆▅▃▃▅▆▇▆█
wandb:         train/mil_loss ▅█▇▆▇▇▆▇▆▇▃▅▇▅▇▂▄█▅█▄▄▂▅█▃▃▆▃▃▄▄▅▄▄▄▃▁▄▆
wandb:      train/policy_loss ▄▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▂▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92223
wandb: best/eval_avg_mil_loss 0.32209
wandb:  best/eval_ensemble_f1 0.92223
wandb:            eval/avg_f1 0.88575
wandb:      eval/avg_mil_loss 0.28919
wandb:       eval/ensemble_f1 0.88575
wandb:           train/avg_f1 0.90308
wandb:      train/ensemble_f1 0.90308
wandb:         train/mil_loss 0.33402
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bumbling-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/26u0g4cf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095234-26u0g4cf/logs
wandb: ERROR Run 26u0g4cf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 7kpgp8l3 with config:
wandb: 	actor_learning_rate: 1.685987179838432e-06
wandb: 	attention_dropout_p: 0.4065086460700109
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.749273350715882
wandb: 	temperature: 0.6150871150099491
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095326-7kpgp8l3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7kpgp8l3
wandb: uploading wandb-summary.json
wandb: uploading history steps 87-100, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇█
wandb: best/eval_avg_mil_loss █▂▄▁
wandb:  best/eval_ensemble_f1 ▁▂▇█
wandb:            eval/avg_f1 ▅▅▄█▆▇▆▆▆▃▆▆▅▇▅▄▄▅▁▅▅▆▄▄▅█▆▆█▇▃▅▃▅▅▂▃▆▃▆
wandb:      eval/avg_mil_loss ▂▃▂█▃▂▂▄▄▂▃▃▃▃▂▃▅▄▃▃▄▂▄▄▃▃▅▄▂▃▁▄▁▂▂▃▂▂▄▁
wandb:       eval/ensemble_f1 ▄█▆▃▃█▆▅▃▅▆▇▅▄▃▆▆▅▅▁▄▄▃▃▁█▅▄▃▃▃▅▇▇▅▅▁▅▂▄
wandb:           train/avg_f1 ▅▅▇▄▅▇▄▅▄▄▅▅▃▅▁▄█▅▇▅▆▆▂▂▅▄▅▇▃▇▄▆▅▃▅▅▄▅▆▁
wandb:      train/ensemble_f1 ▆▃▅▄▄▇▅▅▄▅▆▅▅▃▃▄█▅▄▅█▆▂▄▅▃▄▇▆▇▄▇▃▄▆▁▄▅▄▄
wandb:         train/mil_loss ▆▃▇▆█▄▂▅▄▇▃▁▄▃▆▃▁▄▆▆▄▄▃▃▄▆▅▂▅▆▃▄▄▄▂▃▄▆▄▁
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████████████████▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92644
wandb: best/eval_avg_mil_loss 0.26245
wandb:  best/eval_ensemble_f1 0.92644
wandb:            eval/avg_f1 0.9047
wandb:      eval/avg_mil_loss 0.22007
wandb:       eval/ensemble_f1 0.9047
wandb:           train/avg_f1 0.87123
wandb:      train/ensemble_f1 0.87123
wandb:         train/mil_loss 0.24748
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run ethereal-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7kpgp8l3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095326-7kpgp8l3/logs
wandb: ERROR Run 7kpgp8l3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: crzf2x48 with config:
wandb: 	actor_learning_rate: 1.0945178284835757e-05
wandb: 	attention_dropout_p: 0.3079343858476582
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 97
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2257774795562426
wandb: 	temperature: 8.133558591766123
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095459-crzf2x48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/crzf2x48
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆█
wandb: best/eval_avg_mil_loss █▂▁▄
wandb:  best/eval_ensemble_f1 ▁▄▆█
wandb:            eval/avg_f1 ▅▇█▇▆▅▇▇▅▅▇▆▆█▅▅▆▄▅▅▃▇▄▃▃▃▄▅▂▆▄▆▃▂▃▆▄▁▅▁
wandb:      eval/avg_mil_loss ▃▂▁▃▂▂▂▄▃▅█▄▃▅▁▂▄▃▃▃▅▆▂▅▂▂▂▆▁▃▅▃▅▅▆▅▅▅▆▅
wandb:       eval/ensemble_f1 ▆▇█▇█▇▅▇█▇▆▄▆▇▆▇▃▆▄▆▄▆▄▅▇▃▇▇▆▅▄▄▅▃▆▂▅▁▆▂
wandb:           train/avg_f1 ▇█▇▇█▅█▆▅▆▄▆▅▆▆▇▅▇▅▅▅▇▆▆▅▆▃▅▃▄▅▄▄▄▄▄▁▄▄▁
wandb:      train/ensemble_f1 ▅▆█▇▇▆▇▇▇▅▆▇▅▄▆▇▅▅▆▇▇▄▅▆▄▄▅▃▅▆▄▄▄▄▄▃▁▂▄▃
wandb:         train/mil_loss ▆█▆▆▆▆▇▅▅▅▄▅▄▄▄▄▅▅▃▄▄▃▃▄▄▄▃▄▃▄▃▃▂▂▃▂▃▁▁▁
wandb:      train/policy_loss ▄▄▄▅▄▄▂▄▄▄▄▄█▄▄▄▇▆▁▇▄▃▄▄█▄▄█▄▁▄▄▄▄▄▂▄▄▄▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▇▇▅▁▅▅▃▅▅█▅▂▅▅▅▁▅▅▅▄▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91909
wandb: best/eval_avg_mil_loss 0.2586
wandb:  best/eval_ensemble_f1 0.91909
wandb:            eval/avg_f1 0.85036
wandb:      eval/avg_mil_loss 0.28898
wandb:       eval/ensemble_f1 0.85036
wandb:           train/avg_f1 0.85724
wandb:      train/ensemble_f1 0.85724
wandb:         train/mil_loss 0.20902
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run resilient-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/crzf2x48
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095459-crzf2x48/logs
wandb: ERROR Run crzf2x48 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qg8ji8fi with config:
wandb: 	actor_learning_rate: 9.674909852607551e-05
wandb: 	attention_dropout_p: 0.0847663004185501
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2152829714301745
wandb: 	temperature: 7.106268147648411
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095653-qg8ji8fi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qg8ji8fi
wandb: uploading history steps 188-192, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▇▇█
wandb: best/eval_avg_mil_loss ▆▅▅▇█▁▆
wandb:  best/eval_ensemble_f1 ▁▂▄▄▇▇█
wandb:            eval/avg_f1 ▅▅▅▅▆▅▃▁▁▃▅▆▃▄▃▄▄▆▄▅▅▄▆▅▄▆▄▃▆▄█▃▄▆▆▅█▃▂▇
wandb:      eval/avg_mil_loss ▃▇▅▂▂▅▇▆▄▂▂▆▆▃▆▃▂▁▃▄▂▅▂▄▆▄▁▂▂▆▂▃▇▄▄▄▆█▄▃
wandb:       eval/ensemble_f1 ▃▂▆▃▅▃▆▃▆▅▆▂▅▃▄▅▃▂▆▄▃▆▇▆▂▆▆▁▆▆▆▄▆▅▄▇█▃▄▆
wandb:           train/avg_f1 ▇▂▆▅█▆▆▅▄▅▆▇▇▅▁▆▄▇▅▄▇▂▄▃▅█▃▂▆▃▅█▄▃▃▄▆▄▆▄
wandb:      train/ensemble_f1 ▆▅▇▅▃▆▂▆▆▅▄▄▅▃▆▁▄▇▄▃▇▆▃▂▂▃█▃▄▆▆▅▅▄▇▅▇▆▆▄
wandb:         train/mil_loss █▆▇▆▆▇▆▅▇▆▅▅▄▅▆▅▅▅▄▃▃▃▃▃▃▃▁▂▂▃▃▂▃▂▂▂▃▁▂▁
wandb:      train/policy_loss ▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.25217
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.9145
wandb:      eval/avg_mil_loss 0.26044
wandb:       eval/ensemble_f1 0.9145
wandb:           train/avg_f1 0.88896
wandb:      train/ensemble_f1 0.88896
wandb:         train/mil_loss 0.32868
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fine-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qg8ji8fi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095653-qg8ji8fi/logs
wandb: ERROR Run qg8ji8fi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 87b93str with config:
wandb: 	actor_learning_rate: 5.7915928615901246e-05
wandb: 	attention_dropout_p: 0.19629431921383375
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 136
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6091199012575789
wandb: 	temperature: 9.123435374067515
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100035-87b93str
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/87b93str
wandb: uploading history steps 134-136, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▅▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▆▅█▄▅▇▇▅▄▅▆▅▄▅▇▆▄█▄▇▂▆▇▅█▁▆▃▄▅▄▄▄▃▄▆▄▃▄▃
wandb:      eval/avg_mil_loss ▃█▁▅▄▃▄▃▄▆▁▃▃▅▂▅▆▃▂▄▁▃▇▅▅▃▅▂▄▃▄▇▄▅▃▃▄▃▅▃
wandb:       eval/ensemble_f1 ▅▄▅▄▄▄▇▅▁▄▆▅▇▇▇▅▆▆▃▄▄▂▅▆▇██▄▃▄▃▄▄▃▆▄▃▆▂▄
wandb:           train/avg_f1 ▅▅▄▅▇▅█▆▄▇▄▃▅▆█▆▄▄▄▅▅▄▇▄▅▃▆▅▄▅▄▆▅▄▆▅▁▂▃▃
wandb:      train/ensemble_f1 ▅▇█▄▁▄▃▄▆▄▆▅▇█▅▄▅▅▂▇▅▇▃▃▆▄▆▄▂▅▂▃▅▃▃▃▄▁▂▃
wandb:         train/mil_loss ▃█▆▇▇▆▆▄▅▅▆▃▅▆▇█▅▄▅▅▅▃▄▅▄▄▄▁▅▄▄▃▄▄▇▅▂▄▄▂
wandb:      train/policy_loss ▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▇▅▅▅▅▅▅▅▅▅█▁▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92937
wandb: best/eval_avg_mil_loss 0.22714
wandb:  best/eval_ensemble_f1 0.92937
wandb:            eval/avg_f1 0.89095
wandb:      eval/avg_mil_loss 0.30937
wandb:       eval/ensemble_f1 0.89095
wandb:           train/avg_f1 0.88149
wandb:      train/ensemble_f1 0.88149
wandb:         train/mil_loss 2.59772
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hardy-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/87b93str
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100035-87b93str/logs
wandb: ERROR Run 87b93str errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: mekim345 with config:
wandb: 	actor_learning_rate: 0.0001662066765887502
wandb: 	attention_dropout_p: 0.07262205417677708
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 97
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4332492997270365
wandb: 	temperature: 0.8038691535585274
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100420-mekim345
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mekim345
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇██
wandb: best/eval_avg_mil_loss ▃▁▄█
wandb:  best/eval_ensemble_f1 ▁▇██
wandb:            eval/avg_f1 ▃▂▄▂▅▃▅▅▁▂▃▃▆▃▃▄▆▆▁▄█▄▃▂▄▄▃▄▇▄▄▅▄▃▅▅▅▃▃▆
wandb:      eval/avg_mil_loss ▅▅▁▅█▃▆▅▅▄▃▂▁▃▃▅▅▅▆▅▅▄▄▆▃▅▄▅▄▃▅▆▄▆▃▄▃▇▅▄
wandb:       eval/ensemble_f1 ▃█▂▄▅▂▆▄▃▆▆▅▅▃▆▄▁▄▄▃▃▄▅▆▆▁▄▄▅▃█▇▃▅▅▂▃▅▄▄
wandb:           train/avg_f1 ▄▅▅▄▄▅▆▁▂▄▇▃▄▅▆▄▅█▆▄▄▅▃▅▃▆▅▃▃▆▅▅▆▂▂▇▄▃▄▂
wandb:      train/ensemble_f1 ▁▆▆▆▆▆█▇▅▆▅▆▅▆▇▇▆▅▇▆█▆▇▇▅▇▅▇▆█▆▅▆██▇▇▅█▆
wandb:         train/mil_loss ▇▆▇▆██▅▄▆▄▄▆▄▄▅▃▄▄▄▄▄▂▂▁▃▃▄▄▃▂▂▂▂▃▂▁▂▁▁▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▂▅▅▅▅▅▅▅▅▅▃▅▅▅▁▅▅█▅▅▅▁▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▃▆▆▆▆▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆█▆▆▆▆▁▆▂▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9334
wandb: best/eval_avg_mil_loss 0.31307
wandb:  best/eval_ensemble_f1 0.9334
wandb:            eval/avg_f1 0.88859
wandb:      eval/avg_mil_loss 0.34847
wandb:       eval/ensemble_f1 0.88859
wandb:           train/avg_f1 0.87924
wandb:      train/ensemble_f1 0.87924
wandb:         train/mil_loss 0.76428
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run laced-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mekim345
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100420-mekim345/logs
wandb: ERROR Run mekim345 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: by5t8q58 with config:
wandb: 	actor_learning_rate: 1.2209926892360976e-06
wandb: 	attention_dropout_p: 0.25014049661966603
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 51
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2670301886825862
wandb: 	temperature: 6.19125712397963
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100558-by5t8q58
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/by5t8q58
wandb: uploading wandb-summary.json
wandb: uploading history steps 48-52, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▇█
wandb: best/eval_avg_mil_loss ▇█▇▁▃
wandb:  best/eval_ensemble_f1 ▁▄▆▇█
wandb:            eval/avg_f1 ▅▄▄▂▆▆▆▅▄█▆▄▇▃▃▇▇▄▅▇▃▄▅█▂▃▃▅█▃▃▄▅▂▅▆▅█▁▄
wandb:      eval/avg_mil_loss ▄▃▇▅▅▅▃▃▄▅▄▄▃▁▆▁▆▂▄▅▃▅▅▃▁▆▇▅▄▃▄▃▆▃▇▄▃▂█▅
wandb:       eval/ensemble_f1 ▅▄▄▂▆▃▆▅▄█▆▄▇▃▃▅▇▇▄▅▃▄▅█▂▂▃▃▅▃▃▄▅▂▅▆▅█▁▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▄▂▅▃▂▅▄▃▆▂▅██▆▃▃▄█▆▁▇▆▇▅▂▇▃▄▃▇▄▄▄▅▄▅▂▅▇
wandb:      train/ensemble_f1 ▁▃▂▅▃▁▅▃▆▂▆▅██▆▂▃▃▆▄▇▅▇▄▆▁▇▂▃▂▇▃▄▃▅▃▄▁▄▇
wandb:         train/mil_loss ▅▇▆▄▂▅▃▂▄█▅▅▁▃▆▄▆▄▄▆▁▅▁▅▄▇▄▂▇▃▅▄▁▃▃▃▂▅▂▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄█▄▄█▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅█▅▅█▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91873
wandb: best/eval_avg_mil_loss 0.23339
wandb:  best/eval_ensemble_f1 0.91873
wandb:            eval/avg_f1 0.89296
wandb:      eval/avg_mil_loss 0.30641
wandb:       eval/ensemble_f1 0.89296
wandb:            test/avg_f1 0.90521
wandb:      test/avg_mil_loss 0.19396
wandb:       test/ensemble_f1 0.90521
wandb:           train/avg_f1 0.90044
wandb:      train/ensemble_f1 0.90044
wandb:         train/mil_loss 0.25596
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run crisp-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/by5t8q58
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100558-by5t8q58/logs
wandb: Agent Starting Run: 9bfvg6bs with config:
wandb: 	actor_learning_rate: 0.00024479856229240693
wandb: 	attention_dropout_p: 0.2026519514749396
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 121
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.11046662069375757
wandb: 	temperature: 1.6829499173084017
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100654-9bfvg6bs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9bfvg6bs
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▇██
wandb: best/eval_avg_mil_loss ▇▇▇▇▁█
wandb:  best/eval_ensemble_f1 ▁▃▅▇██
wandb:            eval/avg_f1 ▆▇▅▃▃▂▃▅▃▇▄▄▅▄▅▄▇▄█▅▃▇▁▆▅▄▅▄▇▅▇▄▆▅▅▄▅▄▅▄
wandb:      eval/avg_mil_loss ▃█▆▇▅▃▂▂▄▂▃▆▃▃▁▂▄▄▇▄▁▃▄▅▅▆▃▂▄▃▆▇▃▃▃▃▄▃▃▇
wandb:       eval/ensemble_f1 ▆▆▆▄▃▅▆▄▅▃▆▅▁█▆▅▂▆▅▇█▅▆▄█▇▄▅▆▃▇▆▇▅▄▆▇▆▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▅▅▅▄▅▄▄▄▂▅▁▆▄▃▁█▃▄▃▃▂▆▄▂▄▄▅▄▇▄▄▃▃▄▆▅▁▅▃
wandb:      train/ensemble_f1 ▂▄▂▅▅▄▅▃▅▃▅▄▄▄▅▇▃▃▂▇▁▅▄▄▅▃▃█▄▄▅▂▄▃▆▆▅▅▆▇
wandb:         train/mil_loss █▅▆▄▅▅▆▄▄▄▄▄▄▄▄▃▅▃▃▄▄▃▂▂▃▃▃▂▃▃▃▂▄▂▁▃▂▂▂▁
wandb:      train/policy_loss ██▅███████████████▁█████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃█▃▃▃▆▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.926
wandb: best/eval_avg_mil_loss 0.29701
wandb:  best/eval_ensemble_f1 0.926
wandb:            eval/avg_f1 0.8924
wandb:      eval/avg_mil_loss 0.27666
wandb:       eval/ensemble_f1 0.8924
wandb:            test/avg_f1 0.90768
wandb:      test/avg_mil_loss 0.20745
wandb:       test/ensemble_f1 0.90768
wandb:           train/avg_f1 0.90322
wandb:      train/ensemble_f1 0.90322
wandb:         train/mil_loss 0.40627
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sage-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9bfvg6bs
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100654-9bfvg6bs/logs
wandb: Agent Starting Run: l1fwrpkm with config:
wandb: 	actor_learning_rate: 2.8318445229179417e-05
wandb: 	attention_dropout_p: 0.16325908529567484
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 179
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.005937386942896583
wandb: 	temperature: 8.767392516462833
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100854-l1fwrpkm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l1fwrpkm
wandb: uploading history steps 95-110, summary; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▅▅▆█▆▆▅▄▄▄▄▅▄▅▄▄▆▁▄▂▄▂▄▄▅▂▅▅▂▄▄▄▃▄▆▄▅▄▃
wandb:      eval/avg_mil_loss ▃▅▁▁▃▅▂▁▄▃▄█▃▂▃▄▂▅▂▃▃▄▄▃▆▄▅▄▂▇▃▆▆▂▂▄▂▃▅▃
wandb:       eval/ensemble_f1 ▆▅█▅▆▆▆▅▇▄▄▄▅▅▅▅▅▄▅▁▅▄▄▃▅▅▅▄▄▆▇▆▄▄▆▄▂▁▄▅
wandb:           train/avg_f1 ▅▆▇▃▄▅▇▃▅▅▄▄▆█▅▄▂▂▂▄▅▃▄▃▅▃▄▂▇▄▅▅▆▄▄▅▃▁▂▂
wandb:      train/ensemble_f1 ▆█▄▅▇▇█▅▇▂▄▇▄▇▆▅▄▄▄█▄▃▅▁▂▆▄▃▅▂▄▂▅▇▃▃▅▅▂▃
wandb:         train/mil_loss ▆▄▅▅█▅▆▅▅▄▆▄▂▂▄▃▄▅▄▅▄▅▂▄▆▄▅▄▃▂▂▅▃▂▂▃▁▃▂▄
wandb:      train/policy_loss ▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▄▁▁▁▁▁▁▁▁▃█▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▃▁▁▁▁▁▁▁▁▇▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.22311
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.88291
wandb:      eval/avg_mil_loss 0.45726
wandb:       eval/ensemble_f1 0.88291
wandb:           train/avg_f1 0.87072
wandb:      train/ensemble_f1 0.87072
wandb:         train/mil_loss 0.21068
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run good-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/l1fwrpkm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100854-l1fwrpkm/logs
wandb: ERROR Run l1fwrpkm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5fatw8w8 with config:
wandb: 	actor_learning_rate: 0.0008827112181241951
wandb: 	attention_dropout_p: 0.3926603763100705
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9968240992205856
wandb: 	temperature: 4.506110894514438
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101053-5fatw8w8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5fatw8w8
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▆▇█
wandb: best/eval_avg_mil_loss ▆▆▇▁▄█▇
wandb:  best/eval_ensemble_f1 ▁▄▅▆▆▇█
wandb:            eval/avg_f1 ▂▅▂▅▃▄▃▆▂▃▄▆▅▅▂▆▂▆▄▆▅▃▇▅▄▂▂▆▃▄▃▂█▃▃▄▇▁█▆
wandb:      eval/avg_mil_loss ▄▄▄▇▆▇▃▄▄▃▂▁▅▄▅▂▄▃▇▄▆▄▅█▇▅▃▂▅▃▆▂▇▄▃▅▂▃▄▁
wandb:       eval/ensemble_f1 ▂▅▂▂▅▄▆▃▂▄▅▅▂▄▆▆▄▆▅▅▇▅▂▄▂▂▆▃▄▆▂▆▄▃▃▇▁█▃▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▅▆▅▇▂▅▂▂▅▄▃▄▆▅▇▃▇▄▃▄█▁▅▆▃▅▆▆▃▃▃▄▄▅▄▃▂▂▆
wandb:      train/ensemble_f1 ▁▆▅▆▅▃▂▅▅▂▅▄▄▆▅▇▃▇▄▃▄█▁▅▇▃▅▆▆▃▃▃▄▅▂▄▃▂▂▆
wandb:         train/mil_loss ▇▆▆▆▅▅▄▃▆▆▆▃▆▃▄▆▅▅▅▄▅▅▅▆█▃▄▃▅█▅▅▁▅██▄▇▃▄
wandb:      train/policy_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.33147
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.91161
wandb:      eval/avg_mil_loss 0.2061
wandb:       eval/ensemble_f1 0.91161
wandb:            test/avg_f1 0.86514
wandb:      test/avg_mil_loss 0.30153
wandb:       test/ensemble_f1 0.86514
wandb:           train/avg_f1 0.89856
wandb:      train/ensemble_f1 0.89856
wandb:         train/mil_loss 0.26623
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run avid-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5fatw8w8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101053-5fatw8w8/logs
wandb: Agent Starting Run: anzanutv with config:
wandb: 	actor_learning_rate: 1.054310584265415e-06
wandb: 	attention_dropout_p: 0.13397555118753313
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 148
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4268850196124411
wandb: 	temperature: 8.527511557694712
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101150-anzanutv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/anzanutv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▂▂▃▄▇▇█
wandb: best/eval_avg_mil_loss ▇▆█▆█▇▆█▃▁
wandb:  best/eval_ensemble_f1 ▁▂▂▂▂▃▄▇▇█
wandb:            eval/avg_f1 ▄▃▆▅▃▃▃▅▇▃▅▄▇▅▅▆█▅▂▄▇▂▆▅▅▆▃▆▇▂▄▅▅▃▅▁▅▅▅▂
wandb:      eval/avg_mil_loss ▃▃▃▃▃▃▃▄▃▄▄▂▁▂▃▃▄▃▂▄▄▂▄▂▃▄▂▄▄▃▄▂▃▅▅▃▅▃█▄
wandb:       eval/ensemble_f1 ▅▄▅▃▆▅▅▁▆▆▅██▄▆█▃▆▅█▇▅▄▆▇▆▆▅▅▅▆▆▆▆▅▆▅▃▂▄
wandb:           train/avg_f1 ▅▄▆▅▅▄▂▄▅▂▅▆▅▅▅▆▄▄▄▂▄▃▅▇▇▅▅▄▃▂▆▄▄▆▁█▃▂▄▂
wandb:      train/ensemble_f1 ▅▅▇▅▅█▃▇▆▆▅▅▃▄▅▅▁▄█▄▄▄▅▃▄▄▅▂▃▂▃▂▃▂▆▃▂▇▆▁
wandb:         train/mil_loss ██▇▅▆▅▇▅▄▇▅▅▅▇▄▄▆▆▄▄▆▂▅▅▄▃▄▆▅▃▄▃▃▃▄▄▃▃▁▁
wandb:      train/policy_loss █▂▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▅▃▃▃▅▃▅▃▃▃▅▃▃▅▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▁▄▅▅▅▁▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▇█▅▅▅▅█▅▅▅▅▅▅▇▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.19951
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.88012
wandb:      eval/avg_mil_loss 0.29896
wandb:       eval/ensemble_f1 0.88012
wandb:           train/avg_f1 0.87748
wandb:      train/ensemble_f1 0.87748
wandb:         train/mil_loss 2.75611
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run scarlet-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/anzanutv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101150-anzanutv/logs
wandb: ERROR Run anzanutv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1t36wtxm with config:
wandb: 	actor_learning_rate: 2.0252761989765538e-06
wandb: 	attention_dropout_p: 0.06833007079509135
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 189
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0012800999450944817
wandb: 	temperature: 4.550482926145422
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101425-1t36wtxm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1t36wtxm
wandb: uploading wandb-summary.json
wandb: uploading history steps 184-189, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▆▆▇▇██
wandb: best/eval_avg_mil_loss ▄▆▄▅▆▁█▄▄
wandb:  best/eval_ensemble_f1 ▁▄▄▆▆▇▇██
wandb:            eval/avg_f1 ▅▅▆▄▅█▅▅▇▅▃▅▇▅▁▅█▅▅▇▆▅▅▅▅▆▅▆▇▃▅▆▆█▅▆▄▅█▄
wandb:      eval/avg_mil_loss ▃▇▃▂▅▄▄▇▄▁▂▃▄▆▃▅▃▄▂▆▇▃▂▄▆█▆▅▄▂▃▃▅▂▆▂▆▃▄▂
wandb:       eval/ensemble_f1 ▅▅▃▂▁▄▂▁▄▄█▁▁▆▄▆▄▄▃▃▃▄▆▆▄▇▃▄▄█▅▆▅▄▅▅▃▅▇▄
wandb:           train/avg_f1 ▄▃▂▅▃▆▆▅▃▄▆▃▇▅▆▄▄▅▆▆▅▅▅▄▃▆▆█▁▇▄▄▅▅▅▅▅▃▇▆
wandb:      train/ensemble_f1 ▃▅▅▄█▃▄▄█▅▄▆▂▅▂▃▅▁▄▅▇▄▃▂▅▇█▄▄▂▂▆▆▆▅▃▄▆▆▅
wandb:         train/mil_loss ▆▇▅▆▄█▇▇▇█▃▄▅▅▄▃▂▅▄▂▂▅▃▃▃▆▃▂▃▆▅▃▆▂▇▁▃▅▂▂
wandb:      train/policy_loss ▄▄▄█▄▄██▄█▄▄█▁▄▁▄▄▄▄▁▄▄█▄▄▁▁▁▄▄▄▄█▁▄█▄██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄█▄▄▄▄▄██▄█▄▄███▄█▄█▄▁▄▄▄▄▄█▁█▄▄██▄▄▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.3057
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.91524
wandb:      eval/avg_mil_loss 0.39622
wandb:       eval/ensemble_f1 0.91524
wandb:           train/avg_f1 0.89448
wandb:      train/ensemble_f1 0.89448
wandb:         train/mil_loss 0.30514
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run genial-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1t36wtxm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101425-1t36wtxm/logs
wandb: ERROR Run 1t36wtxm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 4iirqa2k with config:
wandb: 	actor_learning_rate: 2.8467004443237855e-05
wandb: 	attention_dropout_p: 0.3508465517921494
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 77
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10477265713731589
wandb: 	temperature: 0.810879137046403
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101721-4iirqa2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4iirqa2k
wandb: uploading history steps 67-77, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▇▄▇█▆▇▄▆▄▇▆▆▅▃▅▆▆▆▆▄▅▄▄█▆▇▃▇▄▅▅▄▄▁▄▆▃▇▅
wandb:      eval/avg_mil_loss ▄▂▅▄▂▄▂▃▃▄▄▅▁▃▅▁▅▃▁▅▄▃▅▃▆▅▅▆▅▆▄▇▅▆▂█▃▆▃▇
wandb:       eval/ensemble_f1 █▇▄▆▅▇▄▇▄█▃▄▆▆▆▄▃▅▅▆▆█▄▇▄█▃▇▇▇▅▅▅▄▁▃▇▇▅▄
wandb:           train/avg_f1 ▇▇▆▇▇▇█▇▇▄▆▇▆▆▄▆▄▄▆▄▅▅▄▄▄▅▃▄▃▄▇▄▃▅▃▂▂▁▃▂
wandb:      train/ensemble_f1 █▇▇▄█▇██▄▅▆▄▇▇██▇▅▄▆▃▄▆▆▄▅▅▅▃▆▃▁▅▃▁▁▂▄▁▄
wandb:         train/mil_loss ▅█▇▆▇▅▃▆▄▅▆▇▆▅▄▆▄▅▄▅▇▃▅▄▄▂▄▆▃▃▅▃▃▁▃▃▃▄▃▃
wandb:      train/policy_loss ▄▄█▄▄▄▄▄▄▄▄▄▄▅▄▄▁▅▄▇▄▄▄▄▄▄▄▂▄▄▄▄▄▄▇▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▆▅▅▆▅▅▅▅▅▅▅▅▃▅▂▅▅▆▅▅▅▂▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92964
wandb: best/eval_avg_mil_loss 0.2229
wandb:  best/eval_ensemble_f1 0.92964
wandb:            eval/avg_f1 0.89014
wandb:      eval/avg_mil_loss 0.31315
wandb:       eval/ensemble_f1 0.89014
wandb:           train/avg_f1 0.86354
wandb:      train/ensemble_f1 0.86354
wandb:         train/mil_loss 0.26591
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fiery-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4iirqa2k
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101721-4iirqa2k/logs
wandb: ERROR Run 4iirqa2k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mfnjwwxb with config:
wandb: 	actor_learning_rate: 1.0827499651812607e-06
wandb: 	attention_dropout_p: 0.3507496930923853
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 161
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1592774272580989
wandb: 	temperature: 2.893104958767684
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101854-mfnjwwxb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mfnjwwxb
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▄█
wandb: best/eval_avg_mil_loss ▁█▄▅▇
wandb:  best/eval_ensemble_f1 ▁▃▃▄█
wandb:            eval/avg_f1 ▃▄▅▃▂▃▄█▁▆▂▁▃▆▃▅▄▃▆▆▂▃▁▄▂▂▅▂▃▃▂▄▃▆▅▄▆▃▂▅
wandb:      eval/avg_mil_loss ▃▄▅▂█▄▅▄▃▃▄▃▅▅▂█▃▄▂▄▄▄▃▃▄▄▂▄▃▄▂▂▆▃▄▃▄▂▁▃
wandb:       eval/ensemble_f1 ▄▄▃▅▅▆▄▅▃██▄▄▅▅▅▇▁▅▁▂▄▂▇▅▇▅▂▅▇▆▃▂▅▆▅█▅▃▇
wandb:           train/avg_f1 ▄▅▃▁▃▅▄▃▃▃▃▄▄▂▂▅▂▄▅▅▄▄▃▃▂▄▁▃▂▅▁█▆▅▃▄▅▅▅▂
wandb:      train/ensemble_f1 ▃▄▆▃▄▅▄▄▄▃▃▅▅▄▄▂▆▃▅▅█▅▇▄▅▆▄▆▅▄█▇▅▄▅▆▆▅▄▁
wandb:         train/mil_loss ▅▇▆▇▂▂▄▇▄▅█▆▄▄▃▅▇▆▄▂▂▄▅▃▃▃▄▄▆▅▅▆▅▄▆▇▅▁▅▂
wandb:      train/policy_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▃▃▃▄▃▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93725
wandb: best/eval_avg_mil_loss 0.34053
wandb:  best/eval_ensemble_f1 0.93725
wandb:            eval/avg_f1 0.90786
wandb:      eval/avg_mil_loss 0.28973
wandb:       eval/ensemble_f1 0.90786
wandb:           train/avg_f1 0.87697
wandb:      train/ensemble_f1 0.87697
wandb:         train/mil_loss 2.92337
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rose-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mfnjwwxb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101854-mfnjwwxb/logs
wandb: ERROR Run mfnjwwxb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: vplm0yvv with config:
wandb: 	actor_learning_rate: 0.0007795534551381014
wandb: 	attention_dropout_p: 0.31247235267933215
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 144
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.13530877223115567
wandb: 	temperature: 3.039838918568447
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102048-vplm0yvv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vplm0yvv
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃██
wandb: best/eval_avg_mil_loss █▇▁▃
wandb:  best/eval_ensemble_f1 ▁▃██
wandb:            eval/avg_f1 ▅▃▃▄▄▅▃▅▁▃▁▅▄▅▆█▃▃█▅▇▆▆▅▄▃▃▃▄▄▄▂▃▃▃▅▃▄▁▃
wandb:      eval/avg_mil_loss ▃▄▂▅▄▇▃▁█▃▃▇▄▅▄▃▄▃▅▄▄▄▄▃▃▄▆▄▄▇█▅▇▅▄▃▄▂▇▃
wandb:       eval/ensemble_f1 ▆▆▅▆▅▇▃▃▄▃▆▅▆▄▇▇▅▆▆▆▅▄▄█▆▆▄▅▅▄▄▁▅▅▅▅▃▅▄▆
wandb:           train/avg_f1 ▇▅▆▄▃▇█▄▅▃▂▄▄▅▁▅▁▂▆▅▄▆▄▅▄▄▁▂▂▄▄▅▄▃▄▄▂▃▄▄
wandb:      train/ensemble_f1 ▄█▂▆▅▃▂▄▄▁▅▁▁▁▄▆▅▄▅▆▂▅▅▅▅▄▂▇▂▃▄▆▄▅▇▃▅▅▄▄
wandb:         train/mil_loss ▃▄▄█▃▃▅▁▄▃▄▄▃▃▅▂▄▁▅▃▅▄▁▂▁▃▄▃▃▃▅▄▃▁▁▃▄▃▄▃
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃█▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93007
wandb: best/eval_avg_mil_loss 0.25223
wandb:  best/eval_ensemble_f1 0.93007
wandb:            eval/avg_f1 0.90449
wandb:      eval/avg_mil_loss 0.26292
wandb:       eval/ensemble_f1 0.90449
wandb:           train/avg_f1 0.88218
wandb:      train/ensemble_f1 0.88218
wandb:         train/mil_loss 0.22211
wandb:      train/policy_loss 0.06682
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.06682
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run charmed-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vplm0yvv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102048-vplm0yvv/logs
wandb: ERROR Run vplm0yvv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1uiu5bpk with config:
wandb: 	actor_learning_rate: 0.0002288451775098652
wandb: 	attention_dropout_p: 0.0639371857524238
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4966236237170194
wandb: 	temperature: 3.689211808165125
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102314-1uiu5bpk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/j8anb0nv
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1uiu5bpk
wandb: uploading history steps 77-87, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▅▆█
wandb: best/eval_avg_mil_loss █▇▄▁▁▃
wandb:  best/eval_ensemble_f1 ▁▂▅▅▆█
wandb:            eval/avg_f1 ▃▆▁▄▃▄▄▄▅▃▂▃▄▂▄▃▃▂▅▇▅▆▅▃▅▅▅▄▅▄▅▄▃▄▆▅█▅▄▃
wandb:      eval/avg_mil_loss ▇▁▇▃▅▄▂▅▄▂▃▅▆▁▃▅█▂▅▂▂▅▃▇▃▄▅▃▄▃▄▄▅▄▄▄▄▅▆▃
wandb:       eval/ensemble_f1 ▇▃▅▃▅▇▂█▃▅▃▇▆▄▃▆█▄▁▅▄▇▆▂▅▆▅▇▄▅▇▅▇▆▆▆▄▇▅▆
wandb:           train/avg_f1 ▄▆▅▃▅▆▄▄▆█▄▅▄▄▁▇▆▃▆▃▁▃▅▄█▇▅▆▆▅▅▃▆▅▄▆▁▄▃▆
wandb:      train/ensemble_f1 █▅▂▄▇▄▅█▇▄▆▄▆▅▁▇▆▃▇▆▄▃▁▃▃█▅▇▅▆▅▅▇▅▃▅▆▇▅▆
wandb:         train/mil_loss █▆▅▇▇▅▇▆▃▇▄▂█▆▄▅▅▄▃▄▃▄▅▄▃▃▂▅▃▃▂▃▁▁▇▄▄▄▄▂
wandb:      train/policy_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▂█▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████▁█████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93715
wandb: best/eval_avg_mil_loss 0.27892
wandb:  best/eval_ensemble_f1 0.93715
wandb:            eval/avg_f1 0.89657
wandb:      eval/avg_mil_loss 0.32469
wandb:       eval/ensemble_f1 0.89657
wandb:           train/avg_f1 0.89736
wandb:      train/ensemble_f1 0.89736
wandb:         train/mil_loss 0.24612
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run earthy-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1uiu5bpk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102314-1uiu5bpk/logs
wandb: ERROR Run 1uiu5bpk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 624, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_ilse.py", line 563, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_ilse:
wandb: ERROR 	Missing key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
