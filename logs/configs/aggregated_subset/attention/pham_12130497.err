wandb: ERROR Error while calling W&B API: Post "http://anaconda2.default.svc.cluster.local/validate": read tcp 10.54.18.4:50674->10.55.247.53:80: read: connection reset by peer (<Response [500]>)
wandb: Agent Starting Run: zqb0317q with config:
wandb: 	actor_learning_rate: 0.0003611871497378264
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 181
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8418804767901192
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041715-zqb0317q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zqb0317q
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json
wandb: uploading history steps 162-181, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇▇███████
wandb: best/eval_avg_mil_loss █▃▄▅▅▂▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇▇███████
wandb:            eval/avg_f1 ▅▇▇▆█▂▇▂▄▇█▂▁▇▄▆▆▃▆▇█▇█▂▁██▇▅▃▄▂█▇▇▂▇▇▂▁
wandb:      eval/avg_mil_loss ▇▃▄▄▄▆█▁▁▇▄▅▄▂▂▃▅▁▄▁▆▅▃█▅▅▄▅▇▅▁▅▅▄▇▅▇▅▂▁
wandb:       eval/ensemble_f1 ▂▇▆▁█▂▃▂▇██▆▅█▇█▇▄▇█▇▇▄▃▅▂▂▂▂▄▇▂▇▇▂▇▂▇▇▇
wandb:           train/avg_f1 ▆▃▃▇▅▇▅▅▃▅▇▇▅▃▃▂▇▆▁▅▅▆▁▅▂▄▄▆▆▅▂██▃▇▆▄▅▆▄
wandb:      train/ensemble_f1 ▃▁▆▂▅▄▇▂▂▅█▅▇▃▇▅▆▆▆▄▆▆▆▅▂▆▄▆▂▇▄▅▁▆▄▇▅▅▂▄
wandb:         train/mil_loss ▄▆▇▁▇▄▄▁▄▆▆▇▅▄▅▅▅▆█▄▄▂▆▃▄▆▂▆▄▁▅▆▅▆▇▁▄▄▆▂
wandb:      train/policy_loss █████▇█▁█████████████████████████████▃██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▁▄█▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9186
wandb: best/eval_avg_mil_loss 0.30767
wandb:  best/eval_ensemble_f1 0.9186
wandb:            eval/avg_f1 0.8768
wandb:      eval/avg_mil_loss 0.33692
wandb:       eval/ensemble_f1 0.8768
wandb:           train/avg_f1 0.73994
wandb:      train/ensemble_f1 0.73994
wandb:         train/mil_loss 1.32649
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vibrant-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zqb0317q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041715-zqb0317q/logs
wandb: ERROR Run zqb0317q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: zg2ha9w8 with config:
wandb: 	actor_learning_rate: 0.0004540544197588698
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 181
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7776980108301699
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_041934-zg2ha9w8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zg2ha9w8
wandb: uploading history steps 134-143, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▂▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▆▁▆▇▆▃█▆▅▇▃▆▆▇▆▇▁▆▄▆▅██▄▆▆█▅▅▆█▇▅▇▇▅▅▆▃▃
wandb:      eval/avg_mil_loss ▅▆▂▄▂▇▅▁▃▅▃▁▁▅▃▁▄█▅▃▁▄▁▃▁▃▃▆▃▅▃▄▄▄▃▅▃▇▄▄
wandb:       eval/ensemble_f1 ▆▆▁▄▄▅▅▅▄▇▂▆▇▇▁▂▂▂▇▄█▄▇▇█▅▆▂▃█▇▅▃▃▆▁▅▃▁▃
wandb:           train/avg_f1 ▃▅▄▄▅▇▂▆▅▃▁▄▆▁▅▆▅▅▅▅▄▄▂▄▄▁▄▄▇▇▄▆▅█▄▇▃▆▂▄
wandb:      train/ensemble_f1 ▅▆▆▄▄▇▅▄▃▆▄▁▄▄▆▅▃▆▅▅▃▆▆▆▅▄▄▃▄▇▆▅▅▄▆▆▆█▇▄
wandb:         train/mil_loss ▄▁▄▄▂▅▅▆▄▄▇▃▅▅▄▅▂▆▄▆▃▅▄▄▆▅▆▅▃▆▄▄▅▃█▄▄▆▄▇
wandb:      train/policy_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92612
wandb: best/eval_avg_mil_loss 0.25147
wandb:  best/eval_ensemble_f1 0.92612
wandb:            eval/avg_f1 0.91414
wandb:      eval/avg_mil_loss 0.24478
wandb:       eval/ensemble_f1 0.91414
wandb:           train/avg_f1 0.84267
wandb:      train/ensemble_f1 0.84267
wandb:         train/mil_loss 0.43682
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cerulean-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zg2ha9w8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_041934-zg2ha9w8/logs
wandb: ERROR Run zg2ha9w8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wrut1ry0 with config:
wandb: 	actor_learning_rate: 4.253030271860168e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 121
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.07730794005753461
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042204-wrut1ry0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wrut1ry0
wandb: uploading history steps 119-121, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss ▆▁▄█
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▇▄▂▄▇█▃█▁▆▆▆▆█▇▇▅▄▃▅▅█▄▇▃▃▆▇█▃▇▇▅▄█▅▇▇▃▄
wandb:      eval/avg_mil_loss ▆▆▄▃▁▃▃▅▁█▁▁▂▃▃▁▂▇▄▂▃▅▁▅▆▁▄▁▁▅▂▂▄▂▄▂▄▁▄▃
wandb:       eval/ensemble_f1 ▅▅▆▃▇▇▅▅▆█▃▅▅▁█▂▇▇▅█▄▆▇▄▄▇▇▁▇█▇▄▇▂▅▇▇▃▅▅
wandb:           train/avg_f1 ▅▆▄▇▁▄▆▄▅▆▅▃▅▃▅▅▇▆▄▅▄▄▆▅▅▄▄▇▄▅▇█▆▃▆▅▅▄▆▅
wandb:      train/ensemble_f1 ▆▆▇▁▄▄▆▃▆▅▅▅▅▂▅▄▄▅▇█▅▅▆▄▇▄█▇▃▃▃▆▅▄▄▅▆▄▇▄
wandb:         train/mil_loss ▇▄▅▂▃▂▃▃▅▅▃▄▅▃▃▂▂▅▂▃▄▄▅▃▃▁▃▄█▄▃▂▆▄▆▃▁▃▃▁
wandb:      train/policy_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▁▅▇▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91069
wandb: best/eval_avg_mil_loss 0.2985
wandb:  best/eval_ensemble_f1 0.91069
wandb:            eval/avg_f1 0.84153
wandb:      eval/avg_mil_loss 0.48572
wandb:       eval/ensemble_f1 0.84153
wandb:           train/avg_f1 0.83925
wandb:      train/ensemble_f1 0.83925
wandb:         train/mil_loss 0.36735
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vivid-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wrut1ry0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042204-wrut1ry0/logs
wandb: ERROR Run wrut1ry0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9dxdht8z with config:
wandb: 	actor_learning_rate: 1.0702250552462067e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 194
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6281277245683898
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042413-9dxdht8z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9dxdht8z
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▆▆█
wandb: best/eval_avg_mil_loss █▂▁▄▆█▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▆▆█
wandb:            eval/avg_f1 ▇▃▄▆▆▅▇▇▇▃▆▅▇▇▁█▇▇█▇▅▇█▆▃▇▇▇▇▆█▅▇▄▅▅█▇▆▆
wandb:      eval/avg_mil_loss ▁▂▁▁█▁▅▂▁▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁▂▁▁▂▂▇▂▇▂▂▁▂▂▆▁▅
wandb:       eval/ensemble_f1 ▇█▇██▇▇▇▅█▇▇▁▇▇▇▇▇▅█▅▇█▆▇▅▇█▇▅▇█▇▇▅▅▇▇▆█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅▇▃▅▅▆▇▂▅▆▆▆▄▅▆▄▃▁▆▂▆▅▄▆▆▅▆▆▆▅▃▇▄▆▆▅▅▄▅
wandb:      train/ensemble_f1 ▆▄█▄▇▄█▃▄▄▄▇▆▄▅▃▆▃▁▆▄▂▇▁▅▂▇▃▆▄▃▆▆▃▆▆▇▅▆▅
wandb:         train/mil_loss ▂▂▄▇▇▅▃▅▁▃▅▁▃▆▃▇▃▂▄▃█▅▅▅▄▆▄▁▄▁▂▄▁▅▂▅▄▄▂▁
wandb:      train/policy_loss ▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.22972
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.90706
wandb:      eval/avg_mil_loss 0.21666
wandb:       eval/ensemble_f1 0.90706
wandb:            test/avg_f1 0.91226
wandb:      test/avg_mil_loss 0.24504
wandb:       test/ensemble_f1 0.91226
wandb:           train/avg_f1 0.89452
wandb:      train/ensemble_f1 0.89452
wandb:         train/mil_loss 0.29021
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fresh-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9dxdht8z
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042413-9dxdht8z/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: r0qhopzk with config:
wandb: 	actor_learning_rate: 1.5164243902197432e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 195
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6422805606996442
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042705-r0qhopzk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r0qhopzk
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 189-196, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁████
wandb: best/eval_avg_mil_loss █▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁████
wandb:            eval/avg_f1 ▄▅█▃▇▇▇▆▆▇▅▇█▇▇▄▆▄▇▆█▇▇▇▁▆█▆▄▇▅▇▁▇▆█▆███
wandb:      eval/avg_mil_loss ▆▂▃▁▂▂▅▇▆▄▁▁▂▁▁▁▁▂▂▂▁▂█▄▂▁▅▄▂▂▂▆▆▁▁▅▁▁▁▁
wandb:       eval/ensemble_f1 ▄▆▇▇▇▆▇▄▃▄▇▇▇▇▇▇▄▇▇██▆▆▇▇█▇█▇▇▆▁▇▅█▆██▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▅▆▄▇▇▄▅▆▇▆▃▇▅▅▆▄▆█▇█▅▂▄▄▇▂▇▄▅▄▁▄▆▆▄▆▇▁
wandb:      train/ensemble_f1 ▆▅▄▄▄▆▅▅▃▄▁▄▅▄▅▆█▄▄▅▇▅▄▅▂▃▅▄▃▆▄▄█▅▄▅▃▇▄▄
wandb:         train/mil_loss ▄▅▄▂▄▄▅▃▃▃▁▄▁▃▄█▇▁▂▆▄▂▂▃▃▆▂▅▃▃▄▃▅▃▄▇▅▄▃▁
wandb:      train/policy_loss ███████████████████████████████████████▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▅▅▁▁█▅▅▁█▅▁▅▅█▅▁▅▅▅▅█▁▁▅▅▁▅▁▅▅▅▅▅▅▅▁▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.27064
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.9183
wandb:      eval/avg_mil_loss 0.27064
wandb:       eval/ensemble_f1 0.9183
wandb:            test/avg_f1 0.75912
wandb:      test/avg_mil_loss 1.04306
wandb:       test/ensemble_f1 0.75912
wandb:           train/avg_f1 0.84172
wandb:      train/ensemble_f1 0.84172
wandb:         train/mil_loss 0.34028
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run eager-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r0qhopzk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042705-r0qhopzk/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: h0dg2p53 with config:
wandb: 	actor_learning_rate: 1.1671056253595047e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 197
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.94155635755614
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_042955-h0dg2p53
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/h0dg2p53
wandb: uploading history steps 148-151, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▄▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▇▃▆▃▇▃▃▅▅▆▃▄▇█▃▆▅▅▃▄▁▄▇▇▄▇▆▆▃▂▇▅▃▃▃▂▄▆▃▄
wandb:      eval/avg_mil_loss ▆▆▃▄▅▅▅▂▄▃▃▄▅▃▄▁▆▃▇▂▄▆▆▅▇▃▆▄▃█▄▄▅▅▃▄▄▃▂▆
wandb:       eval/ensemble_f1 ▇▂▄▃▁▅▇▇▄▅▆▃█▁▃▅▅▃▃▄▇▃▄▆▁▅▅▃▄▃▇▅▃▆▂▅▄▂▅▄
wandb:           train/avg_f1 ▂▆▇▅▇▄▂▄▂▇▇▁▇▃█▆▅▃▄▄▅▆█▅▆▄▇▆▅▆▇▅▇▄▆▇▄▆█▃
wandb:      train/ensemble_f1 █▂▂▇▄▅▃█▅▅▅▄▄▅▁█▅▁▄▄▂▇▅▄▆▅▆█▆▅▆▅▆▆▇▆▇▇▆▄
wandb:         train/mil_loss █▄▃▆▆▇▆▄▆▇▅▄▅▅▄▂▂▇▇▁▅▄▄▃▄▃▁▄▃▅▄▅▄▅▃▄▁▁▄▃
wandb:      train/policy_loss ▅▂▅▅▁▅▅▅▅▅▅▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█████▆███▅███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9112
wandb: best/eval_avg_mil_loss 0.26261
wandb:  best/eval_ensemble_f1 0.9112
wandb:            eval/avg_f1 0.77367
wandb:      eval/avg_mil_loss 0.92889
wandb:       eval/ensemble_f1 0.77367
wandb:           train/avg_f1 0.75775
wandb:      train/ensemble_f1 0.75775
wandb:         train/mil_loss 0.66626
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/h0dg2p53
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_042955-h0dg2p53/logs
wandb: ERROR Run h0dg2p53 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: blubfyxz with config:
wandb: 	actor_learning_rate: 1.560646110481649e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 196
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5796799515136295
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043236-blubfyxz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/blubfyxz
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 187-197, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆▇▇█
wandb: best/eval_avg_mil_loss ▅█▁▄▂▁▅
wandb:  best/eval_ensemble_f1 ▁▄▆▆▇▇█
wandb:            eval/avg_f1 ▇▇▃▆▇▇▇▂▆███▅▇▇▅█▆█▃▇█▄▇█▂█▇▄█▄▄▇▇▅█▇▁▆▇
wandb:      eval/avg_mil_loss █▂▂▅▃▂▄▆▃▃▄▂▂▅▂▁▄▅▁▅▂▂▆▂▃▁▁▂▇▂▁▆▅▆█▆▁▆▆▇
wandb:       eval/ensemble_f1 ▅▇█▇▅▅█▆▆▆▅▇▅█▆▇██▅▇▅▁█▇▇▅██▅▇█▆▇▄▇█▅▃▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▄█▅▆█▃▄▃▆▆▅▆▄▇▅▆▄▅▄▆▃▆█▅▄▅█▅▆▄▅▅▁▅▄▇▅▇▅
wandb:      train/ensemble_f1 ▅▅▅▄▃▅▅▃▅▆▆▃▄▄▂▃▃▃▅▄▆▆▄▆▄▅█▇▅▅▅▂▇▃▄▁▆▆▇▃
wandb:         train/mil_loss ▂▃▃▅▅▄▅▂▁▂▃▄▂▅▂▄▇▅▂▇▄▃▄▅█▄▆▁▂▁▃▅▂▃▂▅▄▂▄▄
wandb:      train/policy_loss ▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁█▄█▄▄▁██▄█▄▄█▄█▄█▁▄█▄▄▄▄▄▄▄▄▄▄▄█▁▄▄▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.30757
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.86656
wandb:      eval/avg_mil_loss 0.35971
wandb:       eval/ensemble_f1 0.86656
wandb:            test/avg_f1 0.74801
wandb:      test/avg_mil_loss 0.70427
wandb:       test/ensemble_f1 0.74801
wandb:           train/avg_f1 0.83007
wandb:      train/ensemble_f1 0.83007
wandb:         train/mil_loss 0.44733
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run revived-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/blubfyxz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043236-blubfyxz/logs
wandb: Agent Starting Run: r4o9yswr with config:
wandb: 	actor_learning_rate: 1.991553899989447e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 198
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6569108298694123
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043520-r4o9yswr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r4o9yswr
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄█
wandb: best/eval_avg_mil_loss ▂█▁▁
wandb:  best/eval_ensemble_f1 ▁▄▄█
wandb:            eval/avg_f1 ▇▇▆▆▇▇▄▄▅▄▁▄▆▄█▆▇▆▅█▇█▃▆▆▅▂▇▇▄▅▃▄▇▄▂█▁▁▃
wandb:      eval/avg_mil_loss ▂▅▁▄▃█▇▅▅▂▇▁▁▁▂▆▁▂▂▄▂▆▁▃▅▄▂▄▂▂▅▄▁▂█▃▅▁▅▂
wandb:       eval/ensemble_f1 ▆▇▆▄▄█▃▅▃█▇▆█▄▇▇▆▇█▇▇█▆▁▄▇▆▇▅▅▅▇▇▄█▅▄▃▇▇
wandb:           train/avg_f1 ▅▁▅▆▄▃▄▄▆▂▄▆▃▇▂▄▆▄▄▇▃▂▅▂▆▂▁▃▄▄▆█▆▇█▄▄▃▆▆
wandb:      train/ensemble_f1 ▂▅▄▄▄▆▄▃▇▃▆▅▄▇▅▇▅▅▇▄▅▄▄▆▃▇▇▇▃▅▅▁▅▅██▅▃▆▃
wandb:         train/mil_loss ▆▇▄▃▄▅▅▇▄▄▃▃▅▃▅▆▅▅▃▂▆▁▂▅▂▃▅█▆▅▄▃▂▄▆▆▅▃▁▅
wandb:      train/policy_loss █▅▅▅▁▅██▅█▅▅▅▅▁▅▁█▅▅▁▅▅▁▅▁▁▅▅▅▅▅▅▅▅▁▁▅█▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91482
wandb: best/eval_avg_mil_loss 0.30644
wandb:  best/eval_ensemble_f1 0.91482
wandb:            eval/avg_f1 0.9112
wandb:      eval/avg_mil_loss 0.26653
wandb:       eval/ensemble_f1 0.9112
wandb:           train/avg_f1 0.84895
wandb:      train/ensemble_f1 0.84895
wandb:         train/mil_loss 0.58399
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run comic-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/r4o9yswr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043520-r4o9yswr/logs
wandb: ERROR Run r4o9yswr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 014llkp1 with config:
wandb: 	actor_learning_rate: 7.756060870041826e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 153
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.15514306410442835
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043703-014llkp1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/014llkp1
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 148-153, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▁▄▆▇▇█
wandb: best/eval_avg_mil_loss ▃█▂▂▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▁▁▄▆▇▇█
wandb:            eval/avg_f1 ▇▇▇█▄█▅█▇▇▁▇▆▅▇▅█▅▆▇▅▇█▇▅▅▇█▇▅▅▄▄█▄▇██▅█
wandb:      eval/avg_mil_loss ▂▄▁▁▁▇▂▁█▁█▄▃▂▅▃▅▅▁▆▆▄▂▆▄▁▂▄▄▁▄▂▂▁▇█▁▁▆▁
wandb:       eval/ensemble_f1 ▇▇▇▇▆████▇█▅▅▄██▆▅▇▅█▅▇▅▇██▇█▇▆▅██▁▅▅▇██
wandb:           train/avg_f1 ▅▆▄▇█▄▅▇▁▆▇▆▃█▇▆▅▅▄▅▆▅▅▆▅▆▅▆▅▆▅▅▅▆▂▆▆▅▆▆
wandb:      train/ensemble_f1 ▄▅▇▆█▅▅▅▄▄▆▆▂▂▅▆▅▅▇█▆▆▅█▅▇▄▆▄▄▆▃▇▅▁▅▆█▂▇
wandb:         train/mil_loss ▆▃▄█▅▃▃▂▁▅▄▅▅▇▆▄▅▄▄▄▅▄▆▂▃▄▄▂▄▄▄▅▄▆▄▄▅▅▃▃
wandb:      train/policy_loss ▄▄▁█▁▁▄▁▄█▁▁▄▁▄▁▄▄▁▁▁██▄▄▄▄▄███▄█▁▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▄▄▄▄▁▄▄▄▄▄▄█▁▄██▄▄▄▄█▄▄▄▄█▁▄█▄▁█▄▄▄▄▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.2488
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.9026
wandb:      eval/avg_mil_loss 0.24609
wandb:       eval/ensemble_f1 0.9026
wandb:           train/avg_f1 0.81744
wandb:      train/ensemble_f1 0.81744
wandb:         train/mil_loss 0.72078
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worldly-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/014llkp1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043703-014llkp1/logs
wandb: ERROR Run 014llkp1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 8amfb352 with config:
wandb: 	actor_learning_rate: 0.0003058988327069652
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 58
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3010258250072888
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_043912-8amfb352
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8amfb352
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading history steps 44-58, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▆▆▇█
wandb: best/eval_avg_mil_loss █▁█▃▂▂▁
wandb:  best/eval_ensemble_f1 ▁▄▄▆▆▇█
wandb:            eval/avg_f1 ▅▁▃▆▄▅▆▄▆▇█▆█▂▇▃▃▄▇▇▇▅▆▇▂▄▄▃▅▆▅▇▄▅▆▇▅▅▂▄
wandb:      eval/avg_mil_loss ▃▇▄▁▃▃▄█▃▁▁▃▁▆▁▄▄▃▃▁▃▃▁▁▅▃▃▄▃▂▄▃▃▃▃▄▄▅▅▁
wandb:       eval/ensemble_f1 ▆▂▄▆▅▆▆▁▇▇█▆▆█▃▇▆▄▄▄▇▇▅▆▇▃▅▅▃▅▄█▅▇▅▅▇▇▅▇
wandb:           train/avg_f1 ▄▆▆▅▇▃▂▃▇▁▆▇▄▃▁▆▅▆▇▆▃▅▆▆▄▇▆▅▇▅▄▂▃▇▂▆▂▃▃█
wandb:      train/ensemble_f1 ▄▅▄▆▇▂▅▃▂▆▅▆▄▁█▄▅▅▁▁▄▅▄▄▅▅▄▆▅▄▃▆▆▂▃▅▂▃▂▇
wandb:         train/mil_loss ▅▄▅▃▁▂▇▄▆▇▆▅▅▇▆▆▄▂▆▃▆▃▅█▆▇▅▃▆▅▃█▆▆▇▄▅▄▇▄
wandb:      train/policy_loss █▁▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▃▁▁▁▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.926
wandb: best/eval_avg_mil_loss 0.21432
wandb:  best/eval_ensemble_f1 0.926
wandb:            eval/avg_f1 0.88918
wandb:      eval/avg_mil_loss 0.27428
wandb:       eval/ensemble_f1 0.88918
wandb:           train/avg_f1 0.88584
wandb:      train/ensemble_f1 0.88584
wandb:         train/mil_loss 0.61191
wandb:      train/policy_loss 0.06392
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.06392
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run avid-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8amfb352
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_043912-8amfb352/logs
wandb: ERROR Run 8amfb352 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 1h5poui7 with config:
wandb: 	actor_learning_rate: 1.732028957341321e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 171
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2715991685661133
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044020-1h5poui7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1h5poui7
wandb: uploading history steps 117-123, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss █▃▁
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▅▄▁▅▆▅▆▂▅▃▂▅▆▅▅▅▃█▄▆▃▆▄▇▆▆▄▅▃▅▂▅▆▆▄▄▆▄▅▄
wandb:      eval/avg_mil_loss ▄▅▃▂▅▄▆▅▅▄▇█▄▅▂▇▁▅▄▄▆▃▅▅▅▃▅▅▄█▂▇▄▆▃▇▄▂▇▆
wandb:       eval/ensemble_f1 ▇▃▅▃█▄▂▆▆▄▃▃▆▅▅▄▆▃▅▁▄▂▆▅▆▄▆▄▂▇▅▆▄▄▄▆▄▃▄▆
wandb:           train/avg_f1 ▆▂▅▅█▅▅▁▄█▂▅▄▂▇▇▅▄▃▄▃█▄▃▆▁▇▆▅▅▅▅▄▄▄▁▃▅▅▄
wandb:      train/ensemble_f1 ▆▂▄▆█▃▂▄▅▅▆▄▇▅▂▂▇▇▅▃▄▄▆▁▅▄▃▃▅▅▇▃▂▄▅▅▅▄▆▃
wandb:         train/mil_loss ▆▆▃▄▅▆▃▆▅▄▆▅▅▅▆▅▄▄▅▂▆▄▆▆█▃▅▆▆▄▇▄▆▂▆▆▄▄▅▁
wandb:      train/policy_loss ▁▁▅█▁▅▁▁█▅██▁▅▅▅▁▅█▅█▁█▅▁▁█▁▁█▁▅▁▁▁█▅▅▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.89963
wandb: best/eval_avg_mil_loss 0.27693
wandb:  best/eval_ensemble_f1 0.89963
wandb:            eval/avg_f1 0.76611
wandb:      eval/avg_mil_loss 0.83172
wandb:       eval/ensemble_f1 0.76611
wandb:           train/avg_f1 0.79198
wandb:      train/ensemble_f1 0.79198
wandb:         train/mil_loss 0.74996
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smart-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1h5poui7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044020-1h5poui7/logs
wandb: ERROR Run 1h5poui7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: pcxcj6e9 with config:
wandb: 	actor_learning_rate: 1.1557028098655475e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 108
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8583386837872405
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044234-pcxcj6e9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pcxcj6e9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▃▇▄▆▁▂▅▅▄▆▅▇█▅▇▂▄▄██▇▅▇█▇▇▅▇▆▇▆▇▇▆▅▅▆▃▅
wandb:      eval/avg_mil_loss ▁▄▅▅▇▄▄▄▃▄▄▄▄▄▄▂▅▆█▃▄▂▃▁▅▄▂▁▄▄▂▇▄▃▃▇▂▃▂▁
wandb:       eval/ensemble_f1 ▄▆▃▇▇▅▆▅▃█▃▄▅█▅▅▇█▃▇▇▆█▇▃▇▁▆▅▇▇▆▇▅▂▄▇▆█▁
wandb:           train/avg_f1 ▄▆▇▃▅▄▇▄▆▆█▇▄▆▄▆▅▄▃▆▅▃▆▄▄▁▄▃▅▅▄▇▆█▅▂▄▂▆▆
wandb:      train/ensemble_f1 ▅▄▄▆▅▄▇▅▄█▁▄▆▃▃▂█▂▅▃▅▅▇▆▄▃▄▄▆▆▆▅▇▆▅▅▅█▅▆
wandb:         train/mil_loss ▆▄▆▄▆▆▃▂▇▃▄▇▁▃▃▅▅▇▄▇▅▆▅▂▄▁▄▁▅▅▃█▇▆▅▄▇▃▆▂
wandb:      train/policy_loss ▁▁▁▁▁▃▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9038
wandb: best/eval_avg_mil_loss 0.44117
wandb:  best/eval_ensemble_f1 0.9038
wandb:            eval/avg_f1 0.81381
wandb:      eval/avg_mil_loss 0.73978
wandb:       eval/ensemble_f1 0.81381
wandb:           train/avg_f1 0.85068
wandb:      train/ensemble_f1 0.85068
wandb:         train/mil_loss 0.85016
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run breezy-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pcxcj6e9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044234-pcxcj6e9/logs
wandb: ERROR Run pcxcj6e9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ompsvct3 with config:
wandb: 	actor_learning_rate: 6.5813711307992945e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 90
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10871702913803728
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044434-ompsvct3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ompsvct3
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▇▇██
wandb: best/eval_avg_mil_loss █▄▂▂▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▇▇██
wandb:            eval/avg_f1 ▄▆▇▆▇▇▇▆▅▅▆▆▄▆▃▄▅▄▇█▇▃▅▆█▁███▄▅▇▆▇▇█▇█▁▆
wandb:      eval/avg_mil_loss ▆▃▂▂▂▂▁▂▂▂▂▅▃▄▂▄▁▇▄▄▂█▅▃▂▁▁▁▄▆▄▄▄▆▁▃▁█▁▁
wandb:       eval/ensemble_f1 ▆▇▆▇▇▆▆▇▅▅▆▇▇▆▅▇▆▆▃▄▃▇▇▇██▁██▄▅▆▇▇▇▂▇█▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▄▄▅▄▁▄▃▆▃▃▁▂▅▃▄▄▅▃▅▆▄▆▄▆▆▃█▆▃▃▃▄▇▅▃▅▂▆
wandb:      train/ensemble_f1 ▂▄▂▄▅▆▃▄▃▄▁▂▅▄▃▄▅▄▅▆▅▂▆▄▃▆▃██▆▂▄▆▇▄▅▃▂▅▆
wandb:         train/mil_loss ▁▄▅▄▄▆▃▆▁▄▆▅▃█▂▃▆▅▃▆▃▄▃▄▅▄▅▆▄▄▄▃▄▅▅▅▅▅▂▃
wandb:      train/policy_loss █████████████████▁██████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████▁█████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90363
wandb: best/eval_avg_mil_loss 0.25822
wandb:  best/eval_ensemble_f1 0.90363
wandb:            eval/avg_f1 0.89963
wandb:      eval/avg_mil_loss 0.26776
wandb:       eval/ensemble_f1 0.89963
wandb:            test/avg_f1 0.91253
wandb:      test/avg_mil_loss 0.20998
wandb:       test/ensemble_f1 0.91253
wandb:           train/avg_f1 0.84135
wandb:      train/ensemble_f1 0.84135
wandb:         train/mil_loss 0.46284
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fiery-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ompsvct3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044434-ompsvct3/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: me2xaw8k with config:
wandb: 	actor_learning_rate: 4.362505583212767e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 50
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6722302846413669
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044625-me2xaw8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/me2xaw8k
wandb: uploading wandb-summary.json
wandb: uploading history steps 40-51, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss █▃▁▄
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▆▆▂██▇▇▅▇▁▇▇▃█▇▇▇▇▇▅▇▂█▅▇▅█▇███▇▆▇█▇▅██▇
wandb:      eval/avg_mil_loss ▃▃▅▂▁▂▃▃▂▆▂▃▂▇▃▃▁▂█▂▃▅▂▄▂▃▃▂▁▂▂▃▃▁▃▅▃▂▂▂
wandb:       eval/ensemble_f1 ▆▆▂█▃▇▆▅▇▁▇▇▃█▆▇▇▇▆▅▂█▅▇▇█▇▇██▇▆▇██▅█▇▅▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▆▆▄▂▄▇▅▇▅▅▅▆▆▂▇▄▆▇▄▃▇▅▇▆▅▆▆▆▅▇█▅▆▆▇▄▁▇▅
wandb:      train/ensemble_f1 █▆▆▄▂▄▇▅▇▆▅▆▆▂▅▄▆▇▄▅▇▅▇▆▆▅▆▆▆▅▇█▅▆▆▇▇▁▇▅
wandb:         train/mil_loss ▂▄▄▂▅▄▁▅▁▂█▂▁▃▃▂▂▂▃▂▂▁▄▃▂▃▃▂▃▄▅▄▅▂▂▂▁▄▅▁
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▁▅▅▁▅▅▅▁▅▁▅▅▅▅█▅▅▅▅▁▅▅▅▁▅▅▅▅▅▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▁▅▅▁▅▅▅▁▅▅▅▅▅▅█▅▅▅▅▁▅▅▅▁▅▅▅▅▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.926
wandb: best/eval_avg_mil_loss 0.28133
wandb:  best/eval_ensemble_f1 0.926
wandb:            eval/avg_f1 0.89702
wandb:      eval/avg_mil_loss 0.32037
wandb:       eval/ensemble_f1 0.89702
wandb:            test/avg_f1 0.93845
wandb:      test/avg_mil_loss 0.17713
wandb:       test/ensemble_f1 0.93845
wandb:           train/avg_f1 0.88447
wandb:      train/ensemble_f1 0.88447
wandb:         train/mil_loss 0.23378
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run neat-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/me2xaw8k
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044625-me2xaw8k/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: b2fp99zg with config:
wandb: 	actor_learning_rate: 0.00010817584292266056
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 170
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.31706992082642815
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044748-b2fp99zg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/b2fp99zg
wandb: uploading history steps 127-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇█
wandb: best/eval_avg_mil_loss █▁██
wandb:  best/eval_ensemble_f1 ▁▆▇█
wandb:            eval/avg_f1 ▇▇▅▃▄▅▄▁█▇▇▅▇▅▆▆▅▅▇▄█▇▂▆▆▆▇▇▅▅▅▇▃▅▇▇▆▆▇▅
wandb:      eval/avg_mil_loss ▅▂▁▃▂▆▁▃▁▆▁▅▆▅▁▁▄▄▂▁▁▄▄▂▁█▆▁▆▃▅▁█▄▆▆▅▇▄▅
wandb:       eval/ensemble_f1 ▂▅▂▇██▄█▅▄█▄▇▄▅▇▇▄▂▇▇▅▇▁▇▄▇▅▇▃▄▄▇▂█▄▇▇▅▇
wandb:           train/avg_f1 ▁▅▄▆▂▆▅▄▅▂▄▆▁█▄▅▅▄▅▃▃▂▄▃▅▃▂▃▂▄▄▁▄▄▅▆▄▄▅▂
wandb:      train/ensemble_f1 ▃▁▃▃▆▇▅▂▃▄▁▅▅▂▆▆▇▄▇▄▄▄█▆█▃▆▂▇▅▅▅▂▆▇▇▂▅▅▃
wandb:         train/mil_loss ▅▃▃▃▂▂▅▆▁█▃▂▅▅▆▆▂▆▅▆▂▅▂▃▅▁▃▅▄▅▇▃▃▆▃▃▃▅▂▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.30835
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.78073
wandb:      eval/avg_mil_loss 0.77012
wandb:       eval/ensemble_f1 0.78073
wandb:           train/avg_f1 0.84612
wandb:      train/ensemble_f1 0.84612
wandb:         train/mil_loss 0.5591
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devoted-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/b2fp99zg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044748-b2fp99zg/logs
wandb: ERROR Run b2fp99zg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mq2906oa with config:
wandb: 	actor_learning_rate: 0.00016886526712504317
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5275697684644662
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_044946-mq2906oa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mq2906oa
wandb: uploading config.yaml
wandb: uploading history steps 163-175, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇████
wandb: best/eval_avg_mil_loss █▂▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇████
wandb:            eval/avg_f1 ▄▆▇▆▇▄▇▆▇▄▄█▅▇█▁▄▅▅▆▇▇▇▆██▆██▆▇▅▂▅██▄▇▅▅
wandb:      eval/avg_mil_loss ▁▁▅▁▃▃▆▁▁▄▄▁▃▆▁▅▂▂▁▂▃▄▂▁▄▁▄▁▃▁█▁▃▃▂▁▂▃▃▁
wandb:       eval/ensemble_f1 ▅▄██▅▄▇▇▇▆▇▄▁▅▆█▂▃▆▆▇▄▇██▇▄▆█▆▃█▇▅▆▅▅▇█▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▁▅█▅▇█▂▂▄▇▂▂▄▃▅▆▄█▆▃▅▄▁▅▃▅▅▃▂▇▂▅▄▆▄▃▅▅▆
wandb:      train/ensemble_f1 ▆▄▇▆▂█▆▆▅▃▁▆▇▆▃▅▆▆▆▇▇▄▅▃▅▄▆▄▄▆▄▄▅▃▅▄▅▅▄▄
wandb:         train/mil_loss ▄▄▆▅▃▅▆▄▄▆▁▂▃█▅▇▄▄▃▄▄▂▃▃▄▅▃▄▄▄▂▂▃▂▆▄█▃▂▅
wandb:      train/policy_loss ▄▄▄▁▄▄▄▄██▄▄▄▁▄▄▄▄▄▄▄▄▄█▁█▁▄▄▁▄▄▁█▁▄▄▄▄▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91087
wandb: best/eval_avg_mil_loss 0.26361
wandb:  best/eval_ensemble_f1 0.91087
wandb:            eval/avg_f1 0.86431
wandb:      eval/avg_mil_loss 0.37267
wandb:       eval/ensemble_f1 0.86431
wandb:            test/avg_f1 0.82725
wandb:      test/avg_mil_loss 0.42925
wandb:       test/ensemble_f1 0.82725
wandb:           train/avg_f1 0.83105
wandb:      train/ensemble_f1 0.83105
wandb:         train/mil_loss 0.96197
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run jolly-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mq2906oa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_044946-mq2906oa/logs
wandb: Agent Starting Run: 4qzr8b08 with config:
wandb: 	actor_learning_rate: 0.0003492518507333557
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 169
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5724683230037436
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045216-4qzr8b08
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4qzr8b08
wandb: uploading history steps 158-169, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇▇██
wandb: best/eval_avg_mil_loss █▁▂▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆▇▇██
wandb:            eval/avg_f1 ▄▁▅▁▁▅▃▃▂▅▅█▆▆▇▆▅▄▆▄▁▄▆▅██▅▁▆▇▂▁▆▅▄▇▇▂██
wandb:      eval/avg_mil_loss ▃▁▁█▆▆▇▅▇▅▇▄▇▃▇▅▅▆▁▃▃▅▅▅▄▃▄▅▆▄▂▃▅▇▆▃▅▄▆▇
wandb:       eval/ensemble_f1 ▅▇▆▃▂▄▆▁█▆▃▆▆▆▄▆▅▃█▄▅▄▅▅▅▇▄▅▅▆▂▅▄█▄▄▃▆▂█
wandb:           train/avg_f1 ▇▅▂▃▅▂▅▅▄▃█▃▃▃▄▁▁▃▆▄▅▃▆▄█▄▇▆█▄▃▃▆▇▂▃▂▃▃▃
wandb:      train/ensemble_f1 ▂▆▇▆▂▃▃▂▄▄▅▅▁▆▆▃▃▄▂▃▃▂▄▅▂▆▆▂▄▄▅▃▃▆█▃▂▂▆▅
wandb:         train/mil_loss ▂▇▂▅▆▆▇▅▇▅▅█▄▅▄▆▅▄▅▄▅▄▄▅▂▆▆▆▅▃▄▄▁▃▄▄▅▆▄▂
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▅▆▆▆▆▆▆▁▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9145
wandb: best/eval_avg_mil_loss 0.24813
wandb:  best/eval_ensemble_f1 0.9145
wandb:            eval/avg_f1 0.89963
wandb:      eval/avg_mil_loss 0.2699
wandb:       eval/ensemble_f1 0.89963
wandb:           train/avg_f1 0.80401
wandb:      train/ensemble_f1 0.80401
wandb:         train/mil_loss 0.77895
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cerulean-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4qzr8b08
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045216-4qzr8b08/logs
wandb: ERROR Run 4qzr8b08 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 	size mismatch for task_model.mlp.0.weight: copying a param with shape torch.Size([64, 22]) from checkpoint, the shape in current model is torch.Size([256, 22]).
wandb: ERROR 	size mismatch for task_model.mlp.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
wandb: ERROR 	size mismatch for task_model.mlp.3.weight: copying a param with shape torch.Size([2, 64]) from checkpoint, the shape in current model is torch.Size([2, 256]).
wandb: ERROR 
wandb: Agent Starting Run: 1rwwpg6a with config:
wandb: 	actor_learning_rate: 1.1478792869923586e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 187
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.547454803751833
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045522-1rwwpg6a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1rwwpg6a
wandb: uploading wandb-summary.json; uploading history steps 98-116, summary
wandb: uploading history steps 98-116, summary
wandb: uploading data
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss ▆█▁
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 █▂▇▃▁████▇▇██▆█▇▅▂█▇▂▁█▇█▇▁█████▁█▇█████
wandb:      eval/avg_mil_loss ▁▁▃▄█▁▅▁▄▁▁▅▃▂▄▁▁▅█▄▄▁▅▇▁▃▄▃█▁▄▁▁█▃▇▁▅▁▁
wandb:       eval/ensemble_f1 █▂▂▂█▂███▇███▃▇▆▁█▇▂█▂▇▁▇█▇█▇▁▇██▇▃█▁███
wandb:           train/avg_f1 ▆▅▇▄▅▂▅▇▇▆▆▇▄▁▆▅▆▅▄▅▄▆▇▄▅▅▆▂▆▅▄▃█▆▄█▄▇▆▇
wandb:      train/ensemble_f1 ▆▄▅▇▆▇▄▁▇▄▇▅█▆▅▄▄▅▅▆▅▇▅▅▄▅▅▃▆▇▆▄█▅▄█▄▇█▅
wandb:         train/mil_loss ▄▂▃▆▇▅▇▅▇▆▄▆▅▅▆▇▄▅▁▄▃▇▂█▅▄▅▄▇▄▄▇▇▄▅▅▅█▁▄
wandb:      train/policy_loss ▁▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▃▅▅▅▅▅▁▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▁▅█▅▅▁▅▅▅▅▅▅▅▅▅▅█▅█▁▁▅▅▅▃▅▅▅█▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91778
wandb: best/eval_avg_mil_loss 0.26113
wandb:  best/eval_ensemble_f1 0.91778
wandb:            eval/avg_f1 0.90725
wandb:      eval/avg_mil_loss 0.29343
wandb:       eval/ensemble_f1 0.90725
wandb:           train/avg_f1 0.81212
wandb:      train/ensemble_f1 0.81212
wandb:         train/mil_loss 0.74734
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lively-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1rwwpg6a
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045522-1rwwpg6a/logs
wandb: ERROR Run 1rwwpg6a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 57ezkmrg with config:
wandb: 	actor_learning_rate: 2.946463204207924e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7181566378930058
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045659-57ezkmrg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/57ezkmrg
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 76-89, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇▇▇███
wandb: best/eval_avg_mil_loss █▁▂▂▁▁▂▁▂
wandb:  best/eval_ensemble_f1 ▁▇▇▇▇▇███
wandb:            eval/avg_f1 ▃▇▇▇▇▇▇▇▆▇▇▇▁██▇▇▁▇▇▅▇█▆█▇▇█▇▇▇██▇▇▅▇▇▇▅
wandb:      eval/avg_mil_loss ▂▂▂▁█▁▂▂▂▂▁▁▁▁▂▁▂▆▁▂▂▁▂▂▁▁▁▂▂█▁▂▂▁▁█▂▂▂▂
wandb:       eval/ensemble_f1 ▃▇▇▇▇▇▇▇█▇▇▇▇█▇█▇▁████▆█▆▇█▇▇▇▇▇▇▇▆▇██▇█
wandb:           train/avg_f1 ▆▆█▃▅▇▆▇▅▁▃█▅▆▆▇▆▆▄▇▆▃▆▅▇▇▆▄▆▇▇▄▇▄▄▂▆▇▄▇
wandb:      train/ensemble_f1 ▆▄▇▄▆█▆▇▁█▅▇▇▆▆▇▆▆▇▅▆▄▅▅▇▃▆▅▇██▇▇██▄▆▂▇█
wandb:         train/mil_loss ▁▂▂▂▅▂▁▂▁█▂▄▂█▃▂▂▂▂▆▁▂▃▂▃▂▄▄▂▁▂▅▄▂▃▂▁▁▂▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▁▄▄▄▄▄█▄▄▄██▄▄▁▄▁▄▄▄▄▄▄▄▁▁▁███▄▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██▅▅█▅▅▅▁▁▅▅▅▅▅▅▅▅▁▁▅▁▁█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.37478
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.90236
wandb:      eval/avg_mil_loss 0.39289
wandb:       eval/ensemble_f1 0.90236
wandb:           train/avg_f1 0.9034
wandb:      train/ensemble_f1 0.9034
wandb:         train/mil_loss 0.26795
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sweet-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/57ezkmrg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045659-57ezkmrg/logs
wandb: ERROR Run 57ezkmrg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ljtmf28q with config:
wandb: 	actor_learning_rate: 3.95619449739594e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 63
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8236292476084578
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045835-ljtmf28q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ljtmf28q
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss ▂█▁
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 █▆█▅▇▅▆▇▅▆▆▆▇█▆▆▅▇▅█▆█▆▅▆█▄▅▄▆██▇▁█▅▅▆██
wandb:      eval/avg_mil_loss ▁▃▄▆▁▆▄▂▃▅▃▂▄▁▃▅▁▆▂▆▆▃▁█▇▃▄▇▃▆▄▃▁▁▁▇▆▅▅▅
wandb:       eval/ensemble_f1 ▆█▇█▂▇▅▆█▆█▇█▅▇█▅▆▆▆█▆▅▆██▅▆▅▄▆▆▇▁█▆▆█▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▃▅▅▅█▇▇▅▃▇▅▇▆▄▆▇▃█▄▆▃▃▆▄▇█▂▁█▆▆▄▆▅▆▃▃▄
wandb:      train/ensemble_f1 ▄▄▃▅▄▄▇▆▆▄▆▅▄▆▅▄▄▅▃▂▄▂▆▅▆▁▃▇▃▇▅▃▅▅▃▅▂▃▄█
wandb:         train/mil_loss ▂▄▃▄▇▄▃▃▄▄▃▄▅▄█▃▁▃▃▄▃▄▆▄▄▅▃▇▂▃▄▃▄▃▄▇▅▅▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████▁████████████▅███████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91886
wandb: best/eval_avg_mil_loss 0.25863
wandb:  best/eval_ensemble_f1 0.91886
wandb:            eval/avg_f1 0.89726
wandb:      eval/avg_mil_loss 0.98345
wandb:       eval/ensemble_f1 0.89726
wandb:            test/avg_f1 0.87982
wandb:      test/avg_mil_loss 0.80032
wandb:       test/ensemble_f1 0.87982
wandb:           train/avg_f1 0.87846
wandb:      train/ensemble_f1 0.87846
wandb:         train/mil_loss 1.01891
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run zesty-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ljtmf28q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045835-ljtmf28q/logs
wandb: Agent Starting Run: yq3mzo5c with config:
wandb: 	actor_learning_rate: 3.0350393511609658e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 196
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8639572820726955
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_045931-yq3mzo5c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yq3mzo5c
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▇█
wandb: best/eval_avg_mil_loss ▃█▇▂▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃▆▇█
wandb:            eval/avg_f1 ▇▅▄█▂▇██▆▆▆█▅▅▆▅▃▇▅▄▁▅▄▅█▇▅▇▅█▅▆█▇▄▇█▅▇▃
wandb:      eval/avg_mil_loss ▅▂▁▂▂▄▃▆▁▄▄▆▁▄▄▅█▅▃▃▂▄▅▁▁▂▁▃▆▄▆▆▂▄▃▆▃▁▂▆
wandb:       eval/ensemble_f1 ▇▄▅▅▇█▃█▆█▅▇▅▄▄▁▅▅▆▅▅█▇▇█▅▅█▄█▇█▂▆▆▆▄▇██
wandb:           train/avg_f1 ▄▆▆▆▅▃▅▅▅▄▇▃▇▇▁▄▇▇▅▄▆▆▅▃▅▅▅▄▄█▆▇▄█▆▄▂▅▅▅
wandb:      train/ensemble_f1 ▆▄▄▆▆▇▁▅▄▃▃▇▁▅▃▃▅▅▅▅▃▆▄▄▅▂▆▅▅▆▆▄▆▇▆▇▆▄▄█
wandb:         train/mil_loss ▆█▆▇▆▅▃▃█▇▅▃▄▅▃▆▅▁▅▅▇▃▅▄▄▄▇▅▆▄▅▅▄▆▇▃▆▃▂▆
wandb:      train/policy_loss ▄▄█▄▄▄▄▄██▄▁▁▁█▁▄▁▄▄▄█▄▁█▁▁▄▄▄█▄▄▁█▁▄▄▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▄█▄▄▄█▄▄▄▄█▁▄▄█▁█▄▄▄▄▄▄▄▄▄▄██▁▄▄█▄██▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92271
wandb: best/eval_avg_mil_loss 0.22897
wandb:  best/eval_ensemble_f1 0.92271
wandb:            eval/avg_f1 0.74241
wandb:      eval/avg_mil_loss 1.21414
wandb:       eval/ensemble_f1 0.74241
wandb:           train/avg_f1 0.82799
wandb:      train/ensemble_f1 0.82799
wandb:         train/mil_loss 0.71845
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run young-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yq3mzo5c
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_045931-yq3mzo5c/logs
wandb: ERROR Run yq3mzo5c errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 26dg1z6f with config:
wandb: 	actor_learning_rate: 5.119273058670944e-05
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 150
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.07741403453764772
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050125-26dg1z6f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/26dg1z6f
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 133-151, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▅▇██
wandb: best/eval_avg_mil_loss █▁▂▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▄▅▅▇██
wandb:            eval/avg_f1 ██▇█▁██▇▂█▇▇▇▇█▇█▇▇▂███▇▇█▇█▄▇▁▃█▁█▃▇█▇▇
wandb:      eval/avg_mil_loss ▅▇▃▁▅▄▁▃▄▁▁▅▁▄▄▁▁▁▁▂▄▅▁▁▁█▄▁▃▁▁▁▃▁▁▁▂▅▁▅
wandb:       eval/ensemble_f1 ▇█▄▇█▁▇██▇▇▁▄▇▇█▇▅▂█▇▇▇▇▇████▇█▇▇▇▇█▇▃▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▄▆█▃▆▃▆▆▆▅▅▄█▇▇▅▆▃▂▄▆▃▆▆▆▇▇▃▄▂▆▁▆▇█▅▃▄▇
wandb:      train/ensemble_f1 ▆▆▇▆▇▅▄▆▄█▅▆▇▆▅▅▆▄▆▆▆▅▆▇▇▇▃▃▆▄▆▁▆▅▅▃█▄▄▄
wandb:         train/mil_loss ▂▁▇▅▂▃▂▂▃▄▁▂▃▄▄▂▁▅▄▂▂▂▁█▄▅▄▄▃▄▆█▄▂▃▃▆▅▃▁
wandb:      train/policy_loss ▅▅▅▅▅▅▅█▅█▅▅█▅▅▅▅▅▅▁▅▅▅▅▅▅██▁▅▅▅▅█▁▅▅▅▅█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅█▅▁▅▅█▅▅▁█▅▅▅▅▁▅▅▅▅█▅▁▅▅▅▅▅▅█▅▅▅▁▅▁▅▅▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.29024
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.85859
wandb:      eval/avg_mil_loss 1.56614
wandb:       eval/ensemble_f1 0.85859
wandb:            test/avg_f1 0.9422
wandb:      test/avg_mil_loss 0.13466
wandb:       test/ensemble_f1 0.9422
wandb:           train/avg_f1 0.88164
wandb:      train/ensemble_f1 0.88164
wandb:         train/mil_loss 0.23907
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cerulean-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/26dg1z6f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050125-26dg1z6f/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8akxj2oj with config:
wandb: 	actor_learning_rate: 1.915604283272696e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 157
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.910014827476276
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050439-8akxj2oj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8akxj2oj
wandb: uploading history steps 89-102, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄█▅▂▁▆▆▃▇▆▃▆▃▄▅▆▃▅█▇▃▅▇▆▆▃▆▅▇▆▄▆▇▆▅▃▆██▇
wandb:      eval/avg_mil_loss ▆▄▅▄█▃▆▄▆▃▆▅▂▅▆▄▄▄▃▅▇▄▁▂▅█▃▃▄▅▅▅▃▄▃▆▃▆▅▄
wandb:       eval/ensemble_f1 █▆▆▄▂▁▁▃▁▆▅▃▃▃▅▃▅█▇▇▄▃▄▇▅▇▇▅▇▃▇▆▇▄▆▃▇█▇▅
wandb:           train/avg_f1 ▃▄▃▇▇▅▇▅▃▅▃▃▅▂▃▄▄▄▄▅▅▄▃▆▅▁█▂▄▃▄▅▅▄▇▅▃▇▇▇
wandb:      train/ensemble_f1 ▄▄▄▆▄▅▄▅▆▄▁▅▇▆▅▆▄▅▅▇▅▅▅▅▆▄▄▄▄█▅▅▄▆▃▄▅▇▅▇
wandb:         train/mil_loss ▅▂▆█▇▄▇▃▅▆▃▄▃▇▅▃▃▇▃▄▁▆▆▆▆▅▄▁▄▄▁▂▂▅▃▄▂▆▁▅
wandb:      train/policy_loss ██████████████████████████████████▁█████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████████████████▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.34201
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.91524
wandb:      eval/avg_mil_loss 0.28878
wandb:       eval/ensemble_f1 0.91524
wandb:           train/avg_f1 0.91468
wandb:      train/ensemble_f1 0.91468
wandb:         train/mil_loss 0.25064
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fine-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8akxj2oj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050439-8akxj2oj/logs
wandb: ERROR Run 8akxj2oj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 0507261l with config:
wandb: 	actor_learning_rate: 1.3895148242810273e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 186
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4897502018050327
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050612-0507261l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0507261l
wandb: uploading history steps 183-186, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▆█
wandb: best/eval_avg_mil_loss █▃▃▃▁
wandb:  best/eval_ensemble_f1 ▁▆▆▆█
wandb:            eval/avg_f1 ▆▃▃▆▅▂▆▄▁▂▅▄▃▆▅▅▂▇▃▆▅▅▃▅▃█▃▂▅▆▄▄█▄▄▄█▂▅▄
wandb:      eval/avg_mil_loss ▂▇▂▄▆▅▅▃▃▃▅▂▆▃▄▅█▄▅▄▁▄▇▄▄▄▅▂▅▄▆▆▆▆▃▅▇▅▇▄
wandb:       eval/ensemble_f1 ▆▅▁▆▂▄▂▂▂▆▂▆▇▁▅▂▃▂▅▅▅▂▆▆▄▇▃█▄▁▃▆▂▃▂▄▁▄▃▆
wandb:           train/avg_f1 ▄▅▂▆▅▁▅▃▅▂▄█▄▄▅▂▆▄▄▅▄▅▄▃▅▆▄▂▆▃▅▅▅▇▅▃▇▇▆▅
wandb:      train/ensemble_f1 ▄█▅▄▄▅▅▄▅▇▃▆▇▆▅▃▆▇▅▇▅▆▅▅▄▃▄▇▆▄▁▃▅█▅▄▆▅▅▆
wandb:         train/mil_loss ▇▇▅▇▆▄▅█▇▆▇▆▅▅▄▅▇▇▄▅▅▅▅▅▅▅█▇▇▁▄▅▇▇▅▇▇▅▃▇
wandb:      train/policy_loss ▄▄█████▁▁▄██▁█▁▄▄█▁█▁███▁█▁▄▁███▁█▁▄████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91482
wandb: best/eval_avg_mil_loss 0.22048
wandb:  best/eval_ensemble_f1 0.91482
wandb:            eval/avg_f1 0.80957
wandb:      eval/avg_mil_loss 1.27666
wandb:       eval/ensemble_f1 0.80957
wandb:           train/avg_f1 0.74145
wandb:      train/ensemble_f1 0.74145
wandb:         train/mil_loss 1.1549
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glamorous-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0507261l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050612-0507261l/logs
wandb: ERROR Run 0507261l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: az1lsibq with config:
wandb: 	actor_learning_rate: 1.2016142109494789e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 132
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7494108516954057
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_050939-az1lsibq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/az1lsibq
wandb: uploading history steps 123-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆█
wandb: best/eval_avg_mil_loss █▅▁▁
wandb:  best/eval_ensemble_f1 ▁▄▆█
wandb:            eval/avg_f1 ▅▄▄▄▆██▅▅▄▅▅▆▇▃▂▇▆▇█▂▅█▅▅▄▁▁█▄▄▂▂▄▄▄▅▅▅▅
wandb:      eval/avg_mil_loss ▄▇▃█▃▃▄▃▇▄▃▆▆▅▃▃▇▅▁▆▄▆▇▁▁▃▇▁▅▆▁▅▄▅▄▅▆▄▄▇
wandb:       eval/ensemble_f1 ▄▄▅▄▆█▆▅▇▆▅▅▃▄▅▁▇▆▁▇▄▅▅▄▅▇▄█▂▆▂█▇▆▅▂█▅▅█
wandb:           train/avg_f1 ▄▇▂▅▆▅▇▁▆▅▂▆▆▂▇▅▅▆▃▄▅▄▅▆▅▆▁▆▂▆▇█▇▅▇▄▄▇▆▅
wandb:      train/ensemble_f1 ▆▆▅▅▅▆▁▆▃▆▁▅▅▅▅▄▃▃▃▄▅▁▃▂▅█▆▃▅▆▃▆▄▆▅▃▅▅▅▅
wandb:         train/mil_loss ▁▄▅▃▃▄▄▅▅▆▅▆▄▃▃▃▂▄█▆▄▆▂▂▄▄▄▄▁▅▄▅▅▄▅▆▃▄▂▂
wandb:      train/policy_loss ▅▅▅▁█▅▅▅▁█▁▅▁▅▅▁▅▅█▅▅▅▅█▅▅▅▅▅▅█▁▅▅▅▅▅▁▅█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅█▅██▅█▅▅▅█▅▁▅▅▅█▅▅▅█▁██▁▁▅▅▁▅▅▅▅███▅▅▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9105
wandb: best/eval_avg_mil_loss 0.27414
wandb:  best/eval_ensemble_f1 0.9105
wandb:            eval/avg_f1 0.73484
wandb:      eval/avg_mil_loss 2.21561
wandb:       eval/ensemble_f1 0.73484
wandb:           train/avg_f1 0.77154
wandb:      train/ensemble_f1 0.77154
wandb:         train/mil_loss 1.22767
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run denim-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/az1lsibq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_050939-az1lsibq/logs
wandb: ERROR Run az1lsibq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: x5pi7mey with config:
wandb: 	actor_learning_rate: 0.0004986861987900722
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 99
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05880241171963107
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051137-x5pi7mey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x5pi7mey
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 95-99, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss ██▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ██▄█▂▇▄▂▁▂█▂▁▆█▄██▅▄▆█▇█▅█▇▇█▇▇▇▂██▂▁█▂▆
wandb:      eval/avg_mil_loss ▂▄▂▁▃▁▄▁▂▅▁██▅▂▃▁▃▅▅▃▆▇▂▁▁▂▅▁▃▅▃▅▁▇▄█▂▁▂
wandb:       eval/ensemble_f1 █▇██▂▄█▆▇▄▁▄▂▇▇▇▇█▇▄█▅█▂▇▇▅▇▇▇▁▇▆██▆█▇▁▇
wandb:           train/avg_f1 ▄▄▇▅▄▅▃▅▄▅▃▅▃█▁▆▄▅▄▇▃▆▂▆▅▄▆▃▅█▅▇▇▅▇▅▄▅▅▄
wandb:      train/ensemble_f1 ▅▂▄▆▆▅▇▄▅▅▆▁▅▇▂▂▇▅▅█▆▆▅▅▅▆▅▅▅▄▆▅▆▅▅▆▅▅▄▇
wandb:         train/mil_loss ▅▅▅▄▆▆▆█▆▄▂▃▆▄▃▆▃▅▃▆▄▃▅▅▃▄▆▇▂▆▅▅▅█▁▄▆▄▄▅
wandb:      train/policy_loss █████████████████████████████████████▁██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92281
wandb: best/eval_avg_mil_loss 0.32441
wandb:  best/eval_ensemble_f1 0.92281
wandb:            eval/avg_f1 0.89312
wandb:      eval/avg_mil_loss 1.29437
wandb:       eval/ensemble_f1 0.89312
wandb:           train/avg_f1 0.83557
wandb:      train/ensemble_f1 0.83557
wandb:         train/mil_loss 1.03374
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run honest-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x5pi7mey
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051137-x5pi7mey/logs
wandb: ERROR Run x5pi7mey errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 0oleuv4o with config:
wandb: 	actor_learning_rate: 0.0005540688190108043
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 125
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4011252236110291
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051310-0oleuv4o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0oleuv4o
wandb: uploading wandb-summary.json
wandb: uploading history steps 114-126, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇██████
wandb: best/eval_avg_mil_loss █▂▂▂▂▂▁▂▁
wandb:  best/eval_ensemble_f1 ▁▇▇██████
wandb:            eval/avg_f1 ███▆███████████████▃█▇██████▁▇██▇█▇▇▃█▃█
wandb:      eval/avg_mil_loss ▂▂▂▁▂▂▁▁▁▂▂▂▆▂▁▁▂▂▂▂▁▂▂▃▃▁▂█▁▂▁▁▄▁▇▇▃▂▁▁
wandb:       eval/ensemble_f1 ▇▄▇▇▇█████▇▇█▇█▇▇▇▇████▆▇██▇▆▁▇█▇▇▇▆▇▇██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▄▇▆▇▄▅▄▆▅▅▆▇▄▃▄▆▇▆▁▅▁▂▅▄▂█▄▇▅▆▇▇▃▅▇▆▄▆▄
wandb:      train/ensemble_f1 ▄▇▁▃▆▅▆▅▅▅▅▇▅▂▅▂▄▄▃▅▄▃▅▃▄▆▅▅▆▅▅▅█▄▇▆▆█▇▅
wandb:         train/mil_loss ▄▄▁▃▄▅▆▁▃▁▃▄▁▁▇▇▅▂▁▆▁▂▁█▅▅▁▃▅▆▃▅▅▃▆▇▁▃▆▃
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄█▄█▄▄▁▄▄▄██▄▄█▁▁▄▄▄▄▄▄▄▄▄▄▄▄█▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████▁█████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91813
wandb: best/eval_avg_mil_loss 0.25487
wandb:  best/eval_ensemble_f1 0.91813
wandb:            eval/avg_f1 0.89873
wandb:      eval/avg_mil_loss 0.25595
wandb:       eval/ensemble_f1 0.89873
wandb:            test/avg_f1 0.92999
wandb:      test/avg_mil_loss 0.13694
wandb:       test/ensemble_f1 0.92999
wandb:           train/avg_f1 0.85684
wandb:      train/ensemble_f1 0.85684
wandb:         train/mil_loss 0.42823
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run blooming-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0oleuv4o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051310-0oleuv4o/logs
wandb: Agent Starting Run: ro7f19zq with config:
wandb: 	actor_learning_rate: 0.0007051797173640962
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 157
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5275383522950099
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051459-ro7f19zq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ro7f19zq
wandb: uploading history steps 98-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▂▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 █▆▆▂▇▃█▅▅▆▄▆▇▁▄▆▆█▃▆▅▄▄█▅▅█▄▄▆▄▄▇▁▃▅▁▆▇▆
wandb:      eval/avg_mil_loss ▁▅▅▁▅▃▂▄▅▄▃▇▁█▆▆▂▁▃▇█▇▅█▃▃▆▅▆▅▇▃█▇█▅▃▁▆▃
wandb:       eval/ensemble_f1 ▅▃█▄▃▄▃▇▄▆▄▆▆▆▄▄▅▆▃▃▁▄▄█▅▆▄▄▅▅▅▄▃█▃▁▃▂▅▆
wandb:           train/avg_f1 ▆▅▆▄▁▃▆▅▅▅▂▂▅▇▅▆▁▆▆▅▆▃▆▅█▅▇▅▄▆▆▆▆▅▆▆▅▆▅▅
wandb:      train/ensemble_f1 ▆▆▅▄▆▄▇▅▆▇█▇▆▄▁▆▆▅▆▆▄▄▆▃▄▅▅▅█▄▅▅▆▆▆▄▄▆▆▄
wandb:         train/mil_loss ▃▄▁▄▃▄▅▂▅█▅▄▁▃▄▅▅▆▇▃▃▅▅▃▅█▄▃▅▃▆▅▇▆▅▂▆▄▃▃
wandb:      train/policy_loss ▅▅██▅▅█▁▅█▅▁▁█▁█▅▁█▅▅██▁▁▁▁▁▁▅▅▅██▁▁▁▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄██▁▄█▁▁█▄█▄▁▄▁▁▁██▄▄█▁▄▁▁▄▁▁▄▄█▁▄▄██▄▄▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90283
wandb: best/eval_avg_mil_loss 0.25488
wandb:  best/eval_ensemble_f1 0.90283
wandb:            eval/avg_f1 0.83154
wandb:      eval/avg_mil_loss 0.61582
wandb:       eval/ensemble_f1 0.83154
wandb:           train/avg_f1 0.80573
wandb:      train/ensemble_f1 0.80573
wandb:         train/mil_loss 0.99594
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run summer-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ro7f19zq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051459-ro7f19zq/logs
wandb: ERROR Run ro7f19zq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 0k8bsmzn with config:
wandb: 	actor_learning_rate: 0.0005795801635478494
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 54
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4789669032226256
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051709-0k8bsmzn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0k8bsmzn
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 42-54, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▃█▃█▅▅▇▁▇▆▇▇▂▇▇▇▇▃▇▇▄▇▇▁▃▅▆▆▆▂▃█▂▃▆█▆▇▁▇
wandb:      eval/avg_mil_loss ▄▁▅▁▃▂▂▅▃▂▂▁▅▁▇▁▂▂▅▁▃▁▄▁▅▄▂▄▁▅█▅▁▁█▂▆▁▄▂
wandb:       eval/ensemble_f1 ▄▄█▆▆▇▂▇▇▅▇▃▇▇▁▇▇▄▇▇▅▇▄▇▃▆▆▄▇▆▄█▇▃▃▃█▆▂▇
wandb:           train/avg_f1 ▅▆▄▅▅▅▄▅▃█▄▂▃▄▄▅▄▆▅▂▅▅▅▆▁▃▇▄▃▄▂█▇▃▃▆▆▅▅▁
wandb:      train/ensemble_f1 ▅▆▄▅▅▅▄▅█▃█▂▃▄▄▅▄▅▅▁▅▅▅▆▁▃▇▄▃▄▂█▇▃▃▆▆▅▆▁
wandb:         train/mil_loss ▅▄▃▄▆▆▄▄▂▄▁▅▄▃▇▂▄▅▃▄▆▆▂▅▆▆▂▄▃▇▅▁▃▅█▆▄▂▂▃
wandb:      train/policy_loss ▁▅▅▅▅█▁▁▁█▁█▁▅▅▅▁█▅█▅█▅▁██▅█▅█▅█▅▁▁▅██▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄▄▄█▁▁█▁▄▁█▁▄▄▄▁▄███▄▁██▄█▄██▄█▁▁▄██▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92249
wandb: best/eval_avg_mil_loss 0.23378
wandb:  best/eval_ensemble_f1 0.92249
wandb:            eval/avg_f1 0.89963
wandb:      eval/avg_mil_loss 0.29352
wandb:       eval/ensemble_f1 0.89963
wandb:           train/avg_f1 0.84509
wandb:      train/ensemble_f1 0.84509
wandb:         train/mil_loss 0.40876
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0k8bsmzn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051709-0k8bsmzn/logs
wandb: ERROR Run 0k8bsmzn errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: yaj2y5df with config:
wandb: 	actor_learning_rate: 5.338695990004126e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 112
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4623155714048989
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051812-yaj2y5df
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yaj2y5df
wandb: uploading wandb-summary.json; uploading history steps 93-111, summary
wandb: uploading history steps 93-111, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇▇█
wandb: best/eval_avg_mil_loss █▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇▇█
wandb:            eval/avg_f1 █▁█▇▂█▃██▂█▇▂▂▁█▂▄██▁█▆▆▄▂▇███▇▆███▇██▄█
wandb:      eval/avg_mil_loss ▆▄▁▁▄▂▁▁▁▃▅▄▆▆▂▁▁▅▃▁▃▅▂█▅▃▁▁▁▁▁▃▁▁▅▃▁▁▃▁
wandb:       eval/ensemble_f1 ▄▆█▁▇▁▇███▂▇█▂█▇▂████▂▆█▆▅▂▁█▅████▆███▂█
wandb:           train/avg_f1 ▇▆▄▇▇█▃▆█▆▄▆▄▆▇▅▄▁█▆▄▆▅█▂▃▇▃▅▆▅▇▆▄▇▆▆▅▄▆
wandb:      train/ensemble_f1 ▇▆▇▆▇▆▃█▃▆▃▆▇▃▃▅▄▃▄▃▅▃▅▃▂▅▆▇▆▂▄▅▇▅▆▁▅▄▅▆
wandb:         train/mil_loss ▄▄▆█▃▅▄▇▃▁▆▄▆▇▅▃▄▆▄▄▄▁▃▅▅▂▃▅▄▂▁▂▄▄▂▂▂▂▄▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▁▁▁▁▅▅▅▅▅▅▁▅▁█▅▅▅▅▁▃▅▅▅▁▁▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▁▅▅▁▅▅▁▅▁▅▁▅▅▅█▅▅▅▅▁▅▁▃▅▅▅▁▅▅▅▁▁▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93007
wandb: best/eval_avg_mil_loss 0.36912
wandb:  best/eval_ensemble_f1 0.93007
wandb:            eval/avg_f1 0.89485
wandb:      eval/avg_mil_loss 0.42964
wandb:       eval/ensemble_f1 0.89485
wandb:           train/avg_f1 0.83183
wandb:      train/ensemble_f1 0.83183
wandb:         train/mil_loss 0.5263
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lemon-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yaj2y5df
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051812-yaj2y5df/logs
wandb: ERROR Run yaj2y5df errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xou0udua with config:
wandb: 	actor_learning_rate: 8.776424713560277e-06
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 68
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8611017299708021
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_051950-xou0udua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xou0udua
wandb: uploading history steps 52-68, summary
wandb: uploading data
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇█
wandb: best/eval_avg_mil_loss █▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇█
wandb:            eval/avg_f1 ▇▄▄▇▇▆▄▇▇██▇▆▇▄▇▇▄▄▇▇▇██▆▁▇▆▇▅█▄██▇█▅▄█▇
wandb:      eval/avg_mil_loss ▅▄▁▁▁▆▁▁█▃▁▁▆▁▂▁▁█▅▁▇▁▄▁▁▁▇▁▅▂█▃▁▄▅▁▅▁▄▅
wandb:       eval/ensemble_f1 ▇▃▇▇▇▅▇▇█▃▅▇▇█▇█▃▂▇▅▇▇▇▇█▇▄█▃▄█▅▇▁▃▃▇▇▃▇
wandb:           train/avg_f1 ▅▄▄▃▅▁▄▇▆▁██▅▂▆▃▇▃▆█▆▅▆▂▃▆▇▆▃▄▅▅▃▄▃▂▇▆▄▇
wandb:      train/ensemble_f1 ▄▃▂▆█▄▆▅▂▄▆▇▄▁▆▅▄▇▅█▂▄▅▆▅▂▄▂▅▄▄▁▄▂▂▂▇▅▄▆
wandb:         train/mil_loss ▃▆█▆▄▂▄▇▃▅▄▂▄▄▃▆▆▃▄▄▄▄▃▄▇▁▃▅▅▃▃▄▅▂▆▁▂▆▁▅
wandb:      train/policy_loss ▄▄▄▁▁██▄▁█▄▄▄▄▄▄█▄▄▄▄██▁▄▄▄▄▁▄▄▁▄█▄▁▄▁█▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████▁██████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91909
wandb: best/eval_avg_mil_loss 0.29283
wandb:  best/eval_ensemble_f1 0.91909
wandb:            eval/avg_f1 0.87537
wandb:      eval/avg_mil_loss 1.13423
wandb:       eval/ensemble_f1 0.87537
wandb:           train/avg_f1 0.87076
wandb:      train/ensemble_f1 0.87076
wandb:         train/mil_loss 0.89583
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vibrant-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xou0udua
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_051950-xou0udua/logs
wandb: ERROR Run xou0udua errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: tnhatydn with config:
wandb: 	actor_learning_rate: 6.340074782229502e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 182
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4221644215275578
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052058-tnhatydn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tnhatydn
wandb: uploading history steps 137-145, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇████
wandb: best/eval_avg_mil_loss █▂▃▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇████
wandb:            eval/avg_f1 ██▆▄▇▇████▄██▇██▇▇▇█████▇▁██████████▅███
wandb:      eval/avg_mil_loss ▂▅▁▁▃▅▆▂▄▁▂▁▂▁▂▁▂▁▇▁▁▁▆▂▂▂▅▁▅▁▂▁▁▁▁█▆▆▁▆
wandb:       eval/ensemble_f1 ▂██▂▇▃▃███▁▂▇▇███▅▂▇█████▆█▅▇▅▆█▇▂▇█▅███
wandb:           train/avg_f1 ▂▃█▂▆▅▄▃▃▇▄▃▄▅▂▄▆▅▆▂▃▃▅█▅▄▆▆▆▅▇▄█▆▅▁▅▃▇▆
wandb:      train/ensemble_f1 █▃▅▅▆▆▇▄▅▆▄▅▅▃█▃▁▅█▇▅▂▅▄▆▄▇▂▇█▆▅▇▇▆▅▃▇▄▆
wandb:         train/mil_loss ▄▃▆▃▆█▃▆▄▆▅▄▃▇▄▅▁▆▁▁▂▄▃▄▅▃▃▅▂▆▂▆▁▂▆▃▅▅▇▂
wandb:      train/policy_loss ████████████████▆███████▇██████████████▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████▁█████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91161
wandb: best/eval_avg_mil_loss 0.27117
wandb:  best/eval_ensemble_f1 0.91161
wandb:            eval/avg_f1 0.87779
wandb:      eval/avg_mil_loss 1.14951
wandb:       eval/ensemble_f1 0.87779
wandb:           train/avg_f1 0.88488
wandb:      train/ensemble_f1 0.88488
wandb:         train/mil_loss 0.31815
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run resilient-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tnhatydn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052058-tnhatydn/logs
wandb: ERROR Run tnhatydn errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 4c1loiqb with config:
wandb: 	actor_learning_rate: 0.0001405249573010007
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 118
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.11253828324683568
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052312-4c1loiqb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4c1loiqb
wandb: uploading wandb-summary.json
wandb: uploading history steps 112-118, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss ▂▆▁█
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ████▂▂██▄▂▂█▂▂█▇█▂▃█▇█▂▇██▄▇▁▁▂▂▇▇█▇▂█▂█
wandb:      eval/avg_mil_loss ▄▄▃▃▄▃▁▁▁▄▇▄▅▇▄▁▅▁▁▁█▄▁▁▅▁▁▆▄█▄▆█▇▃▄▁▅▆█
wandb:       eval/ensemble_f1 █▂▃█▇██▂█▃▄▃▂▂▂▂▃██▇▃█▇▇█▇█▁▂▂▃▇█▁▃█▇█▂█
wandb:           train/avg_f1 ▅▄▃▁▄▁▄▆▄▁▄▃▆▆▄█▆▂▃▃▄▅▆█▄▇▃▄█▆▅▂▂▅▅▂▃▃▅▆
wandb:      train/ensemble_f1 ▅▁▇▅▃▄▆▇▆▃▄▆▇▆▅█▃▄▇▇▅▅▆▂▆▄▇▇█▇▇▅▆▆▇▇▆▅▇▃
wandb:         train/mil_loss ▆▃▅▄▄▂▄▃▃▅▆▂▅▃▁▄▄▄▄▁▃▄▆▃▃▄▄▅█▁▄▂▅▃▅▄▄▅▃▅
wandb:      train/policy_loss ████████████████████████████▁███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▁▅█▅▅▅▅▅▅▅▅▅▅▁▅▅▁▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9105
wandb: best/eval_avg_mil_loss 0.38974
wandb:  best/eval_ensemble_f1 0.9105
wandb:            eval/avg_f1 0.90325
wandb:      eval/avg_mil_loss 0.40539
wandb:       eval/ensemble_f1 0.90325
wandb:           train/avg_f1 0.74329
wandb:      train/ensemble_f1 0.74329
wandb:         train/mil_loss 1.28133
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run proud-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4c1loiqb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052312-4c1loiqb/logs
wandb: ERROR Run 4c1loiqb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 2hy0sj0l with config:
wandb: 	actor_learning_rate: 0.0005015367254993286
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 130
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5569132019802567
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052456-2hy0sj0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2hy0sj0l
wandb: uploading history steps 96-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▅█▂▄▅▇▄▇▄▆▄▅▃▅▆█▆▆▅▆▇▅▄▁▂▆▅▄▇▆▅▆█▆▃▆▆▄▂
wandb:      eval/avg_mil_loss ▁▄▄▁▃▆▃▆▄▆▁▅▃▄▇▆▂▃▅▇▄▃▇▅▃▆▃▃▃▆▃▃▁▃▆▁▁█▄▆
wandb:       eval/ensemble_f1 █▄▁▆▂▅▅▂▆▇▅▇▅▅▄▁▅▅▃▇▅▃▄▅▁▂▃▆▄▆▆▇▁▅▅▃▇▂▅▁
wandb:           train/avg_f1 ▄▂▄▁▃▄▆▇▃▄▆▄▃▄▇▄█▆▄▃▄▅▆▅▇▄▅▄▃▆▃▅▅▄▅▅▅▄▃▅
wandb:      train/ensemble_f1 ▄▄▁▃▄▃▅▄▅▆▄▃▅▅▄▅▆▇▇▁▃▁▆▄▇▄▅▇▆▆▄▆▂█▅▇▅▅▅▅
wandb:         train/mil_loss ▅▇▁█▆▆▃▅▄▄▆▅▄▂▄▅▆▃▄▂▃▆▄▂▃▃▅▄▆▃▆▆▅▅▄▃▆▄▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄▁▁▄█▁█▁▁▁███▄▄█▄▄▁▄█▄▁▄▄▄█▁▄▄▁▁▄██▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91524
wandb: best/eval_avg_mil_loss 0.24393
wandb:  best/eval_ensemble_f1 0.91524
wandb:            eval/avg_f1 0.68464
wandb:      eval/avg_mil_loss 1.39777
wandb:       eval/ensemble_f1 0.68464
wandb:           train/avg_f1 0.8225
wandb:      train/ensemble_f1 0.8225
wandb:         train/mil_loss 0.87136
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rural-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2hy0sj0l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052456-2hy0sj0l/logs
wandb: ERROR Run 2hy0sj0l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ttbt731e with config:
wandb: 	actor_learning_rate: 1.3862987397811845e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 182
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.20290671275703764
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052655-ttbt731e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ttbt731e
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▆█▅▆▆▆▆▇▇▆▄▄▆▄▇▆▇▇▆▆▅▁▅▆▄▃▆▅▇▆▇▆▆▆▅▅▂▆▆▇
wandb:      eval/avg_mil_loss ▃▃▂▁▃▃▄▂▄▂▃▃▅▃▄▂▃▂▂▂▂▂▂▃▂▂▂▂▁▃▂▂▂█▂▃▃▂▂▂
wandb:       eval/ensemble_f1 ▁▅▆▆▆▆▄▇▆▅▅▄▄▆▇▆▇▆▇▇▆▅▆▇▅▆▃▃▆▆▇▂▇▅▇▃▆█▄▄
wandb:           train/avg_f1 ▆▁▄▄▃▃▇▇▇▄▃▄▃▆▆▆▂▆▆▂▃▇██▇▆▆▄▅▄▂▅▅▃▃▄▃▄▅▅
wandb:      train/ensemble_f1 ▄▃▂▂▆▅▆▆▁▃▆▆▃▂█▇▅▃▆▄▅▅▃▅▅▁▁▄▄█▄▂▆▆▇▆▄▆▃▄
wandb:         train/mil_loss ▆▅▃▂▁▇▄▅▃▃▂▂▁▄▂█▃▃▆▁▆▆▆▅▂▅▃▅▅▂▁▆▅▄▄▆▄▇▂▇
wandb:      train/policy_loss █▄▁█▁█▁███▄▄▁▁█▁▄██▄▄▄███▄██▄█████▁▄▁▄▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.926
wandb: best/eval_avg_mil_loss 0.21256
wandb:  best/eval_ensemble_f1 0.926
wandb:            eval/avg_f1 0.86444
wandb:      eval/avg_mil_loss 0.36444
wandb:       eval/ensemble_f1 0.86444
wandb:           train/avg_f1 0.884
wandb:      train/ensemble_f1 0.884
wandb:         train/mil_loss 0.25856
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cool-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ttbt731e
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052655-ttbt731e/logs
wandb: ERROR Run ttbt731e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5tynn9q1 with config:
wandb: 	actor_learning_rate: 5.370354296041898e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.013171697156451834
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_052859-5tynn9q1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5tynn9q1
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇███
wandb: best/eval_avg_mil_loss █▆▄▁▁▁▂
wandb:  best/eval_ensemble_f1 ▁▄▅▇███
wandb:            eval/avg_f1 ▄▆▄▆▅▆▄▆▅▆██▂▄▆▄▆▄▃▅▄▁▄▇▅▆▆▅▅▆▆▅▅▆▇▆▆▃█▆
wandb:      eval/avg_mil_loss ▅▃▆▃▅▃▅▃▃▂▃▁▆▆▃▆▅▇▅▅▄▂▃▁▄▄▄▄▃▁▆▅▅▃▃▅█▁▂▅
wandb:       eval/ensemble_f1 ▄▄▆▄▆▇▆▅▆▆█▂▄▆▆▆▄▃▅▅▁▄▇▁█▆▅▆█▆▅▅▆▇▇▆▃█▇▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▃▆▃▆▄▆▃▂▅▃▅▄▅▅▁▅▆▃▅▂▇▅▁▂▅▅▆▄▃▃▃▆█▇▅█▆▅▆
wandb:      train/ensemble_f1 ▃▃▆▃▆▄▃▁▆▃▅▄▅▆▂▆▄▄▂▇▂▃▆▇▅▆▄▃▅▃▆█▇▄▄█▆▆▅▆
wandb:         train/mil_loss ▂▁▇▃▅▄▃▂▄▅▅▅▂▃▄▆▄▆▇▄▅▄▂▅▆▅▅▅▄▄▄█▄▆█▃▅▁▃▃
wandb:      train/policy_loss █▄██▄█▄██▁▁▁▄▄▄▄▄█▄▄▁█▁▄▄▁▁█▄▁▁█▁▁▁▄▁█▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁▁▄█▁█▄███▁▁▄█▄█▄▄▄█▁▄▄▄▄█▁▁█▁▁▁▁█▁█▄▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.89983
wandb: best/eval_avg_mil_loss 0.37057
wandb:  best/eval_ensemble_f1 0.89983
wandb:            eval/avg_f1 0.81021
wandb:      eval/avg_mil_loss 1.27664
wandb:       eval/ensemble_f1 0.81021
wandb:            test/avg_f1 0.81941
wandb:      test/avg_mil_loss 0.83197
wandb:       test/ensemble_f1 0.81941
wandb:           train/avg_f1 0.81815
wandb:      train/ensemble_f1 0.81815
wandb:         train/mil_loss 0.69839
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run peachy-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5tynn9q1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_052859-5tynn9q1/logs
wandb: Agent Starting Run: 7vlclpy8 with config:
wandb: 	actor_learning_rate: 8.594685939333766e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 169
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10283865968694672
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053005-7vlclpy8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7vlclpy8
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading history steps 102-116, summary
wandb: uploading history steps 102-116, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▅▁▂▇▇▄█▇▃▅▅▇█▂▇▇▇▇▇▇█▇▂▇▆▇▆▇▂▇▇▇▅▇▃▇█▆▇▃
wandb:      eval/avg_mil_loss ▁▃▂▄█▄▁▅▂▂▂▁▂▁▃▆▁▅▁▁▂▄▁▅▅▂▆▆▃▄▁▂▅▂▄▂▄▁▁▃
wandb:       eval/ensemble_f1 ▇▄▄█▇▅▇▁▅██▆███▄▅▅▆▅█▇▄▇▁▅███▆█▇▇▂▄▄▇▇▇▆
wandb:           train/avg_f1 ▆▆▆▄▅▆▄▇▃▁▅▅▄▄▅▅▅▄▅▇▇▇▆▇▂▄▅█▆▅▆▅██▅▂▆▇▆▆
wandb:      train/ensemble_f1 ▅▁▃▅▅▃▆▆▂▁▃▆▂█▃▄▃▃▄▄▃▂▆▅▆▅▄▅▇▃▇▁▃▄▄▄▃▅▆▄
wandb:         train/mil_loss ▄▄▄▅▁▅▄▃▃▃▅▅▂▄▅▆▂▄▄▃█▄▃▄▆▅▄▄▆▅▄▂▅▄▂▅▅▄▃▄
wandb:      train/policy_loss ████████████████████████▁███████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅█▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.24472
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.8502
wandb:      eval/avg_mil_loss 0.45357
wandb:       eval/ensemble_f1 0.8502
wandb:           train/avg_f1 0.85242
wandb:      train/ensemble_f1 0.85242
wandb:         train/mil_loss 0.74159
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run twilight-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7vlclpy8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053005-7vlclpy8/logs
wandb: ERROR Run 7vlclpy8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kye8nxc0 with config:
wandb: 	actor_learning_rate: 1.5572829940534458e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5247598145756838
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053204-kye8nxc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kye8nxc0
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss ▇▇█▁
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ▇▄▄▄▃▃▅▅▂▁▅▂▃▆▅▅▃▇▇▃▆▄▆▅▄▆▅▅▃▅▆▆▅██▃▆▅▅▃
wandb:      eval/avg_mil_loss ▁▄▃▂▃▆▁▂▂█▃▁▁▂▁▇▇▃▂▂▁▂▂▂▁▁▃▁▂▁▂▂▂▁▂▇▃▁▃▃
wandb:       eval/ensemble_f1 ▄▄▄▄▅▁▅▅▄▃▄▆▃▄▄▆▅▄▇▅▃▂▅▆▄▅▇▆▅█▇▅▇▅▆▇▄▃▄▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▁▃▃▅▁▅▄▂▃▂▇▄▆▄▇▂▅▆▆▅▆▅█▆▄▆▃▄▆▅▃▃▅▅▇▃▅▆▅
wandb:      train/ensemble_f1 ▂▃▄▃▆▅▃▄▆▃▅▄▆▅▃▃▃▂▆▆▅▃▆▄▃▄█▄▃▄▆▃▂▄▅▃▅▆▃▁
wandb:         train/mil_loss ▅▆▄█▄█▆▆▇▅▅▄▆▅▇▃▅▆▃▃▇▄█▃▂▆▅▅▁▅▅▅▅▅▅▄▅▃▆▅
wandb:      train/policy_loss ▁▄▄█▄▁█▄▁█▁▄▄█████▄▁▁▁▁▄▁██▁▁▄▁▁█▄▄█▁▁▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████▁███████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92987
wandb: best/eval_avg_mil_loss 0.23285
wandb:  best/eval_ensemble_f1 0.92987
wandb:            eval/avg_f1 0.87885
wandb:      eval/avg_mil_loss 0.30705
wandb:       eval/ensemble_f1 0.87885
wandb:            test/avg_f1 0.93824
wandb:      test/avg_mil_loss 0.17772
wandb:       test/ensemble_f1 0.93824
wandb:           train/avg_f1 0.9065
wandb:      train/ensemble_f1 0.9065
wandb:         train/mil_loss 0.25453
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rich-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/kye8nxc0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053204-kye8nxc0/logs
wandb: Agent Starting Run: 5hmtdk7e with config:
wandb: 	actor_learning_rate: 2.4862251946331192e-05
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 179
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9523689272621172
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053429-5hmtdk7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5hmtdk7e
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄██
wandb: best/eval_avg_mil_loss █▄▁▁
wandb:  best/eval_ensemble_f1 ▁▄██
wandb:            eval/avg_f1 ▆▅▇▃▅▅▄▆▃▇▃▁▆▃▅▄▆▂▆▆▆▄▆▇▇▇▅▇▆█▇▃▅█▇▅▃▆▂▅
wandb:      eval/avg_mil_loss ▄▅▅▅▅▄▅▁▃█▄▆▁▅▆▃▅▄▅▃▃▃▁▄▃▅▄█▃▇▅██▄▃▄▄▅▄▅
wandb:       eval/ensemble_f1 ▄▄▅▆▄▂▅▄▇▆▆▃▆▅▁▂▆▂▆▃▅▃▅█▅▇▆▅▁▄▆▇▁█▄▆▃▇▂▂
wandb:           train/avg_f1 ▃▆▂▄▇▆▁▆▅▆▇▃▄▅▆▄▂▃▇▇▂▄▅▆▄▃█▄▇▂▃▆▄▄▆█▅▅▄▅
wandb:      train/ensemble_f1 ▄▄▄▄▅▆▂▁▃▇▆▅▅▆▅▃▆▅▅▆▃▆▆█▃▆▄▅▆▄▇█▄▆▄▅▄▇▅▄
wandb:         train/mil_loss ▆▆▆▅▄▇▆▄▄█▄▆▄▄▇▆▄▆▆▆▄▆▄▆▅▄▅▄▃▅▄▅▃▄▅▇▇▁▄▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▄▁█▄▄▄▁▁▁▁█▁█▁▁██▄▁▄█▁▁█▁▁▁▁▄██▄█▄▄▄██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.88619
wandb: best/eval_avg_mil_loss 0.28637
wandb:  best/eval_ensemble_f1 0.88619
wandb:            eval/avg_f1 0.77933
wandb:      eval/avg_mil_loss 1.42938
wandb:       eval/ensemble_f1 0.77933
wandb:           train/avg_f1 0.70353
wandb:      train/ensemble_f1 0.70353
wandb:         train/mil_loss 0.87828
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run valiant-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5hmtdk7e
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053429-5hmtdk7e/logs
wandb: ERROR Run 5hmtdk7e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ybmqekus with config:
wandb: 	actor_learning_rate: 9.956015840137674e-06
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 160
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8532267946235748
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053756-ybmqekus
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ybmqekus
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆█
wandb: best/eval_avg_mil_loss █▅▅▁▄
wandb:  best/eval_ensemble_f1 ▁▄▅▆█
wandb:            eval/avg_f1 ██▇▂██▂██▇█▇█▆█▇████████▁▇██▇██▇██▇█▁██▂
wandb:      eval/avg_mil_loss ▂▁▅█▁▁▂█▁▆▂▁▁█▁▂▂▂▂▆▁▁▁▁▁▆▂▂▁▂▆▁▂▂▁▁▂▁▁▂
wandb:       eval/ensemble_f1 █▇█▂█▂█▇▁████▇▇████▇▇█▂▇███▂▇▇██▂█████▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▄▅▁▅▄▇▇▆▁▃▄▂▅▅▇▇▇▇█▆▆▆▅▇▆▅▅▂▇▆▅▆▇▇▇▇▄▇▃
wandb:      train/ensemble_f1 ▆▅▅▇▆▄▇▇▄▆▇▇█▅▆▇▇▆▅▅▅▆▇▅▆▁▆▆▇▄█▃█▇▄█▆▇▄▇
wandb:         train/mil_loss ▃▃▅▃▃▃█▁▄▃▅▃▁▁▅▃▃▁▃▃▅▃▄▁▃▃▁▃▃▂▃▁▁▃▆▄▇▁▂▃
wandb:      train/policy_loss ▄█▁███▄▄▄▄▄▄█▄▄█▄▄▁▄█▄█▁▄▄▁▄▄▄▄██▄▄▄▁▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄██▄▄█▄▄▁▄▄▁▁▁▄▄▄▁▄▄█▁▄█▄▄▄▄▄███▄▄▄██▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91919
wandb: best/eval_avg_mil_loss 0.26392
wandb:  best/eval_ensemble_f1 0.91919
wandb:            eval/avg_f1 0.60955
wandb:      eval/avg_mil_loss 1.20057
wandb:       eval/ensemble_f1 0.60955
wandb:            test/avg_f1 0.92999
wandb:      test/avg_mil_loss 0.12173
wandb:       test/ensemble_f1 0.92999
wandb:           train/avg_f1 0.87077
wandb:      train/ensemble_f1 0.87077
wandb:         train/mil_loss 0.54221
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run proud-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ybmqekus
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053756-ybmqekus/logs
wandb: Agent Starting Run: q642gvb2 with config:
wandb: 	actor_learning_rate: 0.0008472301789468481
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 59
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6141530198172529
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_053949-q642gvb2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/q642gvb2
wandb: uploading history steps 55-59, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▂▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▃▇▁▆▆▅▃▇▇▇▆▄▄▄▇▇▇▇▇▄▃▅▇▇▆▅▄▇▆▇█▃▅▇▅██▆▇▆
wandb:      eval/avg_mil_loss ▅▃▁█▂▁▁█▃▆▅▄▁▃▁▁▁▅▆▄▂▇▃▆▄▂▂▂▂▇▂▃▁▃▁▂▃▁▁▂
wandb:       eval/ensemble_f1 ▁▅▇▅▇▇▇▃▅▂▂▇▅▇▆██▂▁▃▇▅▃▂▄▆▇▆█▁▃▇▄▇▇▅▇▇▇▆
wandb:           train/avg_f1 ▅▆▇▅▅▃▃▆▆▅█▄▂▇▃▇▅▆▃▃▄▇▆▇▆▂▇▅▃▆▅▇▆▁▅▁█▆▃▅
wandb:      train/ensemble_f1 ▅▅▆▅▃▃▆▆▅▅▆▄▂▇▆▃▅▇▅▆▅▄▇▆▆▂▅▅▆▃▇▆▁▅▅▁█▆▃▅
wandb:         train/mil_loss ▃▄▆▄▆▅▅▅▄▇▅▅█▂▆▄▂▅▅▄▁▅▄▄▃▂▆▄▄▃▄▄▃▇▃▃▇▃▂▄
wandb:      train/policy_loss ███████████████████████████████▁████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████████▁█████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91161
wandb: best/eval_avg_mil_loss 0.23117
wandb:  best/eval_ensemble_f1 0.91161
wandb:            eval/avg_f1 0.88198
wandb:      eval/avg_mil_loss 0.36766
wandb:       eval/ensemble_f1 0.88198
wandb:           train/avg_f1 0.8726
wandb:      train/ensemble_f1 0.8726
wandb:         train/mil_loss 0.44595
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run crimson-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/q642gvb2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_053949-q642gvb2/logs
wandb: ERROR Run q642gvb2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 468qxog8 with config:
wandb: 	actor_learning_rate: 3.9967526658685935e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 146
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.21850180637701055
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054118-468qxog8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/468qxog8
wandb: uploading history steps 122-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▇█
wandb: best/eval_avg_mil_loss ▅█▆▁▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▆▇█
wandb:            eval/avg_f1 ▆▇▇█▇▅▄▅▇▄█▅▇▇▁▅▅▇▂▅▁▅▇▆▇▂▅▃▆▇▇▄▃▇▇▅▃▅▇▅
wandb:      eval/avg_mil_loss ▃▁▄▂▄▃▃▁▃▁▁▁▄▄▄▄▃▂▁▆▃▃▁▃▃▂█▂█▁▁▁▁▄▅▆▁▄▄▇
wandb:       eval/ensemble_f1 ▆▇▅▅▄▄▄▇▄▅█▅▆▇█▇▅▆▅▅▂▆▅▇▆▆▂▆▆█▆█▇▄▄▇▄▅▄▁
wandb:           train/avg_f1 █▄▅▃▂▄▇▂▅▆▂▄█▆▆▇▃▃▆▄▇▅█▆▁▅▄▄▂▅▅▅▆▇▅▆▃▅▄▆
wandb:      train/ensemble_f1 ▇▆▆▁▃▇▆▅▃▅▆▆▆▅▅▄▆▅▆▇▅▅▆▆▄▃▆▆▆▅▆▅▆▆▆█▆▅▄▃
wandb:         train/mil_loss ▃▃▆▅▇▄▂▅▂▄▅▃▃▃█▃▆▃▃▆▅▄▃▃▂▅▃█▁▄▃▄█▆▂▃▅▄█▆
wandb:      train/policy_loss ████████████▁███████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▁█▁▁█████▁█▁██▁▁█▁▁█▅▁▁▁▅▅██▁▅▁▅▅▅▁▁█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91511
wandb: best/eval_avg_mil_loss 0.21059
wandb:  best/eval_ensemble_f1 0.91511
wandb:            eval/avg_f1 0.87838
wandb:      eval/avg_mil_loss 0.3426
wandb:       eval/ensemble_f1 0.87838
wandb:           train/avg_f1 0.83794
wandb:      train/ensemble_f1 0.83794
wandb:         train/mil_loss 0.83104
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devoted-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/468qxog8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054118-468qxog8/logs
wandb: ERROR Run 468qxog8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: k261su7p with config:
wandb: 	actor_learning_rate: 0.00015262332364058553
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 196
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6671335498977113
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054351-k261su7p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k261su7p
wandb: uploading wandb-summary.json; uploading history steps 180-196, summary
wandb: uploading history steps 180-196, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▃▄▅▆█
wandb: best/eval_avg_mil_loss ▇▄▂█▁█▂▁
wandb:  best/eval_ensemble_f1 ▁▁▃▃▄▅▆█
wandb:            eval/avg_f1 ▇▇▆▇▇▇▆▆▆▇▇▆▆▇▆▁▆▅▇▇▇▇▇▇▆▇▇▇▇▇█▄▄▆▇▇▆▇▆▆
wandb:      eval/avg_mil_loss ▁▁▁▁▂▁▂▁▁▁▁▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▂▂▂▃▂█▃▁▂▁▂▁▂▁
wandb:       eval/ensemble_f1 ▇▇▇▆▅▁▇▇▃▇▇▇▆▇▇▇▇▇▇▇▇▅▇██▇▅▇▅▇▇▃▇▇▅▇▇▇▇▇
wandb:           train/avg_f1 ▃▅▄▃▆▄▃▄▅▄▅▆▆▆▅▄▅▆█▁▄▄▅▆▆▆▇▆▅▇▃▄▃█▆▆▆▄▆▆
wandb:      train/ensemble_f1 ▅▆▂▅▆█▆▅▃▇▆▆▅▄▄▅▁▃▆▆▅▄▄▇▅▄▇▅▇▆▄▆▂▅▄▆▅▆▂▅
wandb:         train/mil_loss ▂▂▄▁▂█▂▅▂▁▂▂▇▃▅▄▅▂▆▅▂▆▃▃▄▂█▁▄▃▁▂▂▄▂▂▄▆▃▄
wandb:      train/policy_loss ▁▁▄▄▄█▄▁▄▄█▁▁█▄▄▄▄██▄▄█▄▄▁██▄▄▁▁▄▄▁█▁▁▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄█▄▄█▄▄▄▄█▁▄█▁▄▄▁▁▁█▄▄██▄▄▁▄▄▁▁▄▄▄▄▁▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93016
wandb: best/eval_avg_mil_loss 0.2575
wandb:  best/eval_ensemble_f1 0.93016
wandb:            eval/avg_f1 0.9105
wandb:      eval/avg_mil_loss 0.23677
wandb:       eval/ensemble_f1 0.9105
wandb:           train/avg_f1 0.88721
wandb:      train/ensemble_f1 0.88721
wandb:         train/mil_loss 0.27534
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run firm-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k261su7p
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054351-k261su7p/logs
wandb: ERROR Run k261su7p errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 8eb7r3b8 with config:
wandb: 	actor_learning_rate: 7.350901874956819e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 54
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3093564576101532
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054641-8eb7r3b8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8eb7r3b8
wandb: uploading history steps 51-54, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▇█
wandb: best/eval_avg_mil_loss █▄▆▅▁
wandb:  best/eval_ensemble_f1 ▁▂▃▇█
wandb:            eval/avg_f1 ▆▆▆▁▃▃▆▆▄▆▆▃▂▅▂▆▂▆▂▇▅▅█▅▂▂▃▃▅▃▃█▅▃▆▁▂▆▄▄
wandb:      eval/avg_mil_loss ▄▁█▅▃▄▂▂▄▁▆▆▇▅█▃█▄▆▂▃▂▂▂▃▆▇▃▂▄▆▂▅▃▄██▂▅▆
wandb:       eval/ensemble_f1 ▆▆▆▁▃▆▄▆▄▆▅▂▅▂▄▂▆▂▇▂█▅▇▇▂▄▂▃▃▅▂▃█▅▃▂▁▂▆▄
wandb:           train/avg_f1 ▂▅▃▅▆▇▅▁▅▂▅▆▆▆▁█▅▅▆▄▆▇▆▆▂▇▂▇▃▄▆▆▄▄▅▅▁▆▅▆
wandb:      train/ensemble_f1 ▅▃▅▅▄▅▁▅▂▅▆▆▁█▅▆▆▄▃▄▆▇▆▆▂▇▂▇▃▄▆▆▄▄▅▅▆▅▇▆
wandb:         train/mil_loss ▃▂▂▅▆▄▆▄▂▃▅▃▅█▅▅▅▅▆▅▆▄▅▅▃▄▄▄▆▄▂▄▂▇▅▅▃▁▅▃
wandb:      train/policy_loss ██████████████████████▁█████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████▁█▁██████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.89535
wandb: best/eval_avg_mil_loss 0.30708
wandb:  best/eval_ensemble_f1 0.89535
wandb:            eval/avg_f1 0.69377
wandb:      eval/avg_mil_loss 2.02298
wandb:       eval/ensemble_f1 0.69377
wandb:           train/avg_f1 0.69498
wandb:      train/ensemble_f1 0.69498
wandb:         train/mil_loss 1.41154
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run prime-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8eb7r3b8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054641-8eb7r3b8/logs
wandb: ERROR Run 8eb7r3b8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: d59nllhz with config:
wandb: 	actor_learning_rate: 0.00014519936905673465
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6343368116791726
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054743-d59nllhz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d59nllhz
wandb: uploading history steps 51-66
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆█████
wandb: best/eval_avg_mil_loss █▇▃▂▁▁▁▂▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆█████
wandb:            eval/avg_f1 ▁▄▅▆▄▄▇▁▇▂▄▁▇▂▇▄▅█▄▇▄▇▆▅▇█▅▅▇█▇▇█▄▇▆▄█▇▄
wandb:      eval/avg_mil_loss █▆▃▂▁█▁▄▄▁▅▂▃▇▃▁▃▄▁▁▃█▄▅▁▃▁▄▄▂▁▁▂▄▃▅▂▁▁▆
wandb:       eval/ensemble_f1 ▂▄▆▆█▅▅▇▃▁▅█▇█▅▅█▅▃▅███▆█▇█▇▇▇█▅▇▇▆████▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃█▄▇█▄▇▄▇▄█▇█▆▆▇▅▆▆▆▃▆▁▅▄▄█▆▆▇▆▃▇▄▇▂▇▆▇▅
wandb:      train/ensemble_f1 ▂▄▃▃▅▇▃▃▆▃▇█▅▆▆▇▆▆▄▅▃▅▅▃█▇▆▆▆▂▅█▇▅▁▇▅▅▆▄
wandb:         train/mil_loss ▃▅▄▅▄▆▅▅▅▆▅▆▄▂▅▃▆▄▂▃▃█▄▁▄▄▃▃▅▂▄▆▆▃▇▃▄▅▄▆
wandb:      train/policy_loss ▁▅▅▅▅▅██▁▁▅▁▅▅█▅▁▅▁▁██▁▅▅▅▁▁█▅▅▅▅▅▁▅▅█▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅▅▅▅▅▅█▅██▅▁▁▅▅▁▅▁▁▅▅█▁▅▁▁▁▁▅▅▅▁▅▅▅█▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91909
wandb: best/eval_avg_mil_loss 0.22543
wandb:  best/eval_ensemble_f1 0.91909
wandb:            eval/avg_f1 0.72518
wandb:      eval/avg_mil_loss 1.63934
wandb:       eval/ensemble_f1 0.72518
wandb:            test/avg_f1 0.83391
wandb:      test/avg_mil_loss 0.50028
wandb:       test/ensemble_f1 0.83391
wandb:           train/avg_f1 0.81569
wandb:      train/ensemble_f1 0.81569
wandb:         train/mil_loss 0.86992
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rural-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d59nllhz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054743-d59nllhz/logs
wandb: Agent Starting Run: oudogo0t with config:
wandb: 	actor_learning_rate: 0.00048127670329342674
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 85
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5310335507007667
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_054850-oudogo0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/oudogo0t
wandb: uploading history steps 84-85, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▄▆▅▅▆▃▆▅▁▂█▅▅▂▅▇▄▃▆▇▆▂▂▅█▆▅▅█▃▅▅█▅▆▇▆▄▅
wandb:      eval/avg_mil_loss ▁▃▄▃▄▅▅▆▄▄▃█▃▃▅▃▆▅▃▄▆▆▄▆▁▃▃▅▃▄▆▄▅▁▅▃▃▁▄▅
wandb:       eval/ensemble_f1 ▄▄█▄▂▆▆▅▁▅██▂█▇█▆▇▂▃█▄▅▆▅▆▇▅█▇▅█▄▅█▆▇▆█▅
wandb:           train/avg_f1 ▂▅▃█▅▅▅▇▆▆▅▅▅▃▅▇▄▄▂▆▆▃▅█▄▆▅▄▅▆▇▃▁▆▄▆▇▂▆▆
wandb:      train/ensemble_f1 ▇▇▁▅▅█▆▄▇▆▅▄▃▂▄▁▅▆▆▃█▄▅▅▄▃▃▇▃▄▆▄▁▆▆▇▂▄▅▅
wandb:         train/mil_loss ▅▇▄▄▅▇▆▃▅█▆▅▇▅▅▁▇▅▅█▅▅▅▄▅▅█▄▆▅▃▃▆█▆▄█▅▂▅
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9046
wandb: best/eval_avg_mil_loss 0.25735
wandb:  best/eval_ensemble_f1 0.9046
wandb:            eval/avg_f1 0.71385
wandb:      eval/avg_mil_loss 1.42481
wandb:       eval/ensemble_f1 0.71385
wandb:           train/avg_f1 0.76484
wandb:      train/ensemble_f1 0.76484
wandb:         train/mil_loss 1.78618
wandb:      train/policy_loss 0.16962
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.16962
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run comic-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/oudogo0t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_054850-oudogo0t/logs
wandb: ERROR Run oudogo0t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: wbdowogr with config:
wandb: 	actor_learning_rate: 0.0003177746617422737
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 140
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2662440145387509
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055013-wbdowogr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wbdowogr
wandb: uploading history steps 131-140, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▆█
wandb: best/eval_avg_mil_loss █▃▁▁▁▂
wandb:  best/eval_ensemble_f1 ▁▄▅▆▆█
wandb:            eval/avg_f1 ██▃▅▇█▆▆▁▅█▅███▅▇▂█▆█▅▃▃▅█▅▅▅█▅▂▄▁▆▇▆▂█▅
wandb:      eval/avg_mil_loss ▃▅▃▁▃▁▇▁▃▄▇▄▁▄▁▁▁▆▆▃▄▁▅▂▆▁█▃▁▁▁▆▅▆▂▆▄▆▇▇
wandb:       eval/ensemble_f1 ▇██▃▆█▂▇██▇▂██▅▇▆▆█▆▄▆▂▃▅▇▇▅▇▅▄▆▁█▂▁█▅▇▃
wandb:           train/avg_f1 ▅▄▄▆▄▂▃▃▃▇▅▁▃▅█▂▅▅▃▆▅▁▅▄▃▆▂▇▄▄▂▅▆▇▃▂▅▇▂▄
wandb:      train/ensemble_f1 ▃▅▅▃▃▆▅▄▆▄▄▅▆▄█▆▅▆▁▄▅▄▃█▄▇▇▄▄▄▅▂▃▂▄▅▃▁▆▆
wandb:         train/mil_loss ▅▅▅▂▄▄▆█▃▅▇▂▇▃▄▁▄▆▄▅▃▄▅▆▇▅▄▂▄▅▅▅▃▆▆▄█▁▅▃
wandb:      train/policy_loss ██▅▅▅▁██▁▅▅▅█▅███▅▅██▁▅▅▁▅▅▅▅▅▅██▁▅▅▅▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91194
wandb: best/eval_avg_mil_loss 0.28452
wandb:  best/eval_ensemble_f1 0.91194
wandb:            eval/avg_f1 0.73706
wandb:      eval/avg_mil_loss 1.51932
wandb:       eval/ensemble_f1 0.73706
wandb:           train/avg_f1 0.74876
wandb:      train/ensemble_f1 0.74876
wandb:         train/mil_loss 1.69677
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glamorous-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wbdowogr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055013-wbdowogr/logs
wandb: ERROR Run wbdowogr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: bz81ds4u with config:
wandb: 	actor_learning_rate: 0.00030488404285729617
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7666347995405883
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055227-bz81ds4u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bz81ds4u
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 72-88, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃████
wandb: best/eval_avg_mil_loss █▇▃▁▁▁
wandb:  best/eval_ensemble_f1 ▁▃████
wandb:            eval/avg_f1 ▂▃█▂▃▇▂▃▃▅▄█▃▃▃█▂██▇▇▃▃█▂▇▁█▃▃▇▇▇█▄█▂█▇▇
wandb:      eval/avg_mil_loss ▄▃▄▅▅▇▅▃▄▃▁▄▂█▂▃▅▁▁▁▅▄▇▇▄▁▅▇▁▅▅▁▄▁▅▁▂▁▁▅
wandb:       eval/ensemble_f1 ▂▃▇█▃▂▇▃▇▇██▇▃█▂▂▇▃▁▃▄▃█▇▂▇███▄▂▇▇▃▇▇█▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▇▅▅▆▇▇▃▇▆▄▅▄▂▅▆▅▃▃▃▁▆▇▄▆▇▄▅▅▅▆▅▅▂▆█▃▆▆▅
wandb:      train/ensemble_f1 █▆▄▂▃▆█▂▆▆▄▇▇▃▆▄▄█▅▅▄▄▅▄▃▆▄▄▁▇▄▅▅▂▅▆▅▅█▇
wandb:         train/mil_loss ▄▂▃▆▁▅█▃▄▂▄▆▄▄█▄▄▃▅▃▃▄▄▃▂▄▅▇▆▆▂▄▃▇▁▄▃▄▅▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅█▅▅▅▅▅▁██▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9105
wandb: best/eval_avg_mil_loss 0.25463
wandb:  best/eval_ensemble_f1 0.9105
wandb:            eval/avg_f1 0.8544
wandb:      eval/avg_mil_loss 1.69225
wandb:       eval/ensemble_f1 0.8544
wandb:            test/avg_f1 0.59441
wandb:      test/avg_mil_loss 2.15411
wandb:       test/ensemble_f1 0.59441
wandb:           train/avg_f1 0.72518
wandb:      train/ensemble_f1 0.72518
wandb:         train/mil_loss 0.87448
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devout-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bz81ds4u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055227-bz81ds4u/logs
wandb: Agent Starting Run: z8uv4fw8 with config:
wandb: 	actor_learning_rate: 1.386715186681691e-06
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 147
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7961681544134133
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055350-z8uv4fw8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z8uv4fw8
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃██
wandb: best/eval_avg_mil_loss █▂▂▁▄
wandb:  best/eval_ensemble_f1 ▁▃▃██
wandb:            eval/avg_f1 ▆▇▆▆▄▆▇███▇█▇▇▇█▄▇█▇▇▅▅▇▇▃▇█▇▆▇▇▇▇▇█▇▆▇▁
wandb:      eval/avg_mil_loss ▂▄▃▄▁▁▁▁▁▁▁▂▁▂▁▅▁▂▆▁▄▂▂▂▂▂▂▂▂▁▁▃▁▂▃▁▄▂▁█
wandb:       eval/ensemble_f1 ▇▆▅▄▆▃▇█▇▁▆▂▇█▁▇▃▆▃▇▆▆▆▇▆▇▅▇▆▄▇▆▇▇▂▁▅▇█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▅█▅▄▅▁▇▅▅▄▂▅▄▅▂▆▄▇▃▆▄▆▄▆▅▄▅▆▇▅▅▅▅▄▆▅▄▄
wandb:      train/ensemble_f1 ▅▇▅▅█▆▆▆▆▄▇▅▆▅▄▆▄▄▄▇▇▆▆▆▆▇▆▇▆▆▅▁▆▆▄▆▆▆▆▆
wandb:         train/mil_loss ▅█▅▂▁▃▇▄▅▅▅▆▄▆▅▅▄▄▄▇▄▂▄▄▄▅▂█▇▄▄▅▂▅▃▄▇▃▆▁
wandb:      train/policy_loss ▁██▄█▁▁█████▁▁███▁▁▁▁█▁██▁███▁▁▁▁▁▁██▄▄▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▄█▁█▄█▄▁▁▁▁▁██▁▁▁██▁▁████▁▄▁▄▁▁▄▁▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90799
wandb: best/eval_avg_mil_loss 0.29476
wandb:  best/eval_ensemble_f1 0.90799
wandb:            eval/avg_f1 0.81387
wandb:      eval/avg_mil_loss 0.55054
wandb:       eval/ensemble_f1 0.81387
wandb:            test/avg_f1 0.92406
wandb:      test/avg_mil_loss 0.17745
wandb:       test/ensemble_f1 0.92406
wandb:           train/avg_f1 0.86527
wandb:      train/ensemble_f1 0.86527
wandb:         train/mil_loss 0.29585
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fast-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z8uv4fw8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055350-z8uv4fw8/logs
wandb: Agent Starting Run: p9e7xusb with config:
wandb: 	actor_learning_rate: 3.7677387277305614e-05
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 70
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1710140007017078
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055620-p9e7xusb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/7oyxzwid
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p9e7xusb
wandb: uploading history steps 66-71, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▆▇█
wandb: best/eval_avg_mil_loss ▇▆█▆▁▄
wandb:  best/eval_ensemble_f1 ▁▂▅▆▇█
wandb:            eval/avg_f1 ▇▇▁▄▇▅█▇▅▇▇▇▁▁▇▂▇▇▇▅▄██▂▁█▇▄▇█▂▇▇██▃▇▇▄▇
wandb:      eval/avg_mil_loss ▁▁▃▁▅▂▅▅▆▇▁▁▃▁▃▁▁█▁▂▁▁▁▁▁▅▂▁▁▂▂▁▁▁▇▃▅▅▁▃
wandb:       eval/ensemble_f1 ▇▇▁▅█▆▄▇▇▇▇█▃▃▃█▇▆█▅█▄▇█▇▇█▅▄▇▇▇███▇▅▃▇▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▇▅▂▅▆▃▇▇▇▆▇▆▄▅█▅▇▆▅▅▃▆▃▇▇▄▆▆█▅▅▁▆▃█▆▂▅
wandb:      train/ensemble_f1 ▄▇▄▅▂▆▅▆▇▇█▅▇▆▅▆▄▅▅▆▆▅▆▆▆▄▅▃▄▆▇▇▂▃▆▅▁▃█▅
wandb:         train/mil_loss ▃▅▅▄▅▄▂▃▆▅▄▇▃█▄▅▄██▁▂▁▅▅▄▃▃▅▂▅▄▃▃▁▁▂▅▇▃▂
wandb:      train/policy_loss ▁▄▄▄█▁▁▄▁▁▄█▁▄▄▁▄█▄▁█▄▁▁▁▄▄█▄█▁▁▁▄▄██▁▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▄▄▄█▁▁▄▁▁▄▁▄█▄▁▄▁█▁▄▄▄▁▁▁▄▄▄▄▄█▄▁▄▄██▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91467
wandb: best/eval_avg_mil_loss 0.24754
wandb:  best/eval_ensemble_f1 0.91467
wandb:            eval/avg_f1 0.83911
wandb:      eval/avg_mil_loss 0.46073
wandb:       eval/ensemble_f1 0.83911
wandb:            test/avg_f1 0.9228
wandb:      test/avg_mil_loss 0.23387
wandb:       test/ensemble_f1 0.9228
wandb:           train/avg_f1 0.87258
wandb:      train/ensemble_f1 0.87258
wandb:         train/mil_loss 0.29883
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run deep-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p9e7xusb
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055620-p9e7xusb/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: xhq6x1mh with config:
wandb: 	actor_learning_rate: 2.1786776000940597e-05
wandb: 	attention_dropout_p: 0.3750008439920331
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 195
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5460227785101878
wandb: 	temperature: 4.841646849125286
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_055750-xhq6x1mh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xhq6x1mh
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json
wandb: uploading history steps 176-196, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▁▃▆█
wandb: best/eval_avg_mil_loss ▅▃▄▄█▁
wandb:  best/eval_ensemble_f1 ▁▁▁▃▆█
wandb:            eval/avg_f1 ▅▇▄█▂▃█▁▇██▇▆██████▃█▄███▃█████▄▇█▇█▇█▁▄
wandb:      eval/avg_mil_loss ▄▁▆▁▇▁▁▁▁▃▁▃▁▁▁▁█▂▄▅▁▃▁▁▇▁▅▁▆▁▁█▂▇▁▁█▁▁█
wandb:       eval/ensemble_f1 ██▇▂█▇▃██▄███▇▃█▆██▃▄▁██▇██████████▇██▁▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▅▆▆▅▄▅▅▂▁▆▅▆▄▆▇▅▆▅▄▂▅▅▄▆▅▄▅█▆▃█▇▆▆▃▇▆▆
wandb:      train/ensemble_f1 ▅▃▄▅▃▇▄▆▅▆▆▄▆▅▄▁▄▅▂▄▁▄▆▄▃▅▇▇▅█▃▅▆▂▇▃▆▇▄▅
wandb:         train/mil_loss ▅▁▃▄▅▁▄▄▅▂▄▃▁▃▂▄▄▂▂▅▃▅▅▄▄▂▁▂▂▃▂▃▄▄█▄▃▅▂▂
wandb:      train/policy_loss ██████████████████████████████▁█████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▅▅▅▃▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▂▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92922
wandb: best/eval_avg_mil_loss 0.20381
wandb:  best/eval_ensemble_f1 0.92922
wandb:            eval/avg_f1 0.56911
wandb:      eval/avg_mil_loss 2.15497
wandb:       eval/ensemble_f1 0.56911
wandb:            test/avg_f1 0.92596
wandb:      test/avg_mil_loss 0.15181
wandb:       test/ensemble_f1 0.92596
wandb:           train/avg_f1 0.79277
wandb:      train/ensemble_f1 0.79277
wandb:         train/mil_loss 0.36901
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cool-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xhq6x1mh
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_055750-xhq6x1mh/logs
wandb: Agent Starting Run: flwvrw8q with config:
wandb: 	actor_learning_rate: 0.0008877959157771573
wandb: 	attention_dropout_p: 0.28976751959991853
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 172
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.12133058633249338
wandb: 	temperature: 5.458266961560282
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060024-flwvrw8q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/flwvrw8q
wandb: uploading history steps 119-131, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▇█
wandb: best/eval_avg_mil_loss ▆█▃▃▁
wandb:  best/eval_ensemble_f1 ▁▂▄▇█
wandb:            eval/avg_f1 ▃▃▅▆▁▃▄▄▅▇▆▄▅▃▆▅▂▄▅▅▄█▅▆▅▅▆▅▅▆▅▆▇▆▆▄▆▆▇▃
wandb:      eval/avg_mil_loss ▆▆▆▄▄▄▃▅▇▄▂▃▃▅▂▃▅▂▂▄▅▄█▂▁▅▇▂▂▂▁▄▁▄▃▄▄▃▃█
wandb:       eval/ensemble_f1 ▄▅▆▄▄▄▇▂▄▁▁▅▄▄█▄▅▆▃▄▆▃▆▂▅▅▅▆▄▅▅▆▄▇█▆▃▄█▁
wandb:           train/avg_f1 ▆▅▄▅▅▅▅▂▄█▅▁▇▇█▇▆▄▆▄▄█▅▇▃▄▃▄▇▆▅▂▄▅▆▅▄▇▇▆
wandb:      train/ensemble_f1 ▆▅▁▅▄▄▅▄▇█▃▅▃▅▆▅▄▄▄▁▃▄▆▄▃▂▇▆▂▄▁▃▃▃▅▃▄▅▇▆
wandb:         train/mil_loss ▄▄▂▅▅▆▄▂▅▃▅▄▄▆▆▃▆▄▆▅▇▄▆▅▇▅▆▂▅▄▃█▅▃▁▁▂▃▅▂
wandb:      train/policy_loss ████████████████████████████████████▁███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████▁█████████████████▆▅██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9145
wandb: best/eval_avg_mil_loss 0.23252
wandb:  best/eval_ensemble_f1 0.9145
wandb:            eval/avg_f1 0.81736
wandb:      eval/avg_mil_loss 0.8321
wandb:       eval/ensemble_f1 0.81736
wandb:           train/avg_f1 0.81311
wandb:      train/ensemble_f1 0.81311
wandb:         train/mil_loss 0.71156
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run chocolate-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/flwvrw8q
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060024-flwvrw8q/logs
wandb: ERROR Run flwvrw8q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4448jn3y with config:
wandb: 	actor_learning_rate: 4.930054653123006e-06
wandb: 	attention_dropout_p: 0.43165213048321355
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9984060104089852
wandb: 	temperature: 9.960456978082856
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060251-4448jn3y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4448jn3y
wandb: uploading wandb-summary.json
wandb: uploading history steps 162-170, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss ▁█▆
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 █▃██▁▄█▄▄▁█▁█▅▄█▅▁▄▂█▃█▄▄▄▄█▃████▄▇█▄▅█▅
wandb:      eval/avg_mil_loss ▄▇█▃▄▅▂▂▂▂▁▄▁▄▁▄▃▃▄▇▂▄▄▃▅▃▆▃▃▁▅▁▆▂▁▂▂▄▃▂
wandb:       eval/ensemble_f1 ██▄█▂▂▄█▃▅▄▄█▃▄▄██▃█▄▄▁█▄██▃██▂▄█▅▄█▄▁▄▄
wandb:           train/avg_f1 ▇▆▆▁▆▆▄▅▆█▅▅▄▇█▅█▂▅▇▃▃▃▅▇█▁▃█▄█▅▅▆█▃▇▃▂▇
wandb:      train/ensemble_f1 █▅▄▆▆▆▇▆▆▅▃▇▄▄▆▁█▄▅▆▆▇▆▅▅▆▆▄▇▆▅▃▆▆▆▆▇▆▇▄
wandb:         train/mil_loss ▂▇▇▅▂▅▄▅▂▄█▅▃▅▄▁▅▄▃▅▄▄▂▂▅▃▅▄▃▆▄▃▂▆▃▃▂▄▂▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▃▄▄▁▄█▄▄▄▄▄▄▄▄█▆▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.36626
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.64935
wandb:      eval/avg_mil_loss 0.83065
wandb:       eval/ensemble_f1 0.64935
wandb:           train/avg_f1 0.6597
wandb:      train/ensemble_f1 0.6597
wandb:         train/mil_loss 0.64683
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sweepy-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4448jn3y
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060251-4448jn3y/logs
wandb: ERROR Run 4448jn3y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ixg7der1 with config:
wandb: 	actor_learning_rate: 0.00010473543636660006
wandb: 	attention_dropout_p: 0.4328101342272287
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 93
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05246899978816855
wandb: 	temperature: 6.414414184937199
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060506-ixg7der1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ixg7der1
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 80-93, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▇██
wandb: best/eval_avg_mil_loss █▁▂▁▂▁▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄▇██
wandb:            eval/avg_f1 ▂█▄█▄▇█▇███▇▄██▇█▇█▄██▁▄▇█▇▅█▇█▂█▇▇█▄▇██
wandb:      eval/avg_mil_loss ▆▁▁▂▇▅▁▃▃▂▁▁▁▃▆▄▅▁▃▁▁▁▃▁█▁▁▁▁▃▇▂▁▃▂▆▁▁▁▁
wandb:       eval/ensemble_f1 ▇▇▂▇▇██▆███▇▁▇▇▇▇▆▇█▇▃▂▇▇▇██▄▇▇▇▇█▃▃▇▇▇█
wandb:           train/avg_f1 █▆▅▅▆█▆▅█▆▁▂▆▄▅▃▄▆▅▆▂▆▆▅▄█▆▅▅▆▂▇█▅█▁▄▆▃█
wandb:      train/ensemble_f1 ▆▅▆▇█▄▆▆▅▇▁▆▅█▆▆▅▄▅▆▃▅▆▆▄▇▆▅▆▅█▆▇▄▇▇▂▅▄█
wandb:         train/mil_loss ▃▂▅▅▅▆▄▄▂▂▂▄▆▄▄▃▇▂▄▆▃▄▃▆▂▄▃▅▄▃▇█▃▁█▃▃▁▄▃
wandb:      train/policy_loss ▆▆▆▆▆▆▂▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▃█▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████▅██████▅███████████▁█████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92964
wandb: best/eval_avg_mil_loss 0.25374
wandb:  best/eval_ensemble_f1 0.92964
wandb:            eval/avg_f1 0.91414
wandb:      eval/avg_mil_loss 0.32535
wandb:       eval/ensemble_f1 0.91414
wandb:           train/avg_f1 0.86711
wandb:      train/ensemble_f1 0.86711
wandb:         train/mil_loss 0.71132
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run effortless-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ixg7der1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060506-ixg7der1/logs
wandb: ERROR Run ixg7der1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9f77rhq6 with config:
wandb: 	actor_learning_rate: 5.337153653349163e-05
wandb: 	attention_dropout_p: 0.18420673905129056
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 195
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7776436634283742
wandb: 	temperature: 7.251928634662654
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060623-9f77rhq6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9f77rhq6
wandb: uploading history steps 185-195, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▆▇█
wandb: best/eval_avg_mil_loss ▆█▁▁▁▂
wandb:  best/eval_ensemble_f1 ▁▁▄▆▇█
wandb:            eval/avg_f1 █▄▅▅▅█▆▄▆█▄▄▅▆▆▅█▅█▅███▆▇▅▅▆▁▄▆▅▅█▄▇▅▅█▇
wandb:      eval/avg_mil_loss ▂▂▄▆▂▃▂▆▃▅▆▁▆▂▂▆▃▅▅▂▂▃▂▂▂▂▅█▁▄▂▂▃▂▂▁▅▁▁▂
wandb:       eval/ensemble_f1 ▂▆▆▄▁▆▅█▃▆▄▄▅▆▅▄▄▄▁▄██▄█▄▆▄▇▄▇▅██▆▆▅█▆▃█
wandb:           train/avg_f1 ▁▂▂▄▁▄▃▅▂▂▆▆▅▆▃▅▅▄▅▃▆▅▇▆▅▃▄▅▇▅█▁▄▇█▅▄▅▆▆
wandb:      train/ensemble_f1 ▅▂▆▄▄▄▂▁▄▃▁▃▄▁▆▄▃▂▃▁▅█▃▆▆▅▄▆▅▃▇▅▅█▇▇█▅▅▆
wandb:         train/mil_loss ▅▅▅▆▆▅▇▄▇▅▆▆▄▂▆▅▆█▅▂▅▁▃▄▃▅▂▅▄▄▁▅▂▂▅▄▃▄▅▆
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▆▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.31194
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.8992
wandb:      eval/avg_mil_loss 0.29291
wandb:       eval/ensemble_f1 0.8992
wandb:           train/avg_f1 0.76488
wandb:      train/ensemble_f1 0.76488
wandb:         train/mil_loss 1.06278
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run swept-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9f77rhq6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060623-9f77rhq6/logs
wandb: ERROR Run 9f77rhq6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: akynmkyj with config:
wandb: 	actor_learning_rate: 0.00045334785675111026
wandb: 	attention_dropout_p: 0.4714195386822193
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 156
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5276035937008845
wandb: 	temperature: 4.512925390472296
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_060908-akynmkyj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/akynmkyj
wandb: uploading history steps 145-157, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆██
wandb: best/eval_avg_mil_loss █▅▅▂▁▂
wandb:  best/eval_ensemble_f1 ▁▅▆▆██
wandb:            eval/avg_f1 ▆▄▅▅▃▆▂▄▆▅▄▄▃▃▅▄▆▅▆▄▄▁▃▄▅▅▂▃▂▃█▅▄▃▆▃▆▃▇▅
wandb:      eval/avg_mil_loss ▇▄▄▅▅▅▁▄▆█▅▅▆▄▅▅▆▄▄▅▃▆▃▅▅▃▄▅▃▇▆▅▅▆▄▄▃▆▃▂
wandb:       eval/ensemble_f1 ▇▆▃▄▇█▆▅▅▂▅▅▇▁▅▅▅▅▅▅▂▅▆▄▄▄▃▆▃▄▄▆█▂▄▃▅▆▄▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▇▆▂▅▃▄▁▅▄▆▇▂▄▆▆▆▃▆▄██▆▆▆▇▇▆▄▃▄▄▄▄▆▆▃▆▃▆
wandb:      train/ensemble_f1 ▅▃▃▃▅▅▃▄▂▂▁▅▂▃▄▅▁▅▄█▆▁▇▅▄▆▅▆▄▅▃▄▆▇▇▅▃▅▃▆
wandb:         train/mil_loss ▄▂▅▆▄█▂▃▃▃▃▂▃▁▅▄▂▆▃▅▅▃▄▄▃▆▂▃▃▄▁▃▁▄▄▁▅▅▅▃
wandb:      train/policy_loss ███▁███▅████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅██████████▃█████████████████▁█████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.89279
wandb: best/eval_avg_mil_loss 0.70843
wandb:  best/eval_ensemble_f1 0.89279
wandb:            eval/avg_f1 0.69106
wandb:      eval/avg_mil_loss 1.48995
wandb:       eval/ensemble_f1 0.69106
wandb:            test/avg_f1 0.50511
wandb:      test/avg_mil_loss 2.64815
wandb:       test/ensemble_f1 0.50511
wandb:           train/avg_f1 0.66847
wandb:      train/ensemble_f1 0.66847
wandb:         train/mil_loss 1.65149
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run swept-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/akynmkyj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_060908-akynmkyj/logs
wandb: Agent Starting Run: jz6wueqa with config:
wandb: 	actor_learning_rate: 2.1576115973016788e-06
wandb: 	attention_dropout_p: 0.113849149223401
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4409844807422324
wandb: 	temperature: 3.577427074161783
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061158-jz6wueqa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jz6wueqa
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 80-87, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇██
wandb: best/eval_avg_mil_loss █▁▁▁▁▂
wandb:  best/eval_ensemble_f1 ▁▇▇▇██
wandb:            eval/avg_f1 ▅█▅█▇██▃█▁█▇█▇▃▂██▂▂▁▁█▇▄█▂█▂███▇▂███▇▂█
wandb:      eval/avg_mil_loss ▂▂▁▂▁▁▃▆▂▁▁▄▃▂▁▂▅▂▂█▅▁▄▁▂█▁▅▃▁▁▁▃▂▆▁▆▂▁▄
wandb:       eval/ensemble_f1 ▄██▁█▄▃▁▁██▇▂▂█▁▁███▄▇███▁▂█████▄▅█▂▄▅█▄
wandb:           train/avg_f1 ▆▄▆▇▃▅▇▄▁▅▆▆▇▅▆▅▆▇▄▇▅▄▆▅▅▆█▆▄▇▅▄▅██▄▅▄▆▅
wandb:      train/ensemble_f1 ▅▅▃▆▇▅▅▇▅▃▃▇▄▃▅▇▃▂▇▃▅▅▅▅▁▅█▄▄▃▇▆▄▅▄▃█▄▄▂
wandb:         train/mil_loss ▂▁▆▁▆▇▁▄▄▄▂▄▄▆▂▁▁▅▃▂▄▂▆▇▃▃▅▂▃▄▃▂▆▃▇▂▆█▂▂
wandb:      train/policy_loss ▁███████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.3477
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.58809
wandb:      eval/avg_mil_loss 2.0618
wandb:       eval/ensemble_f1 0.58809
wandb:           train/avg_f1 0.59593
wandb:      train/ensemble_f1 0.59593
wandb:         train/mil_loss 1.42371
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worthy-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jz6wueqa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061158-jz6wueqa/logs
wandb: ERROR Run jz6wueqa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 99wawcxg with config:
wandb: 	actor_learning_rate: 3.0329339811975447e-06
wandb: 	attention_dropout_p: 0.4305614339655348
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 106
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.747667786996881
wandb: 	temperature: 1.3241855417043769
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061311-99wawcxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/99wawcxg
wandb: uploading history steps 92-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██████
wandb: best/eval_avg_mil_loss █▂▂▁▂▂▁
wandb:  best/eval_ensemble_f1 ▁██████
wandb:            eval/avg_f1 █▅▅█▄█▇█▄██████████▄▇▅█▅█▇▅█▇▁▇▁▅██▅▅██▅
wandb:      eval/avg_mil_loss █▂▃▂▄▂▅▂▂▄▂▂▂▁▁▂▂▄▃▂▂▄▂▃▂▃▂▅▂█▅▁▄▃▁▁▂▂▅▄
wandb:       eval/ensemble_f1 ▇▃██▂▁▇█████▂███▇█▆▇▆▂▇▇█▇▇▇█▃█▇▇▂▃█▇█▃▃
wandb:           train/avg_f1 ▆▅▃▃▄▆▆▃▅▄▁▂▆█▅▇▄▃█▅▃▄▅▃▃▆▄▅▆▅▅▆▇▇▆▆▄▅▇▇
wandb:      train/ensemble_f1 ▆▅▃▅▂▇▆▇▇▁█▅▁▇▇▃▇█▁▃▃▄▆▃▄▆▄▅▄▁▅▆▃▅▃▆▅▅█▄
wandb:         train/mil_loss ▄▄▁▃▄▆▄▃▂▃▄▇▆▄▇▄▁▇▄▅▅▇▅▂▄▁▂▇▃▂▃▃▄█▂▂▂▅▆▅
wandb:      train/policy_loss ▆▆▁▆▆▆▆▆▆▅▆▆▆█▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▁▃█▃▃▃▃▃▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92573
wandb: best/eval_avg_mil_loss 0.23215
wandb:  best/eval_ensemble_f1 0.92573
wandb:            eval/avg_f1 0.81381
wandb:      eval/avg_mil_loss 0.50412
wandb:       eval/ensemble_f1 0.81381
wandb:           train/avg_f1 0.86757
wandb:      train/ensemble_f1 0.86757
wandb:         train/mil_loss 0.37006
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dry-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/99wawcxg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061311-99wawcxg/logs
wandb: ERROR Run 99wawcxg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ujtqce08 with config:
wandb: 	actor_learning_rate: 0.00017913634215676612
wandb: 	attention_dropout_p: 0.15196220697611895
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 147
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.27305139271227485
wandb: 	temperature: 2.0215728299715696
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061444-ujtqce08
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ujtqce08
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 █▄▄▅▁▆▇▆▂▄▅▆▃▄▄▆▇▅▆▅▄▆▆█▅▃▄▇▇▅█▄▃█▆▄▂▄▄▃
wandb:      eval/avg_mil_loss ▁▅▄▂▄▃▂▆▅▅▆▇▇▁▂▂▄▂▄▆▄▁▅▅▃▄▄▄▃▅▄▃▆▁▅▅▄▁▅█
wandb:       eval/ensemble_f1 ▄▅▅▄▇▆▅▃▇▆▅▆▇▇▆█▆▅▇▅▅▄▆▆▆▆▆▅▁▆█▅▃▄▅▄▄▇▅▄
wandb:           train/avg_f1 ▅▄▆▂▅▄▅▆▅▆▄▄▅▅▂▄▇▄▂▁▁▂▆▅▆▄▆▄▄▄▆▇▇▂█▅▅▂▄▅
wandb:      train/ensemble_f1 ▅▆▂▆▄▆▄▆▃▆▅█▅▆▃▁▁▄▄▅▆▂▇▁▄▄▆▄▄▆▆▃▂▇▇█▄▄▂▅
wandb:         train/mil_loss ▅▆▅▃▆▅▆▂▇▃█▅▄▂▃▆▅▄▆▃▆█▆▃▁▁▇▅▁▄▄▄▅▃▂▂▃▅▅▃
wandb:      train/policy_loss ██████▅███████████▁████████████▆████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄█▄▁▄▄▄▄▄▄▄▄▄▄▇▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91497
wandb: best/eval_avg_mil_loss 0.25947
wandb:  best/eval_ensemble_f1 0.91497
wandb:            eval/avg_f1 0.66377
wandb:      eval/avg_mil_loss 1.84311
wandb:       eval/ensemble_f1 0.66377
wandb:           train/avg_f1 0.79722
wandb:      train/ensemble_f1 0.79722
wandb:         train/mil_loss 0.70258
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fluent-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ujtqce08
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061444-ujtqce08/logs
wandb: ERROR Run ujtqce08 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: bjc6mq54 with config:
wandb: 	actor_learning_rate: 7.866924644614167e-06
wandb: 	attention_dropout_p: 0.0655144199690505
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 131
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10334588883768414
wandb: 	temperature: 6.707625330593634
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061638-bjc6mq54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bjc6mq54
wandb: uploading history steps 101-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▂▄▆▅▄▄▆▁▅▄▃▁▂▇▁▆▇▅▄▆▆▃▆▆██▅▂▅█▅▃▃▃█▇▃█▆▅
wandb:      eval/avg_mil_loss ▃▄▁▅▁█▁▆▁▆▃▆▄▄▄▇▅▃▂█▆▁▂▆▅▆▃▄▆▁▅█▃▄▄▆█▂▇▃
wandb:       eval/ensemble_f1 █▆█▄▆▄▆▆█▆█▆▂▆▂▂▇▅▄▃▆▃▄▇▃█▇▄▄▇▃▆▇▄▄▃▁▆▆▇
wandb:           train/avg_f1 ▄▆▃▃▃▁▄▂▃▃▄▆▄▅▄▂▃▇▆▆▇▅▆▃▃▁▄▄▃▁▁▄▆▄▅█▃▃▅▄
wandb:      train/ensemble_f1 ▆▃▁▃▂▂▃▃▃▄▄▆▄▆▄▅▃▅▄▇▄▄▅▄▆▄▄▄▁▆▅▆█▃▃▅▅▁▃▄
wandb:         train/mil_loss ▄▅▂▆▃▄▄▆▂▅▂▃▅▄▂▃▄▆▃▄▇▃▃▄▇▃▄█▃▅▄▄▅▂▃▁▂▅▂▃
wandb:      train/policy_loss ▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃▃█▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92236
wandb: best/eval_avg_mil_loss 0.22709
wandb:  best/eval_ensemble_f1 0.92236
wandb:            eval/avg_f1 0.878
wandb:      eval/avg_mil_loss 0.78202
wandb:       eval/ensemble_f1 0.878
wandb:           train/avg_f1 0.7848
wandb:      train/ensemble_f1 0.7848
wandb:         train/mil_loss 0.81462
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hopeful-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bjc6mq54
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061638-bjc6mq54/logs
wandb: ERROR Run bjc6mq54 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: hpc8xj7o with config:
wandb: 	actor_learning_rate: 2.4886124273660947e-05
wandb: 	attention_dropout_p: 0.2674479333834804
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22821302142379973
wandb: 	temperature: 1.480537426787053
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_061831-hpc8xj7o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hpc8xj7o
wandb: uploading history steps 86-92, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▆▇█
wandb: best/eval_avg_mil_loss ▆█▄▅▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆▆▇█
wandb:            eval/avg_f1 ▇█▇▆▇▄▃▆▇▇█▇▇▇▇▇█▇▇▆▅▇▂▁▇▆▆▇▇▆▇▂█▇▇▃▄▇▆▇
wandb:      eval/avg_mil_loss ▁▂▃▃▂▃▄▂▁▁▃▁▃▁▁▁▁▁█▁▁▂▁▁▁▁▃▅▂▁█▁▂▁▁▁▂▂▁▂
wandb:       eval/ensemble_f1 ▆▇▇▆▆▆▇▆▇▇▇▆▇▇▆▆▇▁▇▇█▆▆▇▆▇▇▆▇█▅▇▃▇▇▅▂▆▇▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆█▂▆▂▆▆▆▁▄▃▅▇▅▅▇▇▅▆▅▂▂▇▇▅▃▅▅▆▂▇▇▁█▄▅▆▅▆▅
wandb:      train/ensemble_f1 ▇█▄▅▆▂▇▅▄▇▅▄▄▂▇▇▁▆▆▅▇▄█▅▆▇▆▆▅▅▃▅▃█▅▆▆▅▇▄
wandb:         train/mil_loss ▄▃▃▃▅▅▃▁▄▂█▂▄▃▆▄▂▅▂▄▂▄▃▂▄▄▆▄▃▃▅▄▄▃▃▁▅▁▂▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▄▁▁▁▁▁█▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92964
wandb: best/eval_avg_mil_loss 0.23861
wandb:  best/eval_ensemble_f1 0.92964
wandb:            eval/avg_f1 0.89327
wandb:      eval/avg_mil_loss 0.28491
wandb:       eval/ensemble_f1 0.89327
wandb:            test/avg_f1 0.91168
wandb:      test/avg_mil_loss 0.19932
wandb:       test/ensemble_f1 0.91168
wandb:           train/avg_f1 0.88081
wandb:      train/ensemble_f1 0.88081
wandb:         train/mil_loss 0.26909
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dazzling-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hpc8xj7o
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_061831-hpc8xj7o/logs
wandb: Agent Starting Run: zclrrrs1 with config:
wandb: 	actor_learning_rate: 0.00025587687371741817
wandb: 	attention_dropout_p: 0.4527958838728934
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 90
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8345184743694471
wandb: 	temperature: 1.4755999681722751
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062014-zclrrrs1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zclrrrs1
wandb: uploading wandb-summary.json
wandb: uploading history steps 77-90, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▇▄▁▄▄▇███▂▅▇█▄▇▅█▂▇█▇▅██▂███▄█▃▃██▇▁▄▃█
wandb:      eval/avg_mil_loss ▁▄▇▅▂▁▃▄▁▄▂▃▃▄▁▅▄▂▅▃▂▁▄▁█▃▄▅▂▃▃▃▇▄▅▂▄▄▁▅
wandb:       eval/ensemble_f1 ▇▁▄▄██▂▃▅█▄▇▅▇█▇▅▄▇██▄█▃▂█▂███▂▃▄▇▃▄▄██▃
wandb:           train/avg_f1 ▆▇▇▄▄▅▄█▆▅▃▆▇▇▇▆▇▅▆▆▂▇▄▃▄▅▇▇▅▆▇▇▅▆▁▄▆▆▆▆
wandb:      train/ensemble_f1 ▇▆▆▇▄▆▂▆▇▇▅▄▆▄▂▁▇▄▆▂▄▅▅▆█▇▃▄▆█▆▅▆▆▃▇▅▆▆█
wandb:         train/mil_loss ▃▃▃▃▃█▃▄▆▆▂▆▅▃▅▂▁▅▂▄▃▄▄▁▆▅▃▅▂▆▆▃▆▂▂▂▆▂▂▄
wandb:      train/policy_loss ▇▇▇▇▇▇▇▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄█▄▄█▁▄▄▄▄▄▄█▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄█▄█▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91482
wandb: best/eval_avg_mil_loss 0.34945
wandb:  best/eval_ensemble_f1 0.91482
wandb:            eval/avg_f1 0.44448
wandb:      eval/avg_mil_loss 2.19057
wandb:       eval/ensemble_f1 0.44448
wandb:           train/avg_f1 0.77486
wandb:      train/ensemble_f1 0.77486
wandb:         train/mil_loss 0.45609
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bumbling-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zclrrrs1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062014-zclrrrs1/logs
wandb: ERROR Run zclrrrs1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5kt2r8li with config:
wandb: 	actor_learning_rate: 1.9744662297506503e-05
wandb: 	attention_dropout_p: 0.0949176905025554
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 165
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2872335963640149
wandb: 	temperature: 2.2893781050152895
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062131-5kt2r8li
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5kt2r8li
wandb: uploading history steps 162-166, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁███
wandb: best/eval_avg_mil_loss █▇▁▁
wandb:  best/eval_ensemble_f1 ▁███
wandb:            eval/avg_f1 ▃▃▃▄▆▆▃▄▁▄▆▄▅▁▅▄▄▃▅▃▄▄▄█▆▄▆▄▄▇▆▅▄▆▄▃▆▆▆▄
wandb:      eval/avg_mil_loss ▆▃▃▅▆▃▅▄▃▅▆▄▃▄▃▂▇▅▄▃▅▇▇▁▃▄▄▂▃▁▆▁█▃▂▅▄▄▂█
wandb:       eval/ensemble_f1 █▃▆▅█▄▃▆▆▁▆▄▄▅▅▄▃▃▆▃▃▆▂█▅▇▄▅▆▆▄█▅▅▄▅▅▆▄▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▅█▆▄▂▇▅▄▄▁▆▅▅▅▆▄▅▄▅▃█▆▃▇▇▇█▅▃▆▆█▄▆▆▅▅▆
wandb:      train/ensemble_f1 ▃▅▇▄▅▄▅▇▅▇▅▄▅▄▁▄▅▆▆▆▅▆▃██▄▆▇▅▆▇▆▄▇▄█▆▆▅▅
wandb:         train/mil_loss ▆▅▅▅▅▃▄█▆▃▄▅▂▆▅▆▄▆▄▄▇▅▃▄▅▅▃▆▄▄▁▃▅▃▄▂▃▂█▂
wandb:      train/policy_loss ▆▄▆▆▆▆▆▆▆▆▆▂▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▅▆▆▆▆▆█▆▆▆▆▆▆▆▆▂▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9103
wandb: best/eval_avg_mil_loss 0.30141
wandb:  best/eval_ensemble_f1 0.9103
wandb:            eval/avg_f1 0.55396
wandb:      eval/avg_mil_loss 1.56945
wandb:       eval/ensemble_f1 0.55396
wandb:            test/avg_f1 0.72957
wandb:      test/avg_mil_loss 0.74979
wandb:       test/ensemble_f1 0.72957
wandb:           train/avg_f1 0.75221
wandb:      train/ensemble_f1 0.75221
wandb:         train/mil_loss 0.98034
wandb:      train/policy_loss -0.1698
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.1698
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run jolly-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5kt2r8li
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062131-5kt2r8li/logs
wandb: Agent Starting Run: k01cckmu with config:
wandb: 	actor_learning_rate: 1.2383045779212704e-06
wandb: 	attention_dropout_p: 0.26253854355842277
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 188
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5180604482160881
wandb: 	temperature: 3.651771373330397
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062356-k01cckmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k01cckmu
wandb: uploading wandb-summary.json
wandb: uploading history steps 175-188, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃██████
wandb: best/eval_avg_mil_loss ██▅▆▂▃▁▁
wandb:  best/eval_ensemble_f1 ▁▃██████
wandb:            eval/avg_f1 ▅█▁▄▅▁▂▁▁▄▂▄▁█▂██▃█▂█▁▂▁▁▅▆▁▁▂▆▁█▃▂▃▃▁▁▁
wandb:      eval/avg_mil_loss █▅▇▇▃▆▇▅▇▆▃▄▇▇▅▅▁▄▇▆▅▅█▆▅█▂▃█▄▆█▇▆▇█▃▆▆▄
wandb:       eval/ensemble_f1 ▅▂█▅▄▁▅▄▂▅▂▁▂▆▂██▁▄▂▁▆▁▅▅▂▁▂▁▂▁▁▁█▁▇▂▃▇▂
wandb:           train/avg_f1 ▂▂▅▆▆▄▃▆▇▁▃▅▅▇▅▄▅▄▄▄▂▅▃▃▅▂▇▆▂▄▆▅▆▇▅▆▆▇█▁
wandb:      train/ensemble_f1 ▂▄▂▆▁▇▅▇▅▅▂▅▂▅▇▄▂▄▄▆▂█▅▄▆▅▃▄▄▅███▆▃▃▇▆▃▁
wandb:         train/mil_loss ▃▇▆▅▅▆▆▇▇▅▄▄▆▇▄▄█▄▃▇▅▃▃▂▅▅▆▇▅▅▅▇▁▅▅▆▄▂▃▄
wandb:      train/policy_loss ███████████████████████▂██████████▁█████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████▂██████████████▁███
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90325
wandb: best/eval_avg_mil_loss 0.28989
wandb:  best/eval_ensemble_f1 0.90325
wandb:            eval/avg_f1 0.3426
wandb:      eval/avg_mil_loss 2.69172
wandb:       eval/ensemble_f1 0.3426
wandb:           train/avg_f1 0.46611
wandb:      train/ensemble_f1 0.46611
wandb:         train/mil_loss 1.98638
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run restful-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k01cckmu
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062356-k01cckmu/logs
wandb: ERROR Run k01cckmu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 7d93yqwl with config:
wandb: 	actor_learning_rate: 1.2310884145268782e-05
wandb: 	attention_dropout_p: 0.12199876368472112
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 74
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3119023653224603
wandb: 	temperature: 1.0391069215999515
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062631-7d93yqwl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7d93yqwl
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▃▇█
wandb: best/eval_avg_mil_loss █▇▂▃▃▁
wandb:  best/eval_ensemble_f1 ▁▃▃▃▇█
wandb:            eval/avg_f1 ▆▇▇▃▇▅▇▅▅▅█▇▅█▅▃▅▄▆▇▆▅▄▇▅▄▁▃▇▂▄█▅▁▆▃▇█▄▅
wandb:      eval/avg_mil_loss ▄▂▇▄▄▂▄▄▆▅▃▆▁▅▇▄▂▅▄▃▄▄▅▃▅▃▄▄▆▂▇▆▄▄█▇▄▅▆▃
wandb:       eval/ensemble_f1 ▇▇▅▇▅▇▆▇▅▇█▇▆█▅▆▅▇▇▆▇▅▅▄▇▆▃▇▄▆▆▁▇▃▇█▄▆▁▆
wandb:           train/avg_f1 ▆▆▅▁▇▂▂▆▆▆▆▃▄▅▅▁▅▆▆▅▅▅▃▅█▇█▇▅▆▆▄▃▂▇▅▇▇▅▅
wandb:      train/ensemble_f1 ▆▆▄▁▇▇▆▂▆▄▄▃▃▄▅▅▁▆▅▆▄▄▅▅▆█▃█▇▆▄▇▆▃▂▃▅▅▇▆
wandb:         train/mil_loss ▃█▆▅▂▄▃▅▃▆▆▃▇▅▅▅▄█▅▄▄▇▆▅▄▆▃▃▄▅▅▄▅▄▇▅█▅▅▁
wandb:      train/policy_loss ▃▃▁█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▄▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▁▄▄▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90344
wandb: best/eval_avg_mil_loss 0.26662
wandb:  best/eval_ensemble_f1 0.90344
wandb:            eval/avg_f1 0.73934
wandb:      eval/avg_mil_loss 1.1674
wandb:       eval/ensemble_f1 0.73934
wandb:           train/avg_f1 0.71834
wandb:      train/ensemble_f1 0.71834
wandb:         train/mil_loss 0.84395
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lucky-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/7d93yqwl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062631-7d93yqwl/logs
wandb: ERROR Run 7d93yqwl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: n1v2tvl7 with config:
wandb: 	actor_learning_rate: 6.146031656084934e-06
wandb: 	attention_dropout_p: 0.16385900110454676
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 59
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2524635045686202
wandb: 	temperature: 3.947773975454012
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062754-n1v2tvl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n1v2tvl7
wandb: uploading history steps 54-60, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▆█
wandb: best/eval_avg_mil_loss █▅▆▃▁
wandb:  best/eval_ensemble_f1 ▁▂▅▆█
wandb:            eval/avg_f1 ▁▁▂▂▅▂▂▂▃▂▅▆▁▁▂▄▁▂▁▂▂▁▃▃█▆▁▅▃▂▆▄▆▂▃▂▂▂▂▂
wandb:      eval/avg_mil_loss ▅█▇▇▆▇▆█▇▆▆▆▆▆▃▇▆▆▇▅▅▇▇▅▅▆▅▁▇▄▅▆▇▄▇▄▆▇▆▆
wandb:       eval/ensemble_f1 ▁▂▁▂▂▆▂▂▂▃▄▂▆█▃▂▂▂▂▁▄▂▁▃▃▂█▁▄▂██▄▂▂▃▂▂▂▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▃▅▃▃▆▄▅▃▄▂▄▂▅▃▃▃█▄▄▂▆▆▄▄▅▆▇▆▅▃▄▆▅▃▃▃▅▅▁
wandb:      train/ensemble_f1 ▆▁▅▂▁▅▃▄▃▂▁▃▃▅▁▂▄▂█▂▁▃▃▃▄▄▇▆▄▄▆█▄▄▂▂▁▅▁▃
wandb:         train/mil_loss ▆▇▆▇▅▄▇▅▅▆▆▇▅▇▅▅▆▇▇▄▄▇▅▆▄▆▅▄▄▅▆▆▅▆█▇▅▆▁▆
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▁▅▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.88231
wandb: best/eval_avg_mil_loss 0.52201
wandb:  best/eval_ensemble_f1 0.88231
wandb:            eval/avg_f1 0.41284
wandb:      eval/avg_mil_loss 3.19408
wandb:       eval/ensemble_f1 0.41284
wandb:            test/avg_f1 0.38062
wandb:      test/avg_mil_loss 3.68283
wandb:       test/ensemble_f1 0.38062
wandb:           train/avg_f1 0.48465
wandb:      train/ensemble_f1 0.48465
wandb:         train/mil_loss 2.83687
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glad-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n1v2tvl7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062754-n1v2tvl7/logs
wandb: Agent Starting Run: z39q9sc9 with config:
wandb: 	actor_learning_rate: 8.477867394945432e-05
wandb: 	attention_dropout_p: 0.03807401104135061
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 142
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9701630757031292
wandb: 	temperature: 1.9736013162256285
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_062851-z39q9sc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z39q9sc9
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▇█
wandb: best/eval_avg_mil_loss ▅█▃▁
wandb:  best/eval_ensemble_f1 ▁▃▇█
wandb:            eval/avg_f1 ▆▄▆▆▃▆▆▁▃▄▃▄▅▅▇▆█▅▆▅▄▄▅▄▅▄▃▆█▂▃█▂▃▄▂▆▆▇▅
wandb:      eval/avg_mil_loss ▃▂▃▇▃▅▄█▂▄▃▄▄▃▆▃▆▃▅▃▃▅▄█▂▆▂▁▄▄▅▃▂▆▆▁▁▃▃▃
wandb:       eval/ensemble_f1 ▅▄▇▆▅▇▂▄▅▄▆▇▇▇▅█▅▄▆▆▇▁▃▇▆▇▅▆█▃▆▄▃▅▆▄▆▆▇▇
wandb:           train/avg_f1 █▄▅▅▂▄▆▄▆▃▇▅▆▇▆▇▆▃▇▃▃▄▁▇▆▃▃█▆▅▅█▇▆▂▇▃▆▅▄
wandb:      train/ensemble_f1 ▄▃▅▆▂▇▃▆▄▂▃▄▄▃▃▅▁▃▅▇██▅▁▇▇▅▃▂▅▅▄█▄▆▄▇▄▇▅
wandb:         train/mil_loss ▄▁▃▅▄█▃▅▅▃▃▅▄▅▃▂▃▁▂▃▂▅▂▅▃▃▆▅▃▂▁▂▃▃▅▆▃▃▂▃
wandb:      train/policy_loss ▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▁▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90363
wandb: best/eval_avg_mil_loss 0.2562
wandb:  best/eval_ensemble_f1 0.90363
wandb:            eval/avg_f1 0.79895
wandb:      eval/avg_mil_loss 0.83354
wandb:       eval/ensemble_f1 0.79895
wandb:           train/avg_f1 0.76435
wandb:      train/ensemble_f1 0.76435
wandb:         train/mil_loss 1.06502
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smooth-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/z39q9sc9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_062851-z39q9sc9/logs
wandb: ERROR Run z39q9sc9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jpk9ef47 with config:
wandb: 	actor_learning_rate: 9.150260511031313e-05
wandb: 	attention_dropout_p: 0.4651100507574969
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 165
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04236521469325216
wandb: 	temperature: 6.258501640290288
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063120-jpk9ef47
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jpk9ef47
wandb: uploading history steps 155-165, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄██
wandb: best/eval_avg_mil_loss █▇▃▁▁
wandb:  best/eval_ensemble_f1 ▁▃▄██
wandb:            eval/avg_f1 ▆▂▆▅▄▅▃▅▅▆▅▅▆▃▇▆▇▆▃▅▅██▅▆▆▅▄▁▄▇█▄▅▆▇▆▆▆▃
wandb:      eval/avg_mil_loss ▆▃▆▂▂▆▆▆▁▅▅▆▄▃▄▁▂▅▆▅█▅▃▂▂▄▃▁▃▂▄▆▂▃▄▃▅▄▆▂
wandb:       eval/ensemble_f1 ▆▂▅▅▃▁▆▁▅▅▅▆▃▅▃▆▄▅▆▃▇▅▆█▅▆▆▅▆▆▆▆▄▂▄▆▅▅▃▅
wandb:           train/avg_f1 ▃▃▅▃▇▆▁█▇▃▂▆▃▄▃▇█▄▇▇▇▁▁▅▃▅▆▇▄▆▆▇▄▄▃▆▂▇▃▃
wandb:      train/ensemble_f1 ▄▄▇▃▃▁▄▇▄▅▆▅▄▃▄▄▃▅█▄▇▆▅▃▇▇▅▄▆▄▇▆▆▇▆▆▄▅▇▆
wandb:         train/mil_loss ▇▆▇▅▆▇▇▇▆▃▆▅▄█▆▇▅█▅▄▇▇▆▄▂▃▃▄▇▄▃▁█▆▅▂▂▇█▆
wandb:      train/policy_loss █████████▆█▁███████▆▆███████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████▁██████████████████▅███████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8992
wandb: best/eval_avg_mil_loss 0.21249
wandb:  best/eval_ensemble_f1 0.8992
wandb:            eval/avg_f1 0.5905
wandb:      eval/avg_mil_loss 1.7958
wandb:       eval/ensemble_f1 0.5905
wandb:           train/avg_f1 0.73441
wandb:      train/ensemble_f1 0.73441
wandb:         train/mil_loss 1.23251
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run easy-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jpk9ef47
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063120-jpk9ef47/logs
wandb: ERROR Run jpk9ef47 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 4be6bm1f with config:
wandb: 	actor_learning_rate: 2.318268244280028e-05
wandb: 	attention_dropout_p: 0.11753174151079632
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 142
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.29676595668674133
wandb: 	temperature: 3.2884334172907694
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063421-4be6bm1f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4be6bm1f
wandb: uploading history steps 141-142, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▇███
wandb: best/eval_avg_mil_loss █▅▅▂▁▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▇███
wandb:            eval/avg_f1 ▆▆▆█▅█▅▂▆▆▂▆▇██▃▅█▆█▅█▅▁▅▅██▄▆▆▄█▆▅▆▇█▅▆
wandb:      eval/avg_mil_loss ▂▅▁▂▂▄▅▂▁▂▂▂▃▁▁▂▁▄▂▃▄▁▁▃▁▂▁▂▂▁▁▄▁▂▂▃▁▂█▁
wandb:       eval/ensemble_f1 ▆▆▇█▇▄▆▁▆██▁▃▅█▇█▅▆█▇▅▅███▆▃▆▄▅▆▇▇▄▅█▄▆▅
wandb:           train/avg_f1 ▄▅▄█▅▄▄▅▄▄▅▆▂▁▄▂█▁▃▅▅▆▅█▆▄▄▆▄▄▆▃▆▃▁▃▃▅▆▄
wandb:      train/ensemble_f1 █▇▅▄▇▅▅▆▄▃▅▇▄█▄▄▂▄▄▅▆▄█▁▆▇▄▃▇▇▅▅▇▆▇▆▄▆▅▆
wandb:         train/mil_loss ▄▄▃▃▅▅▇▂▇▅▄▆█▃▁▄▄▁▅▄▂▃▆▃▃▃▄▅▂▅▄▇▂▃▃▄▄▅▂█
wandb:      train/policy_loss ████▁██████████████████████████████████▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.26531
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.71187
wandb:      eval/avg_mil_loss 1.35615
wandb:       eval/ensemble_f1 0.71187
wandb:           train/avg_f1 0.8174
wandb:      train/ensemble_f1 0.8174
wandb:         train/mil_loss 1.31967
wandb:      train/policy_loss -0.06696
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.06696
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dark-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4be6bm1f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063421-4be6bm1f/logs
wandb: ERROR Run 4be6bm1f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: lcytuca5 with config:
wandb: 	actor_learning_rate: 4.651077487530821e-06
wandb: 	attention_dropout_p: 0.4169082629225236
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.08535046313588135
wandb: 	temperature: 7.107116799356455
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063630-lcytuca5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lcytuca5
wandb: uploading wandb-summary.json; uploading history steps 106-122, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇█
wandb: best/eval_avg_mil_loss █▇▄▅▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇█
wandb:            eval/avg_f1 ▆▆█▅██▂▇▆▇▂▆▆▆█▆▅▆▅▆█▇▆▆▅▄▆▆▆▅▄▆▅▁▇▂█▇▇▅
wandb:      eval/avg_mil_loss ▄▃▄▂▄▇▄▄▅▄▅▅▁▁▆▆▃▂▃▄▃▃▁▃▅█▂▃▇▅▄▁▅▂▃▇▂▅▄▃
wandb:       eval/ensemble_f1 ▅█▆▆▇█▃▃█▂▆▇▂▆▆█▆▂▇█▆▂▁█▂▆▂▅▆█▆▆▆▃▆██▅▁▅
wandb:           train/avg_f1 ▅▃▃▃▁▅▆▃▅▄▁▃▃▂▃▆▃█▄▄▄▄▃▃▁▆▃▅▄▃▆▄▅▄▂▅▆▄▅▂
wandb:      train/ensemble_f1 ▅▅▆▅▇▇▇▇▄▅▇▅▇▆▆█▄▄▅▄▁▅█▅▃▇▆▆▃▅▆▆▄▇▆▆▆▆▆▅
wandb:         train/mil_loss ▅▂▅▅▇▄▇▄▇▃▄█▃▅▃▅▃▄▆▅▃▄▁▃▃▅▁▃▂▃▆▇▃▃▄▁▄▆▃▅
wandb:      train/policy_loss ▁█████████████████████████████████████▆█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁▁▁▄▄██▆▄█▃█▆█▄▄█▄▄█▄▄▄█▁█▄█▄▁▃▄▄██▁▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.27036
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.72685
wandb:      eval/avg_mil_loss 2.38826
wandb:       eval/ensemble_f1 0.72685
wandb:           train/avg_f1 0.73635
wandb:      train/ensemble_f1 0.73635
wandb:         train/mil_loss 1.6364
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run still-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lcytuca5
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063630-lcytuca5/logs
wandb: ERROR Run lcytuca5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mnflvvkz with config:
wandb: 	actor_learning_rate: 2.409310646688514e-06
wandb: 	attention_dropout_p: 0.3815551408063677
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 108
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9500890656524132
wandb: 	temperature: 2.504749694275165
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063818-mnflvvkz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mnflvvkz
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▁▂▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▅▁▄█▅▅▅▁▅▄▅▅▅▅▅█▂▅▁▅▂▅▅▁████▆█▄▅▅▅▂▅▁▅▅▅
wandb:      eval/avg_mil_loss ▅▇▁█▄▁▄▅▄▃▆▄▄▁▆▄▃▃▁▄▁▃▄▃▃▅▄▁▁▁▃▆▂▄▅▄▄▃▂▃
wandb:       eval/ensemble_f1 ▅▃▅▅▃█▆▅▃▃▅▅▆▅█▅█▆▃▅▅▅█▇█▁█▃▅▃▇▅█▆▆▇▃▃▆▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▁▄▆▄▄▁▆▃▄▅▂▅█▃▃▄▂▆▃▅▂▅▇▅▅▄▆▅▃▅▃▅▄▁▃▅▄▆▆
wandb:      train/ensemble_f1 ▂▄▃▄▅▆▄▆▅▇▃▁▃▅▅█▆▆▃▇▄▄▂▅▅▃▆▄▄▄▄▃▅▃▅▅▄▄▅▆
wandb:         train/mil_loss ▇▇▄▇▅▂▅▃▇▃▅▅▆▅▇▆▄▂▄▅▇█▄▄▄▄▁▄▇▄▇▃▃▄▄▆▅▆▂▆
wandb:      train/policy_loss ▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▃▃▄▄▄▄▄▄▄▄▃▄▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92937
wandb: best/eval_avg_mil_loss 0.3013
wandb:  best/eval_ensemble_f1 0.92937
wandb:            eval/avg_f1 0.79074
wandb:      eval/avg_mil_loss 0.67697
wandb:       eval/ensemble_f1 0.79074
wandb:            test/avg_f1 0.80972
wandb:      test/avg_mil_loss 0.37385
wandb:       test/ensemble_f1 0.80972
wandb:           train/avg_f1 0.80545
wandb:      train/ensemble_f1 0.80545
wandb:         train/mil_loss 0.65736
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run denim-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mnflvvkz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063818-mnflvvkz/logs
wandb: Agent Starting Run: g63jyxex with config:
wandb: 	actor_learning_rate: 0.0001801374602086915
wandb: 	attention_dropout_p: 0.3044137492239417
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 151
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.46481595092850136
wandb: 	temperature: 6.567912751740495
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_063956-g63jyxex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g63jyxex
wandb: uploading history steps 132-150; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃███
wandb: best/eval_avg_mil_loss █▃▂▂▁
wandb:  best/eval_ensemble_f1 ▁▃███
wandb:            eval/avg_f1 █▅▄▄▄▂█▄▄▂▄▄▅▃▄▃█▄▃▁▃▅▅▅▂▅▅▄▂██▄▅▅▅▅▄▁▅▅
wandb:      eval/avg_mil_loss ▃▆▇▅▄▃█▃█▂▅▅▇▄▅▅▂▂▇▂▂▂▂▂▂▂▅▅▁▅▅▇▂▂▅▇▂▅▄▅
wandb:       eval/ensemble_f1 ▂▅▄▄▄█▄▂▄▂▄▄█▅▂▃▃▄▅▁▅▂▂▅▅▄█▁██▃▅▅▅█▁▁▅▅▂
wandb:           train/avg_f1 ▃▂▅▂▃▇▂▄▄▅▄▄▂▃▃▄▆▂▅▄▅▄▄▄▇▂▁▂▄▂▃▄▆▅▆▂▁█▅▄
wandb:      train/ensemble_f1 ▁▃▂▃▄▂▅▃▃▅▂▃▄▄▅▅▂▂▄▆▂▅▂▆▂▅█▆▃▄▅▇▁▁▂█▇▆▂▆
wandb:         train/mil_loss ▂▄▄▅▅▅▆▆▆▃█▂▄▃▄▃▄▄█▁▃▅▂▄▆▃▄▆▄▅▂▅▁▇▄▅▅▃▆▂
wandb:      train/policy_loss ██████████▁██▁██████████████▅███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▅██████████▁████████████▅██▅██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91467
wandb: best/eval_avg_mil_loss 0.32185
wandb:  best/eval_ensemble_f1 0.91467
wandb:            eval/avg_f1 0.39974
wandb:      eval/avg_mil_loss 2.20486
wandb:       eval/ensemble_f1 0.39974
wandb:           train/avg_f1 0.56902
wandb:      train/ensemble_f1 0.56902
wandb:         train/mil_loss 1.11777
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bright-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g63jyxex
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_063956-g63jyxex/logs
wandb: ERROR Run g63jyxex errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: nmvncgd3 with config:
wandb: 	actor_learning_rate: 1.100276914536888e-05
wandb: 	attention_dropout_p: 0.08545410870841263
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 105
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9086033247214296
wandb: 	temperature: 0.16077718688284004
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064211-nmvncgd3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nmvncgd3
wandb: uploading wandb-summary.json
wandb: uploading history steps 92-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▆▇▇█
wandb: best/eval_avg_mil_loss ▁█▄▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▁▄▆▇▇█
wandb:            eval/avg_f1 ▃▇▂▇▇▇██▃▇█▃▂█▇▇▇█▃▃▃█▇█▃█▇▂▁▃▃▃██▇█▃▁▇▇
wandb:      eval/avg_mil_loss ▅▁▅▆▃▃▁▆▃▃▂▆▆▄▃▅▇▄▁▁▆▆▇▆▆▁▁▅▃█▄▁▃▇▁▁▁▆▄▁
wandb:       eval/ensemble_f1 ▃▂▃▇▇█▄▇▄▄█▅▂▇▇▄▁▇▇▄█▁█▄██▃▂█▇███▇▇█████
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▄▆▆█▆▇▆▇▃▆▅▇▂▇█▄▅▅▆█▄▅▁▅▅▄▄▇▅▄▃▂▃▇▅▅▆▄
wandb:      train/ensemble_f1 ▅▆█▆▅█▆▄▇█▅▆▅▆▁▃▂▇▆▇▃▅▃▄▅▅▅▇▃▄▅▄▃▄▆▅▆▅▆▄
wandb:         train/mil_loss ▃▆▁▅▁▂▄▂▆▁▆▃▅▄▅▄▆▄▄▄▄▃▄▃▄▃▂▄▃▄█▁▅▇▃▂▃▄▄▂
wandb:      train/policy_loss █████▂██▁███████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93328
wandb: best/eval_avg_mil_loss 0.24713
wandb:  best/eval_ensemble_f1 0.93328
wandb:            eval/avg_f1 0.86417
wandb:      eval/avg_mil_loss 0.36554
wandb:       eval/ensemble_f1 0.86417
wandb:            test/avg_f1 0.43582
wandb:      test/avg_mil_loss 1.90037
wandb:       test/ensemble_f1 0.43582
wandb:           train/avg_f1 0.80372
wandb:      train/ensemble_f1 0.80372
wandb:         train/mil_loss 0.52244
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nmvncgd3
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064211-nmvncgd3/logs
wandb: Agent Starting Run: tsykheky with config:
wandb: 	actor_learning_rate: 8.42485509882352e-06
wandb: 	attention_dropout_p: 0.25819518933022223
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 177
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6260921663637279
wandb: 	temperature: 9.295688682752363
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064344-tsykheky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tsykheky
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 169-174, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃▆▆▇▇█
wandb: best/eval_avg_mil_loss ██▁▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▁▃▆▆▇▇█
wandb:            eval/avg_f1 ▇█▃██▄▇█▅█▇▄▁█▂██▅█▇██▃██▄▂▆█▁▆▆█▄▃█▅▂██
wandb:      eval/avg_mil_loss ▇▁▃▁▄▆▃▄▄▅▂█▂▁▅▅▅▃▃▅▄▁▄▃▂▅▄▅▄▂▂▁▃▂▆▅▅▁▄▅
wandb:       eval/ensemble_f1 █▇▆▅███▆▂█▄▁█▂▂▇██▂███▃█▆█▅███▄██▄▃██▂▄▅
wandb:           train/avg_f1 ▂▅▅▃▄▅▇▁▆▇▅█▄▅▇▂▂▆▅▂▇▇▃▇▃▅▁▆▂▃▆▆▄▄▂▄▆▆▅▆
wandb:      train/ensemble_f1 ▃▃▇▆▅▇▄▅▅▃▇▆▇▄▄█▃▂▇▃▄▆▆▆▁▆▆▅▂▆▃▄▄▇▇▅▆▇▄▅
wandb:         train/mil_loss ▄▅▄▅▄█▅▂▃▅▆▄▇▆▄▃▄▃▄▆▄▂▅▆▂▁▅▇▅▅▅▃▆▃▁▄▆▄▄▃
wandb:      train/policy_loss ████████▁██▆██████▁████▇████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████████▁███████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91104
wandb: best/eval_avg_mil_loss 0.28415
wandb:  best/eval_ensemble_f1 0.91104
wandb:            eval/avg_f1 0.87871
wandb:      eval/avg_mil_loss 2.14335
wandb:       eval/ensemble_f1 0.87871
wandb:           train/avg_f1 0.76819
wandb:      train/ensemble_f1 0.76819
wandb:         train/mil_loss 1.29142
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/tsykheky
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064344-tsykheky/logs
wandb: ERROR Run tsykheky errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 4m1j4fo6 with config:
wandb: 	actor_learning_rate: 0.00046570371214183664
wandb: 	attention_dropout_p: 0.3224735109010155
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 153
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7428630387964165
wandb: 	temperature: 9.154708017511435
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064609-4m1j4fo6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4m1j4fo6
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 112-128, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▇▅▅▇▅██▇█▄▅▂▃▃▁▃▄█▇▃█▁███▃▇█▇▃▄▄▅▄▇▄▅▄█▇
wandb:      eval/avg_mil_loss ▁▂▁▆▁▄▄▂▄▂▆▂▁▆▆▃█▄▂▂▂▅▄▄▁▃▁▃▂▃▅▇▂▂▄▂▃▃▁▁
wandb:       eval/ensemble_f1 █▄██▄▅██▄█▃▄▃▇▃▇████▄▄▇▅██▅▅▅▂█▁▄█▅▅▇▃█▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▃▄▄▆▄▅▅▃▁▄▄▅▃▄▄▅▇▂▄▄▄▂▆▄▃▃▃▄▄▂▁▇▄▅▃▅█▆▅
wandb:      train/ensemble_f1 ▆█▁▆▆▄▂▅▁▄█▇▆▆▅▃▆▂▆▃▅▄▇▇▆▆▃▃▄▅▆▆▇█▆█▅▁█▁
wandb:         train/mil_loss ▅▇▃▁▂▂▅▂▇▅▅▃▃▇▄▄▃▂▂▅▄▄▇▅▃▂▁▃▄▂▄▅▃█▅▃▆▆▄▄
wandb:      train/policy_loss ██████████████████████████▁█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91813
wandb: best/eval_avg_mil_loss 0.38586
wandb:  best/eval_ensemble_f1 0.91813
wandb:            eval/avg_f1 0.9105
wandb:      eval/avg_mil_loss 0.40598
wandb:       eval/ensemble_f1 0.9105
wandb:            test/avg_f1 0.90463
wandb:      test/avg_mil_loss 0.2735
wandb:       test/ensemble_f1 0.90463
wandb:           train/avg_f1 0.71943
wandb:      train/ensemble_f1 0.71943
wandb:         train/mil_loss 0.83276
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run expert-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/4m1j4fo6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064609-4m1j4fo6/logs
wandb: Agent Starting Run: ai1vktmn with config:
wandb: 	actor_learning_rate: 2.9859631058581894e-06
wandb: 	attention_dropout_p: 0.03974653216419749
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 197
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1992324353251297
wandb: 	temperature: 4.330440365883321
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_064758-ai1vktmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ai1vktmn
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇▇▇███
wandb: best/eval_avg_mil_loss █▃▅▅▃▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇▇▇███
wandb:            eval/avg_f1 ▃██▆▅▅▄▄▅▂█▇▇▁▇▆▆▇▄▆▆▃▆▆▂▂▂▄▁▇▇▂▅▆▆▆▅▅▇▅
wandb:      eval/avg_mil_loss ▃▇▃▂▂▁▄▆▄▄▇▅▆▃▃█▅▂▃▃▄▂▄▄▇▆▇▅▄█▅▅▄▂▄▇▅▃▄▂
wandb:       eval/ensemble_f1 ▂▁▆▇▇▆▄▁▄▄▆▂▂▆▇▇▆▆▆█▄▄▇▅▇▂▁▇█▁▆▇▆▅▁█▄▃▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▅▄▄▂█▇▅▄▄▁▅▅▃▁▇▄▅▅▃▄▅▇▂▅▄▂▆█▄▅▆▄▄▃▇▅▆▇
wandb:      train/ensemble_f1 ▁▅▄▅▅▇▃▂▃█▄▇▅▇▄▄▃▃▅▄▇▇▅▂▇▇▅▇▅▄▅▇▅▄▃▄▃▃▅▅
wandb:         train/mil_loss ▄▇▄▄▂▅▇▅▅▅▃▄▅█▅▂▅▃▅▁▆▆▄▄▄▅▆▄▂▃▆▅▂▄▅▄▄▆▂▃
wandb:      train/policy_loss ▂▂▇█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄█▂▄▄▄▄▄▇▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90646
wandb: best/eval_avg_mil_loss 0.25635
wandb:  best/eval_ensemble_f1 0.90646
wandb:            eval/avg_f1 0.74365
wandb:      eval/avg_mil_loss 1.3233
wandb:       eval/ensemble_f1 0.74365
wandb:            test/avg_f1 0.65471
wandb:      test/avg_mil_loss 2.01423
wandb:       test/ensemble_f1 0.65471
wandb:           train/avg_f1 0.75554
wandb:      train/ensemble_f1 0.75554
wandb:         train/mil_loss 1.31137
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run prime-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ai1vktmn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_064758-ai1vktmn/logs
wandb: Agent Starting Run: mfsjzgty with config:
wandb: 	actor_learning_rate: 1.266570277867515e-06
wandb: 	attention_dropout_p: 0.28573121544878016
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 175
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.21499990818371809
wandb: 	temperature: 6.77539079742998
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065139-mfsjzgty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mfsjzgty
wandb: uploading history steps 173-176, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▅▆▆▇█
wandb: best/eval_avg_mil_loss ▆██▂▂█▃▁
wandb:  best/eval_ensemble_f1 ▁▄▅▅▆▆▇█
wandb:            eval/avg_f1 ▆▃▄▁▃▄▆▁▂▅▄▂▄▄█▆▇▁▇▃▆▅█▆▆▆▇▇▆▂▃▆▁▇█▇▁▇▅▂
wandb:      eval/avg_mil_loss ▇▆█▇▅▄▄▇▆▃▇▅▄▄▅▅▅▅▄▅▄▂▅▅▆▄▆▆█▃▃▂▃▆▂▆▇▃▁▆
wandb:       eval/ensemble_f1 ▃▂▂▁▄▄▆▄▆▄▁▅▆▃▁▃▇▅▁▃▄▄▃▅▅▇▇▃▃▅▅▅▅▆▅▁▃▁▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▃▁▂▃▃▅▃▅▄▃▄▅▂▆▅▅▇▂▅▄▅▅▄▅▆▅▃▃▆█▅▅▆█▆▇▇▇▅
wandb:      train/ensemble_f1 ▃▅▁▂▄▃▃▃▅▁▃▄▃▄▃▄▅▅▅▄▆▄▅▃▄▄▅▄▅▃▃▃▇█▅▆▅▅▆▇
wandb:         train/mil_loss ▇▆▇▇▅█▅█▅▄█▆▄▆▃▅▆█▃▇▃▄▆▄▆▄▅▆▂▁▁▄▄▄▂▄▃▂▃▂
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁████▇██▂███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.65905
wandb: best/eval_avg_mil_loss 0.8228
wandb:  best/eval_ensemble_f1 0.65905
wandb:            eval/avg_f1 0.37271
wandb:      eval/avg_mil_loss 2.91287
wandb:       eval/ensemble_f1 0.37271
wandb:            test/avg_f1 0.61383
wandb:      test/avg_mil_loss 0.99803
wandb:       test/ensemble_f1 0.61383
wandb:           train/avg_f1 0.51958
wandb:      train/ensemble_f1 0.51958
wandb:         train/mil_loss 1.65302
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mfsjzgty
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065139-mfsjzgty/logs
wandb: Agent Starting Run: vifv4u4h with config:
wandb: 	actor_learning_rate: 9.637507958317216e-06
wandb: 	attention_dropout_p: 0.11076624954734438
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7032308547717373
wandb: 	temperature: 8.095363741729768
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065419-vifv4u4h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vifv4u4h
wandb: uploading history steps 74-91, summary; uploading wandb-summary.json
wandb: uploading history steps 92-92, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃███
wandb: best/eval_avg_mil_loss ▇▇█▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▃███
wandb:            eval/avg_f1 ▃▄█▂█▇██▇▄██▃▄▂██▄█▁███▂▄▅█▅▄███▄█▃▇▂▂▄▄
wandb:      eval/avg_mil_loss ▄▃▆▂▃▄█▄▄▂▂▄▆▁▄▃▂▆▅▁▄▆▂▄▆▁▁▂▃▂▁▁▁▂▁▂▅▇▁▆
wandb:       eval/ensemble_f1 ▄▃▄▅██▇█▂▇▂█▄▄▁▄▄█▇▁▇▇▂█▄▂▁▄█▄▇▅█▇█▄█▇▇▄
wandb:           train/avg_f1 ▃▃▃▅▄▅▅▆▇▂▄▆▃█▆▆▇▅▇▄▇▆▅▇▃▆▆▇▁▅▆▆▄▇▅▅▄▆▇▁
wandb:      train/ensemble_f1 ▃▃▃▃▂▃▃▅▄▂▅▂▄▅▅▄▁▄▃▆▄█▄▆▂▄▅▅▁▄▆▆▅▄▅▅▄▅▄▁
wandb:         train/mil_loss ▃▂▆▄▄▃▅█▅▃▆▂▆▅▆▄▄▄▄▅▅▄▅▇▁▅▆▄▄▂▅▇▆▂▆▅▃▆▅▅
wandb:      train/policy_loss ███████████████████████▁████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄█▄▄█▄▄▄█▄▄▄▁█▁▄▄▄▄▁▄▁▄██▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91813
wandb: best/eval_avg_mil_loss 0.32823
wandb:  best/eval_ensemble_f1 0.91813
wandb:            eval/avg_f1 0.63149
wandb:      eval/avg_mil_loss 1.04953
wandb:       eval/ensemble_f1 0.63149
wandb:           train/avg_f1 0.55031
wandb:      train/ensemble_f1 0.55031
wandb:         train/mil_loss 1.02468
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run peachy-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vifv4u4h
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065419-vifv4u4h/logs
wandb: ERROR Run vifv4u4h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5fpve8fo with config:
wandb: 	actor_learning_rate: 1.9114673786080992e-06
wandb: 	attention_dropout_p: 0.3760853983522699
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 152
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.946205765093049
wandb: 	temperature: 8.37405239149835
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065551-5fpve8fo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5fpve8fo
wandb: uploading history steps 151-153, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▄▅▅▇██
wandb: best/eval_avg_mil_loss ███▃▄▄▅▁
wandb:  best/eval_ensemble_f1 ▁▁▄▅▅▇██
wandb:            eval/avg_f1 ▃▄▁▂▂▅▆▂▆▃▆█▆▅▁▅▄▂█▅▇▄▂▄▃▅▆▄▄▂▃▅█▇▅▆▄▄▃▄
wandb:      eval/avg_mil_loss █▆▅▅▅▅▂▆▄▆▄▂▄▃▄▅▃▃▆▅▅▁▅▂▂▃▁▄▄▁▂▄▆▃▅▄▃▂▂▁
wandb:       eval/ensemble_f1 ▄▄▃▁▂▃▂▃▄▄▆█▆▅▁▄▁▆▄█▆▇▇▃▅▄▅▇▄▇▃▇▇▅▆▃▇▄▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▁▃▃▃▄▄▅▄▅▆▃▅▇▄▅▂▃▆▅▄▅▃▆▆▅▅▆▄▇▅▄█▆▅▇▅▆▆
wandb:      train/ensemble_f1 ▃▃▇▂▃▅▅▃▃▁▄▄▆▅▆▁▆▃▅▆▆▅▇▇▃▆▆▄█▆▇▇▇▅▆█▇▅█▇
wandb:         train/mil_loss ▇█▇██▆▇▄▃▄▇▆▅▅▃▃▆▄▅▄▃▄▇▅▂▇▂▃▂▄▄▄▄▅▃▄▁▄▄▅
wandb:      train/policy_loss ▆▁▆▆▆▅▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▅▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁███████▇███▆█████████▄█████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.88215
wandb: best/eval_avg_mil_loss 0.38718
wandb:  best/eval_ensemble_f1 0.88215
wandb:            eval/avg_f1 0.88215
wandb:      eval/avg_mil_loss 0.38718
wandb:       eval/ensemble_f1 0.88215
wandb:            test/avg_f1 0.65357
wandb:      test/avg_mil_loss 1.31664
wandb:       test/ensemble_f1 0.65357
wandb:           train/avg_f1 0.73452
wandb:      train/ensemble_f1 0.73452
wandb:         train/mil_loss 1.06036
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run summer-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5fpve8fo
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065551-5fpve8fo/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qy1gp378 with config:
wandb: 	actor_learning_rate: 4.521310608845224e-06
wandb: 	attention_dropout_p: 0.20820253033291924
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 128
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.14244501814576116
wandb: 	temperature: 8.713077718351629
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_065852-qy1gp378
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qy1gp378
wandb: uploading history steps 121-129, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇██
wandb: best/eval_avg_mil_loss █▄▇▅▁▂
wandb:  best/eval_ensemble_f1 ▁▄▇▇██
wandb:            eval/avg_f1 ▃█▇▇█▅▁▇█▆▆▆▄▄███▂▆██▇▃▇▆▆█▇▆▅█▅▄▃▅▄▇▆██
wandb:      eval/avg_mil_loss ▃▃▅▃▁▁▃▁▁▃▄▄▃▁▅▂▃▃▄▁▅▆▂█▅▅▂▁█▃▂▄▅▇▁▇▂▅▂▅
wandb:       eval/ensemble_f1 ▇█▅█▄▄▅█▇▇▆▅▅▅▆▄▆▆▆█▂█▆▇▇▂▅▁▁█▆█▃▅▅▂▇▆██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▇▇▃▇▇▅█▆▁▅▄▆▄▅▄▄▆▆▅▅▁▂▆█▆▆█▆▇▄▅▃█▇▆█▅▃▂
wandb:      train/ensemble_f1 ▇▆▂▅▅▁▇▆▄▃▆▅▄▄▆▅▂█▅▄▂▆▅▄▅▆▃▄█▅▃█▄▂▆▆▃▆▇▄
wandb:         train/mil_loss ▂▆▂▄▅▄▅▄▃▇▆▂▂▇▃▃▃▃▆█▄▅▄█▇▃▆▇▅▄▃▄▆▃▁▅█▇▅▅
wandb:      train/policy_loss ██████████▁█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████▆███████████████████████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.31336
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.75726
wandb:      eval/avg_mil_loss 1.02166
wandb:       eval/ensemble_f1 0.75726
wandb:            test/avg_f1 0.77723
wandb:      test/avg_mil_loss 0.47302
wandb:       test/ensemble_f1 0.77723
wandb:           train/avg_f1 0.7609
wandb:      train/ensemble_f1 0.7609
wandb:         train/mil_loss 0.74148
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fearless-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qy1gp378
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_065852-qy1gp378/logs
wandb: Agent Starting Run: lmbjpcnj with config:
wandb: 	actor_learning_rate: 0.00045212723914902846
wandb: 	attention_dropout_p: 0.2164793240727867
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.05134473710260079
wandb: 	temperature: 2.6449141797554256
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070051-lmbjpcnj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lmbjpcnj
wandb: uploading history steps 97-100, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▇█
wandb: best/eval_avg_mil_loss █▃▁▁▁
wandb:  best/eval_ensemble_f1 ▁▃▆▇█
wandb:            eval/avg_f1 ▇▇▁▅█▅▄█▇█▃▅▇▄▅▅▅▅▆▇▁▄▇▇▇▇▇▇█▄▆▅▃▅▇▇▇▇▃▆
wandb:      eval/avg_mil_loss ▃▅▃▆▄▂▅▁▁▃▆▇▄▁▂█▂▄▃▃▂▂█▁▆▃▃▁▃▃▇▅▂▃▂▄▇▁▄▄
wandb:       eval/ensemble_f1 ▃▆█▆▄██▆██▇▅▇▆▆▄▆▅▅▇▇▅▄█▇▇▅▇█▇▅▄▁▆▆▇▆▇▅▅
wandb:           train/avg_f1 ▆█▄▆▄▁▆▄▆▄▆▇▅▄▆▅▄▆▇▆▆▅▅▅▄▇▅▃▇▅█▄▆▆█▅▇▃▆▅
wandb:      train/ensemble_f1 ▄▄▆▆▃▃▅▂▁▃▄▄▄▇▃▄▂▄▄▅▃▄▃▄▅▃▄▅█▁▄▄▆▄▃▅▂▄▅▅
wandb:         train/mil_loss ▄▂▇▃▇▁▂▂▇▄▂▃▄▄▄▅▇▃▁▄▃▅▅▄▅▄▃▄▆▆▇█▄▁▆▃▃▄▃▃
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92573
wandb: best/eval_avg_mil_loss 0.26956
wandb:  best/eval_ensemble_f1 0.92573
wandb:            eval/avg_f1 0.86024
wandb:      eval/avg_mil_loss 0.32663
wandb:       eval/ensemble_f1 0.86024
wandb:           train/avg_f1 0.81385
wandb:      train/ensemble_f1 0.81385
wandb:         train/mil_loss 0.5084
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fragrant-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lmbjpcnj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070051-lmbjpcnj/logs
wandb: ERROR Run lmbjpcnj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: t2c236v2 with config:
wandb: 	actor_learning_rate: 2.6368426030140062e-05
wandb: 	attention_dropout_p: 0.3952243242628392
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17722777007407498
wandb: 	temperature: 5.855783749984291
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070244-t2c236v2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t2c236v2
wandb: uploading history steps 97-107, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▅▆▆█
wandb: best/eval_avg_mil_loss █▇▅▂▄▄▁
wandb:  best/eval_ensemble_f1 ▁▂▅▅▆▆█
wandb:            eval/avg_f1 ▂▆▇▁▃▇▇▂▃▇▄▂▆▃█▅▃▂▅▆▇▅▇█▇▇▆▆▇▄▅▅▄▄▅▃█▆▆▆
wandb:      eval/avg_mil_loss ▇▇▆▆▄▇▆▅▃█▄▇▄▅▂▇▃▅▂▂▄▂▄▁▃▅▄▃▂▅▃▃▅▃▅▂▄▃▂▃
wandb:       eval/ensemble_f1 ▄▅▂▅▁█▃▆▁▄▅▁▂▄▃▂▆▅▁▆▄▅▁▄▇█▆▆▃▃▅▂▃▃▇▁▅▇▅▄
wandb:           train/avg_f1 ▃▃▃▃▅▂▃▄▄▆▃▃▂▄▅▄▂▄▂▆▅▇▃▅▆▅▅▃█▅▄▆▇▇▁▆▅▅▄▇
wandb:      train/ensemble_f1 ▃▃▅▃▃▅▄▃▄▄▂▃▃▂▄▆▄▄▂▄▂▆▄▄▄▅▅▄▄▅▇▇▁▅█▅▄▆▇▆
wandb:         train/mil_loss ▂▆▆▁█▅▄▅▅▃▆▅▅▅▅▅▄▅▃▄▅▄▂▄▃▄▄▂▂▄▃▁▂▂▂▃▁▂▃▃
wandb:      train/policy_loss ▆▁▆▆▆▆▅▆▆▆▆▆▆▆▆▆█▆▆▆▆▄▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▆▇▇▇▇▇▇▁▆▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91173
wandb: best/eval_avg_mil_loss 0.28748
wandb:  best/eval_ensemble_f1 0.91173
wandb:            eval/avg_f1 0.79905
wandb:      eval/avg_mil_loss 0.46795
wandb:       eval/ensemble_f1 0.79905
wandb:           train/avg_f1 0.83002
wandb:      train/ensemble_f1 0.83002
wandb:         train/mil_loss 0.42284
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run morning-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/t2c236v2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070244-t2c236v2/logs
wandb: ERROR Run t2c236v2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: bh81u9kc with config:
wandb: 	actor_learning_rate: 0.0006344534279305844
wandb: 	attention_dropout_p: 0.204126182632062
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 66
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.23201103886208263
wandb: 	temperature: 4.042254870601484
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070449-bh81u9kc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bh81u9kc
wandb: uploading wandb-summary.json
wandb: uploading history steps 56-66, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄████
wandb: best/eval_avg_mil_loss █▇▅▃▁▁
wandb:  best/eval_ensemble_f1 ▁▄████
wandb:            eval/avg_f1 ▂▄███▅█▃▅█▄█▇▄▁█▁▄▄▄▃███▂█▅▃▂▄█▄▅▂▁▅▂▄▅▅
wandb:      eval/avg_mil_loss ▅▅▄▂▁▃▅▂▃▂▂▄▃▁▃▃▄█▂▂▃▁▁▃▅▂▃▃▆▃▂▅▆▂▂▅▃▃▂▂
wandb:       eval/ensemble_f1 ▂▄██▄▃▂█▃█▅█▇▄▄▄▄█▁▄▄▂▃█▂██▅▃█▄█▃▂▅▃▂▅▅▃
wandb:           train/avg_f1 ▃▅▅▃▃▂▅▂█▆▄▄▁▃▄▄▄▅▇▄▅▄▃▂▆▃▄▃▃▇▅▄▂▃▆▆▅▆▃▆
wandb:      train/ensemble_f1 ▃▅▅▃▃▂▅█▆▅▅▄▄▇▄▅▁▄▃▄▇▄▂▄▂▄▁▂▃▃▇▅▄▅▄▃▆▃▅▃
wandb:         train/mil_loss ▄▄▃▃▇▃▅▃▆▅▅▇█▅▄▆▄▄▅▂▃▃▆▁▁▃▅▇▅▄▄▄▄▂▃█▂▄▅▃
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▁▄▄█▄█▄▁▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████▁████████████████████████████▂█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.35691
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.62207
wandb:      eval/avg_mil_loss 0.95252
wandb:       eval/ensemble_f1 0.62207
wandb:           train/avg_f1 0.76864
wandb:      train/ensemble_f1 0.76864
wandb:         train/mil_loss 0.87587
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run solar-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bh81u9kc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070449-bh81u9kc/logs
wandb: ERROR Run bh81u9kc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: qi95z0dc with config:
wandb: 	actor_learning_rate: 0.00031066600427007515
wandb: 	attention_dropout_p: 0.2404099530046111
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 92
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4209732365111992
wandb: 	temperature: 2.973312994999351
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070551-qi95z0dc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qi95z0dc
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂██
wandb: best/eval_avg_mil_loss ▃█▇▁▁
wandb:  best/eval_ensemble_f1 ▁▁▂██
wandb:            eval/avg_f1 ▆▆▅▆█▇▇▇▅▅▄█▇▇▅▃▆▇▇█▇▇▅▆▇▅▇▇█▇▅▇▇▇▆▆▇▇▁▅
wandb:      eval/avg_mil_loss ▅▃▄▆▂▄▄▆▂▃▅▂▃▅▂▃█▃▁▁▂▄▂▃▂▆▂▂▁▅▃▄▂▂▄▃▂▄▅▄
wandb:       eval/ensemble_f1 ▆▃▆▆█▂▆▃▄▃▅▆▅█▄▆▃▅▇▆▅██▇▆▆▄▆▆██▁▅▇▆▆▆▆▃▄
wandb:           train/avg_f1 ▆▄▃▄▅▅▆▂▅▇▅█▅▄▅▃▁▅▄▁▂▄▂▃▆▄▆▄▇▄▄▃▆▇▄▄▂▄▆▃
wandb:      train/ensemble_f1 ▁▅▄▅▅▅▃▃▇▃▆▄▅▅▃█▅▅▅▄▆▆▅▃▃▃▅▄▃▅▄▅▄▆▇▆▅▅▅▅
wandb:         train/mil_loss ▅▂▂▂▃▅▃█▄▁▅▃▅▃▂▄▇▄▄▃▆▂▂▂▃▃▃▂▂▃▇▂▂▄▅▃▂▂▁▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▁▅▃▅▅▅▅▅▅▅▅▃▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄▄▄▄▃▄▄▁▄▂▄▃▄▄█▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.26286
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.72463
wandb:      eval/avg_mil_loss 1.00118
wandb:       eval/ensemble_f1 0.72463
wandb:           train/avg_f1 0.77647
wandb:      train/ensemble_f1 0.77647
wandb:         train/mil_loss 0.60473
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run silvery-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qi95z0dc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070551-qi95z0dc/logs
wandb: ERROR Run qi95z0dc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: u5mghsv4 with config:
wandb: 	actor_learning_rate: 7.522776964127412e-06
wandb: 	attention_dropout_p: 0.05511842106296155
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 60
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2058095108226491
wandb: 	temperature: 0.9088674062567892
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070739-u5mghsv4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/u5mghsv4
wandb: uploading history steps 55-60, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇█
wandb: best/eval_avg_mil_loss █▆▂▁▁
wandb:  best/eval_ensemble_f1 ▁▅▆▇█
wandb:            eval/avg_f1 ▂▆▆▇█▅▂▂▆▆▁█▆▅▆▅▁▅▃▆█▄▇▂▄█▅▇▆▃▅▁▇▂▁▅▃▅▃█
wandb:      eval/avg_mil_loss ▄▃▁▁▃▄▅▂▂▁▁▃▄▅▂▅▇▃▂▄▇▄▄▂▆▁▃▂▄▂▆█▅▂▅▅▆▂▆▂
wandb:       eval/ensemble_f1 ▆▆▇█▄▆█▄▄▇█▄█▆▆▇▆▆▄▄▃█▄▆▇▆█▇▇▆▆▁▄▄▅▄▇▆▅▅
wandb:           train/avg_f1 ▄▁▆▁▆▂▆▆▆▄▃▅▅▆▄▄▅▅▄▆▃▅▆▅▃▄▅▇▅▆▄█▄▂▆▆█▂▅▂
wandb:      train/ensemble_f1 ▄▁▆▁▆▄▂▆▄▆▃▅▇▅▄▄▇▄▅▅▇▃▅▆▄▅▃▅▇▄█▅▄▂█▆▆█▂▄
wandb:         train/mil_loss ▄▄▆▆▁▃▄▄█▃▂▄▃▄▂▆▃▂▁▅▃▁▃▂▄▅▄▃▄▅▂▅▃▄▅▄▇▂▄▄
wandb:      train/policy_loss ▁▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▂▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▃▃▃▃▃▃▃▃▃▃▃▃▃█▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▂▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.22453
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.8964
wandb:      eval/avg_mil_loss 0.51907
wandb:       eval/ensemble_f1 0.8964
wandb:           train/avg_f1 0.80953
wandb:      train/ensemble_f1 0.80953
wandb:         train/mil_loss 0.84773
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run frosty-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/u5mghsv4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070739-u5mghsv4/logs
wandb: ERROR Run u5mghsv4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5kr9lyzj with config:
wandb: 	actor_learning_rate: 2.2915075215841347e-06
wandb: 	attention_dropout_p: 0.2383566172703224
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 129
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.22962712277086297
wandb: 	temperature: 1.0762304214464211
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_070852-5kr9lyzj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5kr9lyzj
wandb: uploading history steps 123-129, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁███
wandb: best/eval_avg_mil_loss █▁▁▁
wandb:  best/eval_ensemble_f1 ▁███
wandb:            eval/avg_f1 ▆▆▆▃█▅▇▆▆▆▆▄▄▅█▆▆▄█▅▄▅▄▆▅▅▆▆▇▅▆▁▄▆█▆▅▃▆▆
wandb:      eval/avg_mil_loss ▆▃▃▄▂▄▄▁▅█▅▄▄▄▄▄▆▃▅▃▃▁▄▅▆▃▅▃▅▄▃▃▃▁▅▅▃▅▂▄
wandb:       eval/ensemble_f1 ▆█▆█▂▅▅▆▃▆█▅█▇▄▅▆▇▇▂█▄▇▂▅▅▅▆▇▆▃▄▆▅▆▄▅▂▁▅
wandb:           train/avg_f1 ▃▇▆▆▂▇▄▃▅▃▅▆▆▃▆▆▅▅▆▃█▄▆▃▄▆▄▆▁▃▁▇▆▆▄▇▆▆▅▂
wandb:      train/ensemble_f1 ▂█▅▃▅▇▅▃▂▂▅▅▅▆▇▇▇▅▆▄▆▆▅▄▂▇▇▅▃▄▁▃▇▃▆▆▄▄█▂
wandb:         train/mil_loss ▆▂▇▃▅▆▅▄▄█▄▇▃█▄▆▅▇▅▄▆▄▅▇▆▆▅▁█▄▄▇▇▅▆▄▃█▄▂
wandb:      train/policy_loss ▅▅▅▄▂▅▅▅▁▂▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▃█▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90236
wandb: best/eval_avg_mil_loss 0.24542
wandb:  best/eval_ensemble_f1 0.90236
wandb:            eval/avg_f1 0.77295
wandb:      eval/avg_mil_loss 0.98178
wandb:       eval/ensemble_f1 0.77295
wandb:           train/avg_f1 0.73565
wandb:      train/ensemble_f1 0.73565
wandb:         train/mil_loss 0.67432
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run earnest-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5kr9lyzj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_070852-5kr9lyzj/logs
wandb: ERROR Run 5kr9lyzj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: i6cqnzpj with config:
wandb: 	actor_learning_rate: 1.5764276575381284e-06
wandb: 	attention_dropout_p: 0.3621218311252414
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 61
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.97418447425363
wandb: 	temperature: 6.7157697355634705
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071122-i6cqnzpj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i6cqnzpj
wandb: uploading history steps 51-62, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▅▆█
wandb: best/eval_avg_mil_loss ▁▁█▂▁
wandb:  best/eval_ensemble_f1 ▁▃▅▆█
wandb:            eval/avg_f1 ██▇▇▅▆▆█▇▅▆▅▅▆▄▄▅▅█▇▆███▇█▁▇█▅█▆▆███▅█▇█
wandb:      eval/avg_mil_loss ▁▁▁▂▂▃▁▂▅▄▄▄▃▃▂▆▅▁▃▃▁▁▃▅▄▃█▄▁▃▁▃▃▁▁▂▃▃▁▁
wandb:       eval/ensemble_f1 ▇▅▇▄▆▇▅▂▆▅█▄▃█▅▅▅█▇▇█▇▅▅▅█▇▁▇█▄▆█▅█▆█▇██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▁▅▁▁██▆▃▄▄▅▅▄▃▅▅▄▅▆▂▄▄▂▆▄▃▃▄▄▅▇▇▄▇▅▇▃▂▅
wandb:      train/ensemble_f1 ▄▁▆▆▁▂▁▅▃█▄▇▄▅▅▅▂▃▅▅▆▃▄▆▇▇▆▃▄▆▅▇█▄▇▃▁▂▇▅
wandb:         train/mil_loss ▃▄▂▅▂▇▄▅▄▃▃█▄▅▂▄▆▅▄▅▅▅▇▅▃▄▂▂█▁▅▇▆▄▆▂▆▂▃▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▄▆▆▆▇▆▆▆▆▆▆▆▆▁▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▇▆▆▆▆▆▆▆▆▆▁▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91796
wandb: best/eval_avg_mil_loss 0.28997
wandb:  best/eval_ensemble_f1 0.91796
wandb:            eval/avg_f1 0.8924
wandb:      eval/avg_mil_loss 0.28286
wandb:       eval/ensemble_f1 0.8924
wandb:            test/avg_f1 0.89926
wandb:      test/avg_mil_loss 0.24534
wandb:       test/ensemble_f1 0.89926
wandb:           train/avg_f1 0.8005
wandb:      train/ensemble_f1 0.8005
wandb:         train/mil_loss 0.99552
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run bumbling-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i6cqnzpj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071122-i6cqnzpj/logs
wandb: Agent Starting Run: k2j3ygar with config:
wandb: 	actor_learning_rate: 0.0002262959128035346
wandb: 	attention_dropout_p: 0.04711542416611392
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 105
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9855794899099266
wandb: 	temperature: 8.110339479086504
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071223-k2j3ygar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k2j3ygar
wandb: uploading wandb-summary.json
wandb: uploading history steps 90-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▆▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 █▃▅▂█▁▇▆▁▆▁▃▆▁▁█▃▂▇▂█▅▂▃█▁▂▂▂▂▃▃▂▂█▃██▆▅
wandb:      eval/avg_mil_loss ▅▄▄▄▇██▁▁█▄▁▆▆▃▄▇▄▅▂▃▄▄▃▄▃▄▄▄▄█▅▆▆▁▇▇▁▄▂
wandb:       eval/ensemble_f1 ▃▂▂▅▁█▂▇▇▁▁▇▇▅▆▃▃▂▂▂▂▃▁▁▄█▂▁▃▃▁█▂▁▃█▃▃▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▃▄▄▁▅█▇▇▅▅▂▅▆▄▅▅▅▆▆▅▅▆▅▄▄▃▄▅▇▅▆▄▅▅█▅▆▄
wandb:      train/ensemble_f1 ▂▆▃▃▄▃▃▅▂█▆▆▄▅▄▄▅▆▅▁▆█▅▂▄▃▅▄▅▃▄▃▄▅▆▄▄▂▂▄
wandb:         train/mil_loss ▇▄▃▄▄▅▄▅▆▃▄▅▃▆▆█▆▃▂▄▆▅▅▇▄▃▃▄▆▇▃▅▁▃▂▄▆▃▁▄
wandb:      train/policy_loss ▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁█▂▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▃▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.26018
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.65581
wandb:      eval/avg_mil_loss 0.54336
wandb:       eval/ensemble_f1 0.65581
wandb:            test/avg_f1 0.35601
wandb:      test/avg_mil_loss 3.36408
wandb:       test/ensemble_f1 0.35601
wandb:           train/avg_f1 0.60233
wandb:      train/ensemble_f1 0.60233
wandb:         train/mil_loss 2.19036
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run divine-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/k2j3ygar
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071223-k2j3ygar/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: uhzdbayn with config:
wandb: 	actor_learning_rate: 6.577350141823249e-05
wandb: 	attention_dropout_p: 0.15530466862121284
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 183
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6482432289004448
wandb: 	temperature: 5.212188647411544
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071407-uhzdbayn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uhzdbayn
wandb: uploading wandb-summary.json
wandb: uploading history steps 162-174, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▇███
wandb: best/eval_avg_mil_loss █▅▇▆▆▄▁
wandb:  best/eval_ensemble_f1 ▁▂▃▇███
wandb:            eval/avg_f1 ▂▁▁▂▁▇▁▁▇▃█▃████▂▂▇▇█▂▁▂██▅▃▁▂▂█▇█▂█▃▂█▃
wandb:      eval/avg_mil_loss ▄▃▆▃▇▃▄▅▃▄▅▅▄▁▄▄▄▁▆█▅▆▅▄▄▆▅▄▂▆▄▄▄▁▆▃▂▄▅▄
wandb:       eval/ensemble_f1 ▄▁▂▅█▁▇▅▃███▃▂▁▅█▃█▂▁▇▂▂██▁▃▂▂▇▃▁▂▂▃█▂▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▆▅▂▅▂▄▆█▄▃▃▂▇▆▂▆█▆▅▄▃▅▆▅▃▂▆▂▄▄▃▄▆▄▁▄▆▄▆
wandb:      train/ensemble_f1 ▅▃▃▅▅▄█▄▃▃▂▇▆█▄▆▃▂▃▃▁▄▅▅▅▂▃▇▇▆▃▄▆▅▅▄▇▆▆▅
wandb:         train/mil_loss ▃▆▆█▆▇▆▇▆▇▆▄▅▁▁▆█▇▅▆▇▇▅▆▄▅▆█▆▃▅▇▆▃▆▄▃▄▃▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▁▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▁█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.26679
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.45671
wandb:      eval/avg_mil_loss 3.1219
wandb:       eval/ensemble_f1 0.45671
wandb:            test/avg_f1 0.53146
wandb:      test/avg_mil_loss 2.36894
wandb:       test/ensemble_f1 0.53146
wandb:           train/avg_f1 0.66606
wandb:      train/ensemble_f1 0.66606
wandb:         train/mil_loss 1.46854
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run easy-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uhzdbayn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071407-uhzdbayn/logs
wandb: Agent Starting Run: fo04m2kc with config:
wandb: 	actor_learning_rate: 2.4775241308708745e-05
wandb: 	attention_dropout_p: 0.07524304262002635
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 80
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6720414570524121
wandb: 	temperature: 0.6747334656287851
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071637-fo04m2kc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fo04m2kc
wandb: uploading history steps 73-81, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▇▇▇▇███
wandb: best/eval_avg_mil_loss ▅▇▃█▁▂▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▂▇▇▇▇███
wandb:            eval/avg_f1 ▄▄▄▃▄██▄▅▄▄▄▆▇█▄▄▁█▇▇███▅███▄▄▄▄█▄██▇▄▅▄
wandb:      eval/avg_mil_loss ▃▇▅▃▄▃▃▃▅▅▆▁▂▃▃▇█▃▃▃▄▃█▃▃▅▆▆▃▃▃▄▄▄▃▁▃▆▄▄
wandb:       eval/ensemble_f1 ▄▄▄▃▅█▆▆▄▄▆▇█▅▇█▄▁▇█▅▄██▅▅▄█▄█▅▄███▄▅▅▄▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▂▅▆▆▁▇▅▇▃▆▄█▅█▇▅▆▇▄▇█▃▇▅▇▆▄▇▆▅▄▅▃▅█▄▄▂
wandb:      train/ensemble_f1 ▇▂▄▆▄▆▁▅▇▃▅▃▅▆▅█▅██▄▁▇▅▆▆█▅▇▄▄▅▇▄▇▄▄▅▃▆▃
wandb:         train/mil_loss ▃▆▄█▆▅▇▄▅▆▃▆▇▄▄▇▆▄▆▆▄▆▆▅▄▅▄█▆▃▁▅▅▅▆▇▆▅▇▃
wandb:      train/policy_loss ████████████▆███████▆██▁████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▅▆▆▆▁▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91813
wandb: best/eval_avg_mil_loss 0.32133
wandb:  best/eval_ensemble_f1 0.91813
wandb:            eval/avg_f1 0.63149
wandb:      eval/avg_mil_loss 0.93254
wandb:       eval/ensemble_f1 0.63149
wandb:            test/avg_f1 0.59751
wandb:      test/avg_mil_loss 1.29009
wandb:       test/ensemble_f1 0.59751
wandb:           train/avg_f1 0.79804
wandb:      train/ensemble_f1 0.79804
wandb:         train/mil_loss 0.52545
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run magic-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fo04m2kc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071637-fo04m2kc/logs
wandb: Agent Starting Run: ogzo83q4 with config:
wandb: 	actor_learning_rate: 1.5602295019828245e-06
wandb: 	attention_dropout_p: 0.35596160155935896
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 138
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.296698971463483
wandb: 	temperature: 2.2215704243812153
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071749-ogzo83q4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ogzo83q4
wandb: uploading wandb-summary.json
wandb: uploading history steps 109-116, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▄██▇███▄████▅▅▁▂██▅▃█▁▂█▂▅▆▂████▂██▅████
wandb:      eval/avg_mil_loss ▁▂▁▁▁▃▃▅▁▅▄▁▁▁▅▃▁▅▅▁▄▂▁▄▂▅█▁▁▂▁▁▂▅▂▁▂▁▃▁
wandb:       eval/ensemble_f1 █▄▅█▁▁▇█▄█▅█▇██▂███████▅▄█▇▇█▁▄▄██▄█▇▁██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▆▅▅▆▅▆▃▆▅▅▅▇▆▂▅▁▃▅▄▅▆▄▇▄▆█▆▅▆▆▄▆▄▅▅▅▅▆▇
wandb:      train/ensemble_f1 ▅▅▆▇▄█▄▇▅▅▆▅▇█▁▇▇▄▆▅▅▇▅▆▇▇▇▇▇██▅▇▇▆▆▇▂▇█
wandb:         train/mil_loss ▆█▇▃▃▃▁▃▂▃▃▅▃▅▄▇▁▂▃▄▃▅▃▂▂▄▃▃▄▄▅▂▄▂▂▆▁▅▃▆
wandb:      train/policy_loss █▄▄▄▄▄▁▄▁▄▁▄█▄▁▁█▄█▁▁█▄▄▄▄▄█▄█▁▄▄▄▄▁▄█▄▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▅█▅▅▅▁█▅▅▅▅▅▅▅▅██▅▁▅█▁▅▅▅▅▅▅▅▁▅▅▅▅▅▅█▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.2418
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.90646
wandb:      eval/avg_mil_loss 0.35533
wandb:       eval/ensemble_f1 0.90646
wandb:            test/avg_f1 0.93426
wandb:      test/avg_mil_loss 0.11714
wandb:       test/ensemble_f1 0.93426
wandb:           train/avg_f1 0.88535
wandb:      train/ensemble_f1 0.88535
wandb:         train/mil_loss 0.42961
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run denim-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ogzo83q4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071749-ogzo83q4/logs
wandb: Agent Starting Run: 97sad1pa with config:
wandb: 	actor_learning_rate: 8.532227200855418e-05
wandb: 	attention_dropout_p: 0.32416197408419145
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 85
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.0572249662180665
wandb: 	temperature: 6.848685349830113
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_071933-97sad1pa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/97sad1pa
wandb: uploading wandb-summary.json
wandb: uploading history steps 71-86, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁████
wandb: best/eval_avg_mil_loss █▁▂▂▁
wandb:  best/eval_ensemble_f1 ▁████
wandb:            eval/avg_f1 █▄▆█▄██▁▁█▄▄███▁▄▅▄▄▄▅█▃▄█▄▄▄▄██▂▇█▄▄█▅▇
wandb:      eval/avg_mil_loss ▃▂▁▂▃▁▂▁▆█▂▂▆▄▂▅▄▃▂▄▂▂▁▂▄▂▂▃▅▂▅▂▁▅▁▁▂▃▁▁
wandb:       eval/ensemble_f1 ▂█▄█▆▅█▄█▁▄█▁█▄▄▂▁▄▃██▄█▄▄▄▄▁▄▆█▂▇█▄▄█▅▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▁▇▆▅▃▇▇▇▇█▄▄▁▄▄▂▆▆▇▂▅▃▄▆▄▅▆▇▅▅▆▄▅▆▅█▅▅▄
wandb:      train/ensemble_f1 ▂▃▆▇▅▁▅▄▇▅█▆▄▄▃▃▄▅▅▆▇▅▂▆▄▃▅▄▅▆▅▆▂▄▅▅▅▅▇▆
wandb:         train/mil_loss ▂▅▅▅▁▇▄▁▇▅▅▄▄▅▄▆▂▃▄▄▆▄▃▅▃█▆▄▃▄▆▇▆▂▇▂▄▄▆▇
wandb:      train/policy_loss ██████████████████████▂█▁█████████▄█████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████▁███████████▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92573
wandb: best/eval_avg_mil_loss 0.34849
wandb:  best/eval_ensemble_f1 0.92573
wandb:            eval/avg_f1 0.88259
wandb:      eval/avg_mil_loss 0.32397
wandb:       eval/ensemble_f1 0.88259
wandb:            test/avg_f1 0.41015
wandb:      test/avg_mil_loss 2.44692
wandb:       test/ensemble_f1 0.41015
wandb:           train/avg_f1 0.77393
wandb:      train/ensemble_f1 0.77393
wandb:         train/mil_loss 1.33514
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run glad-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/97sad1pa
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_071933-97sad1pa/logs
wandb: Agent Starting Run: gw7wo163 with config:
wandb: 	actor_learning_rate: 0.000211464539641324
wandb: 	attention_dropout_p: 0.2574749847100511
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 135
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5674524881148417
wandb: 	temperature: 6.137452965822183
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072051-gw7wo163
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gw7wo163
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 91-104, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▂█▂▃███▁▂▃▅▄█▁▃▂▂▃▁▁▂▂▇▂█▅▂▂▂▂▇▂▃▂▂█▂▂▂▂
wandb:      eval/avg_mil_loss ▄▇▅▆▂▂▅▃▄▆▇▅▁█▄▅▆▅▄▄▇█▅▆▄▅▆▁█▄▂▄▆▅█▅█▇▆▅
wandb:       eval/ensemble_f1 ▂█▃▂▁████▂▂▁██▂▆▂▃▁▂▄█▂▇█▂▅▁▂▁▂▁▃▃▁▁█▂▂▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▃▅▄▆▅▄▆▄▂▄▄▆▂▂▆▃▇▅▄▅▅▄▆▆█▆▅▄▄▄▃▆▆▃▅▁▆▅▄
wandb:      train/ensemble_f1 ▂▅▅▅▇▅▃▃▂█▂▃▁▄▅▁▄▇▄▅▂▆▄▃▄▄▅▆▆▅▆▅▄▄▆▃▃▅▇▇
wandb:         train/mil_loss ▂▄▄▃▃▅▅▂▆▅▇▅▃▂▄▁▅▂▂▆▂▅▅▄▇▆▆█▁▂▅▆▅▂▃█▁▆▃▅
wandb:      train/policy_loss ████████████████████████████████████▁███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▁▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.26346
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.36577
wandb:      eval/avg_mil_loss 2.49869
wandb:       eval/ensemble_f1 0.36577
wandb:            test/avg_f1 0.90521
wandb:      test/avg_mil_loss 0.57348
wandb:       test/ensemble_f1 0.90521
wandb:           train/avg_f1 0.5408
wandb:      train/ensemble_f1 0.5408
wandb:         train/mil_loss 2.0821
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dainty-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gw7wo163
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072051-gw7wo163/logs
wandb: Agent Starting Run: 1rxai4yi with config:
wandb: 	actor_learning_rate: 1.6443651450617705e-05
wandb: 	attention_dropout_p: 0.3169601050371865
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 83
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8678427988609606
wandb: 	temperature: 3.574858875515984
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072223-1rxai4yi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1rxai4yi
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 71-84, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss █▂▁
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 ▄▁▄▂▂▂▃▄▂▃▃▃▃▂▃█▃█▃▁▇▂█▁█▃▄█▇█▄▄▄████▁▆▃
wandb:      eval/avg_mil_loss ▃▄█▅▄▆▅▅▅▁▄▃▅▇▅▅█▆▂▃▅▄▆▃▇▆▁▃█▄▃▆▅▁▁▃▅▁▇▂
wandb:       eval/ensemble_f1 ▇▂▁▁▁▃██▃▂██▃▃▂▂▂▄█▂▃▂▇▇▂▁█▁▂▃▇▇▄▄█▇█▄▆▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▂▃▄▄▆▂▅▄▅▆▂▁█▄▇▃▆▃▇▅▄▅▄▆▃▁▃▅▃▃▃▄▅▃▄▄▂▆▂
wandb:      train/ensemble_f1 ▂▄▂▃▄▆▅▅▃▆▁█▄▆▁▆▃▄▆▆▅▄▇▅▆▁▃▄▃▅▆▄▂▄▇▄▂▃▇▅
wandb:         train/mil_loss ▅▇▂▅▄▇▃▆▁▆█▄▅▄▃▇▅▆▅▁▅▄█▅▃▅▅▇▅▆▅▃▃▇▁▃▆▅▁▄
wandb:      train/policy_loss ▆██▁███████████████████▂████▁███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██▁█████████████████████▂███████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.27188
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.65381
wandb:      eval/avg_mil_loss 0.74989
wandb:       eval/ensemble_f1 0.65381
wandb:            test/avg_f1 0.34336
wandb:      test/avg_mil_loss 2.69737
wandb:       test/ensemble_f1 0.34336
wandb:           train/avg_f1 0.53694
wandb:      train/ensemble_f1 0.53694
wandb:         train/mil_loss 1.77461
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run valiant-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1rxai4yi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072223-1rxai4yi/logs
wandb: Agent Starting Run: n0sit1yj with config:
wandb: 	actor_learning_rate: 8.600832816545013e-06
wandb: 	attention_dropout_p: 0.19468771472574825
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 130
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.1267083723483724
wandb: 	temperature: 8.172058414928815
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072403-n0sit1yj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n0sit1yj
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▄▃▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ▇██▅▇█▅█▅▆▂█▅█▅▄▅▆███▃█▇▇▆▇▇▂▃▅▆▁████▁▅█
wandb:      eval/avg_mil_loss ▅▅▂▁▆▅▃▂▁▄▄█▄▅▃▄▄▂▅▄▄▁▁▃▁▁▄▂▆▃█▇▅▇▁▁▅▅▃▃
wandb:       eval/ensemble_f1 ▆█▇▃▅▇██▅▆██▅▁▅█▅▅▄▇▅██▆▆▅▇▇▇▅▃▅▇██▅▆▅▁▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▇▃▃▃▇▂▅▄▆▅█▆▅▂▆▇▇▇▃▅▆▃▄▁▄█▆▇█▅▅▆▆▇▃▅██▃
wandb:      train/ensemble_f1 ▁▄█▃▂▄▄▇▄▇▃▇▄▇▄▇▆▆▅▃▆█▇▃▄▅▁▅▅▆▇▃▂▇▄▃▅██▃
wandb:         train/mil_loss ▄▄▃▆▅▃▃▆▄▄▄█▃▃▇▄▅▅▄▅▂▅▃▃▅▄▁▂▃▂▃▃▃▃▄▁▅▆▃▅
wandb:      train/policy_loss ▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▁▅▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.23556
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.89327
wandb:      eval/avg_mil_loss 0.75288
wandb:       eval/ensemble_f1 0.89327
wandb:            test/avg_f1 0.50182
wandb:      test/avg_mil_loss 2.04011
wandb:       test/ensemble_f1 0.50182
wandb:           train/avg_f1 0.75165
wandb:      train/ensemble_f1 0.75165
wandb:         train/mil_loss 1.02261
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run astral-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n0sit1yj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072403-n0sit1yj/logs
wandb: Agent Starting Run: yjvl8tup with config:
wandb: 	actor_learning_rate: 1.3435393295793707e-06
wandb: 	attention_dropout_p: 0.3922582527151732
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7776717738223946
wandb: 	temperature: 5.816361095433233
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072546-yjvl8tup
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yjvl8tup
wandb: uploading history steps 118-128, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁███
wandb: best/eval_avg_mil_loss █▁▁▁
wandb:  best/eval_ensemble_f1 ▁███
wandb:            eval/avg_f1 ██▅██▂▆█▅▄█▇▃▅▁▇▇▅▅▆▅███▇▇█▆▄▇▅▅██▄█▇███
wandb:      eval/avg_mil_loss █▁▂▁▁▁▁▄▁▁▁▁▄▇▁▁▁▁▄▁▃▁▁▆▂▁▁▄▁▂▁▁▁▁▆▁▁▁▅▆
wandb:       eval/ensemble_f1 ▃█▆██▃██▃▃█▇▇██▁█▇▇█▃▇▇▂███▃█▇▇████▇█▄█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▅▇▇▆▄▆▃▇▄▄█▇▆▅▇▅▅▆█▂▆█▇▁▆█▇▇▆▆▆▆▇▆▆▃▇▂
wandb:      train/ensemble_f1 ▆▃▇▆▆▇▃▃▆▅▄▆▆▂▁▃▆▆▅▆▅▆▇▆▇▅▅▅▆▆▄▄▅▄▅▄▅█▅▃
wandb:         train/mil_loss ▅▄▂▆▂▅▃▂▅█▅▂▂▄▄▄▂▄▂▄▄▂▃▄▄▁▇▂▄▂▄▅▂▃▂▄▃▂▃▄
wandb:      train/policy_loss ████████████████████▁███████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████▁██████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.29076
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.91845
wandb:      eval/avg_mil_loss 0.29654
wandb:       eval/ensemble_f1 0.91845
wandb:            test/avg_f1 0.93076
wandb:      test/avg_mil_loss 0.13666
wandb:       test/ensemble_f1 0.93076
wandb:           train/avg_f1 0.84526
wandb:      train/ensemble_f1 0.84526
wandb:         train/mil_loss 0.43754
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run solar-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yjvl8tup
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072546-yjvl8tup/logs
wandb: Agent Starting Run: qhw2q1bu with config:
wandb: 	actor_learning_rate: 2.2253708535904764e-06
wandb: 	attention_dropout_p: 0.4375437877819952
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 136
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8184584009944175
wandb: 	temperature: 7.297857627783877
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072745-qhw2q1bu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qhw2q1bu
wandb: uploading history steps 100-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▇▅████▄█▄▄█▅████▁██▇█▅▄██▄█▇▅▇█▁▇█▃████
wandb:      eval/avg_mil_loss ▁▃▂▄▄▁▁▁▂▁▄▂▁▃█▁▃▁█▁▂▄▁▁▁▁▃▁▆▃▃▃▂▄▆▃▁▇▁▁
wandb:       eval/ensemble_f1 █▄▅█▆███▄▅▇██▅▁█▅█▅▄▇▃██▄▅▆▆▆▆▅█▇▅▇▆████
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▂▇▅▇▆▅▄▅▅▅▆▆▇▆█▅▅▆▅▅▂▆▂▃▆▅▄▆▅▅▅▅▁▇▃▆▆▅
wandb:      train/ensemble_f1 ▄▃▃▄▃▇▃▁▅▅▇▆▅▃▅▁▅▅▄▇▆█▇▅▆▄▅▅▄▅▃▂▅▅▄▅▃▅▁▃
wandb:         train/mil_loss ▁▁▂▂▂▅▅▅▅▄▂▃▂▃▂▂▂▂▆▄▅▄▂▂▃▁▂█▃▂▃▁▃▄▂▂▅▄▁▄
wandb:      train/policy_loss ███████████████████████▁████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▁▅▅█▁▅▁▁▅█▁▅▁▁▁▁▅▅▅▅▁▅█▁▅▅▅▁██▅▅█▅▅▅▅█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.28647
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.92558
wandb:      eval/avg_mil_loss 0.30755
wandb:       eval/ensemble_f1 0.92558
wandb:            test/avg_f1 0.89399
wandb:      test/avg_mil_loss 0.18293
wandb:       test/ensemble_f1 0.89399
wandb:           train/avg_f1 0.85007
wandb:      train/ensemble_f1 0.85007
wandb:         train/mil_loss 0.50187
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run upbeat-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qhw2q1bu
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072745-qhw2q1bu/logs
wandb: Agent Starting Run: j6a7qb47 with config:
wandb: 	actor_learning_rate: 1.4726792602994571e-06
wandb: 	attention_dropout_p: 0.373778899355803
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 118
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7786217304438653
wandb: 	temperature: 7.404587257364543
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_072924-j6a7qb47
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/j6a7qb47
wandb: uploading history steps 118-119, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss █▁▂
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 █▅█▅▇▂▇▄▄▅▂▅▇▃▇▂▂█▆▁▅▅▅▅▅▆▆▃▅▅█▆██▅██▂▅▅
wandb:      eval/avg_mil_loss ▃▄▃▃▅▁▄▄▄▄▄▃█▃▇▄▄▂██▄▄▃▂▁▅▃▂▄▄▄▃▁▄▁▃▁▄▄▃
wandb:       eval/ensemble_f1 █▅▅▇▅▂▂▅▅▅▇▅▁▅▅▄▁▅▅▅▅█▅█▂▂▂▅▃██▅█▃▂█▂▅▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▇▄▅▂▅▁▂▃▅▄▇▅█▆▆▇▁▅█▄▅▄▅▁▄▆▅▃▄▆▆▅▄▁▅▆▆▄
wandb:      train/ensemble_f1 ▃▇▁▅▅▄▃▆▂▅▅▇▅▆▃▂▇█▄▄▆▆▇▂▃▇▅▆▇▄▆▄█▂▅▅▇█▃▆
wandb:         train/mil_loss ▅▄▅▇▃▇█▄▆▃▄▄▅▆▃▂▃▂▃▆▃▄▅▁▁▄▃▂▃▃▂▁▂▂▃▄▂▁▃▁
wandb:      train/policy_loss █████████▁██████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▄▆██████▁█████████████████████████▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.31979
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.68436
wandb:      eval/avg_mil_loss 0.67734
wandb:       eval/ensemble_f1 0.68436
wandb:            test/avg_f1 0.70764
wandb:      test/avg_mil_loss 0.8261
wandb:       test/ensemble_f1 0.70764
wandb:           train/avg_f1 0.69704
wandb:      train/ensemble_f1 0.69704
wandb:         train/mil_loss 0.55024
wandb:      train/policy_loss -0.13404
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.13404
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fresh-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/j6a7qb47
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_072924-j6a7qb47/logs
wandb: Agent Starting Run: smf6ej9j with config:
wandb: 	actor_learning_rate: 2.535038403019846e-06
wandb: 	attention_dropout_p: 0.39304168226700614
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 138
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9133098760114062
wandb: 	temperature: 9.422765711193712
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073118-smf6ej9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/smf6ej9j
wandb: uploading output.log; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂▆▇██
wandb: best/eval_avg_mil_loss ▆█▃▃▅▁▁
wandb:  best/eval_ensemble_f1 ▁▁▂▆▇██
wandb:            eval/avg_f1 ▄▅▅▆▂▆▅▁▇▆▅▆▆▇██▃▇▃█▆▅▅▆▇██▅▄▇▅▃▆▇▅▆▅▇▃█
wandb:      eval/avg_mil_loss ▆▆▅▃▄▄▄█▃▄▅▃▅▅▄▆█▆▃▃▁▁▁▅▄▄▁▆▄▅▅▄▃▂▃▄▁▂▄▂
wandb:       eval/ensemble_f1 ▆█▆▅▆▅▆▄▂▅██▆▅▂▁█▆▆▆▅███▆▆▆██▄▅▃▆▆▃▂▃▆▆▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄█▂▅▇▂▁▁▅▄▃▆▇▂▅▃▃▇▄▄▆▄▄█▄▆▄▄▅▇▆▃▅▅▆▇▇▄▅▃
wandb:      train/ensemble_f1 ▄▄▄▅▅▆▇▅▃▄▄▃▄▁▇▄▄▃▅▇▁▄▆▄▇▅█▂▇▅▅▅▅▄▅▅▆▅▆▄
wandb:         train/mil_loss ▅▅█▆▃▅▄▃▂▁▄▅▅▃▄▅▂▄▄▂▄▅▃▆▃▇▅▂▅▂▂▄▃▄▃▅█▃▂▅
wandb:      train/policy_loss █████████████████▅████████████████▁█████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▁████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90646
wandb: best/eval_avg_mil_loss 0.30819
wandb:  best/eval_ensemble_f1 0.90646
wandb:            eval/avg_f1 0.878
wandb:      eval/avg_mil_loss 0.77477
wandb:       eval/ensemble_f1 0.878
wandb:            test/avg_f1 0.44941
wandb:      test/avg_mil_loss 1.25854
wandb:       test/ensemble_f1 0.44941
wandb:           train/avg_f1 0.67203
wandb:      train/ensemble_f1 0.67203
wandb:         train/mil_loss 1.22158
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run efficient-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/smf6ej9j
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073118-smf6ej9j/logs
wandb: Agent Starting Run: 2o0qo6hl with config:
wandb: 	actor_learning_rate: 1.973205299751745e-06
wandb: 	attention_dropout_p: 0.3939713103718733
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 135
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.842208373892611
wandb: 	temperature: 6.248123183006085
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073327-2o0qo6hl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/lycs9zpn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2o0qo6hl
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▄█
wandb: best/eval_avg_mil_loss █▁▃▁▃
wandb:  best/eval_ensemble_f1 ▁▃▄▄█
wandb:            eval/avg_f1 ▇▅▁▆▆▂▄▆▆▇▆▅█▄▃▅▅▃▃▅▄▃▆▅▅▄▄▅▆▆▅▇▄▅▆▆▆▆▅▄
wandb:      eval/avg_mil_loss ▅▄▃▅▃▄▄▅▂▃▅▅█▂▅▇▃▃▆▄█▅▃▅▅▆▆▄▆▃▇▄▅▁▇▃▅▅▃▇
wandb:       eval/ensemble_f1 ▆▅▂▆▄▂▄▄▆▅▇▇▃▃█▄▄▃▇▃▅▅▆▅▄▂▂▂▆▃▅▇▅▆▄▄▅▆▄▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▆▃▁▅▂▆▅▁▄▆▂▅▄▁▃▅█▆▄▇▇▆▇▆▄▇▅▆▆▆▆▅▃▆▅▅▇▇
wandb:      train/ensemble_f1 ▄▂▅▆▆▁▄▅▅▆▆▅▆▃▅▃█▁▅▅▆▆▄▆▆▃▄▅▆▆▅▅▆▅▅▆▅▆▇▆
wandb:         train/mil_loss ▇█▇▄▇▅▅▃▅▅▄▇▄▆▄▆▃▄█▄█▇▅▄▇▅▁▂▇▄▅▃▅▂▅▂▄▄▂▄
wandb:      train/policy_loss ███████████████▅███▂████▁███████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▃█████████▇██████▆█▁█▆█████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.88591
wandb: best/eval_avg_mil_loss 0.92127
wandb:  best/eval_ensemble_f1 0.88591
wandb:            eval/avg_f1 0.57301
wandb:      eval/avg_mil_loss 2.24786
wandb:       eval/ensemble_f1 0.57301
wandb:            test/avg_f1 0.67246
wandb:      test/avg_mil_loss 1.3512
wandb:       test/ensemble_f1 0.67246
wandb:           train/avg_f1 0.71659
wandb:      train/ensemble_f1 0.71659
wandb:         train/mil_loss 1.21405
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run classic-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2o0qo6hl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073327-2o0qo6hl/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 0ynzdfd2 with config:
wandb: 	actor_learning_rate: 0.0006557550527354232
wandb: 	attention_dropout_p: 0.2295487274958678
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 98
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8948874478650376
wandb: 	temperature: 9.250950200610715
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073642-0ynzdfd2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0ynzdfd2
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading history steps 82-99, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇▇▇▇▇▇█
wandb: best/eval_avg_mil_loss █▁▁▁▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▆▇▇▇▇▇▇█
wandb:            eval/avg_f1 ▁▇▇▇▇▇▇▆▇▆▆███▁▆▇▇▇▇▇█▇█▇█▇▇▇▇▇▇▆▇▇▆█▇█▇
wandb:      eval/avg_mil_loss █▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▆▁▁▁▁▁▁▁▁█▃▁▁▁▁
wandb:       eval/ensemble_f1 ▇▇█▇▇▆▇▇▆█▆█▇█▇▁██▇▇████▇▇▂██▇█▇▇▁▇█▇▄█▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▁▂▆▇▄▃▄▆▆▅▅▅▆▆▇▇▅▄▅▆█▅▃▆▆▇▆▆▆▆▃▅▃█▅▅▆▂▄
wandb:      train/ensemble_f1 ▃▇▅▂▅▂▅▅▃▄▃▂▆▄▅▇▁▄█▆▅▅▆▆▇▆▄▂▅▆▆▆▆▃▅▅▃█▅▅
wandb:         train/mil_loss ▂▃▁▃▃▃▃▁▁▁▂▂▁▂▁▄▃▃▂▁▂▁▅▃▃▁▁▃▂▁▁▂▁▁▄█▂▃▆▅
wandb:      train/policy_loss ▁████▄▄▄▄▁▄▄▄▄▄▁▄███▁██▄▄▄█▄▄▄▁█▁█▄█▁▁▁▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄█▄▄▄▁▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93715
wandb: best/eval_avg_mil_loss 0.23616
wandb:  best/eval_ensemble_f1 0.93715
wandb:            eval/avg_f1 0.90438
wandb:      eval/avg_mil_loss 0.20299
wandb:       eval/ensemble_f1 0.90438
wandb:            test/avg_f1 0.91963
wandb:      test/avg_mil_loss 0.18576
wandb:       test/ensemble_f1 0.91963
wandb:           train/avg_f1 0.89942
wandb:      train/ensemble_f1 0.89942
wandb:         train/mil_loss 0.52825
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run expert-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0ynzdfd2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073642-0ynzdfd2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: s8s25vl1 with config:
wandb: 	actor_learning_rate: 0.0004399644090579088
wandb: 	attention_dropout_p: 0.3644597458662244
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 75
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.988522059208011
wandb: 	temperature: 7.432094391873986
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073825-s8s25vl1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s8s25vl1
wandb: uploading history steps 74-76, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▆▆▇█
wandb: best/eval_avg_mil_loss █▄▃▂▂▃▁
wandb:  best/eval_ensemble_f1 ▁▂▅▆▆▇█
wandb:            eval/avg_f1 ▅▅▄▆▅▄▄▄▅▄▄▄▆▁▅▁▁▆▆▆▆▄▅▄▇▅▃▆▄▇▇▆▆██▃█▇▅▇
wandb:      eval/avg_mil_loss ▂▁▂▂▂▂▁▂▁▂▂▂▂▁▁▂▁▁▂▂▁▃▁▂▂▁▁▂▁▁▁▁▁▁▂▂▁▁█▂
wandb:       eval/ensemble_f1 ▆▆▆▇█▆▁▆▆▆▆▆▇▄▆▄▇▇▇▇▇▆▆▇▆▇▇▆███▆▆▇▆███▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▅▄▄▄▅▇▅█▆▄▄▅▆▆▅██▇▅▄▅▇▆▆▇▅▅▇▇▅██▇▇▇▇▅▇▄
wandb:      train/ensemble_f1 ▆▁▄▄▄▇▅▃▄▆▆▅█▆▅▆▅▇▇▅▅▆▆▆▆▇▃▄▇▅▅██▅▇▇▇▅▇▆
wandb:         train/mil_loss ▃▃▇▂▃▃▄▆█▄▂▄▆▃▂▂▄▃▄▃▃▂▇▄▅▂▂▄▂▇▄▁█▁▄▂▂▂▃█
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▁▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▅▆▆▆█▆▁▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93693
wandb: best/eval_avg_mil_loss 0.21467
wandb:  best/eval_ensemble_f1 0.93693
wandb:            eval/avg_f1 0.90773
wandb:      eval/avg_mil_loss 0.2635
wandb:       eval/ensemble_f1 0.90773
wandb:            test/avg_f1 0.91373
wandb:      test/avg_mil_loss 0.20554
wandb:       test/ensemble_f1 0.91373
wandb:           train/avg_f1 0.89855
wandb:      train/ensemble_f1 0.89855
wandb:         train/mil_loss 0.24332
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run likely-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s8s25vl1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073825-s8s25vl1/logs
wandb: Agent Starting Run: ai0covd1 with config:
wandb: 	actor_learning_rate: 3.827246143155911e-06
wandb: 	attention_dropout_p: 0.03177852752302085
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 90
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2688001079046559
wandb: 	temperature: 5.074735626393951
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_073932-ai0covd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ai0covd1
wandb: uploading wandb-summary.json
wandb: uploading history steps 82-90, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▅▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▅▃▅█▂▄▄▆█▃█▅▃▃▅▄▂▁▃▅▁▆▄█▆█▄▄▅▆▆▆▁█▂▄▇▇▅▃
wandb:      eval/avg_mil_loss ▁▃▆▃▄▇▄▃▂▁▆▁▆▁▆▃▄▄▅▅▄▂▄█▅▄▃▁▅▅▅▄▁▆▄▂█▄▁▆
wandb:       eval/ensemble_f1 ▇▅▅▃▅▂▆▂▂▂█▃▂▅▅▃▅▁▇█▅▁▄▄▆██▄▃▆▃█▆▅▄▄▃▇█▇
wandb:           train/avg_f1 ▅▆▄▄▄▅█▄▅█▅▆▁▃▄▆▂▂▅▅▂▆▅▆▅▃▄▄▄▅▅▄▃▅▅▄▄▃▆▄
wandb:      train/ensemble_f1 ▅▆▃▂▆▄▅▄█▅▃▅▆▁▃▃▆▂▅▅▂▆▆▇▃▄▅▅▄▃▃▅▄▇▆▄▅▆▄▇
wandb:         train/mil_loss ▆▆▅█▂▄▅▆▅▃▆▄▃▂▁▃▇█▇▅▆▂▅▃▆▆▂█▄▆▄▂▅▂▄▃▄▇▄▄
wandb:      train/policy_loss █████████████▁██████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91796
wandb: best/eval_avg_mil_loss 0.24082
wandb:  best/eval_ensemble_f1 0.91796
wandb:            eval/avg_f1 0.69205
wandb:      eval/avg_mil_loss 2.83476
wandb:       eval/ensemble_f1 0.69205
wandb:           train/avg_f1 0.7803
wandb:      train/ensemble_f1 0.7803
wandb:         train/mil_loss 1.56823
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smart-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ai0covd1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_073932-ai0covd1/logs
wandb: ERROR Run ai0covd1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: frnnpwju with config:
wandb: 	actor_learning_rate: 0.0006675821315533491
wandb: 	attention_dropout_p: 0.3984092114255729
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6525650067395523
wandb: 	temperature: 8.486968904123222
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074046-frnnpwju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/frnnpwju
wandb: uploading history steps 101-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆██
wandb: best/eval_avg_mil_loss █▃▁▁
wandb:  best/eval_ensemble_f1 ▁▆██
wandb:            eval/avg_f1 ▃▄▁▃▅▄▇▃▃█▅▄▅▅▇▁▄▃▃▄▂▆▅▅▆▅█▄█▅▄▇▆▇▂▇▆▆▅▆
wandb:      eval/avg_mil_loss ▅▃▆▅█▁▂▂▄▄▅▄▁▅▃▃▂▄▇▅▃▆▃▅▂▂▂▇▁▄▄▆▃▃▂▁▄▅▂▄
wandb:       eval/ensemble_f1 ▆▆▂▅▁▇█▆▇▂▃▄▂▆▁▄▆▆▅▃▂▃▇▅▆▆▂▇█▅▆▂▄▅▇▄▅▆▃▆
wandb:           train/avg_f1 ▅▆▆▆▇▃▂▆▁▂▃▃▅▃▆▆▃▇█▅▃▇█▂▄▅▃▄██▆▆▆▅▄▇▅▃▅▆
wandb:      train/ensemble_f1 ▅▅▅▅▅▄▆▆▁▆▆▅▅▄▃▃▃▆▄▆█▃▂▆▆▁▄▅▆█▄▃▇▅▅▃▅▅▇▄
wandb:         train/mil_loss ▁▇▇▅▄▇▃▆▃▅▂▄▃▄▆▆▂▄▄▇▃█▅▃▅▄▄▅▆▅▅▅▂▃▅▃▆▇▃▆
wandb:      train/policy_loss ▄██▄█▁███▄▁█▄██▁▁██▄▁▁▁▄▁██▁▄▄▄▄▁██▁▁▄▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████▁███████████████████████▄█████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.21805
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.85682
wandb:      eval/avg_mil_loss 1.41483
wandb:       eval/ensemble_f1 0.85682
wandb:           train/avg_f1 0.81251
wandb:      train/ensemble_f1 0.81251
wandb:         train/mil_loss 1.37585
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run woven-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/frnnpwju
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074046-frnnpwju/logs
wandb: ERROR Run frnnpwju errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: gqzlp813 with config:
wandb: 	actor_learning_rate: 0.0002331540600936535
wandb: 	attention_dropout_p: 0.31306827199536413
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8627850731472052
wandb: 	temperature: 6.684713651025216
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074249-gqzlp813
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gqzlp813
wandb: uploading history steps 99-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆██
wandb: best/eval_avg_mil_loss █▁▁▆
wandb:  best/eval_ensemble_f1 ▁▆██
wandb:            eval/avg_f1 ███▇▁▆▇██▆████▇█▇███▇██▃▇▄▇██▄▆███▇█████
wandb:      eval/avg_mil_loss ▁▁█▁▁▁▁▁▅▁▇▁▁▆▁▁▁▂▃▁▁▁▁▁▃▃▁▁▁▁▃▃▅▂▂▁▁▃▂▁
wandb:       eval/ensemble_f1 ▇▂█▂█▇▇▇█▁▂▁▆▇██▇█▇█▇██▇█▇████▅█▇█▄█████
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▅▂▃▃▄▄▂▄▆▁▄▂▆▃▃▆▇▆▃▃▃▆█▇▄▇▆▅▄▄▃▅▇▃▁▃▄▄
wandb:      train/ensemble_f1 ▅▄▄▄▄▅▅▅▂▅▄▆▄▅▄▃▅▆▇▅▄▄▄▆▇▇█▄▁▆▃▅▅▇▅▅▄▂▃▄
wandb:         train/mil_loss ▅▂▃▇▆▂▂▁▄▄▄▂▅▁▅▂▁▃▄▃▄▂▁▅█▁▂▆▆▄▄▄▃▃▄▅▃▅▂▆
wandb:      train/policy_loss ▅▅▅▁▅▅▅▅▅▅▅▅▅█▅▁▅▅█▅█▁▅▅▅▅█▅▅▅▅▅█▅▅▅▅█▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅█▅▅▅▅▁█▅▅▅▅▅█▅▅▅▅▁█▅▅▅▅▅▅▅█▅▅▁▅▅▁▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.30571
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.91432
wandb:      eval/avg_mil_loss 0.24828
wandb:       eval/ensemble_f1 0.91432
wandb:            test/avg_f1 0.92222
wandb:      test/avg_mil_loss 0.15213
wandb:       test/ensemble_f1 0.92222
wandb:           train/avg_f1 0.87807
wandb:      train/ensemble_f1 0.87807
wandb:         train/mil_loss 0.24746
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run valiant-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gqzlp813
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074249-gqzlp813/logs
wandb: Agent Starting Run: cj3zivxg with config:
wandb: 	actor_learning_rate: 0.0008238625066208179
wandb: 	attention_dropout_p: 0.3128694436792375
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8798320774755176
wandb: 	temperature: 7.574754859429643
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074417-cj3zivxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cj3zivxg
wandb: uploading wandb-summary.json
wandb: uploading history steps 80-95, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆█
wandb: best/eval_avg_mil_loss █▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆█
wandb:            eval/avg_f1 █▅█▅▇██▇▆▆▇▁▆▇█▆██▇▄▇█▆▁▅▆▇▆██▇█▇▄██▆█▇█
wandb:      eval/avg_mil_loss ▁▄▁▁▂▆▄▃▅▄▃▂▂▁▅▂▁▁▂▁▃▁▁▁▆▁▅▆▁▁▁▁▇▂▆▃█▁▁▁
wandb:       eval/ensemble_f1 ▅█▇▅▇▄██▇▆█▆▆▇▁███▇█▅█▆██▁█▅█▁█▄▅▇█▆████
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▅▃▆▇▅▇▃█▃▅▇▄▅▅▄▆▄▃▇▁█▇▃▇▂▅▃▅▄▆▆▂▇▆▆▇▇▇
wandb:      train/ensemble_f1 ▇▆▅▃█▇▇▄▃█▅▄▆▄▆▅▇▃▆█▄▇▁▇▆▆▇▇▂▂▅▅█▃▅▇▄▇▇▃
wandb:         train/mil_loss ▂▆▃▅▅▃▄▂▄▇▄▂█▃▃▆▁▃▁▅▂▂▂▇▁▁▅▂█▇▃▂▆▅▂▁▆▂▇▁
wandb:      train/policy_loss ▅▅▅▅▁▅▅█▅▅▅▅▅▅▅▁▁▅▅▅▅▅█▅▅▁▅▅▅▅▅▅▁▅▅▅▅▅▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▁▁▅▁█▅▅▁▅▁▅▅▁▅█▅▅██▅▅▅▅▅▅▅▅▅█▅▅▅█▅▅▅▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9409
wandb: best/eval_avg_mil_loss 0.25293
wandb:  best/eval_ensemble_f1 0.9409
wandb:            eval/avg_f1 0.92558
wandb:      eval/avg_mil_loss 0.25558
wandb:       eval/ensemble_f1 0.92558
wandb:            test/avg_f1 0.93845
wandb:      test/avg_mil_loss 0.10618
wandb:       test/ensemble_f1 0.93845
wandb:           train/avg_f1 0.82339
wandb:      train/ensemble_f1 0.82339
wandb:         train/mil_loss 0.24584
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run worldly-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/cj3zivxg
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074417-cj3zivxg/logs
wandb: Agent Starting Run: f1s02g51 with config:
wandb: 	actor_learning_rate: 0.0003968188117598157
wandb: 	attention_dropout_p: 0.2903559704447366
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 71
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9871261516648232
wandb: 	temperature: 9.94446563376274
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074535-f1s02g51
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f1s02g51
wandb: uploading history steps 60-71, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▇█▇████▇████▇▁▂▇▁▅████▁█▇█▇▂▇▅▇█▅██▇█▁█
wandb:      eval/avg_mil_loss ▁█▁▁▃▁▂▁▅▁▁▁▄▁▁▁▂▁▆▇▄█▁▃▁▁▆▂▁▁▄▁▁▁▁▁▅▃▁▁
wandb:       eval/ensemble_f1 ▇▄█████████▁█▂█▁█▆██▅▇██▇▇█▅▇▇█▇▁▇███▁▅█
wandb:           train/avg_f1 ▃▇▇▆▆▄█▅▆▁▄▂▅▇▂▇▅▅▇▆▂▆▅▃▆▆▇▆▂▃▃▄▇▆▂▄▇▃▇▁
wandb:      train/ensemble_f1 ██▇▇▇█▆▇▄▄▆▇▆█▅▆▆▇▆▆▇▇▇▇▁▇▆▅▆▅▇▄▄▆█▆▇▅▄▇
wandb:         train/mil_loss ▃▁▅▆█▂▁▁▂▄▄▂▂▂▂▂▃▁▅▃▂▃▅▅▂▁▄▁▂▃▄▂▂▆▄▅▂▁▂▆
wandb:      train/policy_loss ▅█▅▅▁█▅▅▁▅▅▅█▅▅█▁▅▅▅▁▁█▅▅▅▅▅▅▅▅█▅▅▅▅▁▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁█▁▅▅▅▁█▅▅▅▅▅██▅▅▅▁▅▅▁▅█▅▅▅▅▅▅▅▅▅▅▅█▁▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.24883
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.92193
wandb:      eval/avg_mil_loss 0.25806
wandb:       eval/ensemble_f1 0.92193
wandb:           train/avg_f1 0.87626
wandb:      train/ensemble_f1 0.87626
wandb:         train/mil_loss 0.93712
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vague-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f1s02g51
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074535-f1s02g51/logs
wandb: ERROR Run f1s02g51 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: flftdx40 with config:
wandb: 	actor_learning_rate: 0.0001256457839141494
wandb: 	attention_dropout_p: 0.38428299102460944
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 71
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8259453844931133
wandb: 	temperature: 6.050349664348598
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074637-flftdx40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/flftdx40
wandb: uploading history steps 55-72, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss █▂▅▁
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 █▅▂▆█▁█▇▅▅█▆▂▄█▄▇▄▁█▅█▅▃▅▆█▂███▅▇███▄█▅▅
wandb:      eval/avg_mil_loss ▁▄█▂▃▁▁▅▁▄▃▃▃▁▅▁▃▂▄▂▂▄▃▄▄▄▂▁▁▁▁▃▁▅▅▁▁▃▁▁
wandb:       eval/ensemble_f1 ▂▅▁▇▇▇▅▇█▆█▂█▄▄█▄▇▄▃▇▆█▂█▅████▅▃▇███▄█▅▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▄▂██▆▄▇▇▆▇▅█▆▇▇▆▄▆▇▁▃▆▆▄█▇▄▇▇▆▆▅▅▆▇▆▆▆
wandb:      train/ensemble_f1 ▅▅▄▆▆▇▅▃▅▇▄▅▆▇▅▅▄▅▆▁▅▅▅▄▇▇▆▄▆▅▄▄▃▂▇█▅▆▄▆
wandb:         train/mil_loss ▆▃▂▃▄█▂▂▅█▂▄▄▄▇▃▂▁▇▄▄▂▄▂▂▂▄▂▃▅▃▅▂▇▁▃▅▇▃▃
wandb:      train/policy_loss ▁▁█▅▅▅█▅▅▅▁▅█▁███▅▁▁█▅▅▁▁█▁▁▅▅▅▁▅█▅█▅█▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.23636
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.80283
wandb:      eval/avg_mil_loss 1.83441
wandb:       eval/ensemble_f1 0.80283
wandb:            test/avg_f1 0.90493
wandb:      test/avg_mil_loss 0.25178
wandb:       test/ensemble_f1 0.90493
wandb:           train/avg_f1 0.86711
wandb:      train/ensemble_f1 0.86711
wandb:         train/mil_loss 0.95585
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run still-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/flftdx40
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074637-flftdx40/logs
wandb: Agent Starting Run: gqic6by4 with config:
wandb: 	actor_learning_rate: 0.0003061393104552007
wandb: 	attention_dropout_p: 0.3280724540695707
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 80
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9233409236279146
wandb: 	temperature: 6.190185424234952
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074744-gqic6by4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gqic6by4
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅▇█
wandb: best/eval_avg_mil_loss █▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▄▅▇█
wandb:            eval/avg_f1 ▇█████▇█▇▅▇█████▁██▅██▄██▇██▃█▇█▇███▇▇██
wandb:      eval/avg_mil_loss ▃▁▁▁▁▂▃▁▁▁▁▁▂█▁▁▁▁▁▃▁▄▄▁▄▁▄▁▁▁▁▁▅▇▁▁▁▁▁▁
wandb:       eval/ensemble_f1 ███████▇▇▁▇▇████████▇██▇█████▃▇█▇█▇▇▇█▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▆▄▄▆▇▂▃▇▃█▄▂▃▁▃▆▄▄▄▃█▇▇▆█▅▇▁▃▇▆▃▅▄▁▅▅▃▄
wandb:      train/ensemble_f1 ▅▅▇▄▅▇▂▄▇▁▄▂▅▆▇▅▆▅▄▃▄▂█▇█▇█▅▇▄▆▇▅▄▆▆▄▅▅▇
wandb:         train/mil_loss ▄▃▁▃▄▂▄▄▃▁▃▂▂▃▁▅▃▅▄▃▁▅▄▁▄█▁▄▄▂▃▃▃▃▂▆▅▅▄▄
wandb:      train/policy_loss ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.29012
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.91087
wandb:      eval/avg_mil_loss 0.26746
wandb:       eval/ensemble_f1 0.91087
wandb:            test/avg_f1 0.92358
wandb:      test/avg_mil_loss 0.12774
wandb:       test/ensemble_f1 0.92358
wandb:           train/avg_f1 0.8792
wandb:      train/ensemble_f1 0.8792
wandb:         train/mil_loss 0.9053
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run easy-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/gqic6by4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074744-gqic6by4/logs
wandb: Agent Starting Run: sv6u4ug7 with config:
wandb: 	actor_learning_rate: 0.0006482598298324099
wandb: 	attention_dropout_p: 0.33593239365036903
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7093643647533849
wandb: 	temperature: 9.51648079074369
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_074851-sv6u4ug7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sv6u4ug7
wandb: uploading wandb-summary.json
wandb: uploading history steps 120-127, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇███
wandb: best/eval_avg_mil_loss █▁▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇███
wandb:            eval/avg_f1 █▇▄▇▄▆▇▇▇██▃▃██▇▄█▇▁█▄▃▇█▆▄▇▇▁▇▇▇▇▅████▇
wandb:      eval/avg_mil_loss ▆▄▂▃▇▂▁▁▁▁▁▄▁▄▃▁▂▃▁▁▁▁▃▁▁▁▆▁▃▄▁▁▂▃▁▂▂█▁▁
wandb:       eval/ensemble_f1 ▄█▄▆▆▂▇▇▇██▇▁█▁▇▆██▇▄▇▃▇▇▄▇█▄▆▇▅▄▇██▇▇█▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄▆▇▂▄▃▇▆▆▂▆▆▃▇▃▇▅▆▇▅▄▇▆▁▂▇▆▄▅▅▅█▆▅▃▅▅▃▆
wandb:      train/ensemble_f1 ▆▅▆▇▇█▄█▅▅▇▆▇▅█▇██▆▄▁▅▆▆▆▅▇▃▆▇▇▆▆▆█▇▇▇▆▃
wandb:         train/mil_loss ▅▃▂▄▃▂▃▃▄▁█▂▄▃▅▅▂▄▄▆▄▂▂▁▅▂▃▄▂▂▄▅▃▆▁▃▂▄▂▆
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▁▁▄▁▄█▄▄▄▄▄▄▄▄▄▄▄▁▄▄█▄█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄█▄▁▄▄▄▄█▄▄▄▄▄▄▄▄█▁▄█▄▄▄█▄▄▄▄▄▄▄▁█▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.27233
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.7755
wandb:      eval/avg_mil_loss 2.59744
wandb:       eval/ensemble_f1 0.7755
wandb:            test/avg_f1 0.88427
wandb:      test/avg_mil_loss 0.30919
wandb:       test/ensemble_f1 0.88427
wandb:           train/avg_f1 0.79881
wandb:      train/ensemble_f1 0.79881
wandb:         train/mil_loss 1.3277
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run effortless-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sv6u4ug7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_074851-sv6u4ug7/logs
wandb: Agent Starting Run: zolp3m82 with config:
wandb: 	actor_learning_rate: 0.0004449968001349981
wandb: 	attention_dropout_p: 0.16789350818725596
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 108
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8639520008897708
wandb: 	temperature: 8.493118642232542
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075035-zolp3m82
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zolp3m82
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 100-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅█
wandb: best/eval_avg_mil_loss █▁▅▁
wandb:  best/eval_ensemble_f1 ▁▄▅█
wandb:            eval/avg_f1 █▅▇▅██▅███▃█▇█▄█▃▇▁██▇▃█▁█▁▇█▂▇▇█▅█▇██▇█
wandb:      eval/avg_mil_loss ▁▁▂▂▁▁▁▁▂▁▆▁▅▁▄▆▅▂▁▄▁▁▇▄▁▁▅▇▁▁▁█▁▂▃▁▅▁▅▇
wandb:       eval/ensemble_f1 ██▇▆█████▅▆█▃▄█▅▃█▃▇▇▇██▇▁██▇█▇▃▇█▆▅█▁██
wandb:           train/avg_f1 ▆▃▅▅▃▃▆▃▅▄▅▅▅▄█▂▃▁▃▄▄▅▅▄▂▇▅▇▂▃▅▂▅▆▄▇▅▂▆▆
wandb:      train/ensemble_f1 █▄▇▄▆▄▆▂▆▅▇▃▂▁▂▆▅▄▄▃▅▃▅▃▇▇▄▇▅▄▇▅▇▆▄▅▇▄▆▆
wandb:         train/mil_loss ▃▄▂▇▃▇▁▃▅▅▆▃▄▃▅▁▃▆▆▇▃▅▂▅▃█▁▃▄█▄▅▁▇▃▃▃▄▅▅
wandb:      train/policy_loss ██████████████▁█████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▁▅▅▁████▅▅▁▅▅█▅▁▅█▅▁▅▁▅▅▁▅▅▅█▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.26194
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.8926
wandb:      eval/avg_mil_loss 2.55356
wandb:       eval/ensemble_f1 0.8926
wandb:           train/avg_f1 0.87281
wandb:      train/ensemble_f1 0.87281
wandb:         train/mil_loss 0.97919
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dashing-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zolp3m82
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075035-zolp3m82/logs
wandb: ERROR Run zolp3m82 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 9apx8r7b with config:
wandb: 	actor_learning_rate: 0.00037595759107709586
wandb: 	attention_dropout_p: 0.3309684687471713
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 138
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8725364478352147
wandb: 	temperature: 5.915421225691441
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075202-9apx8r7b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9apx8r7b
wandb: uploading wandb-summary.json
wandb: uploading history steps 98-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▂▂▃▅█▇▄▁▄▇█▂▄▂▃▆▂▆▅▇▄▂▇▁▇▇▅▅▃▁▁▃▆▁▄▃▁▃▂▃
wandb:      eval/avg_mil_loss ▄▆▅▄▆▄▅▇▇▃▇▅▄█▅▅▇▂▆▆▅▅█▆▇▆▅▃▇▇▇▁▅▂▅▃▆▆▇▅
wandb:       eval/ensemble_f1 ▇▄▃█▁▂▅█▇▇▂▅▇▄▂▃▆▆▅▅█▃▇▁▇▃▁▇▇▅▃▁█▃▆▄▂▃▁▅
wandb:           train/avg_f1 ██▃▅▅▇▄▅▃█▁▇▇▇▄▂▇▇▃▄▇▇▆▄▄█▇▃▁▅▅▅▅▅▂▂▆█▂▄
wandb:      train/ensemble_f1 ▄▇▃▄▇▆██▇▅█▇▃▇▃▄▅▇▁▅▇▄▅▅▆▇▅▅▄▅█▆▅▃▄▃▇█▄▅
wandb:         train/mil_loss ▅▅▃▄▄▇▆▄▅▂▅▅▄▆█▅▅▆▄▃▄▅▃▄▅▆▄▅▄▁▅▆▄▃▂▆▅▄▇▅
wandb:      train/policy_loss █████████▁█████████████▆█████████████▇██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████████████▁████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9183
wandb: best/eval_avg_mil_loss 0.27845
wandb:  best/eval_ensemble_f1 0.9183
wandb:            eval/avg_f1 0.87537
wandb:      eval/avg_mil_loss 1.37948
wandb:       eval/ensemble_f1 0.87537
wandb:           train/avg_f1 0.70998
wandb:      train/ensemble_f1 0.70998
wandb:         train/mil_loss 2.23642
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devout-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9apx8r7b
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075202-9apx8r7b/logs
wandb: ERROR Run 9apx8r7b errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: py6q7x2u with config:
wandb: 	actor_learning_rate: 0.0003197228059153668
wandb: 	attention_dropout_p: 0.22894051422816564
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 80
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9687727648478192
wandb: 	temperature: 8.35814962868229
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075330-py6q7x2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/py6q7x2u
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇██
wandb: best/eval_avg_mil_loss █▂▁▁▂
wandb:  best/eval_ensemble_f1 ▁▇▇██
wandb:            eval/avg_f1 ▇▃▇▂▂█▁▇█▇█▃██▃█▅████▂▁█▃▄█▂▂█▃▂█▃▂████▃
wandb:      eval/avg_mil_loss ▁▄▃▅▅▆▇▁▁▅▄▁▇▄▃▁▂▃▆▄▄▁▇▅▄█▃▁▅▄▄▃▁▃▄▇▁▁▄▆
wandb:       eval/ensemble_f1 ▂▂▂█▇▂▁▁▁▃▇▂██▃▃▂█▅█▄█▇██▁▃▂▄▃▂▂██▃█▂███
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅▁▇▆▇▆▆█▅▆▅▅▆▆▇▄▅▆▇▆▇▆▄▄▆▄▄▆▃▇▄▄▅▆▆▃▆▅▃
wandb:      train/ensemble_f1 █▅▁▇▆▄▇▆▅▆▅▅▅▆▅▅▅▅▅▅▇▄▅▆▆▄▆▄▅▇▄▄▄▇▅▆▆▆▆▃
wandb:         train/mil_loss ▆▄▆▇▃▄▃▁▅▇▂▃█▆▅▅▆▂▅▄▆▅▂▃▄▆▇▅▇▅▄▆▄▇▅▁▃▇▂▆
wandb:      train/policy_loss ████████████████▁███████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁██▁██████▁████████▁█████████▁▁███████▁█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9145
wandb: best/eval_avg_mil_loss 0.25343
wandb:  best/eval_ensemble_f1 0.9145
wandb:            eval/avg_f1 0.84992
wandb:      eval/avg_mil_loss 2.62357
wandb:       eval/ensemble_f1 0.84992
wandb:            test/avg_f1 0.92222
wandb:      test/avg_mil_loss 0.23266
wandb:       test/ensemble_f1 0.92222
wandb:           train/avg_f1 0.70325
wandb:      train/ensemble_f1 0.70325
wandb:         train/mil_loss 1.3415
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/py6q7x2u
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075330-py6q7x2u/logs
wandb: Agent Starting Run: 5jk4labh with config:
wandb: 	actor_learning_rate: 0.0005805655305946761
wandb: 	attention_dropout_p: 0.36240539603098065
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 87
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7209759604460654
wandb: 	temperature: 8.047868262437426
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075437-5jk4labh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5jk4labh
wandb: uploading history steps 80-87, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁███
wandb: best/eval_avg_mil_loss █▁▁▁
wandb:  best/eval_ensemble_f1 ▁███
wandb:            eval/avg_f1 ▁▇█▄▄█▇▁▆▇█▇█▇██▇██▃▇█▇▇█▆█▇▇▇█▁▇██▁█▇▇▇
wandb:      eval/avg_mil_loss ▄▁▁▁▆▆▁▂▁█▁▄▁▁▁▆▁▁▁▁▁▁▁▄▁▁▁▁▂▆▅▄▂▅▁▁▃▁▁▁
wandb:       eval/ensemble_f1 ██▇▅▄▇▇█▇██▇▇██▇█▇█▇███▆▇▇▇█▂▇▇▆██▆▇▁█▅▇
wandb:           train/avg_f1 ▅▄▇▇█▅▁▆▇▆▆▇▇▆▄▅▅▃▇▂▆██▇▇▆▃█▆▇▅▄▆▄▄▅█▆▆█
wandb:      train/ensemble_f1 ▃▁▇▇▇▅▄▂▃█▆▅▄▆▄▆▆▄▂▄▁▄▅▇▇▆▂▃▅▃▄▅▃▃▃▅▅▅▇▅
wandb:         train/mil_loss ▄▆▇▇▂▄▃▅▁▅▄▅▄▃▅▄▆▃▄▂█▄▅▄▁▅▃▂▆▂▁▄▁▂▂▁▁▃▃▃
wandb:      train/policy_loss ▄▁▁▄█▄▄▁▄▄▄█▄▄▁█▄▁▄▄▄▁▁█▄▁█▄▁▄▄▄▄▁▄██▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▅▅▅▅▅▁▁█▁▅▅▅▅█▅█▁▁▁▅▅▁▅█▁█▅▁██▅▅▅▅▅▁▁▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92976
wandb: best/eval_avg_mil_loss 0.26598
wandb:  best/eval_ensemble_f1 0.92976
wandb:            eval/avg_f1 0.88682
wandb:      eval/avg_mil_loss 0.3809
wandb:       eval/ensemble_f1 0.88682
wandb:           train/avg_f1 0.87717
wandb:      train/ensemble_f1 0.87717
wandb:         train/mil_loss 0.24926
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run autumn-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5jk4labh
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075437-5jk4labh/logs
wandb: ERROR Run 5jk4labh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: aciic9k8 with config:
wandb: 	actor_learning_rate: 0.0006596623567354373
wandb: 	attention_dropout_p: 0.2648293263393907
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 105
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8691705343621111
wandb: 	temperature: 8.986346436245524
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075550-aciic9k8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aciic9k8
wandb: uploading history steps 88-106
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▃▆█
wandb: best/eval_avg_mil_loss ▁▅▃▃█
wandb:  best/eval_ensemble_f1 ▁▃▃▆█
wandb:            eval/avg_f1 ▇▇▇▇█▇█▄▃▇▇▄▄█▄▄▇▇▇█▇▅▂█▅▇▆▂▅▇▃███▅▇▆▁▄█
wandb:      eval/avg_mil_loss ▁▁▁▁▁▅▇▄▁▂▁▃▃▁▁▁▆▂▄▁▁▃▅▅▁▁▁█▄▄▃▅▁▁▄▅▁▁▁▁
wandb:       eval/ensemble_f1 ▇▇▇█▄▃█▁▄▇▇▇▆▆█▅██▇▂▄█▃█▇█▇▇▄█▇██▇▇▂▇▇▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▅▅▄▅▆▇▆▅▆█▅▆▄▅▆▄▁▄▆▃▄▄▇▆▆▅▆▇▃▇▆▅▅▇▅▇▅▅▃
wandb:      train/ensemble_f1 ▃▆▅▅▆▅█▃▅▇▅█▄▆▅▁▄▅▄▄▆▃▄▅▅▆▃▄▆▃▅▇▇▅▅▇▆▄▆▄
wandb:         train/mil_loss ▁▁▄▇▆▂▅▃▄▁▅▄▅▃█▆▅▅▅█▅▄▆█▅█▄▆▅▆▇▄▄▂▄█▇▅▆▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁█▁▁▂▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93328
wandb: best/eval_avg_mil_loss 0.23579
wandb:  best/eval_ensemble_f1 0.93328
wandb:            eval/avg_f1 0.81743
wandb:      eval/avg_mil_loss 1.00143
wandb:       eval/ensemble_f1 0.81743
wandb:            test/avg_f1 0.74418
wandb:      test/avg_mil_loss 1.59502
wandb:       test/ensemble_f1 0.74418
wandb:           train/avg_f1 0.84729
wandb:      train/ensemble_f1 0.84729
wandb:         train/mil_loss 0.60955
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run amber-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/aciic9k8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075550-aciic9k8/logs
wandb: Agent Starting Run: sfcng64y with config:
wandb: 	actor_learning_rate: 7.210229387249357e-05
wandb: 	attention_dropout_p: 0.3224854863298031
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 63
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7492271385905022
wandb: 	temperature: 7.329341818754403
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075728-sfcng64y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sfcng64y
wandb: uploading wandb-summary.json
wandb: uploading history steps 59-63, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇██
wandb: best/eval_avg_mil_loss ▇█▁▁
wandb:  best/eval_ensemble_f1 ▁▇██
wandb:            eval/avg_f1 ▂▇▃▇▄▇▂▇▅▇▂█▄▄▇▇▄▂▅▇▃▅█▄█▂█▇▂▇▇█▇▅▃▁▂▅▁▃
wandb:      eval/avg_mil_loss ▁▅▄▄▂█▄▅▅▅▆▄▅▄▄▇▅▅▄█▄▁▃▁▁▄▇▄▄▄▅▇▁▅▅▂▇▇▂▄
wandb:       eval/ensemble_f1 ▂▇█▇▇█▇▃▇▇▅▇▇▂█▃▄█▇▄▅▇▂▁▃██▂█▃█▇██▇▇▅▄▁▃
wandb:           train/avg_f1 ▅▆▅▄▆▄▅▅▄▅▄▄█▅▆▄▆▃▅▅▆▄▆▁▇▆▅▆▅▇▅▆▅▄▇▄▄▆▇▇
wandb:      train/ensemble_f1 ▅▆▅▆▅▅▄▅▅▄█▅▇▆▆▆▃▄▅▅▄▁▇▄▅▅▆▅▇▅▆▅▄▄▇▇▆▅▃▇
wandb:         train/mil_loss ▄▄▅▂▁▄▆█▁▃▅▄▁▄▇▃▂▄▅▇▅▄▅▆▃▃▆▅▅█▅▅▂▆▁▆▆▄▂▄
wandb:      train/policy_loss ▅▅▁▅▅█▅█▁▅▅▅▅█▅▅▅▅▅▁▁▁▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▁▅▅▁▅▅▁▅▅▅█▅▅▁▅▁▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.38596
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.65612
wandb:      eval/avg_mil_loss 1.75971
wandb:       eval/ensemble_f1 0.65612
wandb:           train/avg_f1 0.85114
wandb:      train/ensemble_f1 0.85114
wandb:         train/mil_loss 1.50946
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run crimson-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sfcng64y
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075728-sfcng64y/logs
wandb: ERROR Run sfcng64y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hmphz1p0 with config:
wandb: 	actor_learning_rate: 0.00015466039147701843
wandb: 	attention_dropout_p: 0.36468087571317154
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 62
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.969996977509948
wandb: 	temperature: 7.3725892428329
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075831-hmphz1p0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hmphz1p0
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss ▁▆█
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 ▇▇▇█▇█▇▇▇██▇██▁█████▇██████████████▇▇▇▁▄
wandb:      eval/avg_mil_loss ▁▅▁▂▁▁█▂▆▆▁▁▁▁▁█▆▁▁▁▁▁▅█▁▁▂▁▁▅▁▁▁▁▁▁▁▂▇▂
wandb:       eval/ensemble_f1 █▇▇▇█▄█▇▇▇█▁▇▇█▇██▇█▇▇█▇██████▇▇▇█████▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▇▇▇▇▅▅▆▂▇▆▅▃▇▄▁▃█▆▅▄▃█▅█▇▇█▅▇▇▃▅▃▇▅▁▅▆▇
wandb:      train/ensemble_f1 ▇▅▇█▆▆▃▂▇▃▅▃▄▇▁█▆▅▂▄▆▄▇▃▇▇▇▇▃▅▃▇▄▃▇▁▅▆▇▃
wandb:         train/mil_loss ▁▃▁▁▄▃▄▁▅▁▄▄▁▁▆▆▃▁▄▁▁▄▃▁▁▃▄▆█▁▃▅▂▁▃▄▃▇▇▄
wandb:      train/policy_loss ████████████████████████████▁███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████▁████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.26685
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.74099
wandb:      eval/avg_mil_loss 0.60554
wandb:       eval/ensemble_f1 0.74099
wandb:            test/avg_f1 0.91594
wandb:      test/avg_mil_loss 0.12735
wandb:       test/ensemble_f1 0.91594
wandb:           train/avg_f1 0.83515
wandb:      train/ensemble_f1 0.83515
wandb:         train/mil_loss 0.99303
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run royal-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hmphz1p0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075831-hmphz1p0/logs
wandb: Agent Starting Run: 08numy12 with config:
wandb: 	actor_learning_rate: 7.68875383519145e-05
wandb: 	attention_dropout_p: 0.2731026447644113
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8650408442029501
wandb: 	temperature: 5.966559884775497
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_075928-08numy12
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/08numy12
wandb: uploading wandb-summary.json
wandb: uploading history steps 40-57, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇▇▇█
wandb: best/eval_avg_mil_loss █▁▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▆▇▇▇█
wandb:            eval/avg_f1 ▆█▆▆▃▇▇▇█▅▅██▃█▅███▆▂▁▂▁█▇█▄█▆██████▇██▇
wandb:      eval/avg_mil_loss ▃▁▁▂▂▂▂▅▄▃▁▃▄▃▁▄▄▁▁▂▁▆▁▇█▁▅▁▆▃▂▂▅▁▁▁▄▁▁▁
wandb:       eval/ensemble_f1 ▇█▆█▆▆▇▃▄████▃████▆█▁▁█▁▇█▄█▆▆█████▇███▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▄▆▆▆▃▃▂▂▇▆▁█▇▆▃▃▄▁▃▂▆█▆▄▁▆▄▅▄▅▆▂▂▃▂▅▅▅▆
wandb:      train/ensemble_f1 ▇▆▄▆▆▅▄▃▃▂▇▇▄▁█▆▃▃▄▁▂█▆▄▆▄▅▆▄▃▄▆▂▁▃▂▅▅▆▆
wandb:         train/mil_loss ▃▅▃▅▅▃▃▄▆▃▄▃▆█▅▄▅▄▄▆▆▄▄▆▅▅▁▃▆▅▄▇▆▃▇▅▅▄▂▁
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄█▄▄▄▁▄▄▄▄▄▁▄▁▄▄▄▄▄▄███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.28061
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.89004
wandb:      eval/avg_mil_loss 2.52157
wandb:       eval/ensemble_f1 0.89004
wandb:            test/avg_f1 0.68409
wandb:      test/avg_mil_loss 2.91853
wandb:       test/ensemble_f1 0.68409
wandb:           train/avg_f1 0.85431
wandb:      train/ensemble_f1 0.85431
wandb:         train/mil_loss 0.22799
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dandy-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/08numy12
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_075928-08numy12/logs
wandb: Agent Starting Run: atxrlks2 with config:
wandb: 	actor_learning_rate: 0.00030701615398042093
wandb: 	attention_dropout_p: 0.2403376653721759
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 106
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8073821950584139
wandb: 	temperature: 8.30048535188
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080020-atxrlks2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/atxrlks2
wandb: uploading history steps 90-107, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▇█
wandb: best/eval_avg_mil_loss ▄▃▁██
wandb:  best/eval_ensemble_f1 ▁▄▅▇█
wandb:            eval/avg_f1 █▇▇▇▇▇▇▇▇▆▅█▇████▇█▇▆██▇▄▇▇█▇▁▇▃▇█▇▇▅█▇▇
wandb:      eval/avg_mil_loss ▁▁▃▁▂▁▁▁▁▄▁▁▁▂▁▁▃▄▁▄▁▁▁▁▁▅▁▁▄▁█▁▁▄▄▁▁▁▄▁
wandb:       eval/ensemble_f1 ██▇▆▃▆▇▇▆█▅▇▅▇█▇▆▇▂▇▁▇▇▇▆▄▃▇█▆▇▆▇▇▆▄█▆▇▁
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅█▄▅▁▁▄▆▁▆▄▆▅▄▆▅▆▇▆▅▆▃▇▅▄▇▅▆▆▅▅▅▆▆▄▅▆▆▆
wandb:      train/ensemble_f1 ▆▅█▅▃▇▂▁▅▆▆▄▆▃▅▆▆▆▅▅▆▄▅▇▄▅▆▆▆▅▆▄▆▇▅▅▅▆▆▃
wandb:         train/mil_loss ▃▆▄█▅▄▂▇▄▆▄▅▁▃█▂▇▇▁▅▅▁▃▃▃▆▂▃▂▅▅▂▆▄▄▁▃▂▁▄
wandb:      train/policy_loss █▅█▅▁▅▅▁▅█▅▁█▅▅▁▅▅▁▅▁█▅▅█▅█▅▁▅███▁▅▅▁▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████▁██████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92634
wandb: best/eval_avg_mil_loss 0.24661
wandb:  best/eval_ensemble_f1 0.92634
wandb:            eval/avg_f1 0.91845
wandb:      eval/avg_mil_loss 0.20125
wandb:       eval/ensemble_f1 0.91845
wandb:            test/avg_f1 0.91566
wandb:      test/avg_mil_loss 0.24385
wandb:       test/ensemble_f1 0.91566
wandb:           train/avg_f1 0.88948
wandb:      train/ensemble_f1 0.88948
wandb:         train/mil_loss 0.41456
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/atxrlks2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080020-atxrlks2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: sco2vjjr with config:
wandb: 	actor_learning_rate: 0.0005197749145630058
wandb: 	attention_dropout_p: 0.17383459244863675
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8975315825624264
wandb: 	temperature: 8.179159281802104
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080203-sco2vjjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sco2vjjr
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇█
wandb: best/eval_avg_mil_loss █▄▅▁
wandb:  best/eval_ensemble_f1 ▁▂▇█
wandb:            eval/avg_f1 ▇▆█▇▁█▆▆▆▇██▅▂▅█▃█▇▄▇▆▅▇▄▇▆▂█▃▆▇▁█▇█▆▇▇█
wandb:      eval/avg_mil_loss ▄▁▄▆▂▁▁▂▁▄█▁▁▁▁▃▂▁▂▁▁▃▁▁▁▅▂▁▃▁▃▃▅▄▃▁▆▂▃▁
wandb:       eval/ensemble_f1 ▇▇▆▂▅▆▇▇▆▁██▂▆▆▅▇▇█▇▃▇█▅▆▆▇▆▂▂▇█▆▆▇▇▆▇▂█
wandb:           train/avg_f1 ▇█▅▄▅▅▅▄▄▅▆▂█▄█▂▅▂▂▇▃▂▂▅▄▃▆▆▃▄▃▄▁▆▆▇▅▄▆▅
wandb:      train/ensemble_f1 █▄▃▂▆▅▃▅▇▆▃▄█▄▇▄▆▆▇▆▃▄▁▂▂▇▄▅▅▇▄▃▃▄█▅▇▄▇▆
wandb:         train/mil_loss ▄▃▂▂▂▅▄▂▅▃▇▃▂▃▂▂▂▄▄▃▃▅▄▄▃▂▂▆▄▁▂▄▃▄█▁▃▃▅▆
wandb:      train/policy_loss ▄▄▁▄▄█▁▄▄▄▄▄▄██▁█▁▄▁█▄▁▁▄▁▄▁█▄██▁▄▄█▄██▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████▁██████████████▆████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.21232
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.91467
wandb:      eval/avg_mil_loss 0.23807
wandb:       eval/ensemble_f1 0.91467
wandb:           train/avg_f1 0.84765
wandb:      train/ensemble_f1 0.84765
wandb:         train/mil_loss 0.46785
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lively-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/sco2vjjr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080203-sco2vjjr/logs
wandb: ERROR Run sco2vjjr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: bp714fc9 with config:
wandb: 	actor_learning_rate: 1.054517491703243e-06
wandb: 	attention_dropout_p: 0.3934038775968901
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 183
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5895962956747095
wandb: 	temperature: 4.1370378649912
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080351-bp714fc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bp714fc9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆█
wandb: best/eval_avg_mil_loss ▆▂▁█
wandb:  best/eval_ensemble_f1 ▁▃▆█
wandb:            eval/avg_f1 █▅▅▇▇▅▃▂▆▃▅▅█▁▃█▅▇▆█▅▃▂▇▃▇▃▃▆▇▆▅▃█▅▅█▅▃▃
wandb:      eval/avg_mil_loss ▃▂▁▂▃▁▆█▃▃▃▆▂▂▂▃▃▂▁▃▃▁▃▁▁▄▂▄▁▁▃▁▂▁▁▂▁▁▅▁
wandb:       eval/ensemble_f1 ██▅▇▅▆█▇▆▇▂▄▆▄▆▇▅▅▅▄▄▆█▄▇▆▇▃▆██▆██▇▇██▁▄
wandb:           train/avg_f1 ▂▅▅▄▅▅▅▁▆▄▆▅▅▃▅▂▅▄▃▆▄▅▂▃▅▁▄▇▅▄▄▅▄▇▆▄█▄▅▇
wandb:      train/ensemble_f1 ▂▆▆▅▆▆▇▆▅▁▅▇▄▇▅█▅▅▄▄▆▄▆▂▄▇▅▄▆▆▄▆▄▅▄▅█▅▅▃
wandb:         train/mil_loss █▇▂▂▅▃▆▄▂▄▆▄▅▅▅▅▆▆▃▁▂▄▇█▂▅▄▂▁▃▄▃▆▁▇▂▂▅▃▄
wandb:      train/policy_loss ████▁███████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄█▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.2499
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.77313
wandb:      eval/avg_mil_loss 1.13058
wandb:       eval/ensemble_f1 0.77313
wandb:           train/avg_f1 0.8046
wandb:      train/ensemble_f1 0.8046
wandb:         train/mil_loss 0.604
wandb:      train/policy_loss 0.09062
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.09062
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rural-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/bp714fc9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080351-bp714fc9/logs
wandb: ERROR Run bp714fc9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: zvuk99rq with config:
wandb: 	actor_learning_rate: 9.716960961869669e-06
wandb: 	attention_dropout_p: 0.17590590943126294
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 181
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5801564342496827
wandb: 	temperature: 9.75887942703739
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080723-zvuk99rq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zvuk99rq
wandb: uploading history steps 106-118, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▃█
wandb: best/eval_avg_mil_loss █▆▄▁
wandb:  best/eval_ensemble_f1 ▁▁▃█
wandb:            eval/avg_f1 █▇▅▅▅██▇▆█▄██▂▂▅▇██▅█▇███▄██▅█▆█▅▅▇▁▅██▃
wandb:      eval/avg_mil_loss ▁▄▃▆▁▄▁▁▃▅▃▁▁▇▁▁▆▆▄▇▁▄▆▁▄▆▁▄▃▁▁▁▁▄▆▆█▁▁▁
wandb:       eval/ensemble_f1 ▇▅▁▅█▅█▇█▅▅▅▇▄██▇█▇█▇█▇▅█▇▅▁█▄▅▇▁████▇█▂
wandb:           train/avg_f1 ▃▇▆▅▇▅▇▅█▄▆▄▅▇▆▇▄▂▆▃▃▄▆▁▆▅▃█▆▆▇▆▂▆▃▅▆▇▆▃
wandb:      train/ensemble_f1 ▃▄▆▅▆▆▄▃▆▅▆▅▆▄▅▇▆▆▆▁▄▇▅▅▆▄▅▄█▃▄▅▃▇▃▄▇▃▆▆
wandb:         train/mil_loss ▅▄▄▄▃▂▄▅▅▄▃▆▅▅▄▇▆▅▆▄▃█▄▂▄▅▄▅▄▃▆▇▁▃▃▄▃▄▃▄
wandb:      train/policy_loss ▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▁█▁▅█▅▅█▁▁▅▅█▁▅█▅▅▅▅██▅█▁▅█▅▅▅▅▁▅▁▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.22151
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.67676
wandb:      eval/avg_mil_loss 1.44383
wandb:       eval/ensemble_f1 0.67676
wandb:           train/avg_f1 0.8629
wandb:      train/ensemble_f1 0.8629
wandb:         train/mil_loss 0.49143
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run apricot-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/zvuk99rq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080723-zvuk99rq/logs
wandb: ERROR Run zvuk99rq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: mnhj0t9s with config:
wandb: 	actor_learning_rate: 5.936592791705304e-06
wandb: 	attention_dropout_p: 0.4671643171663796
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 127
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.24894286823307463
wandb: 	temperature: 7.159764619323256
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_080912-mnhj0t9s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mnhj0t9s
wandb: uploading wandb-summary.json
wandb: uploading history steps 114-127, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▄▆▆█
wandb: best/eval_avg_mil_loss █▁▂▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▄▄▆▆█
wandb:            eval/avg_f1 ▃███▃█▇█▆▃█▁█▆█▆██▄▇█▇▅██▂▅██▃█▇▅▄▄█▄█▄▃
wandb:      eval/avg_mil_loss ▂▅▁▄▅▅▃▂▃▂▄▁▃▄▃▅▁▇▇▅▅▃▁▄▇█▃▂▁▁▃▅▄▄▁▄▆▃▄▁
wandb:       eval/ensemble_f1 ███▇█▃█▁█▃██▃▃▇▇▆█▄▆████▃▃▃▃▅▃█▄▄▇▄▆██▇█
wandb:           train/avg_f1 ▇▅▆▅▄▇▅▅▅▅▁▄▆▆▆▆█▅▅▄▆▅▅▇▅▅▅▅▆▄▆▆▇▇▄▂▃▄▇▅
wandb:      train/ensemble_f1 ▇▃▃▆█▇▃▄▆▅▅▅▇█▅▆▄█▅▆▇▆▆▄▄▆▄▅▄▆▇▇▇▆▃▂█▇▁▄
wandb:         train/mil_loss ▅▄▄▁▆▅▅▆▃▃▁▄▅▂▅▂▃▃▃▃▅▄▃▄▅▇▅▅▃▁▁▆▃▃▄▆▃▆▃█
wandb:      train/policy_loss ▁▅▅▅▅▅▁▅▅▅█▅▁▁▅▅▅▅▅▁▅█▅▅▅▅██▅▅▁▅▅▅▅▅▅▅▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████▁██████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.2513
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.78286
wandb:      eval/avg_mil_loss 1.60067
wandb:       eval/ensemble_f1 0.78286
wandb:           train/avg_f1 0.81605
wandb:      train/ensemble_f1 0.81605
wandb:         train/mil_loss 2.07481
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run zesty-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mnhj0t9s
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_080912-mnhj0t9s/logs
wandb: ERROR Run mnhj0t9s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: i503qan7 with config:
wandb: 	actor_learning_rate: 0.00019879629590319688
wandb: 	attention_dropout_p: 0.038755654773574955
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 147
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7708508807262189
wandb: 	temperature: 7.170663149433617
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081101-i503qan7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i503qan7
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂▇▇▇█
wandb: best/eval_avg_mil_loss ▅▂█▂▆▁▁
wandb:  best/eval_ensemble_f1 ▁▁▂▇▇▇█
wandb:            eval/avg_f1 ▅▆▅▄▄▁▇█▆▄█▄██▆▆▃▇█▇▆▇▆█▂▇▃▇▅▃▇▆▄▇▆▅▃▄▄▆
wandb:      eval/avg_mil_loss ▃▄▄▁▅▇▄▄▃▅▃▇▄▇▁▆▁▃▁█▃▁▇▆▁▄▄▂▂▅▁▅▄▆▆▄▃▃▁▅
wandb:       eval/ensemble_f1 ▅▅▄▆▃▃▄▃▂▇▆▂▅▃▂▆██▆▃▄▃▄█▁▇▁▄▇▆▆▅▃▅▄▅▂▂▄▇
wandb:           train/avg_f1 ▅▄▃▁▆▃▄▃▂▄▃▃▄▄▅▄▅▇▄▅▃▆▂▃▇▇▃▃▄▅█▄▃▅▃▆▄▄▂▃
wandb:      train/ensemble_f1 ▇▅▄▆▄▆▇▅▃▄▇▅▅▅▅▅▆█▅▆▄▅▆▄▄█▆▄▁▄█▅▇▅▅▇▃▄▅█
wandb:         train/mil_loss ▃▂▆▅▃▄▄▄█▄▅▅▃█▄▅▄▃▃▂▃▁▂▄▂▂▂▄▂▁▃▄▂▂▂▄▅▁▄▄
wandb:      train/policy_loss █▁▁█████████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁█████████████████████████▄████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91069
wandb: best/eval_avg_mil_loss 0.22223
wandb:  best/eval_ensemble_f1 0.91069
wandb:            eval/avg_f1 0.84259
wandb:      eval/avg_mil_loss 0.9915
wandb:       eval/ensemble_f1 0.84259
wandb:           train/avg_f1 0.84338
wandb:      train/ensemble_f1 0.84338
wandb:         train/mil_loss 1.02843
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run feasible-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/i503qan7
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081101-i503qan7/logs
wandb: ERROR Run i503qan7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: lrg4avj8 with config:
wandb: 	actor_learning_rate: 0.0007817875934294041
wandb: 	attention_dropout_p: 0.33794260021191563
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.607991116314001
wandb: 	temperature: 8.069983236431668
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081346-lrg4avj8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lrg4avj8
wandb: uploading history steps 105-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇▇██
wandb: best/eval_avg_mil_loss ▆█▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▆▇▇██
wandb:            eval/avg_f1 ▅▇▃▇█▇███▇▇▇▇█▆▆▅▇▅▅▇▇█▇▅▇█▇█▇▄█▄█▆▆▄▆▃▁
wandb:      eval/avg_mil_loss ▄█▂▁▄▁▄▁▅▁▄▇▃▅▁▂▂▁▂▃▇▄▇▁▁▁▁▁▅▃▆▃▁▁▇▆▃▂▇▂
wandb:       eval/ensemble_f1 ▆▇▅▄▇▇▅▁██▄██▇▄▅▅▇▇▅▇█▁▃▅▇▄▆▆█▆▆▅▆▃▆▆▇██
wandb:           train/avg_f1 ▁▂▅▆▁▅▆▅▆▅▆▅▄▆▅▆█▅▃▆▅▇▃▁▅▅▃▆▂▆▃▁▂▃▅▆█▄▆▃
wandb:      train/ensemble_f1 ▃▄▇▃▁▃▄▇▂▅▄▆▄▆█▃█▄▅▆▃▆▆▄▄▇▆▆▃▆▃▅▆▂▁▄▃▁▆▃
wandb:         train/mil_loss ▃▃▄▃█▃▃▄▂▂▃▄▄▄▄▂▅▄▃▅▃▂▁▆▂▄▄▃▂▆▃▃█▄▄▂▇▂▄▄
wandb:      train/policy_loss ▆▆▁▆▃▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆█▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁██████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92587
wandb: best/eval_avg_mil_loss 0.26682
wandb:  best/eval_ensemble_f1 0.92587
wandb:            eval/avg_f1 0.90758
wandb:      eval/avg_mil_loss 0.2852
wandb:       eval/ensemble_f1 0.90758
wandb:           train/avg_f1 0.8117
wandb:      train/ensemble_f1 0.8117
wandb:         train/mil_loss 0.62044
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run still-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/lrg4avj8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081346-lrg4avj8/logs
wandb: ERROR Run lrg4avj8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: um2ms642 with config:
wandb: 	actor_learning_rate: 1.7235845192372547e-06
wandb: 	attention_dropout_p: 0.05475548032213695
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 77
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2951514724188967
wandb: 	temperature: 2.9863485681971733
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081529-um2ms642
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/um2ms642
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▆█
wandb: best/eval_avg_mil_loss ▄▂█▂▃▁
wandb:  best/eval_ensemble_f1 ▁▂▂▃▆█
wandb:            eval/avg_f1 ▆▄▆▆▂▇▃▆▁▆▇▄▇▃▄▅▇▅█▆▅▁▇▇█▅▃▅▆▇▇▃▆▅▄▅▇▆▅▅
wandb:      eval/avg_mil_loss ▁▄▄▂▂▅▂█▂▁▄▁▆▁▁▃▄▁▁▁▇▃▄▂▁▆▂▅▂▂▃▁▂▅▅▃▁▁▁▃
wandb:       eval/ensemble_f1 ▇▆▃▆▆▇▂▆▆▅▇▃▆▇▇█▄▆▆▆▇▇▅▁█▃▇▅▆▅▇▂▇▂▆▄▇▇▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▇▃█▃▄▁█▄▄▇▄▇▂▆▃▄▂▂▅▅▃▅▄▂▃▆▅▅▆▅▂▄▅▅▂▇▃▃▅
wandb:      train/ensemble_f1 ▄▇▅▄▄█▅▆▅▇▅▇▇▄▄▆▆▄▅▆▆▅▅▅▄▆▆█▆▆▆▅▆▁▄▃▇▇▄▇
wandb:         train/mil_loss ▃▃██▄▁▁▃▂▅▇▄▅▄█▃▃▂▂▆▆▆▄▅▆▅▄▅▅▃▄▅▇▇▄▂▄▇▇▃
wandb:      train/policy_loss ▂▂▂▃▂▂▂▁▂▄▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93735
wandb: best/eval_avg_mil_loss 0.21837
wandb:  best/eval_ensemble_f1 0.93735
wandb:            eval/avg_f1 0.85398
wandb:      eval/avg_mil_loss 0.72475
wandb:       eval/ensemble_f1 0.85398
wandb:            test/avg_f1 0.88427
wandb:      test/avg_mil_loss 0.43341
wandb:       test/ensemble_f1 0.88427
wandb:           train/avg_f1 0.86885
wandb:      train/ensemble_f1 0.86885
wandb:         train/mil_loss 0.74832
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run toasty-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/um2ms642
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081529-um2ms642/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 72dqvrul with config:
wandb: 	actor_learning_rate: 4.136593160895814e-05
wandb: 	attention_dropout_p: 0.3992303130650154
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 192
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.943509210515096
wandb: 	temperature: 8.974601506421592
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081707-72dqvrul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/72dqvrul
wandb: uploading wandb-summary.json; uploading history steps 88-104, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss █▁▄
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 █▃▃▂▇▂▁▇▇▃██▃█▅██▇▆▃▂▇▇▆▃█▂▇▆▆▄▇▇▇█▇▃▃▃▃
wandb:      eval/avg_mil_loss ▁▁▁▄▃▁▃▆▁▁▁▁▁▅▄▄▁▁▁▃▄▅▁▄▁▁▅▃▆▁▁▅▃▁▁▅▁▃▆█
wandb:       eval/ensemble_f1 ██▅▅▁▂▇▅▅▆█▇▇▇▅▅██▇▇▆██▇▂▇▅██▁▇▅▅██▅██▅▅
wandb:           train/avg_f1 ▆▄▅▄▅▃█▆▆▂▆▄▃▄▆▅▁▅▄▃▅▃▂▆▅▇▆▄▅▆▄▃█▄▃▄▄▅▆▄
wandb:      train/ensemble_f1 ▄▅▅▄▄▂▂█▂▅▇▃▆▅▆▅▄▂▅▄▆▁▅▅▇▄▆▄▅█▃▃█▅▄▇▆▄▆▄
wandb:         train/mil_loss █▃▂▅▃▇▁▄█▄▄▄▅▇▇▄▅▃▇▁▇▇▃▆▄▃▇▄▂▃▃█▃▄▆▃▃▆▃▄
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▁▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.24182
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.59186
wandb:      eval/avg_mil_loss 4.24671
wandb:       eval/ensemble_f1 0.59186
wandb:           train/avg_f1 0.79013
wandb:      train/ensemble_f1 0.79013
wandb:         train/mil_loss 0.8126
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run giddy-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/72dqvrul
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081707-72dqvrul/logs
wandb: ERROR Run 72dqvrul errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: xl1vgyc4 with config:
wandb: 	actor_learning_rate: 0.0007972088751073179
wandb: 	attention_dropout_p: 0.31381722004625023
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 115
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8148259135402944
wandb: 	temperature: 6.221440789447984
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_081851-xl1vgyc4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xl1vgyc4
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅█
wandb: best/eval_avg_mil_loss █▁▁
wandb:  best/eval_ensemble_f1 ▁▅█
wandb:            eval/avg_f1 ██▅▄█▅▂▁▅▃▃▄▃▇▃▅▅▅▂▅▂▅▃▃█▄▃▁▅▃▇▅▃▄▅▅▅▄▃█
wandb:      eval/avg_mil_loss ▅▁▅▇▂█▇▃▇▇▄█▄▄▃▇▆▄▅▁▇▇▅▁▇▆▇▅▃▅▄▃▅▆▅▅▆▅▅▆
wandb:       eval/ensemble_f1 █▄█▄▁▃▂▅▅▂▄▄▅▇▄▃▅▅▁█▂▂▆▄▃▄▄▃▃▅▇▃▄▄▂▇▄▆▂█
wandb:           train/avg_f1 ▅▅▄▄▃▅█▄▅▁▅▂▃▇▆▁▆▃▅▃▆▅▆▆▅▄▃▅▄▃▄▅▄▄▇▃▇▅▂▅
wandb:      train/ensemble_f1 ▆▅▅▇▆▄▅▃█▁▂▆▇▅▅▃▅▇▃▆▆▅▃▆█▃▃▅▅▃▄▇▆▂▅▄▃▅▄▃
wandb:         train/mil_loss ▄▆▁▆▇▆▄▆▄▂▅▄▄▅▄▃▃▁▄▅▄▃▃▅▅▇▃▃▅▃▇█▃▄▂▃▁▄▆▂
wandb:      train/policy_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▁▄▄▄▄▄▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▁▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91535
wandb: best/eval_avg_mil_loss 0.28679
wandb:  best/eval_ensemble_f1 0.91535
wandb:            eval/avg_f1 0.91184
wandb:      eval/avg_mil_loss 0.25608
wandb:       eval/ensemble_f1 0.91184
wandb:           train/avg_f1 0.70132
wandb:      train/ensemble_f1 0.70132
wandb:         train/mil_loss 2.32785
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run hopeful-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xl1vgyc4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_081851-xl1vgyc4/logs
wandb: ERROR Run xl1vgyc4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 37gbwi5s with config:
wandb: 	actor_learning_rate: 0.00013847489226039838
wandb: 	attention_dropout_p: 0.34606599783448316
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8598260138878141
wandb: 	temperature: 8.177830781098981
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082028-37gbwi5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/37gbwi5s
wandb: uploading wandb-summary.json
wandb: uploading history steps 87-101, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁████
wandb: best/eval_avg_mil_loss █▁▁▂▁
wandb:  best/eval_ensemble_f1 ▁████
wandb:            eval/avg_f1 ██▇▅█▂█▅▆▆█▆▇▃▅█▄▁█▇▇▅▄▅██▅█▇▅██▅▆▅▇▇▇▇▆
wandb:      eval/avg_mil_loss ▃▁▃▄▃▃▄▄▅▅▂▄▆▅█▄▁▂▂▃▂▁▄▇▆▃▁▃▁▃▁▆▄▅▅▃▁▄▄▁
wandb:       eval/ensemble_f1 █▇▅▇▆▆▂█▅▆▆▅▆▆▅▅▁▇█▆▇▅▅▄▅█▇█▇██▆▅██▇▇▇▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▃▁▃▅▃▄▃▇▅▅█▄▄▅▇▂▇▄▇▄▄▇▃▄▁▅▆▅▃▆▄▃▅▅▃▅▅▂▅
wandb:      train/ensemble_f1 ▁▅▅▃▇▇▂▆▄▃▄▅▄▄█▆▅▅▆▆██▃▆▅▇▆▅▃▄▆▄▅▅▆▄▇▇▇▆
wandb:         train/mil_loss ▃▆▅▆▅▄▃▄▆▆▄▄▆▄▃▇▃▁▆▃▄▆▃▇▆▅▅▃█▄▇▃▃▃█▆▃▄▆█
wandb:      train/policy_loss █▄██▄▄▄▁▄▄▁▁██▄▁▁▁▄▄▄▄▄▄▄▄▄▄█▄███▄▄█▁▁▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁██▄▄▁▄▁▄▁▁█▄█▁█▁▄▁█▄▄▄▄▄▁▄█▁▄▄▁█▁▄▄▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91467
wandb: best/eval_avg_mil_loss 0.22875
wandb:  best/eval_ensemble_f1 0.91467
wandb:            eval/avg_f1 0.81748
wandb:      eval/avg_mil_loss 1.26382
wandb:       eval/ensemble_f1 0.81748
wandb:            test/avg_f1 0.73723
wandb:      test/avg_mil_loss 2.45205
wandb:       test/ensemble_f1 0.73723
wandb:           train/avg_f1 0.84357
wandb:      train/ensemble_f1 0.84357
wandb:         train/mil_loss 1.46248
wandb:      train/policy_loss 0.08466
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.08466
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dutiful-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/37gbwi5s
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082028-37gbwi5s/logs
wandb: Agent Starting Run: g9cqcfeq with config:
wandb: 	actor_learning_rate: 1.2049372854798869e-06
wandb: 	attention_dropout_p: 0.30580260708254675
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6153552994034045
wandb: 	temperature: 9.367453840990589
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082202-g9cqcfeq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g9cqcfeq
wandb: uploading wandb-summary.json
wandb: uploading history steps 95-101, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄█
wandb: best/eval_avg_mil_loss █▁▁▁
wandb:  best/eval_ensemble_f1 ▁▃▄█
wandb:            eval/avg_f1 ██▄█▄█▇▁▇█▇██▂██▇▆▆▇████▁▃▄██▇▃█▆▇▃█▄▆▇▇
wandb:      eval/avg_mil_loss ▅▁▃▁█▂▅▁▁▁▆▆▁▂▄▃▁▅▃▄▇▁▃▁▄▁▁▁▁▁▅▆▃▁▁▃▁▄▅▂
wandb:       eval/ensemble_f1 ▆█▄█▇▄▁██▇▂████▇▆▆▇▁█▃▇█▁▆██▇▄▆▇▅▃█▆▄▂▆▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▅▇▅▃▅▅▂▄▆▂▄▆▃▇▆▃▅▅▃▅█▃▇▅▁▅▇▄▄▃▅▅▄▄▅▅▆▆▅
wandb:      train/ensemble_f1 ▁▅▇▅▄█▃▆▆▄▅▆▄▂▆▄▇▄▅▁▅▇▅▅▆█▅▇▁█▂▄▅▇▄▅▆▅▄▆
wandb:         train/mil_loss ▅▃▄█▃▃▁▃▅▁▃▄█▇▄▁▆▄▆▃▃▃▃▄▃▄▃▅▂▅▁▇▂▅▇▃▄▄▄▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.26521
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.86496
wandb:      eval/avg_mil_loss 0.42088
wandb:       eval/ensemble_f1 0.86496
wandb:            test/avg_f1 0.92625
wandb:      test/avg_mil_loss 0.10919
wandb:       test/ensemble_f1 0.92625
wandb:           train/avg_f1 0.85515
wandb:      train/ensemble_f1 0.85515
wandb:         train/mil_loss 0.51613
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cosmic-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/g9cqcfeq
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082202-g9cqcfeq/logs
wandb: Agent Starting Run: p36bfc9w with config:
wandb: 	actor_learning_rate: 1.589104454854089e-05
wandb: 	attention_dropout_p: 0.3492315994987027
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 137
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2924159680607049
wandb: 	temperature: 5.936726164288872
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082330-p36bfc9w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p36bfc9w
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁████
wandb: best/eval_avg_mil_loss █▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁████
wandb:            eval/avg_f1 ▅██▇█▄█▇███▇█▅▆▅▇▄▁███▇▅▇█▇▇▅▅█▇████▆▇▄▇
wandb:      eval/avg_mil_loss █▁▁▁▁▁▁▁▁▆█▁▁▆▁▁▃▂▆▁▂▁▁▁▁▁▇▆▁▇█▁▂█▁▁▁▁▁▆
wandb:       eval/ensemble_f1 ▃█▇▇█▇▇▇▇▇▂▇▃▇██▂▄▅▁▁█▆▇▇█▂▆▆▇▇▃▇▁▃▇████
wandb:           train/avg_f1 ▁▂▃▅▃▄▄▂▄▅▅▆▆▅▇▁▃▆▃██▇▃▇▂▇▅▆▃▃▂▄▆▂▅▆▅▁▇▆
wandb:      train/ensemble_f1 ▅▂▆▇▅▂▇▁▃▆▇▄▇▁▇▆▇▆▅▆▃▆█▇▇█▃▂▂▆▆▆▅▆▃▂▇▆▁▆
wandb:         train/mil_loss ▄▄▃▅▃▂▆▆▃▅▃▃▃▅▅▁▅█▃▂▂▂▄▃▁▄▄▅▂▆▄▄▄▃▄▃▄▃▃▅
wandb:      train/policy_loss ███████████████████████████████▁████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92624
wandb: best/eval_avg_mil_loss 0.25416
wandb:  best/eval_ensemble_f1 0.92624
wandb:            eval/avg_f1 0.92281
wandb:      eval/avg_mil_loss 0.25672
wandb:       eval/ensemble_f1 0.92281
wandb:           train/avg_f1 0.85931
wandb:      train/ensemble_f1 0.85931
wandb:         train/mil_loss 0.54825
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run driven-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/p36bfc9w
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082330-p36bfc9w/logs
wandb: ERROR Run p36bfc9w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 2qlf88sz with config:
wandb: 	actor_learning_rate: 2.5787642814173593e-05
wandb: 	attention_dropout_p: 0.1815326114714132
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 117
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6585852683516172
wandb: 	temperature: 3.1412137870204946
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082545-2qlf88sz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2qlf88sz
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 113-117, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄█
wandb: best/eval_avg_mil_loss █▁▂▂
wandb:  best/eval_ensemble_f1 ▁▄▄█
wandb:            eval/avg_f1 ███▇██▆▇████▇██▇▇▇▇▁█▇██▇█████▇█▂█▇▇██▁█
wandb:      eval/avg_mil_loss ▁▅▁▇▁▁▁▁▁▁▁▅▁▇▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▅▅▅▁▆▁▁
wandb:       eval/ensemble_f1 ████▇████▇█████▇▇██▇██▇█▁█▁█████████▇▇██
wandb:           train/avg_f1 ▆▄▃▄▇▇▅▇█▇▄▃▆▂▄▆▇▅▇▂▂▁▅▇▆▇▆▄▃▅█▄▅▆█▄██▄▄
wandb:      train/ensemble_f1 ▅▅▆▃▆▇█▅▆▄▃▃▂▇▄▄▆▆▄▂▅▇▂█▆▁▆▄▇▃▄█▄▆▇▇█▆█▄
wandb:         train/mil_loss ▁▁▃▂▂▂▂▃▇▃▂▅▂▁▅▄▄▆▂▆▅█▃▃▁▄▁▁▅▄▃▄▄▆▅▃▃▅▁▄
wandb:      train/policy_loss ███████████████████████████████████▁████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▁▁▅▅█▅▁▅▅▁▅▅▁▅▅▁▅█▅▅▁▁▁▅▁▁▅▅▁▅▅▅▅█▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91813
wandb: best/eval_avg_mil_loss 0.27193
wandb:  best/eval_ensemble_f1 0.91813
wandb:            eval/avg_f1 0.77985
wandb:      eval/avg_mil_loss 0.57476
wandb:       eval/ensemble_f1 0.77985
wandb:           train/avg_f1 0.85048
wandb:      train/ensemble_f1 0.85048
wandb:         train/mil_loss 0.85151
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run legendary-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/2qlf88sz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082545-2qlf88sz/logs
wandb: ERROR Run 2qlf88sz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 3axpxefr with config:
wandb: 	actor_learning_rate: 0.00030726235587871494
wandb: 	attention_dropout_p: 0.25794683550111974
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 60
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8988832807002889
wandb: 	temperature: 7.762380141947375
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082723-3axpxefr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3axpxefr
wandb: uploading history steps 52-60, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃█
wandb: best/eval_avg_mil_loss ▃█▁
wandb:  best/eval_ensemble_f1 ▁▃█
wandb:            eval/avg_f1 ▇▁▇▇▇▁▄▇▃▂▇▇▆▂▅▇▃▇▇▂▂▇▅▅█▇▃▇▇▇▁▁▇▆▆▆▇█▇▇
wandb:      eval/avg_mil_loss ▁▄▅▃▂▁▁▄▂▄▁▁▁█▁▂▂▁█▄▄▁▁▇▃▁▁▂▁▄▁▂▅▅▂▄▆▁▄▁
wandb:       eval/ensemble_f1 ▇▇▅▆▇▄▅▇▄▇▇▇▄▆▇▅▇▇▁▄█▆▆█▇▅▇█▄▇▃▄▇▇▇▇█▇▇▆
wandb:           train/avg_f1 ▅█▂▆▂▅▄▂▁▄▃▆▂▅▃▅▃▄▇▃▄▃▆▄▅▂▄▅▁▄▂▂▄▅▅▆▂▆▄▄
wandb:      train/ensemble_f1 ▆▂▇█▄▅▃▁▅▅▇▂▆▃▄▃▄▅▃▇▄▇▂▅▆▆▁▅█▂▅▂▇▆▆▂▇▅▃▇
wandb:         train/mil_loss ▄▄▃▇▅▆▂▅▃▇▅▄▆█▅▂▅▂▃█▅▂▄▄▅▄▄▁▅▆▃▄▅▂▁▄▃▃▂▃
wandb:      train/policy_loss █▄▄▄▄▄█▁█▄█▄▄██▄▄▄▁██▄█▁▁▄▄████▄▄▄▄▄▄▁▁▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄▄▄█▁█▄▁██▁▄▄▄▁▄█▁▁▄▄▄███▄█▄▄▄▄▄▄▁▁▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.24595
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.88704
wandb:      eval/avg_mil_loss 0.28424
wandb:       eval/ensemble_f1 0.88704
wandb:           train/avg_f1 0.89541
wandb:      train/ensemble_f1 0.89541
wandb:         train/mil_loss 0.37901
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run usual-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3axpxefr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082723-3axpxefr/logs
wandb: ERROR Run 3axpxefr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: xkgh7eap with config:
wandb: 	actor_learning_rate: 6.828358932253308e-05
wandb: 	attention_dropout_p: 0.19510921580487683
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 131
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.15545077262568965
wandb: 	temperature: 5.320886228542373
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_082819-xkgh7eap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xkgh7eap
wandb: uploading history steps 108-121, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄█
wandb: best/eval_avg_mil_loss █▁▄▅
wandb:  best/eval_ensemble_f1 ▁▃▄█
wandb:            eval/avg_f1 ▇▆▄▄▆▇▂▄▁▇▄█▄▅▆█▆▄▇█▇▇▇▆█▃▄▄▅▇▅▆▅▄▇▃▇█▅▇
wandb:      eval/avg_mil_loss █▁▆▃▃▆▁▃▁▃▁▄▂▁▁▃▂▁▁▂▃▁▃▁▂▄▄▂▂▃▅▂▁▂▃▅▁▁▅▂
wandb:       eval/ensemble_f1 ▁▆▃▄▂▆█▁█▅█▆▃▇▇▅▆▅▇▅█▄█▄▃▄▇▆▄▆▇██▇▅▅▇█▅▅
wandb:           train/avg_f1 ▆▃▃▆█▃▄▂▇▅▅▄▅▆▃▆▅▇▆▄▁▃▇▄▆▁▄▄▆▅█▅▃▂▇▆▄▆▅▆
wandb:      train/ensemble_f1 ▅▃▅▃▃▆▂▄▆▆▂▃▂▂▄▃▆▅▄▅▄▃▅▁▂▄▂▅▃▃▅▇▅▃▅▃▃▅▄█
wandb:         train/mil_loss ▂▂▂▃▄▃▃█▂▅▁▄▂▄▂▂▁▄▂▃▁▃▁▃▁▃▁▃▁▄▄▂▄▄▂▂▃▂▁▁
wandb:      train/policy_loss █████▁██████████████████████▄███████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████▁███████████████▄██████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92964
wandb: best/eval_avg_mil_loss 0.26553
wandb:  best/eval_ensemble_f1 0.92964
wandb:            eval/avg_f1 0.89354
wandb:      eval/avg_mil_loss 0.39222
wandb:       eval/ensemble_f1 0.89354
wandb:           train/avg_f1 0.86712
wandb:      train/ensemble_f1 0.86712
wandb:         train/mil_loss 0.51772
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xkgh7eap
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_082819-xkgh7eap/logs
wandb: ERROR Run xkgh7eap errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: nub2gnhe with config:
wandb: 	actor_learning_rate: 1.1513533074087305e-05
wandb: 	attention_dropout_p: 0.28722782370516475
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 130
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7629796028748131
wandb: 	temperature: 4.484176983619163
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083040-nub2gnhe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nub2gnhe
wandb: uploading history steps 122-131, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇████
wandb: best/eval_avg_mil_loss █▄▄▁▄▁▁
wandb:  best/eval_ensemble_f1 ▁▆▇████
wandb:            eval/avg_f1 ▄▇▆▄▆▆▂▆▁▅▆▃▄▅▅▂▄▄▃▃▆▄▄▅▅▂▆▅▁▅▃▃▅▄▅▆█▂▄▆
wandb:      eval/avg_mil_loss ▂▅▃▅▄▃▂▃▆▆█▄▄▅▆▂█▄▇▇▁▅▅▁▅▃▃▅▂█▄▄▅▆▁▄▄▃▂▇
wandb:       eval/ensemble_f1 ▅▆▄▄▆▂▆▁▂▄▄▄▄▄▆▅▅▇▃█▄▁▂▅▅▄▅██▇▂▆▆▅▇▅▆█▅▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▃▆▅▄▄▃▆▄▁▅▁▄▄▅▃▄▄▃▇▇▄▄▆▄█▇▆▆▅▇▄▆▅▄▆▇▇▄▆
wandb:      train/ensemble_f1 ▅▅▃▄▅▆▇▆▅▅▆▁▃▄▆▆▄▆▃▆▄▄▅▄▆▆▄▄▇▇▆█▇▆▆▅▇▆▄▆
wandb:         train/mil_loss ▅▄▄▁▅▃▃▄▃▁▅▆▅▇▃▄▄▅▄▆▅▄▅▄▃▃▅▃▄▅▆▂▄▆▄▄█▆▆▂
wandb:      train/policy_loss ████▁███████▁█████████████▁█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▁█████████████████▅████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90687
wandb: best/eval_avg_mil_loss 0.28421
wandb:  best/eval_ensemble_f1 0.90687
wandb:            eval/avg_f1 0.74696
wandb:      eval/avg_mil_loss 2.02059
wandb:       eval/ensemble_f1 0.74696
wandb:            test/avg_f1 0.79022
wandb:      test/avg_mil_loss 1.16154
wandb:       test/ensemble_f1 0.79022
wandb:           train/avg_f1 0.80245
wandb:      train/ensemble_f1 0.80245
wandb:         train/mil_loss 1.60555
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run absurd-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/nub2gnhe
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083040-nub2gnhe/logs
wandb: Agent Starting Run: u0qbm9a0 with config:
wandb: 	actor_learning_rate: 1.065669522070243e-06
wandb: 	attention_dropout_p: 0.4384932760919677
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 101
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.23104189107380557
wandb: 	temperature: 2.457361075154015
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083310-u0qbm9a0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/u0qbm9a0
wandb: uploading wandb-summary.json
wandb: uploading history steps 93-101, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▁▅██▇▅██▂▇▆█████▆▂████████▇███▇▆██▂██▃█▆
wandb:      eval/avg_mil_loss ▁▃▃▁▁▃▁▆▁▁▁▁▂▂▁█▂▆▁▁▁▁▁▁▁▁▁▂▁▅▆▁▆▁▁▁▁▂▁▂
wandb:       eval/ensemble_f1 ██▇▇█▁▇█▇█▇██▁▂█▃████████▇███▆█▇▂▂▆███▂▆
wandb:           train/avg_f1 ▄▅▃▃▅▁▃▇▇▄█▇▅▃▇▂▃▆▄▅▅▂▅▅▄▄█▄▄▆▇▂▃▅▃▇▄▅▄▇
wandb:      train/ensemble_f1 ▃▃▅▁▆▄▇▄▄▃▄▂▄▂▄▃▆▄▄▄▆▅▅▄█▄▆▆▇▅▅▂▆▅▂▇▃▃▅▇
wandb:         train/mil_loss ▁▁▃▂▇▃▃▃▂▁▁▃▁▂▃▂▁▂▃▂▁▅▂▁▂▄▂▂▂▂▅▅▁▂▁█▃▁▅▂
wandb:      train/policy_loss ███████████████████████████▁████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄██▄▄▄▁█▄▄▄██▄▄▄▄▄▁██▄▄██▄▄▄▄▄▄▄▄▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93315
wandb: best/eval_avg_mil_loss 0.24765
wandb:  best/eval_ensemble_f1 0.93315
wandb:            eval/avg_f1 0.91886
wandb:      eval/avg_mil_loss 0.27091
wandb:       eval/ensemble_f1 0.91886
wandb:           train/avg_f1 0.87214
wandb:      train/ensemble_f1 0.87214
wandb:         train/mil_loss 0.7302
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run skilled-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/u0qbm9a0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083310-u0qbm9a0/logs
wandb: ERROR Run u0qbm9a0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 8l6lehnv with config:
wandb: 	actor_learning_rate: 4.2424525663552914e-05
wandb: 	attention_dropout_p: 0.3745240037204649
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 182
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.34093043221355834
wandb: 	temperature: 0.9041547626777512
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083438-8l6lehnv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8l6lehnv
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 175-183, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▅▅▅▆▆▇▇▇▇█
wandb: best/eval_avg_mil_loss █▅▃▁▂▃▃▁▁▂▂▁
wandb:  best/eval_ensemble_f1 ▁▂▅▅▅▆▆▇▇▇▇█
wandb:            eval/avg_f1 ▇▅▆▆█▇▇█▆▅▅▇▇▇▄▆▅█▆▃▁▇█▆▆█▇▇█▇▆▇▇█▇▆▆█▆▇
wandb:      eval/avg_mil_loss ▁▁▇▃▁▂█▁▁▁▁▃▁▃▃▁▁▃▄▇▄▃▁▂▁▄▃▆▁▁▁▅▅▁▁▄▂▂▂▂
wandb:       eval/ensemble_f1 █▅▅█▆█▅▇▇▇██▇█▇▇▇▇▆▆▇▇▇▇▁▆█▆▅██▆█▇▅▆█▆▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▇▃▅▅▂▃▃▅▅▆▃▂▇▄█▇▅▅▇▆▄▅▆▆▁▇▆▂▆▅▃▅█▁▄▂▃▄▃
wandb:      train/ensemble_f1 ▅▁▃▅▅▄▅▆▃▃▂▅▅▃▂▆▃▄▄▄▆█▅▃▆▆▆▄▇▄▄▁▆▇▆▆▇▅▅▄
wandb:         train/mil_loss ▅▅▃▆▄▃▂▇▂▂▅▃▂▅▄▅▃▄▃▃▃▃▃▂▄▂▆▂█▄▄▄▄▁▅▃▂▆▇▂
wandb:      train/policy_loss ▄▁▁██▁▄█▄██▁▄▁██▄▄█▁▁██▁▁▁█▁▁▄▄▁█▄▄▄█▄▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▇▇▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93315
wandb: best/eval_avg_mil_loss 0.198
wandb:  best/eval_ensemble_f1 0.93315
wandb:            eval/avg_f1 0.92558
wandb:      eval/avg_mil_loss 0.19906
wandb:       eval/ensemble_f1 0.92558
wandb:            test/avg_f1 0.737
wandb:      test/avg_mil_loss 1.72477
wandb:       test/ensemble_f1 0.737
wandb:           train/avg_f1 0.88397
wandb:      train/ensemble_f1 0.88397
wandb:         train/mil_loss 0.45967
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run mild-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8l6lehnv
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083438-8l6lehnv/logs
wandb: Agent Starting Run: 9m2f7ihc with config:
wandb: 	actor_learning_rate: 0.0002665565045102378
wandb: 	attention_dropout_p: 0.1552108650896208
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 132
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4740948453128271
wandb: 	temperature: 6.199731399486974
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_083809-9m2f7ihc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9m2f7ihc
wandb: uploading history steps 121-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇███
wandb: best/eval_avg_mil_loss █▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇███
wandb:            eval/avg_f1 ▇▅▄▂▆▇▃▅▆▅█▄▃▆▃▅▃▁▅▄█▄█▅▄█▅▆▅▆▅▄▅▆▇█▆▇▄▄
wandb:      eval/avg_mil_loss ▁▃▃▄▆█▃▅▆▃▄▆▄▁▃▄█▆▇▇▄▁▁▃▄▆▄▁▄▁▅▃▁▁█▄▄▄▁▅
wandb:       eval/ensemble_f1 ▄█▄▄▄▅▂▂▃█▃▇▄▂██▆▆█▇█▄▇▁▅▇▄▆▆▅▇▆▇▇▅▅▅▅██
wandb:           train/avg_f1 ▆▅▅▄▇▄▄▅▅▆▆▅▅▅▃▅▇▇▃▄▆▅▁▂▃▄▄▄▄▇▅▄▅▅▅█▅▃▅▇
wandb:      train/ensemble_f1 ▄▆▄▃▄▃▅▅▆▂▄▂▄▃▅▅▇▅▆▆▂▂▅▂▄▄▃▃▅▃▄▁▆▃▆▅█▃▆▄
wandb:         train/mil_loss ▆█▅▄▅▄▄▄▄▇▃█▃▅▆▂▅▄▃▇▄▃▂▃▅▆▆▄▄█▆▅▇▄▆▁▅▂▄▅
wandb:      train/policy_loss ███████████████▁████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████▁██████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92542
wandb: best/eval_avg_mil_loss 0.22444
wandb:  best/eval_ensemble_f1 0.92542
wandb:            eval/avg_f1 0.80543
wandb:      eval/avg_mil_loss 1.23526
wandb:       eval/ensemble_f1 0.80543
wandb:           train/avg_f1 0.89062
wandb:      train/ensemble_f1 0.89062
wandb:         train/mil_loss 0.79067
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/9m2f7ihc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_083809-9m2f7ihc/logs
wandb: ERROR Run 9m2f7ihc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: ffkxpghj with config:
wandb: 	actor_learning_rate: 1.744581767066708e-05
wandb: 	attention_dropout_p: 0.0028478083147970845
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 194
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.02854590525715528
wandb: 	temperature: 7.964743754649168
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084044-ffkxpghj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ffkxpghj
wandb: uploading history steps 161-165, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▅▆▆▆▇█
wandb: best/eval_avg_mil_loss ▂▅█▂▁▁▂▁▁
wandb:  best/eval_ensemble_f1 ▁▂▄▅▆▆▆▇█
wandb:            eval/avg_f1 ▅▁▆▅▅▆▇▆▆▆▆▇▄▇▂▇▅▄▇▆▄▇█▄▇▃▇▇▇▆▇▇▅▅▇█▇▆█▃
wandb:      eval/avg_mil_loss ▁▃▁▁▃█▁▁▁▁▃▄▃▂▃▁▃▂▅▁▃▁▁▁▄▅▂▂▁▁▃▁▁▄▁▂▃▃▁▁
wandb:       eval/ensemble_f1 ▄▅▄▆▆▅▆▅▇▄▆▁▃▄▁▆█▆▇▅▆▄▃▄▆█▆▃▆▆▅▆▇▃▇▇▆▇▅▇
wandb:           train/avg_f1 ▆▅▃▅▅▇▆▅▄▄▅▅▃▅▇▇▅▁▆▂▇▂▄▆▄▅▅▆▅▄▆▅▅▅▅█▆▇▅▁
wandb:      train/ensemble_f1 ▅▄▅▂▃▄▅▅▆▅▇▄▇▇▄▆▅▆▇▅▆▁▃▆▂▅▄▄▇▅▃▄█▄▅█▅▅▄▆
wandb:         train/mil_loss ▃▅██▃▇▁▇▃▄▆▅▅▄▃▃▇▂▂█▆▃▅▆▆▄█▄▃▇▅▆▆▅▁▇▄▅▃▃
wandb:      train/policy_loss █▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████▁███████▆█████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93328
wandb: best/eval_avg_mil_loss 0.22973
wandb:  best/eval_ensemble_f1 0.93328
wandb:            eval/avg_f1 0.90344
wandb:      eval/avg_mil_loss 0.26925
wandb:       eval/ensemble_f1 0.90344
wandb:           train/avg_f1 0.87683
wandb:      train/ensemble_f1 0.87683
wandb:         train/mil_loss 0.57456
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run polar-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ffkxpghj
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084044-ffkxpghj/logs
wandb: ERROR Run ffkxpghj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 5pkxucx0 with config:
wandb: 	actor_learning_rate: 1.5423349222608305e-05
wandb: 	attention_dropout_p: 0.31019507390679624
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 120
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.48602855131242506
wandb: 	temperature: 2.3063937322628982
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084355-5pkxucx0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5pkxucx0
wandb: uploading history steps 116-120, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▄▅▅▆▆▇▇█
wandb: best/eval_avg_mil_loss █▆▆▅▅▃▃▁▃▄
wandb:  best/eval_ensemble_f1 ▁▄▄▅▅▆▆▇▇█
wandb:            eval/avg_f1 ▆▆▅▅▄▆▆▅▇▅▆▆▇▇▇█▆▅▇▆▇▅▆▆▇█▁▆▇▅▇▇▇▇▇█▆▂▇█
wandb:      eval/avg_mil_loss ▅▅▆▅▆▆▅▆▅▃▂▅▄▅▅▃▄█▄▆▄▅▃▄▃▁▄▄▂▃▃▃▅▃▅▄▄▅▇▅
wandb:       eval/ensemble_f1 ▃▆▆▆▅▄▄▇▄▅▆▄▄▆▆▂▇▆▅▇▄▇▅▅▇▆▇▆▇▆▄▄▆▇▇▅▁▇█▇
wandb:           train/avg_f1 ▃▃▁▁▃▆█▂▃▂▅▁▂▅▂▅▅▅▅▄▃▃▇▃▇▃▃▄▅▅▂▆▂▇▆▂▂▃▆▃
wandb:      train/ensemble_f1 ▄▃▁▄▅▃▆█▂▄▃▃▅▇▂▄▄▅▂▄▄▄▄▇▇▂▄▄▄▃▆▄▆▅▃▆▆▂▃▃
wandb:         train/mil_loss ▂▃▅▃▂▃▂▁█▆▆▄▃▂▅▁▃▂▂▂▂▆▄▁▅▁▃▄▃▂▂▂▂▅▇▃▁▅▂▃
wandb:      train/policy_loss █████▁████████████████████████████████▆█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇▅█▅▅▅▂▅▅▄▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9409
wandb: best/eval_avg_mil_loss 0.23052
wandb:  best/eval_ensemble_f1 0.9409
wandb:            eval/avg_f1 0.92964
wandb:      eval/avg_mil_loss 0.20832
wandb:       eval/ensemble_f1 0.92964
wandb:           train/avg_f1 0.91619
wandb:      train/ensemble_f1 0.91619
wandb:         train/mil_loss 0.21242
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run swift-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5pkxucx0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084355-5pkxucx0/logs
wandb: ERROR Run 5pkxucx0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: s1u4mwlw with config:
wandb: 	actor_learning_rate: 2.320967819270162e-05
wandb: 	attention_dropout_p: 0.3979927928450333
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 190
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9581488696079604
wandb: 	temperature: 5.269330052076679
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084549-s1u4mwlw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s1u4mwlw
wandb: uploading history steps 174-185, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂▆▇▇▇██
wandb: best/eval_avg_mil_loss █▆▇▁▁▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▁▂▆▇▇▇██
wandb:            eval/avg_f1 ▄▄▇▅▇▅▅▇▇▅▅▄▂▂▇▇▄▂▂▇▇▇█▁█▄▆▄▇▆▆▆▆▆▆▄▅█▅▇
wandb:      eval/avg_mil_loss ▃▄▅▁▃▃▃▆▃▁▁▂▃▃▁▁▂▁▁█▄▁▂▆▃▃▂▂▁▄▂▃▃▄▃▃▃▂▂▁
wandb:       eval/ensemble_f1 ▆▆▅█▇▁▄▇██▆▅▇▄▄▃█▄▇▅▇▆▆▆▄▇▃▄▆▆▅▇▇▅▆▇▅▇██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▅▇▇▄▂▆▅▄▆▄▅▄▇▆▇▄▂█▅▄▄▆▁▄▅▅▃▃██▅█▆▅▄▄▃▄▄
wandb:      train/ensemble_f1 ▃▇▆▆▆▄▆▇▅▇▄▆▅▃▅▆▆▅▆▅▇▇▇▆▇▃▄██▅▆▅▆▅▃▇▆▁▅▆
wandb:         train/mil_loss ▄▂▂▃▆▆▁▆▃▄▃▂▄▅▅▅▅▆▅▅▃▅▇▄▅▄▃▅▆▃▅▅▄▂▂▇▂█▅▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▁█▄▁▁▄▄▁▁▁▁▄██▁▁▄▄▁▄▁▁█▁▄▁▄▄▄█▄▄███▄█▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92573
wandb: best/eval_avg_mil_loss 0.19233
wandb:  best/eval_ensemble_f1 0.92573
wandb:            eval/avg_f1 0.82481
wandb:      eval/avg_mil_loss 1.65905
wandb:       eval/ensemble_f1 0.82481
wandb:            test/avg_f1 0.85173
wandb:      test/avg_mil_loss 0.8246
wandb:       test/ensemble_f1 0.85173
wandb:           train/avg_f1 0.85146
wandb:      train/ensemble_f1 0.85146
wandb:         train/mil_loss 0.5381
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run gallant-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s1u4mwlw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084549-s1u4mwlw/logs
wandb: Agent Starting Run: xxs821lf with config:
wandb: 	actor_learning_rate: 0.0009092507311817712
wandb: 	attention_dropout_p: 0.41897401833862347
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 164
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3258535731656551
wandb: 	temperature: 8.157191668244417
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_084920-xxs821lf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xxs821lf
wandb: uploading history steps 117-124, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇████
wandb: best/eval_avg_mil_loss █▂▂▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇▇████
wandb:            eval/avg_f1 █▃▃▇▅▇▁▂▅█▄▆▇▃▅█▄▇▆▅▇▁▆▆▃▆▄▆▇▂▆▇▅▇▆▇█▆▇▄
wandb:      eval/avg_mil_loss ▃▂▂▆▃▁▅▁▃▃▄▆▄▅▁▄█▄█▆▅▅▄▅▅▃▄▂▃▆▃▄▃▅▅▁▃▅▃▇
wandb:       eval/ensemble_f1 ▇██▄▃▇▅█▁▂▅▅▅▄▆▄▆▇▆█▇▁▁▄▃▄▄▇▂▄▇▆▄▇▇▃█▅▅▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▆▅▃▄▆▆▅█▆▆▅▅▃▄▃▆▅▆▄▄▃▄▆▄▅▄▄▆▆▇▅▅▆▃▂▄▅▁▅
wandb:      train/ensemble_f1 ▇▅▇▇▇▇▇▆▄▅▆▄▇▇▇▇█▆▅▄▃▆▇▃▅▄▇▆█▆▆▆▇▅▆▁▆▆▆▆
wandb:         train/mil_loss ▅█▅▅▅▂▄▅▁▃▆▅▃▅▄▅▄▅▄▃▃▅▄▃▂▆▄▁▃▂▇▆▁▄▂▄▆▄▃▄
wandb:      train/policy_loss ████████████████████▁███████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁█▅▅▅▅█▁▁▅█▁▅▁▁▅▅▅▅▅▁▁▅▁▁▅▅█▅▁▅▁█▁█▅▅▅▅█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.19113
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.7763
wandb:      eval/avg_mil_loss 2.49389
wandb:       eval/ensemble_f1 0.7763
wandb:            test/avg_f1 0.69284
wandb:      test/avg_mil_loss 1.8577
wandb:       test/ensemble_f1 0.69284
wandb:           train/avg_f1 0.81511
wandb:      train/ensemble_f1 0.81511
wandb:         train/mil_loss 1.28255
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fanciful-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/xxs821lf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_084920-xxs821lf/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dwrgdmna with config:
wandb: 	actor_learning_rate: 1.0638809379099498e-06
wandb: 	attention_dropout_p: 0.4751366924658339
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 79
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9124860397246376
wandb: 	temperature: 8.417063628108412
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085125-dwrgdmna
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dwrgdmna
wandb: uploading wandb-summary.json
wandb: uploading history steps 73-80, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅█
wandb: best/eval_avg_mil_loss █▁▇▄▆
wandb:  best/eval_ensemble_f1 ▁▃▄▅█
wandb:            eval/avg_f1 ███▇████▇█▂████▇██▁██████▇██▁█▇█▇▇▂▇███▇
wandb:      eval/avg_mil_loss ▁▁▁▁▁▄▁▁▅█▁▅▁▁▁▁▁▁▁▆▁▁▆▁▁▁▁▆▁▁▁▁██▁▁▁▁▁▁
wandb:       eval/ensemble_f1 █▇▇▇▇▆█▇█▇█▇███▇▇█▇▇██▁██▇█▇▂██▂▇▇▇▇▇▇▇▂
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▂█▅▂▃▄▇▄▆▆▄▇▇▆▆▅▇▅▆▂▄▅▅▆▇▂▃▇▇▆▁▇▆█▁▇▇▇▂
wandb:      train/ensemble_f1 ▇▅▃▆▄▇▅▅▆▇▄▅▆▇▁▆█▇▆▅▅▆▇▆▅▇▃▄▇▃▅▂▆▇█▄▇▂▅▃
wandb:         train/mil_loss ▁▁▂▂▂▃▂▂▆▃▁▄▁▂▁▆▅▃█▃▄▂▃▁▄▁▁▄▂▂▃▄▅▁▄▅▃▄▂▃
wandb:      train/policy_loss ▁▁▅▅▁▁▅▅█▅▁▁▅▁▅█▅▁▅▅▅▅▁▁▅██▅█▁▅█▅█▅▅▅▁▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▅▁▁▅▁▅▅▅▅▁▁▁▅▅█▅▁▁▁▅▅▁▅▁▅█▁▁▁▅▅▅▁▅▅▁▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.28101
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.89312
wandb:      eval/avg_mil_loss 0.28002
wandb:       eval/ensemble_f1 0.89312
wandb:            test/avg_f1 0.92307
wandb:      test/avg_mil_loss 0.16173
wandb:       test/ensemble_f1 0.92307
wandb:           train/avg_f1 0.84383
wandb:      train/ensemble_f1 0.84383
wandb:         train/mil_loss 0.63829
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run swift-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dwrgdmna
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085125-dwrgdmna/logs
wandb: Agent Starting Run: mk6lw00s with config:
wandb: 	actor_learning_rate: 0.00010770068133216093
wandb: 	attention_dropout_p: 0.1160824787995532
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 155
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7796282350137056
wandb: 	temperature: 3.7551350921660065
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085238-mk6lw00s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mk6lw00s
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆█
wandb: best/eval_avg_mil_loss ▁█▆
wandb:  best/eval_ensemble_f1 ▁▆█
wandb:            eval/avg_f1 ▅▇▆▄█▅▅▅▅▆▇▆▄▅▅▇▄▇▄▅▄▃▁▅▇▄▂▅▄▄▄▄▄▂▅█▄▄▇▂
wandb:      eval/avg_mil_loss ▃▅▁▅▃▄▇▄▆▂▃▂▃▆▃▆▄▅▃▄▂▃▂█▃▄▄▄▇▂▄▅▅▇▇▄▄▄▅▆
wandb:       eval/ensemble_f1 ▄▇▆▄▄▅▃▆▄▄▄▅▅▆▄█▇▄▄▄▆▇▄▄▃▁▇▅▄▁▃█▅█▂▅▄▆▇█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▅▂▅▃▅▃▃▅▆▄▅▅▁▅▆▃▃▅▄▃▅██▇▃▄▂▂█▃▄▅▃▅▂▄▅▄
wandb:      train/ensemble_f1 ▇▆▇▅▇▇▂▄▃▆▅▂▅▅▅▆▃▃▂▄▃▄▄█▄▆▇▄▃▁▅█▅▃▄▄▄▂▅▃
wandb:         train/mil_loss █▅▆▅▅▃▇▆▃▂█▅▆▅▁▃▅▄▂▄▆▃▆▃▄▇▁▅▅▅▆▃▅▃▃▅▃▂▅▇
wandb:      train/policy_loss ▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▃▆▆▆▆▆▆▆▄▆▆█▆▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆█▆▆▆▆▆█▁▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92193
wandb: best/eval_avg_mil_loss 0.22737
wandb:  best/eval_ensemble_f1 0.92193
wandb:            eval/avg_f1 0.90758
wandb:      eval/avg_mil_loss 0.22786
wandb:       eval/ensemble_f1 0.90758
wandb:            test/avg_f1 0.92704
wandb:      test/avg_mil_loss 0.18855
wandb:       test/ensemble_f1 0.92704
wandb:           train/avg_f1 0.90946
wandb:      train/ensemble_f1 0.90946
wandb:         train/mil_loss 0.25283
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run divine-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mk6lw00s
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085238-mk6lw00s/logs
wandb: Agent Starting Run: v4aenbqr with config:
wandb: 	actor_learning_rate: 7.546320021072715e-06
wandb: 	attention_dropout_p: 0.4228517808045403
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 197
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.02082585553178351
wandb: 	temperature: 8.472123891708444
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085452-v4aenbqr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v4aenbqr
wandb: uploading history steps 120-133, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▄▅▇█
wandb: best/eval_avg_mil_loss █▆▅▂▂▁
wandb:  best/eval_ensemble_f1 ▁▃▄▅▇█
wandb:            eval/avg_f1 ▂▅▄▆▆▃▇▅▇█▃▃▆█▅▅▄▅▃▆▄▇▂▇▂▆▄▆▃▅▆▄▆▅▆▅▁▅█▇
wandb:      eval/avg_mil_loss █▄▃▃█▃▃▃▅▄▅▃▃▅▄▃▅▅▅▁▆█▇▂▄▃▇▅█▂▅▂▅▃▇▅▄▄▆▅
wandb:       eval/ensemble_f1 ▆▇▃▇▇▇▄▇▆▅▄▅▄▆▇▇█▅▅▃▄▆▅▆▅▆▇█▂▄▅▆▅▆▆▁▅▆▅▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁█▇█▂▇▆▂▃▄▆▄▆▄▄▅▆▂▂▁▆▇▄▄▇▂▆█▆▄▅▅▅▅▇▆▃▅▁▄
wandb:      train/ensemble_f1 ▆██▅█▁▃▄▇▇▇▅▆▅▇▅▆▂▃▄▆▆▃▅▇▇▄▅▄▇▅▆▆▆▇▄▅▇▃▃
wandb:         train/mil_loss ▅▄▅▄▇▄█▃▅▇▆▄▄▂▆▆▇▃▁▃▆▃▂▃▄▆▆▅▄▄▅▂▅▅▆▄▄▃▆▆
wandb:      train/policy_loss ██████████▁█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄█▄▄▃▄▄▄▄▄▄▄▂▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91414
wandb: best/eval_avg_mil_loss 0.22325
wandb:  best/eval_ensemble_f1 0.91414
wandb:            eval/avg_f1 0.89983
wandb:      eval/avg_mil_loss 0.63949
wandb:       eval/ensemble_f1 0.89983
wandb:            test/avg_f1 0.8505
wandb:      test/avg_mil_loss 0.63384
wandb:       test/ensemble_f1 0.8505
wandb:           train/avg_f1 0.83872
wandb:      train/ensemble_f1 0.83872
wandb:         train/mil_loss 0.68594
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sage-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v4aenbqr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085452-v4aenbqr/logs
wandb: Agent Starting Run: fu570mxf with config:
wandb: 	actor_learning_rate: 0.00026962566225785436
wandb: 	attention_dropout_p: 0.287464373981729
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 71
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9428321589770282
wandb: 	temperature: 2.958111294325063
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085728-fu570mxf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fu570mxf
wandb: uploading history steps 67-72, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂██
wandb: best/eval_avg_mil_loss ███▁
wandb:  best/eval_ensemble_f1 ▁▂██
wandb:            eval/avg_f1 ▇▇▅▇▇▆▅▇█▇▄▇▆▇▆▇▆▇▇▇▄▅▄█▇▇██▁▆▅▇▆▇▅▇▇▆▇▄
wandb:      eval/avg_mil_loss ▁▁▃▂▁▁▃▁▅▂▂▂▁▂▂▁▆▁▅▁▃▆▃▆▁▁▂▃▁█▆▁▃▁▁▁▁▁▅▆
wandb:       eval/ensemble_f1 ▇▇▇▆▇▇▅█▇▇▇▆▇█▇▇▇▇▃▄▄▄▅▄▇▅▇▅▁▇▅▂▇▇▇▇▇▇▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▂▅▅▆▇▅█▄▆▁▄▇▅▆▅▅▇▆▅▇▅▆▅▅▄▅▆▇▄▆▅▄▆█▅█▅▇▄
wandb:      train/ensemble_f1 ▂▁▄▃▅▄▇▅▇█▅▃▇▆▄▅▅▅▄▄▄▆▄▆▅▅▅▄▅▇█▄▅▇▆▄▇▅▆▅
wandb:         train/mil_loss ▄▇█▃▂▁▂▃▅▃▂▆▂▄▆▅▆▃▂▁▅▄▃▁▃▃▃▄▂▂▇▂▃▅▂▅▆▂▁▄
wandb:      train/policy_loss ▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃█▃▃▃▃▃▃▃▅▃▃▃▃▃▃▃▃▃▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92951
wandb: best/eval_avg_mil_loss 0.19351
wandb:  best/eval_ensemble_f1 0.92951
wandb:            eval/avg_f1 0.83934
wandb:      eval/avg_mil_loss 0.93535
wandb:       eval/ensemble_f1 0.83934
wandb:            test/avg_f1 0.90521
wandb:      test/avg_mil_loss 0.18525
wandb:       test/ensemble_f1 0.90521
wandb:           train/avg_f1 0.87925
wandb:      train/ensemble_f1 0.87925
wandb:         train/mil_loss 0.4319
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run effortless-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fu570mxf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085728-fu570mxf/logs
wandb: Agent Starting Run: ex1y80k8 with config:
wandb: 	actor_learning_rate: 3.403020065406091e-05
wandb: 	attention_dropout_p: 0.42810668793200535
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 129
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4660057500598964
wandb: 	temperature: 7.481207649980015
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_085856-ex1y80k8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ex1y80k8
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss █▁▁
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 ▅▅▅▆▅██▅▅▆▆█▆▂▅█▆▅▆▂▅█▂█▆▅▅▆▁▆▅▃▅▆▅███▅█
wandb:      eval/avg_mil_loss ▃▄▃▁▅▁▂▁▂▁▅▄▁▃▃▄▃█▁█▄▁▁▂▃▄▄▃▇▄▃▅▇▄▃▄▂▁▃▁
wandb:       eval/ensemble_f1 █▄▄▆▅▅▅▆██▅█▅▅█▅▅▅▁▅▁▁▂▇▅▂▇▇▇▄▄▇▄▅▅▂██▂▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▄▄▄▆▅▄▁▅▄▇▁▆▂▆▅▄▅▇█▆▄▄▅▂▃▆▆▅▅▄▇▅▅▄▃▃▄▃
wandb:      train/ensemble_f1 ▄▇▅▅▄▄▅▃▅▃▅▅▅▆▇▅▅▄█▇█▆▅▇▅▅▅▄▁█▄▇▅▆▃▅▄▅▄▄
wandb:         train/mil_loss ▃▇▃▆▅▅▃▃▃▆▅▆▆▆▄▄▁▄▄▆▄▃▃▄▄▃▄▃█▄▆▆▆▂▃▄▅▆▃▂
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████▁███████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.25755
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.9216
wandb:      eval/avg_mil_loss 0.24533
wandb:       eval/ensemble_f1 0.9216
wandb:            test/avg_f1 0.93026
wandb:      test/avg_mil_loss 0.21277
wandb:       test/ensemble_f1 0.93026
wandb:           train/avg_f1 0.74303
wandb:      train/ensemble_f1 0.74303
wandb:         train/mil_loss 1.00414
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run robust-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ex1y80k8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_085856-ex1y80k8/logs
wandb: Agent Starting Run: 148d0zk0 with config:
wandb: 	actor_learning_rate: 0.00028823132636815306
wandb: 	attention_dropout_p: 0.22486833339509657
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.34244837703910314
wandb: 	temperature: 0.6116218545883256
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090101-148d0zk0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/148d0zk0
wandb: uploading history steps 50-53, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅█
wandb: best/eval_avg_mil_loss █▁▂▁
wandb:  best/eval_ensemble_f1 ▁▅▅█
wandb:            eval/avg_f1 ▇▆▆▄▅██▇▆▅▃▆▃▁▇▆▆█▇▅▅▅▆▇▆▅█▂▆▅▃▆▇▆▂▇█▅▆█
wandb:      eval/avg_mil_loss ▂▅▆▂▁▃▄▂▄▄▂▇█▃▇▂▁▁▅▂▄▆▂▄▃▅▁▅▃▄▂▄▃▃▄▂▁▃▂▁
wandb:       eval/ensemble_f1 ▇▆▅▃▅█▅▇▇▆▅▆▃▇▆▇▄▇█▄▆▆▆▅██▂▅▅▇▃▆▇▅▁▇▇▄▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 █▆▃▄▄▅▄▇▆▂▇▇▆▇▅█▃█▂▆▄▁▅▅▅▂▅▆▅▂▆▅▅▆▇▇▂█▃▃
wandb:      train/ensemble_f1 ▆▄▄▄█▅▄▇▆▂▇▇▆▇▅█▃█▂▆▄▁▅▅▅▂▅▅▄▂▅▅▆▅▆▇▇▂█▄
wandb:         train/mil_loss ▃▃▅▃▂▃▃▄▇█▆▄▅▅▂▅▃▅▄▂▁▄▄▃▇▄▄▄▁▂▄▅▂▂▂▂▃▂▄▄
wandb:      train/policy_loss █████████▁█████████████████████████▆████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▁▅▅█▅█▅▁▁█▅▅▁▅▁▁▅▁▅▅▅█▅█▅▁█▅▅█▅▅██▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92558
wandb: best/eval_avg_mil_loss 0.24847
wandb:  best/eval_ensemble_f1 0.92558
wandb:            eval/avg_f1 0.89327
wandb:      eval/avg_mil_loss 0.29605
wandb:       eval/ensemble_f1 0.89327
wandb:            test/avg_f1 0.92061
wandb:      test/avg_mil_loss 0.46378
wandb:       test/ensemble_f1 0.92061
wandb:           train/avg_f1 0.76496
wandb:      train/ensemble_f1 0.76496
wandb:         train/mil_loss 1.47766
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run lemon-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/148d0zk0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090101-148d0zk0/logs
wandb: Agent Starting Run: dej4rezu with config:
wandb: 	actor_learning_rate: 0.0004465570759430152
wandb: 	attention_dropout_p: 0.27821045374758674
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 175
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4339416697087267
wandb: 	temperature: 8.556923795976697
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090153-dej4rezu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dej4rezu
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇█
wandb: best/eval_avg_mil_loss ▇█▁
wandb:  best/eval_ensemble_f1 ▁▇█
wandb:            eval/avg_f1 ▁▆▃▁▂▄▁▆▅▂▃▇▄▃▂▄▃▃▅▂█▁▄█▄▅▁▇▂▂▂▂▂▃▅▅▂▃▃▂
wandb:      eval/avg_mil_loss ▂▆█▄▅▅▃▃▅▄▂▇▃▅▅▆▃▂▃▄▆▂█▂▁▇▂▇▆▄▆▆▅▃▁▇▅▆▅▅
wandb:       eval/ensemble_f1 ▂▄▆▃▅▇▄▄▄▃▄▁▄▄▆▄▄▅▃▇▁█▆▂▇▆▅▅▄▃▄▄▆▅▂▃▃▄█▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▇▁▁▆▆▄▄█▆▃▂▃▅▅▃▅▄▅▇▆▆▄▅▅▂▄▆▇▂▆▆▂▆▇▆▄▅▆▅
wandb:      train/ensemble_f1 ▄▆▇▇▅▅▄▅▇▅█▂▆▆▅▅▅▆▅▄▁▅▅▇▇▄▄▆▅▅▅▆▆▇▅▇▆▇██
wandb:         train/mil_loss ▆▄▂▇▆▆▇█▄▆▂▇▇▆▇▅▃▃▃▅▄▆▃▆▅▅▃▃▄▅▆▅▆▅▃▄▄▂▁▃
wandb:      train/policy_loss ▇▇▁▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▂▇▇▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90687
wandb: best/eval_avg_mil_loss 0.27641
wandb:  best/eval_ensemble_f1 0.90687
wandb:            eval/avg_f1 0.7583
wandb:      eval/avg_mil_loss 2.01989
wandb:       eval/ensemble_f1 0.7583
wandb:            test/avg_f1 0.81204
wandb:      test/avg_mil_loss 1.34998
wandb:       test/ensemble_f1 0.81204
wandb:           train/avg_f1 0.76203
wandb:      train/ensemble_f1 0.76203
wandb:         train/mil_loss 1.81148
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pleasant-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dej4rezu
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090153-dej4rezu/logs
wandb: Agent Starting Run: qbfcsrla with config:
wandb: 	actor_learning_rate: 0.00015912691533047227
wandb: 	attention_dropout_p: 0.30507636408451083
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 128
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9437826490177144
wandb: 	temperature: 9.712598111863697
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090519-qbfcsrla
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/2j3xnbn3
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qbfcsrla
wandb: uploading history steps 120-129, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇▇██
wandb: best/eval_avg_mil_loss █▅▁▁▁▂▁
wandb:  best/eval_ensemble_f1 ▁▄▇▇▇██
wandb:            eval/avg_f1 ▇▅▆▅▆▆▂█▅▆▆▄▆▄▇▄▄▇▆▅█▁▇▄▆▂▇▅▇▂█▄▇▅▃▆▇▇▆▆
wandb:      eval/avg_mil_loss ▄▁▃▁▄▆▁▁▁▄▂▆▄▁▅▁▂▁▁▇▁▄▅▃▆▅▁▁▁▁▁▂▁▂▁▃▂▁█▃
wandb:       eval/ensemble_f1 ▅▅▆▇▆▇▅▅▄▇▇▁▆▆▂▄▅▃▇▄▅▂▇▇█▇▇▆▇▅▆▄▄▃▅▇▇▇▇▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▇▅▆▆█▇▆▆▁▆▃▃▅▄▃██▆▆▃▆▆▅▄█▇▇▇▆▆▂▃█▇▂▆▇▆▄
wandb:      train/ensemble_f1 ▄▅▃▄▅▇▆▁▇▆▄▅▇▇█▅▄▄▆▆▇██▇▇▅▆▆▇██▆▇▇▂█▆▄▄▇
wandb:         train/mil_loss ▇▂▇▅█▃▃▄▆▂▅▄▄▄▄▂▃▄▅▁▅▂▂▇▂▆▁▃▄▆▃▃▄▃▅▅▆▄▃▂
wandb:      train/policy_loss ██████████▁█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92964
wandb: best/eval_avg_mil_loss 0.2106
wandb:  best/eval_ensemble_f1 0.92964
wandb:            eval/avg_f1 0.87478
wandb:      eval/avg_mil_loss 0.2932
wandb:       eval/ensemble_f1 0.87478
wandb:            test/avg_f1 0.9128
wandb:      test/avg_mil_loss 0.20494
wandb:       test/ensemble_f1 0.9128
wandb:           train/avg_f1 0.8648
wandb:      train/ensemble_f1 0.8648
wandb:         train/mil_loss 0.53897
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run earnest-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qbfcsrla
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090519-qbfcsrla/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
wandb: Agent Starting Run: 8c7c45uw with config:
wandb: 	actor_learning_rate: 0.0001666758125412651
wandb: 	attention_dropout_p: 0.4921669887758097
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 89
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3070493311554222
wandb: 	temperature: 8.256612426348385
wandb: Currently logged in as: ninabraakman (ninabraakman-university-of-amsterdam) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090832-8c7c45uw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-1
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8c7c45uw
/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 73-90, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆█
wandb: best/eval_avg_mil_loss █▅▅▁
wandb:  best/eval_ensemble_f1 ▁▄▆█
wandb:            eval/avg_f1 █▇▇█▆▇█▅█▄▇▇██▇█▇▄▆▂▇██▆█▆▇▆▄▆▆▆▁▆▃▆▇▇▇▄
wandb:      eval/avg_mil_loss ▂▂▂▁▃▁▂▁▁▂▃▂▂▅▁▁▁▁▂▂▁▄▃█▁▂▂▃▄▁▂▁▂▄▅▃▂▂▂▄
wandb:       eval/ensemble_f1 ▇██▇▇█▁▇▇▇▇██▄▇▇▇▇▇▇▅██▇█▇█▆▇▇▆▁▆▇▂█▆▇▄▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▆▄▃▇▇▆█▆▄▇▃▇▄▅▇▆▁▆▆▆█▇▅▇▆▄▄▆▅█▆▅▄▅▆▇▂▇█
wandb:      train/ensemble_f1 ▇▆▇▆▇▆▃█▆▄▆▃▅▆▃▄▆▄▆▆▇▇▆▅▃▄▄▅▇▆██▅▅▇▆▅▃▁▄
wandb:         train/mil_loss ▅▂█▂▂█▁▂▅▄▄▁▁▃▇▄▅▄▄▂▅▂▃▇▅▂▂▇▄█▃▂▇▂▂▁▁▁▃▁
wandb:      train/policy_loss ▅▅▅▅▅▅▅▁▅▅▁▅▅▅▅▅▅▅█▅▅▅▅█▅▅█▅▅▅▅▅▅▅██▅▅▁▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▁▄▄▁█▄▄█▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92907
wandb: best/eval_avg_mil_loss 0.25297
wandb:  best/eval_ensemble_f1 0.92907
wandb:            eval/avg_f1 0.69289
wandb:      eval/avg_mil_loss 0.61827
wandb:       eval/ensemble_f1 0.69289
wandb:            test/avg_f1 0.91253
wandb:      test/avg_mil_loss 0.26803
wandb:       test/ensemble_f1 0.91253
wandb:           train/avg_f1 0.83467
wandb:      train/ensemble_f1 0.83467
wandb:         train/mil_loss 0.28434
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run fresh-sweep-1 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8c7c45uw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090832-8c7c45uw/logs
wandb: Agent Starting Run: yx4oumhw with config:
wandb: 	actor_learning_rate: 6.649950137601314e-06
wandb: 	attention_dropout_p: 0.47521382520518535
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 194
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.10649666769415957
wandb: 	temperature: 8.194038044443712
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_090949-yx4oumhw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-2
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yx4oumhw
wandb: uploading history steps 189-194, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃▆▆▆█
wandb: best/eval_avg_mil_loss █▂▂▁▂▁
wandb:  best/eval_ensemble_f1 ▁▃▆▆▆█
wandb:            eval/avg_f1 █▇▇▆▆▅▁▅▆▅▆▅▆▆▄▆▅▆▇▅▆█▆▆▆▅▅▄▄▇▆▇▇▅▇▃▄▅▇▆
wandb:      eval/avg_mil_loss ▄▄▂▅▃▃▄▃▃▂▃▅█▄▇▄▂▁▄▂▁▃▄▂▃▃▅▅▅▄▄▃▂▁▃▂▃▄▄▂
wandb:       eval/ensemble_f1 ▇▇▅▁▇▄▇▇▅▇▄▃▇▄▇▅▄▆▃▅▇█▅▅▄▄▄▁▆▂▅▄▃▆▄▄▄▇▄▄
wandb:           train/avg_f1 ▆▃▇▂▅▃▃▄▃▄▂▄▅▅▅▄▁▄▆▃▂▅▅▇▁▃█▃▂▇█▆▄▆▂▆▂▄▅▅
wandb:      train/ensemble_f1 ▂▂▂▂▇▄▃▅▁▅▁▄▃▅▂▅▅▃▁▅▅▅▇▃▅▄▅▄█▄▄▃▇▅▅▂▇▅▃▅
wandb:         train/mil_loss ▄▅▄█▁▃▇▇▅▄█▅▃▄▄▄▄▄▄▄▃▅▅▁▃▃▅▁▄▁▂▅▁▄▄▂▁▅▆▅
wandb:      train/policy_loss ████▁██████████████████████▁████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████████████████████▁████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92998
wandb: best/eval_avg_mil_loss 0.25773
wandb:  best/eval_ensemble_f1 0.92998
wandb:            eval/avg_f1 0.81381
wandb:      eval/avg_mil_loss 1.1756
wandb:       eval/ensemble_f1 0.81381
wandb:           train/avg_f1 0.7987
wandb:      train/ensemble_f1 0.7987
wandb:         train/mil_loss 0.70525
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run flowing-sweep-2 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/yx4oumhw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_090949-yx4oumhw/logs
wandb: ERROR Run yx4oumhw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 23s2wl6z with config:
wandb: 	actor_learning_rate: 2.512221386131954e-05
wandb: 	attention_dropout_p: 0.4379531276000336
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 65
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.99850967092889
wandb: 	temperature: 5.015068092945832
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091316-23s2wl6z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-3
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/23s2wl6z
wandb: uploading history steps 58-66, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇██
wandb: best/eval_avg_mil_loss █▆▁▂▂▄
wandb:  best/eval_ensemble_f1 ▁▄▇▇██
wandb:            eval/avg_f1 ▃▇▆▃▁▅▃▁▇▆▄▃▇▄▄▆▂▅▂▄▄▆▄▃▄█▆▅▇▄▄▇▅▄▅▃▆▃▁█
wandb:      eval/avg_mil_loss ▄▁▃▅█▅▃▇▅▂▆▆▄▇▄▅▂▄▆▅▂▃█▅▃▁▅▄▆▅▄▃▃▃▆▆▄▇▄▁
wandb:       eval/ensemble_f1 ▅▇▆▃▂▅▅▁▆▇▁▄▃▄▄▂▅▆█▅▄▃▄▆▆▃█▆▇▅▅▄▇▅▄▆▁▃▁█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▅▆▃▂▄▄▄▅▅▅▅▆▃▅▁▃▃▆▆▄▅▅▄▄▃▇▅▅▅▅▅▅▅▄█▄▅▅▆
wandb:      train/ensemble_f1 ▅▅▆▃▂▅▄▄▅▆▄▅▅▅▅▃▆▅▅▆▁▅▅▅▄▄▄▇▅▅▅▅▅▅▄▄█▅▅▅
wandb:         train/mil_loss ▃▆▃▄▃▃▃▇▅▃▄▁▅█▆▄▅▃▆▄▃▂▁▄▃▃▃▃▅▅▅▄▂▅▃▃▄▄▂▂
wandb:      train/policy_loss █████████████▆███████████████████▁██████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███████████▁████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.8782
wandb: best/eval_avg_mil_loss 0.84733
wandb:  best/eval_ensemble_f1 0.8782
wandb:            eval/avg_f1 0.86861
wandb:      eval/avg_mil_loss 0.33246
wandb:       eval/ensemble_f1 0.86861
wandb:            test/avg_f1 0.8768
wandb:      test/avg_mil_loss 0.26996
wandb:       test/ensemble_f1 0.8768
wandb:           train/avg_f1 0.73769
wandb:      train/ensemble_f1 0.73769
wandb:         train/mil_loss 0.70011
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rare-sweep-3 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/23s2wl6z
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091316-23s2wl6z/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: osgxc3ni with config:
wandb: 	actor_learning_rate: 2.5644501603829684e-05
wandb: 	attention_dropout_p: 0.2122726869164256
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 91
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04211724903804426
wandb: 	temperature: 6.853514069233912
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091458-osgxc3ni
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-4
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/osgxc3ni
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▆▇██
wandb: best/eval_avg_mil_loss █▄▆▂▂▁▁
wandb:  best/eval_ensemble_f1 ▁▄▆▆▇██
wandb:            eval/avg_f1 ▅▃▇▆▇▆▅██▅▁▃▃▆▃█▆██▅▆▅▆▆█▂▆▅▂▄▇▅▄▄█▄▃▃▆▂
wandb:      eval/avg_mil_loss ▅▇▃▄▂▁▁▁▃▅▃▇▆▆▃▁▄▄▂▁▅▄▁▄▃▄▃▇▅▂▁▃▃▅█▇▄▁▃█
wandb:       eval/ensemble_f1 ▄▇▇▆▇▆▆▅█▇▄▃▅█▄▄▅▇▆▆▅▇▆▃▁▇▇▇▆█▅▇▃▄▄▆▄▆▇▄
wandb:           train/avg_f1 ▇▆▅▇▁██▆▅▆▆▇▃▅▆▆█▄▃▅▅▅▅▄█▇█▅█▆▇▆▇▆▅▄▇▅▅▄
wandb:      train/ensemble_f1 ▇▆▅▇▆▇▆▆▁▄▃▅▄▅▃▆▆█▆▅▄▄▆▅▅█▅▇▅▅▅▇▂▆███▅▇▂
wandb:         train/mil_loss ▄▅▂▆▃▂▆▄▃▅▃▅▃▇▇▃▃█▆▇▄▅▃▆▆▃▅▂▄▁▅▅▄▂▇▁▂▅▅▄
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91482
wandb: best/eval_avg_mil_loss 0.23281
wandb:  best/eval_ensemble_f1 0.91482
wandb:            eval/avg_f1 0.76871
wandb:      eval/avg_mil_loss 1.14883
wandb:       eval/ensemble_f1 0.76871
wandb:           train/avg_f1 0.78553
wandb:      train/ensemble_f1 0.78553
wandb:         train/mil_loss 0.57876
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run giddy-sweep-4 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/osgxc3ni
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091458-osgxc3ni/logs
wandb: ERROR Run osgxc3ni errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: iyo97ad0 with config:
wandb: 	actor_learning_rate: 3.124882605657151e-05
wandb: 	attention_dropout_p: 0.4588879923574557
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 72
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9860564720564368
wandb: 	temperature: 3.610686063228774
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091637-iyo97ad0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-5
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iyo97ad0
wandb: uploading history steps 72-72, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▇▇█
wandb: best/eval_avg_mil_loss ▇█▄▂▁
wandb:  best/eval_ensemble_f1 ▁▄▇▇█
wandb:            eval/avg_f1 ▇█▅▄▄▆▇▇▂▇▆▇█▇▄█▃▂▁▆▇▅█▄▅▄█▆▆▄▇▆▇▅█▆▃▇█▅
wandb:      eval/avg_mil_loss ▂▃▃▄█▂▂▅▁▄▁▂▁▇▂▂▅▁▄▂▁▇▃▄▂▇▄▁█▆▁▃▂▇▂▅▃▂▄▅
wandb:       eval/ensemble_f1 ▆▆▅▅▄▄▃▆▆▅▂▆▅▆▅▇▇▆█▁▇▄▅▇▄▅▄█▄▇▆▄█▆▇▅▆▃▇▅
wandb:           train/avg_f1 ▆▃▆▅▄▃▆▃▂▁▆▅▄▄▇▅▅▁▅▅█▃▄▅▄▂▄▃▂▂▇▅▂▅▂▄▅▃▂█
wandb:      train/ensemble_f1 ▆▆▅▃▄▆▃▄▃▅▅▃▁▅▆▅▇▄▅▁▅▃▃█▃▄▅▄▂▆▆▇▅▂▂▄▂▄▂█
wandb:         train/mil_loss ▄▄▄▃▆▄▅▆▆▄▃▃▇█▃▃▇▅▃▇▇▃▆▃▆▇▅▄▂▆▄▃▁▂▁▃▅▃▅█
wandb:      train/policy_loss ██████████▁█████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████▁█████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91467
wandb: best/eval_avg_mil_loss 0.24185
wandb:  best/eval_ensemble_f1 0.91467
wandb:            eval/avg_f1 0.84619
wandb:      eval/avg_mil_loss 0.56985
wandb:       eval/ensemble_f1 0.84619
wandb:           train/avg_f1 0.87852
wandb:      train/ensemble_f1 0.87852
wandb:         train/mil_loss 0.62393
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run summer-sweep-5 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/iyo97ad0
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091637-iyo97ad0/logs
wandb: ERROR Run iyo97ad0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: a94tsja2 with config:
wandb: 	actor_learning_rate: 0.0007460057744635761
wandb: 	attention_dropout_p: 0.4979690889443175
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 174
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.15266382426798486
wandb: 	temperature: 8.217816283315294
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_091806-a94tsja2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-6
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a94tsja2
wandb: uploading history steps 172-175, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▂▃▄▅▇▇█
wandb: best/eval_avg_mil_loss █▇▅▆▅▄▂▂▁▂
wandb:  best/eval_ensemble_f1 ▁▂▂▂▃▄▅▇▇█
wandb:            eval/avg_f1 ▄▄▁▅▅▅▄▅▄▅▄▄▅▄▅▁▅▂▂▂▆▆▆▅▅▂▇▆▇▆▇▆▅▇▆▇█▅█▇
wandb:      eval/avg_mil_loss ▆▆▆▆█▆▅▅▅▅▄▄▄▄▇▅▄▃▄▄▃▃▃▃▃▃▃▅▃▂▂▃▂▁▄▂▂▁▁▁
wandb:       eval/ensemble_f1 ▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅▆▂▆▁▆▆▆▆▆▄▆▅▃▇▃▇▇██▆█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▁▄▄▃▃▂▄▃▂▂▄▄▅▅▆▅▅▆▆▄▅▅▆▃▅▆▅▄▆▆▅▄▅▆█▇███
wandb:      train/ensemble_f1 ▁▂▄▃▂▄▂▄▃▅▅▄▅▅▆▆▅▅▅▆▆▄▆▃▇▄▅▇▅▆▇▆▅▇█▇▆▆▅▇
wandb:         train/mil_loss ▇█▇▄▆▆▅▄▅▆▃▃▄▃▅▄▄▄▅▄▂▂▃▄▁▃▂▃▃▃▃▂▂▁▂▃▂▃▂▂
wandb:      train/policy_loss ██████████████▄████████████▆█▁█▁████▇███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.93315
wandb: best/eval_avg_mil_loss 0.20935
wandb:  best/eval_ensemble_f1 0.93315
wandb:            eval/avg_f1 0.92573
wandb:      eval/avg_mil_loss 0.18859
wandb:       eval/ensemble_f1 0.92573
wandb:            test/avg_f1 0.92704
wandb:      test/avg_mil_loss 0.15344
wandb:       test/ensemble_f1 0.92704
wandb:           train/avg_f1 0.91871
wandb:      train/ensemble_f1 0.91871
wandb:         train/mil_loss 0.20702
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run visionary-sweep-6 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/a94tsja2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_091806-a94tsja2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8ui6ldc9 with config:
wandb: 	actor_learning_rate: 0.0008872854847380531
wandb: 	attention_dropout_p: 0.03458433516466469
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 93
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7459036270981984
wandb: 	temperature: 5.6430553158258325
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092122-8ui6ldc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-7
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8ui6ldc9
wandb: uploading history steps 86-93, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇█
wandb: best/eval_avg_mil_loss █▆▁▂
wandb:  best/eval_ensemble_f1 ▁▂▇█
wandb:            eval/avg_f1 ▆▅▃▆▄▄▂▃▆▇▂▄▇▁▇▅█▇▆▅▁▅▂▃▃▇▄▇▃▄▆▂▅▁▃▆▅▄▁▄
wandb:      eval/avg_mil_loss ▂▅▄▃▁▆▁▂█▄▂▂▆▃▃▂▁▁▇▂▇▂▇▅▅▅▁▄▂▃▂▂▃▇▅▄▁▃▅▄
wandb:       eval/ensemble_f1 ▆▅▅█▁▃▃▃▇▅█▇▇▅▇▃▇▃▅▆▄▅▇▅▆▃▆▆▃▄▄▆▇▆▆▄▆▃▂▅
wandb:           train/avg_f1 █▃▃▃▆▄▁▅▄▁▅▂▃▃▂█▃▃▇▅▆▆▅▃▃▅▅▄▄▇▇▆▂▄▅█▆▅▄▅
wandb:      train/ensemble_f1 ▄█▄▁▅▇▆▅▃▆▇▅▃▅▄▄▅▅▃▄▄▅▇▅▆▄▅▆▆▅▇▃▆▇▃▆▇▆▂▆
wandb:         train/mil_loss ▇▆▇█▃▅▆▄█▅▄▆▅▃▄▆▂▇▆▇▄▇▃▆▇▅▁▃▅▆▄▂▃▅▄▅▅▃▅▄
wandb:      train/policy_loss █████████▇█▆████▄████████████▁███████▇██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇█▇
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92223
wandb: best/eval_avg_mil_loss 0.26055
wandb:  best/eval_ensemble_f1 0.92223
wandb:            eval/avg_f1 0.82482
wandb:      eval/avg_mil_loss 0.79066
wandb:       eval/ensemble_f1 0.82482
wandb:           train/avg_f1 0.82596
wandb:      train/ensemble_f1 0.82596
wandb:         train/mil_loss 0.7174
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run stellar-sweep-7 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8ui6ldc9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092122-8ui6ldc9/logs
wandb: ERROR Run 8ui6ldc9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 0a7yqbii with config:
wandb: 	actor_learning_rate: 0.00012694496785094274
wandb: 	attention_dropout_p: 0.24658983605563017
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 195
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6493854977260568
wandb: 	temperature: 1.104950736354714
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092306-0a7yqbii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-8
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0a7yqbii
wandb: uploading history steps 158-165, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂▆▆▇█
wandb: best/eval_avg_mil_loss ▅█▄▂▂▁▁
wandb:  best/eval_ensemble_f1 ▁▁▂▆▆▇█
wandb:            eval/avg_f1 ▅▁▅▅▂▅▄▇▃▃▄▇▅▇█▇▄▅▆▅▇▄▄▆▇▃█▇▆▇█▅▆█▇▄▅▆▅▄
wandb:      eval/avg_mil_loss ▄▆▅▄▃▂▄▄▄▆▅█▂▂▂▅▃▁▂▃▃▄▂▁▁▆▆▃▁▃█▅▄▅▂▄█▄▃▄
wandb:       eval/ensemble_f1 ▅▃▁▅▆▄▄▅▇▅▃▇▆▅█▇█▁▅▆█▂▂▅▅██▅▇▇▇▆▆▅▁▆▅▆▄▅
wandb:           train/avg_f1 ▄▇▅▁▄▅▃▅▆▇▃▅▅▆▅▇▄▆▄▅▆▆▅▅▅▆▅▆█▅▆▆▅▇▇▇▃▃▅▇
wandb:      train/ensemble_f1 ▄▅▁▅▄▅▆▅▃▆▅▇▄▆▆▇▇▇█▄▆▇▆▅▇█▅▆▆▇▆▃▇▇▇▆█▆▆▇
wandb:         train/mil_loss ▆▅▃▆▇▅▂▃▃▅▄▇▂█▅▃▃▅▃▄█▃▆█▃▁▁▃▄▆▃▂▂▅▃▄▆▆▄▄
wandb:      train/policy_loss ▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█▇▇▇▇▇▇
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▁████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9103
wandb: best/eval_avg_mil_loss 0.27465
wandb:  best/eval_ensemble_f1 0.9103
wandb:            eval/avg_f1 0.8014
wandb:      eval/avg_mil_loss 0.79036
wandb:       eval/ensemble_f1 0.8014
wandb:           train/avg_f1 0.80026
wandb:      train/ensemble_f1 0.80026
wandb:         train/mil_loss 0.7651
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run treasured-sweep-8 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0a7yqbii
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092306-0a7yqbii/logs
wandb: ERROR Run 0a7yqbii errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 3083obha with config:
wandb: 	actor_learning_rate: 3.937369917229598e-05
wandb: 	attention_dropout_p: 0.2163113430355666
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7748038139045392
wandb: 	temperature: 2.9832127156222477
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092607-3083obha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-9
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3083obha
wandb: uploading history steps 57-57, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss ▁█
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▃▂▄▅▄▆▆▁▆▅▃▄▄▄▄▅▆▃▄▄▄▄▃▃▃▂▅▁▁▅▃▄▆▄▅▆▃▄█
wandb:      eval/avg_mil_loss ▁▅▆▅▅▅▅▆▃▃▄▄▇▄▄▄▆▅▇▇▄▆▄▅▆▆▄▃██▄▅▄▄▅▅▃▅▅▂
wandb:       eval/ensemble_f1 █▁▃▂▅▄▃▁▄▆▅▃▄▄▄▄▅▆▃▂▅▃▄▄▄▃▃▃▂▇▁▅▅▃▄▄▆▃▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▄▅▅▆▅▃▃█▆▇▇▅▁▆▄▆▇▇▇▅█▅▆▇▆█▅▂▄▂█▂▅▆▃█▅▇
wandb:      train/ensemble_f1 ▄▃▄▅▅▆▅▃▂█▇▅▇▇▅▃▅▇▇▃█▅▅▇▇▃█▄▅▂▄▄▁█▁▅▆▂█▆
wandb:         train/mil_loss ▆▆▅▆▅▅▇▇▅▄█▇▆▅▅▃▇▅▇▄▅▇▄▆▄█▄▆▁▆▅▄▇▅▄▅▄▆▃▅
wandb:      train/policy_loss ▆▆█▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.88652
wandb: best/eval_avg_mil_loss 0.58063
wandb:  best/eval_ensemble_f1 0.88652
wandb:            eval/avg_f1 0.88652
wandb:      eval/avg_mil_loss 0.58063
wandb:       eval/ensemble_f1 0.88652
wandb:            test/avg_f1 0.64348
wandb:      test/avg_mil_loss 1.62014
wandb:       test/ensemble_f1 0.64348
wandb:           train/avg_f1 0.70728
wandb:      train/ensemble_f1 0.70728
wandb:         train/mil_loss 1.4896
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run blooming-sweep-9 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3083obha
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092607-3083obha/logs
wandb: Agent Starting Run: 8qgg6zux with config:
wandb: 	actor_learning_rate: 1.6933074620890775e-06
wandb: 	attention_dropout_p: 0.05867535612452168
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 166
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.23668946458948129
wandb: 	temperature: 2.758826631209146
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092713-8qgg6zux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-10
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8qgg6zux
wandb: uploading wandb-summary.json
wandb: uploading history steps 157-167, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▆▆▇█
wandb: best/eval_avg_mil_loss ▁▆▇▆██▄
wandb:  best/eval_ensemble_f1 ▁▂▃▆▆▇█
wandb:            eval/avg_f1 ██████████▃▇▃█▇▅▇████▃▇█▆██▇▇█▇█▁▅█▇████
wandb:      eval/avg_mil_loss ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▄▂▁█▁▅▁▁▁▁▅▁▂▁▂▁▁▁▁▁
wandb:       eval/ensemble_f1 ████▆█▆█▆█▇█▆█▆▂▆▄█▇█▁███▆██▇██▄█▆████▄█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▇▄▅▇▇▅▄▄▆▇▆▅▂▆▆▄▇██▁▇▇█▆▃▅▇▆▇▆█▇▆█▄▆▄▆▇
wandb:      train/ensemble_f1 ▆▇▆▁▅▃▇▃▃▇▇▃▄▆█▄▃▇█▇▅█▅▇▄▇█▅▃▆▃▆█▃█▅▄▅▆▇
wandb:         train/mil_loss ▆▄▁▂▃▂▁▁▄▃▁▁▃▂▁▁▁▂▃▃▁▁▁▂▃▄█▅▄▁▁▃▁▂▁▃▃▂▃▃
wandb:      train/policy_loss ▁▄▄▄▄█▄▄▄▄▄▄▄▄█▁▄▄▁█▄▄▄▁▄▄▁▁▄▄█▄▄▁█▄▄██▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▁▄▄▄▄▄▄▄▄█▄▄▁▄▁▄█▁▄▄▄▁▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92209
wandb: best/eval_avg_mil_loss 0.32343
wandb:  best/eval_ensemble_f1 0.92209
wandb:            eval/avg_f1 0.84296
wandb:      eval/avg_mil_loss 0.37434
wandb:       eval/ensemble_f1 0.84296
wandb:            test/avg_f1 0.91784
wandb:      test/avg_mil_loss 0.14218
wandb:       test/ensemble_f1 0.91784
wandb:           train/avg_f1 0.89615
wandb:      train/ensemble_f1 0.89615
wandb:         train/mil_loss 0.49849
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run volcanic-sweep-10 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/8qgg6zux
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092713-8qgg6zux/logs
wandb: Agent Starting Run: vo5ivjpd with config:
wandb: 	actor_learning_rate: 2.952703079266372e-06
wandb: 	attention_dropout_p: 0.09257075001672366
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 62
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6876486397270735
wandb: 	temperature: 8.162180271180105
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_092928-vo5ivjpd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-11
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vo5ivjpd
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇██
wandb: best/eval_avg_mil_loss █▃▅▁
wandb:  best/eval_ensemble_f1 ▁▇██
wandb:            eval/avg_f1 ▇▃▃█▃▇▃█▇▁█▇▁▇▇█▇▇▆▇▇▂█▇▇██▂▇▇█▇▇▇▃▁▇▆▄▇
wandb:      eval/avg_mil_loss ▃▃▆▁▁▁▃▁▄▁▁▅▁▃▃▅▄▁▅▄▁▁▁▁▁▄▁▄█▁▁▁▄▄▃▅▅▇▃▁
wandb:       eval/ensemble_f1 ▇▄▃█▄▇█▇▂█▇▁▇▇▇▇██▇▂▇▇█▇█▂▇▇▁▇█▇▇▄▄▂▆▄▁▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆▇▇▇█▅▇▅▁█▆██▆▆▆▆▅▆▆█▇▅▇▇▅▆▆▂▇▇▅▇▆▇▅▄▅▆▆
wandb:      train/ensemble_f1 ▆▆▆▇█▇▅▇▁▅█▄▆▆▆▆▅▆▆██▇▆▆▇▆▂▇▅▆▆▇▄▆▇▅▄▅▆▆
wandb:         train/mil_loss ▄▄▃▄▆▃▄▅▄▂▃▃▃▆▇▅▃▂▁▄▄▃▃▅▂▃▃▃▃▃▃▃▅█▅▄▂▄█▆
wandb:      train/policy_loss █████████████████████████████▁██████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████████████▁██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90646
wandb: best/eval_avg_mil_loss 0.32798
wandb:  best/eval_ensemble_f1 0.90646
wandb:            eval/avg_f1 0.85032
wandb:      eval/avg_mil_loss 0.37514
wandb:       eval/ensemble_f1 0.85032
wandb:            test/avg_f1 0.84214
wandb:      test/avg_mil_loss 0.18276
wandb:       test/ensemble_f1 0.84214
wandb:           train/avg_f1 0.78801
wandb:      train/ensemble_f1 0.78801
wandb:         train/mil_loss 1.12755
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run restful-sweep-11 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vo5ivjpd
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_092928-vo5ivjpd/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jqfjbzjk with config:
wandb: 	actor_learning_rate: 0.00010266024307139768
wandb: 	attention_dropout_p: 0.2405611729696343
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 106
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2616956315673892
wandb: 	temperature: 1.648445395053364
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093031-jqfjbzjk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-12
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jqfjbzjk
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 98-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ▁▄▂▃▇▃██▁▃▂█▇██▂██▃██▂▇█▃▄████▄▄▂▂▄▄▂██▄
wandb:      eval/avg_mil_loss ▁▂█▁▁▁▃▁▇▄▁▁▁▃▂▁▁▃▁█▆▁▁▃▄▃▄▄▁▃▅▆▃▁▅▃▁▁▁▃
wandb:       eval/ensemble_f1 █▃▁▂████▇▇█▇█▄▃█▇██▂▃█▄▂▁▃█▄█▄█▄▄█▇▄▂███
wandb:           train/avg_f1 ▆▄▄▃▅▃▃▄▄▄▆▁▄▄▃▅▄▂▆▆▂▄▆▃▄▆▃▇█▇▅▄▇▃▆▆▂▆█▄
wandb:      train/ensemble_f1 ▃▄▄▅▃▅▅▃▂▇▆▄▂▄▆▄▄▄▆▂▆▃▇▁▃▇▄▅▆▄▅▁▃▆▄▄▄▆█▃
wandb:         train/mil_loss ▅▅▃▃▁▄▄▃▅▅▃▄▃▆▆▃▅▃▇▄▄▂▅▃█▃▄▄▇▄▃▃▇▄▄▄▃▅▅▃
wandb:      train/policy_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91394
wandb: best/eval_avg_mil_loss 0.28028
wandb:  best/eval_ensemble_f1 0.91394
wandb:            eval/avg_f1 0.65381
wandb:      eval/avg_mil_loss 1.11583
wandb:       eval/ensemble_f1 0.65381
wandb:           train/avg_f1 0.75394
wandb:      train/ensemble_f1 0.75394
wandb:         train/mil_loss 0.76163
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run true-sweep-12 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jqfjbzjk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093031-jqfjbzjk/logs
wandb: ERROR Run jqfjbzjk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: jmf7lt5b with config:
wandb: 	actor_learning_rate: 4.783900197690554e-06
wandb: 	attention_dropout_p: 0.4841232938032552
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 139
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.18035406233942253
wandb: 	temperature: 7.68870466099819
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093158-jmf7lt5b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-13
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jmf7lt5b
wandb: uploading wandb-summary.json
wandb: uploading history steps 133-140, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇██
wandb: best/eval_avg_mil_loss █▆▁▄▅▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇██
wandb:            eval/avg_f1 ▃██▇▇███▇█▇▇▄█▄▁██▇▇▃█▂█▇▃▅█▄█▃▇▇▇█▇▂▇█▃
wandb:      eval/avg_mil_loss ▂▃▆▁▄▁▁▁▁█▇▁▇▁▆▂▁▁▂▃▅▅▆▄▂▃█▁▁▃▂▁▁▃▄▄▄▄▁▃
wandb:       eval/ensemble_f1 ▇█▂█████▁▇▇██▃██▆▄▃███▇▇▂██▄▂▇▄▃█▇▇███▂▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▆█▅▆▅▅▁▅▆▇▄▄▇▃▆▆▄▂▆▃▄▆▇▆▄▇▆▅▆▅▅▁▅▆▆▃▂▄▆▅
wandb:      train/ensemble_f1 ▁▆█▄▄█▅▄▆█▇▂▄▆▄▁▄▅▅▇▃▅▂▄▆▅▇▆▄▄▃▂▆▂▃▂▄▃▄▆
wandb:         train/mil_loss ▃▂▃▅▄▃▂▁▃▄▆▄▄▃▃▃▄▃▆▄▃▄▂▃▂▃▂█▂▄█▅▅▃▃▅▄▃▃▅
wandb:      train/policy_loss ▄▄▁▁▄▄▄▄▄▄█▄▄▄▁▄▆█▄▁▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▁▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄▄▄▄▄▄▄▄▁▄▄▆▄█▄▄▄▁▄█▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9216
wandb: best/eval_avg_mil_loss 0.26256
wandb:  best/eval_ensemble_f1 0.9216
wandb:            eval/avg_f1 0.90438
wandb:      eval/avg_mil_loss 0.30195
wandb:       eval/ensemble_f1 0.90438
wandb:            test/avg_f1 0.88136
wandb:      test/avg_mil_loss 1.15121
wandb:       test/ensemble_f1 0.88136
wandb:           train/avg_f1 0.80866
wandb:      train/ensemble_f1 0.80866
wandb:         train/mil_loss 0.70966
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run distinctive-sweep-13 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/jmf7lt5b
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093158-jmf7lt5b/logs
wandb: Agent Starting Run: eyxkcw0l with config:
wandb: 	actor_learning_rate: 1.1172883419379586e-06
wandb: 	attention_dropout_p: 0.1873729909872514
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 86
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9529914191154164
wandb: 	temperature: 4.176845460679351
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093357-eyxkcw0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-14
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eyxkcw0l
wandb: uploading wandb-summary.json
wandb: uploading history steps 75-86, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁███
wandb: best/eval_avg_mil_loss █▁▁▁
wandb:  best/eval_ensemble_f1 ▁███
wandb:            eval/avg_f1 ▂█▂█▃▁▂▃▄▇█▁▄▇▄▂█▆█▂▅▆▁█▃█▅▇▇█▂█▃█▄█▂█▂▂
wandb:      eval/avg_mil_loss ▆▄▇▇▂▃▂▇▅▄▁▃▅▇▃▇▄▅▅▄▇▄▅▁▅▃▆▅▆▅▁█▅▄▅▅▄▄█▆
wandb:       eval/ensemble_f1 ▂█▃▃▁▁▃▅▂▄▄██▄█▁▂█▂▃▂█▇▇▂▇▁▄▃▂▂▄▇███▄▂▅▃
wandb:           train/avg_f1 ▃█▃▆▄▅▃▅▄▆▄▆▅▄▄▆▅▅▆▅▄▆▃▆▅▆▁▆▄█▇▅█▆▄▃▆▆▆▄
wandb:      train/ensemble_f1 ▆▁▅▅▂▃▁▅▅▆▃▅▄▃▄▃▄▅▄▅▅▅▄▃▆▃▅▄▆▆█▃▆▄▇▆▇▆▆▃
wandb:         train/mil_loss ▅▂▄▆▅▄▇▅▄▄▇▂▇▅▃▆▄█▄▄█▄▂▆▃▅▃▇▃▄▆▅▂▆▁▅▂▂▄▄
wandb:      train/policy_loss ▃▃█▃▃▃██▃▃▃▃▃▃▃▃▃█▃▃▁▃▃▃█▃▃▃▃▃▃▃▃█▃▃▃█▃▃
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▃▃▃▃▃▃▃█▃▃▃▁▃▃▃▁▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91069
wandb: best/eval_avg_mil_loss 0.38449
wandb:  best/eval_ensemble_f1 0.91069
wandb:            eval/avg_f1 0.57209
wandb:      eval/avg_mil_loss 1.67005
wandb:       eval/ensemble_f1 0.57209
wandb:           train/avg_f1 0.68577
wandb:      train/ensemble_f1 0.68577
wandb:         train/mil_loss 0.98708
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dandy-sweep-14 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/eyxkcw0l
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093357-eyxkcw0l/logs
wandb: ERROR Run eyxkcw0l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: c9dr46ky with config:
wandb: 	actor_learning_rate: 7.611665203993396e-05
wandb: 	attention_dropout_p: 0.4879884470965411
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 82
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.96073281361581
wandb: 	temperature: 0.28715318214363905
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093515-c9dr46ky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-15
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/c9dr46ky
wandb: uploading history steps 72-82, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▁▁▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ▇▃▇█▁█▇▄▇▂▃▇▇▃▇▇██▇▇▆▆▇█▇█▇▇██▇▇▇██▃▄▅▃▂
wandb:      eval/avg_mil_loss ▅▄▁▁█▅▄▃▂▃▂▁▁▅▅▁▁▃▄▁▁▁▂▁▁▁▃▄▁▁▁▃▁▁▁▂▁▃▂▁
wandb:       eval/ensemble_f1 ▃▇▇█▁▇▇▄▇▂▃▇▃█▃█▃█▇▇▆▇██▂█▇█▇▇▇▇█▇▃▇▅▃▄▇
wandb:           train/avg_f1 ▆▅▄▂▅▆▃▂▃▃▃▄▄▆▄▂▅▂▂▆▆▄▃▃▆▄▅▃▄▅▄▅▅▄▆▃▅▁▃█
wandb:      train/ensemble_f1 ▅▆▄▂▃▃▃▄▄▃▃█▄▅▃▃▇▂▃▅▆▄▄▄▆▅▃▁▄▅▁▆▃▆▅▂▃▁▄█
wandb:         train/mil_loss ▂▅▄▁▂▅▄▆▃▄▃▃▃▅▅▄▅▅▂▃▃▅▄▁▆▇▅▄▂▆▂█▃▂▅▃▃▁▃▄
wandb:      train/policy_loss ██████████████████████████████████████▁█
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▄▄▄█▁▄▄▁▁▄▁▁▁▄▄█▄█▁▁▄▄██▄▄█▄█▄▄█▁▄▁█▄▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91758
wandb: best/eval_avg_mil_loss 0.25373
wandb:  best/eval_ensemble_f1 0.91758
wandb:            eval/avg_f1 0.89279
wandb:      eval/avg_mil_loss 0.3291
wandb:       eval/ensemble_f1 0.89279
wandb:           train/avg_f1 0.89844
wandb:      train/ensemble_f1 0.89844
wandb:         train/mil_loss 0.36859
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run floral-sweep-15 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/c9dr46ky
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093515-c9dr46ky/logs
wandb: ERROR Run c9dr46ky errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5tnowm5w with config:
wandb: 	actor_learning_rate: 1.2914001917263224e-05
wandb: 	attention_dropout_p: 0.28544454523367135
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 199
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5473075968264228
wandb: 	temperature: 6.055832141287913
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093638-5tnowm5w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-16
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5tnowm5w
wandb: uploading history steps 97-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃█
wandb: best/eval_avg_mil_loss █▁▇
wandb:  best/eval_ensemble_f1 ▁▃█
wandb:            eval/avg_f1 ▇██████████████████████▆███████▁████████
wandb:      eval/avg_mil_loss ▂█▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▂▂▁▁▂▂▂
wandb:       eval/ensemble_f1 █▁██████████████████████████████████████
wandb:           train/avg_f1 ▅▃▆▆▄▅▆▆▅▆▆▂▆▅▅▅▆▆▄▇▇▆▇▄▆▇█▇▇▇▁▆▇▇█▇█▃▆▇
wandb:      train/ensemble_f1 █▄▇▆▆▆▅▆█▆▅▇▂▂▇▆█▆▇▇▆▇▄▅▆▇▇█▇▆▅▁▆▃▆▇▄▇▄▇
wandb:         train/mil_loss ▄▄▄▃▇█▅▃▄▄▅▃▅▇▄▄▁▃▆▃▃▆▃▄▇▃▃▅▃▃▁▃▃▄▂▃▃▂▆▄
wandb:      train/policy_loss ▅▅█▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▁▅▅▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████▁██████████████████████▁███████▁██
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91373
wandb: best/eval_avg_mil_loss 0.35477
wandb:  best/eval_ensemble_f1 0.91373
wandb:            eval/avg_f1 0.90599
wandb:      eval/avg_mil_loss 0.35803
wandb:       eval/ensemble_f1 0.90599
wandb:           train/avg_f1 0.90372
wandb:      train/ensemble_f1 0.90372
wandb:         train/mil_loss 0.29869
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run smart-sweep-16 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/5tnowm5w
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093638-5tnowm5w/logs
wandb: ERROR Run 5tnowm5w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: s7eu13fm with config:
wandb: 	actor_learning_rate: 0.0005429441428349776
wandb: 	attention_dropout_p: 0.26187093801785183
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 184
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3447506534582091
wandb: 	temperature: 0.20998940817962383
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_093812-s7eu13fm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-17
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s7eu13fm
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇██
wandb: best/eval_avg_mil_loss █▃▁▂
wandb:  best/eval_ensemble_f1 ▁▇██
wandb:            eval/avg_f1 █▇▅▆▆▂▅▃▃▁█▂█▇▂▃█▆█▂▃██▂▄▄█▁▆▅▇▂█▄▂▇██▃▇
wandb:      eval/avg_mil_loss ▅▁▂▂▂▅▁█▁▅▁▆▅▁█▅▅▅▁▅▇▇▂▁▅▂▆▅▁▃▁▇▆▂▁▁▂▆▅▁
wandb:       eval/ensemble_f1 ████▆▁▇▂█▄▆▄▆▃▇██▂▄▃▃█▆▆█▄▄▇▄▃█▅▃▄▇█████
wandb:           train/avg_f1 ▄▂▄▇▃▄▁▅▇▅▆▇▅▂▄▄█▁▃▂▆▅▄▆▇▇▆▅▅▃▅▅▄▅▅▄▅▃▆▆
wandb:      train/ensemble_f1 ▁▁▅▇▃█▃▄▅▆▆▃█▄▃▅▅▅▆▆▅▃▇▇▇▅▃▆▆▅▄▆▃▅▇▄▃▅▄▆
wandb:         train/mil_loss ▃▃▄▃▄▅▄▂▄▂▅▄▂▃▄▂▄▃▅▅▂▅▃▃█▄▅▃▁▅▄▄▄▂▂▃▅▃▃▄
wandb:      train/policy_loss █▁█████████████████▅███████████▁████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████▁█▅█▁█████▁██████████▁████▁████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91758
wandb: best/eval_avg_mil_loss 0.34591
wandb:  best/eval_ensemble_f1 0.91758
wandb:            eval/avg_f1 0.78501
wandb:      eval/avg_mil_loss 1.44905
wandb:       eval/ensemble_f1 0.78501
wandb:           train/avg_f1 0.79751
wandb:      train/ensemble_f1 0.79751
wandb:         train/mil_loss 1.06679
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sweepy-sweep-17 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/s7eu13fm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_093812-s7eu13fm/logs
wandb: ERROR Run s7eu13fm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: uxvrxflz with config:
wandb: 	actor_learning_rate: 7.61640452576267e-05
wandb: 	attention_dropout_p: 0.3315159986110641
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 67
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7903026128452927
wandb: 	temperature: 6.3327780278767465
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094042-uxvrxflz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-18
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uxvrxflz
wandb: uploading history steps 52-68, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▇███
wandb: best/eval_avg_mil_loss █▄▁▄▁▁
wandb:  best/eval_ensemble_f1 ▁▁▇███
wandb:            eval/avg_f1 ▅▅██▃▆▃▂▁▅▆▇▅▂▃▅█▂▃▇▅█▅▆▅▅▄█▆▂▆█▃▅▅▃▅▄▄▅
wandb:      eval/avg_mil_loss ▇▄▅▁▄▆▄▆▅▆▅▅▂▇█▃▆▇▄▃▅▄▁▄▄▆▅▁█▃▆▁▆▇▆▄▅▇▇▇
wandb:       eval/ensemble_f1 ▅▅▄▃▃▂▁▅▆▆▂▃▃▂▅▅▃▆▇▂▅█▅▅▆▆█▃▂▆▆▅▅▅▆▄▃█▄▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▆▆▆▃▃▅▄▄▄▃▅▅▂▇▆▄▃▅▅▄▃▃▃▄▂▃▄▄▃▆▆▄▄▃▁▆█▅
wandb:      train/ensemble_f1 ▅▇▆▇▇▄▄▆▅▅▆▃▇█▇▅▄▆▆▄▅▄▄▅▃▃▅█▂▇▄▁▅▂▅▅▄▇▆▆
wandb:         train/mil_loss ▆▅▅▃▅▄▆▂▇▂▅▆▅▃▅▇▄▆▄▅▄▃▇▄▇▃▃▂▁▃▇▁█▅▃▃█▇▃▃
wandb:      train/policy_loss █████████████████████████████▁██████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████████████████████▁███████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90018
wandb: best/eval_avg_mil_loss 0.28763
wandb:  best/eval_ensemble_f1 0.90018
wandb:            eval/avg_f1 0.7422
wandb:      eval/avg_mil_loss 1.09002
wandb:       eval/ensemble_f1 0.7422
wandb:            test/avg_f1 0.72624
wandb:      test/avg_mil_loss 1.63631
wandb:       test/ensemble_f1 0.72624
wandb:           train/avg_f1 0.72518
wandb:      train/ensemble_f1 0.72518
wandb:         train/mil_loss 1.5003
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run cool-sweep-18 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/uxvrxflz
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094042-uxvrxflz/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 57ejpc1r with config:
wandb: 	actor_learning_rate: 1.6045296968276115e-06
wandb: 	attention_dropout_p: 0.40250382391121936
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 104
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.06872622173887477
wandb: 	temperature: 1.9738155430297888
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094154-57ejpc1r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-19
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/57ejpc1r
wandb: uploading history steps 98-105, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇███
wandb: best/eval_avg_mil_loss █▁▁▂▁
wandb:  best/eval_ensemble_f1 ▁▇███
wandb:            eval/avg_f1 ▇▇▇▆▆▆▆▆▆▇▅█▇▆▇▆▁▆▃█▇▇▆█▆█▆▆▆▆▅▆▆▆▄▇▇▇▆▆
wandb:      eval/avg_mil_loss ▃▄▃▃▃▃▂▅▂▂▂▂▁▇▄▂▇▂▂▂▃▂▂▂▁▁▂▂▃▃▂█▂▁▂▃▁▃▂▂
wandb:       eval/ensemble_f1 █▇▇▆▇▆▇▆▆█▇▆▇▆▁▅█▃▇▁▆█▆▆█▆▅▆▆▁▄▅▇▇▅▆▇▇▆▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▃▅▆▁▆▄▅▅▃▄▅▃▇▅▆▅▂▃▄▆▅▅▆▃▁▄▆▄▅▆▃▃▆▄▆▄▇▃█
wandb:      train/ensemble_f1 ▄▂▃▆▄▇▇▆▅▆▆▄▅▄▃▃▅▃█▄▇▁▆▄▂▇▃▆▃▅▅▃▄▁█▂▄▆█▅
wandb:         train/mil_loss ▃▃█▁▄▇▁▂▃▂▃▇▁▃▄▄▄▂▂▄▇▆▂▃▃▇▂▂▃▂▅▃▄▄▂▁▂▁▅▃
wandb:      train/policy_loss ▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████▃▁████████████████████████████▃███
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91845
wandb: best/eval_avg_mil_loss 0.24984
wandb:  best/eval_ensemble_f1 0.91845
wandb:            eval/avg_f1 0.89558
wandb:      eval/avg_mil_loss 0.28583
wandb:       eval/ensemble_f1 0.89558
wandb:            test/avg_f1 0.9141
wandb:      test/avg_mil_loss 0.24262
wandb:       test/ensemble_f1 0.9141
wandb:           train/avg_f1 0.92089
wandb:      train/ensemble_f1 0.92089
wandb:         train/mil_loss 0.25777
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run electric-sweep-19 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/57ejpc1r
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094154-57ejpc1r/logs
wandb: Agent Starting Run: d99fxs4f with config:
wandb: 	actor_learning_rate: 0.0001370305254997265
wandb: 	attention_dropout_p: 0.16939373118113216
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 100
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.45898401470441386
wandb: 	temperature: 7.518377484204125
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094353-d99fxs4f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-20
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d99fxs4f
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁██
wandb: best/eval_avg_mil_loss █▁▁
wandb:  best/eval_ensemble_f1 ▁██
wandb:            eval/avg_f1 █████▇██▂██▇█▆█▃▄▇█████▂▃▇██▇██▇▁▂▃▁█▇██
wandb:      eval/avg_mil_loss ▁▁▁▁▄▁▆▆▁▁▁▁▁▁▃▁▁▄▃▄▆▇▁▄▁▁▅▁▄▁▁▅▁█▃▁▁▁▁▁
wandb:       eval/ensemble_f1 ▂███▇▇▁█████▅▇█▂█▁▂██▁█▁▃███▄▂█▇██▇█▃███
wandb:           train/avg_f1 ▅▂▇▅▂▅▆▁▃▆▇▅▄▅▅▇▄▆▃▇▆▅▆▆▁▄▆▄█▅▆▆▄▄▄▄▅▃▅█
wandb:      train/ensemble_f1 ▇▅▂▇█▄▃▁▇▇▅▄▃▇▇▃▇▅▅▆▁▄▇▅▄█▅▄▆▅▄▂▆▆▅▆▃▅▄▅
wandb:         train/mil_loss ▅▄▂▄▃▃▃▂▂▅▄▃▁▂▂▄▃▂▂▄▄▅▂▂▂▃▄▄▅▆▃▂▄▃▅▄▂█▂▂
wandb:      train/policy_loss ██▄██▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▁▄▁▄▄▄█▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅█▅█▅▅▅▅▅▁▅▁▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅█▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91778
wandb: best/eval_avg_mil_loss 0.28572
wandb:  best/eval_ensemble_f1 0.91778
wandb:            eval/avg_f1 0.91778
wandb:      eval/avg_mil_loss 0.2839
wandb:       eval/ensemble_f1 0.91778
wandb:           train/avg_f1 0.84421
wandb:      train/ensemble_f1 0.84421
wandb:         train/mil_loss 0.47214
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run olive-sweep-20 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/d99fxs4f
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094353-d99fxs4f/logs
wandb: ERROR Run d99fxs4f errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hiynwmf6 with config:
wandb: 	actor_learning_rate: 0.00013187692182470658
wandb: 	attention_dropout_p: 0.4335472372877428
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 165
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.19405331884230503
wandb: 	temperature: 8.127329669569884
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094526-hiynwmf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-21
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hiynwmf6
wandb: uploading wandb-summary.json
wandb: uploading history steps 154-166, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆▇█
wandb: best/eval_avg_mil_loss █▆▆▁▁
wandb:  best/eval_ensemble_f1 ▁▆▆▇█
wandb:            eval/avg_f1 ███▇████▇██▁▁▁█▁█▃██████████████▇▇▇▇████
wandb:      eval/avg_mil_loss ▄▄▁▄▂▁▁▁█▆▂▂▄▂▁▄▁▂▂▁▂▂▂▁▁▂▁▅▁▁▁▂▂▄▁▄▂▂▂▁
wandb:       eval/ensemble_f1 █████▂█▄▇▁██▇█▁██▃██████▇█████▄█▇▇▇█▇█▅█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▇▆▆▇▅▆▆▇▃▆▇▆▅▇▇█▄▆▇▇▃▄█▇█▆▄▂▃▆▇▇▃▅▁▇▅█
wandb:      train/ensemble_f1 ▇▇▄█▅▅▇█▄▇▆▅█▇▅▅▂▅▇▆▇▅▄▅▄█▇▆▇█▅▄▆▇▅█▁▇▅▇
wandb:         train/mil_loss ▁▂▄▃▃▅▅▃▃▃▂▁▁▂▄▁▁▃▂▄▁▁▁▂▁▄▁▁▁▁▄▅▃█▃▁▂▁▁▃
wandb:      train/policy_loss ▁█▁▅▅▅▅▅▅▅▁▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▁▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▁▅▅▅▅▁▅▅▅▅▅▅▅▅▁█▅█▅▅▅▅▁▅▅▁█▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92542
wandb: best/eval_avg_mil_loss 0.24626
wandb:  best/eval_ensemble_f1 0.92542
wandb:            eval/avg_f1 0.91394
wandb:      eval/avg_mil_loss 0.27131
wandb:       eval/ensemble_f1 0.91394
wandb:            test/avg_f1 0.92596
wandb:      test/avg_mil_loss 0.12897
wandb:       test/ensemble_f1 0.92596
wandb:           train/avg_f1 0.86619
wandb:      train/ensemble_f1 0.86619
wandb:         train/mil_loss 0.66241
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run classic-sweep-21 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/hiynwmf6
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094526-hiynwmf6/logs
wandb: Agent Starting Run: 3urx7w4z with config:
wandb: 	actor_learning_rate: 6.762336624211239e-05
wandb: 	attention_dropout_p: 0.4441611410334853
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 137
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9684900351198646
wandb: 	temperature: 4.020802803396341
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_094742-3urx7w4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-22
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3urx7w4z
wandb: uploading history steps 126-138, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▆▇▇█
wandb: best/eval_avg_mil_loss ▆█▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▁▆▇▇█
wandb:            eval/avg_f1 ▂▃▂▄▄█▃▁▆▆▁▇▆█▃▆▇▄▅▇█▇▇▇▁▄▆▇▆▇▅▇▇▄█▆▂▅▆▄
wandb:      eval/avg_mil_loss ▂▂▅▃▅▇▂▅▂▁▆▂▆▁▂▂▂▅▂▃▁▂▁▆▂▂▁▅▂▆█▅▂▅▅▂▆▅▅▆
wandb:       eval/ensemble_f1 ▅▇█▇▆▅▇▅▄▇▃▇▇▇▁███▇▆▆▇▃▃▄█▆▄▅███▆█▆▇▇▂▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▁▇▅▁▄▆▆▃▅█▃▅▇▄▇▆▄▇▇▄▄▃▄▇▅▅▅▇▁▆▇▇▅▄▅▅██▇
wandb:      train/ensemble_f1 ▅▄▃▃▄▄▂▅▄▅▅▄▅▇▆▄▅▃▃▄▄▁█▄▄▅▄▅▄▅▆▅▃▆▅▄▇▆▇▅
wandb:         train/mil_loss ▃▅▂▆▄▅▁▄▅▆▅▂▅▄█▆▆▄▅▃▃▆▅▆▅▆▅▅▂▂█▄▃▄▄▇▆▆▄▃
wandb:      train/policy_loss ███▂█████████████████▁██████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92261
wandb: best/eval_avg_mil_loss 0.24257
wandb:  best/eval_ensemble_f1 0.92261
wandb:            eval/avg_f1 0.77933
wandb:      eval/avg_mil_loss 0.78973
wandb:       eval/ensemble_f1 0.77933
wandb:            test/avg_f1 0.87293
wandb:      test/avg_mil_loss 0.27112
wandb:       test/ensemble_f1 0.87293
wandb:           train/avg_f1 0.84597
wandb:      train/ensemble_f1 0.84597
wandb:         train/mil_loss 0.60393
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run splendid-sweep-22 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/3urx7w4z
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_094742-3urx7w4z/logs
wandb: Agent Starting Run: m3xx7wg1 with config:
wandb: 	actor_learning_rate: 0.0002351829971807688
wandb: 	attention_dropout_p: 0.35102959265826617
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 56
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8553385289439946
wandb: 	temperature: 0.027662860250360355
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095017-m3xx7wg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-23
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/m3xx7wg1
wandb: uploading history steps 53-56, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 █▄▇█▆▇▇█▇▆▆▇▇▇▅▇█▇▇█▇▁▇▇▇▇▇▇▄█▇▇▇▅▅▇▃▇▅▇
wandb:      eval/avg_mil_loss ▂▂▁▂▁▁▁▁▂▂▁▁▃▂▁▂▁▂▁█▂▁▂▁▂▂▂▁▁▁▂▁▂▂▂▁▁▁▃▁
wandb:       eval/ensemble_f1 █▅▇▇███▆█▇█▇▅▇█▇█▇▁▄█▇█▇▇▇▄██▇█▇▇▅▅▇▄█▇▆
wandb:           train/avg_f1 ▅▅▄▇▄▇▆▆▆▅▂▁▅▅▅▆▅▅▄▅█▅▆▄▅▂▂▂▃▅▅▃▆▆▄▅▅▂▆▅
wandb:      train/ensemble_f1 ▅▅▃▄▇▆▆▄▇▅▄▅▄▆▂▆▅▅▄▅█▅▆▆▅▂▁▂▄▅▅▃▆▄▃▅▄▁▆▄
wandb:         train/mil_loss ▂▇▂▅█▃▃▆▃▁▂▂▃▁█▄▇▂█▇▁▅▂▂▂▆▂▂▃▃▂▂▂▆▂▇▃▅▂▂
wandb:      train/policy_loss █▅▅▁▅▅▅█▁▅▁▅▅▁▅▅█▅█▅▅▁▁▁▅▅█▅▁█▁▅█▅▅▅▅▁▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▁▄█▄█▁▄▄▁▄▁███▄▁▄▁▁▄▄▄▄▄▁█▄▁█▄▄▁▄▄▁█▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91758
wandb: best/eval_avg_mil_loss 0.25784
wandb:  best/eval_ensemble_f1 0.91758
wandb:            eval/avg_f1 0.89312
wandb:      eval/avg_mil_loss 0.28897
wandb:       eval/ensemble_f1 0.89312
wandb:           train/avg_f1 0.88406
wandb:      train/ensemble_f1 0.88406
wandb:         train/mil_loss 0.27883
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run soft-sweep-23 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/m3xx7wg1
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095017-m3xx7wg1/logs
wandb: ERROR Run m3xx7wg1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 747zbtng with config:
wandb: 	actor_learning_rate: 5.446871493996085e-05
wandb: 	attention_dropout_p: 0.24088806064303053
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 94
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9471846256307596
wandb: 	temperature: 0.0902846941456248
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095118-747zbtng
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-24
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/747zbtng
wandb: uploading wandb-summary.json
wandb: uploading history steps 77-95, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁
wandb: best/eval_avg_mil_loss ▁
wandb:  best/eval_ensemble_f1 ▁
wandb:            eval/avg_f1 ▃▇█▂▇▃▇▃▇█▆▇█▄█▇▃█▁▃▃▇▅▂▇▇▇▇█▆▂▇██▇▇▇▇▇█
wandb:      eval/avg_mil_loss ▁▄▄▆▄▁█▄▁▄▄▄▆▁▁▁▁▁▆▁▅▆▁▁▄▄▄▆▅▃▆▂▁▁▁▂▁▄▄▄
wandb:       eval/ensemble_f1 ▇▃█▂▇▇██▃▆▃▆▇▆▅▁▇█▁▇▂▄█▇▇▆▁▇█▃█▇▇▇█▇█▇██
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▂▂▄▁▃▂▂▃▅▂▃▅▃▇▆█▃▄▅▄▃▄▃▂▅▅▄▁▅▂▄▅▃▁▅▃▃▂▅▅
wandb:      train/ensemble_f1 ▃▄▂▅▄▅▂▁▃▅▆█▅▅▅▃▅▁▄▇▅▃▃▆▆▂▇▆▅▃▂▃▆▅▆▅▄▃▄▄
wandb:         train/mil_loss ▆▅▁▆▄▅▅▃▃▂▂█▄▃▃▄▅▃▆▂▄▆▅▁▂▄▃▃▄▅▃▃▄▄▃▄▁▂▂▄
wandb:      train/policy_loss █████████████████████████████████▁██████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.90599
wandb: best/eval_avg_mil_loss 0.4274
wandb:  best/eval_ensemble_f1 0.90599
wandb:            eval/avg_f1 0.89963
wandb:      eval/avg_mil_loss 0.3237
wandb:       eval/ensemble_f1 0.89963
wandb:            test/avg_f1 0.93076
wandb:      test/avg_mil_loss 0.24036
wandb:       test/ensemble_f1 0.93076
wandb:           train/avg_f1 0.70968
wandb:      train/ensemble_f1 0.70968
wandb:         train/mil_loss 0.66249
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run royal-sweep-24 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/747zbtng
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095118-747zbtng/logs
wandb: Agent Starting Run: byxpzky8 with config:
wandb: 	actor_learning_rate: 0.00099198934739014
wandb: 	attention_dropout_p: 0.26310195958770494
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 132
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9115050412175368
wandb: 	temperature: 8.209561595887441
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095241-byxpzky8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-25
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/byxpzky8
wandb: uploading history steps 125-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▅█
wandb: best/eval_avg_mil_loss █▇▃▁
wandb:  best/eval_ensemble_f1 ▁▅▅█
wandb:            eval/avg_f1 ▇▇▅▅▆█▇▇▁▇▇▇█▇█▅▇▇▇█▇▇▆▅█▇▄▄▅█▆▇▇▆▇▄▇▇▇▇
wandb:      eval/avg_mil_loss ▁▅▅▁█▂▁▁▅▁▁▁▄▁▄▄▄▃▃▃▅▄▁▁▂▁▂▄▅▁▄▂▁▅▁▁▃▁▁▁
wandb:       eval/ensemble_f1 ▅▇▄▄▇▅▁█▇█▇██▅█▇▇█▆██▆▃█▅▃▂▇▇▃▅▆▆▅▃▄▇█▇▇
wandb:           train/avg_f1 ▄▅▄▂▅▆▅▄▅▅▄▆▄▆▅▇▃▆▆▆▇▅▅▆▃▃▅▅▁▅▃▃█▃▆▆▂▇▃▆
wandb:      train/ensemble_f1 ▅▆▆▁▅▅▆▇▅▃▃▅▄▃▅▆▅█▄▆▄▅▄▆▆▃▃▆▅▆▆▅▆▆▅▆▄▇▅▅
wandb:         train/mil_loss ▂▆▄▅▅▅▂▁▁▆▄▃▃▇▃▅▄▃▅▅▇▅▃▂▃▅▄▂▃▂▆▅▇█▃▂▄▄▃▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.23227
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.9038
wandb:      eval/avg_mil_loss 0.28378
wandb:       eval/ensemble_f1 0.9038
wandb:           train/avg_f1 0.82635
wandb:      train/ensemble_f1 0.82635
wandb:         train/mil_loss 0.41568
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run dauntless-sweep-25 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/byxpzky8
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095241-byxpzky8/logs
wandb: ERROR Run byxpzky8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 92ttvk4t with config:
wandb: 	actor_learning_rate: 3.1793815679197725e-06
wandb: 	attention_dropout_p: 0.3471022333229003
wandb: 	attention_size: 128
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 180
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4994279916604113
wandb: 	temperature: 6.976248560292171
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095511-92ttvk4t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-26
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/92ttvk4t
wandb: uploading history steps 175-180, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▆▇██
wandb: best/eval_avg_mil_loss ▆█▂▇▁▁
wandb:  best/eval_ensemble_f1 ▁▄▆▇██
wandb:            eval/avg_f1 ▃▁▇▄▇▇█▃▃▅▃▂▃▅▇▅▄▄▇▆▄▇▅▃▆▇▅▃▅▆▂▂▄▆▁▅▄▅▆▅
wandb:      eval/avg_mil_loss ▃▇▅▄▆▆▁▂█▄▄▄▄▅▄▅▆▃▁▁▁▂▅▆▅▁▅▂▅▃▃▄▅▃▁█▆▄▃▃
wandb:       eval/ensemble_f1 ▂▂▄▇▁▃▂▃▅▅▆▂▅█▅▅▃▄▇▃▄█▅▂▂▅▂▅▇▅▅█▂▅▄▇▆▅▇▂
wandb:           train/avg_f1 ▇▄▅▅▅█▆▆▄▅▆▂▃▇▃▁▅▆▅▆▄▁▇▄▇▂▃▃▅▃▅▄▆█▆▃▁▄▅█
wandb:      train/ensemble_f1 ▂▅▄▅▄▇▃█▆▆█▅▄▅▇▅▃▇▆▁▃▄▇▂▃▄▄▃▆▅▄▅▄▂▄▂▅▄▅▇
wandb:         train/mil_loss ▇▅█▃▅▂█▆▅▄█▄▄▂▃▄▄▄▃█▄▄▅▇▂▃▆▇▇▅▃▅▆▄▁▃▂▆▃▂
wandb:      train/policy_loss █████████████████▁██████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▁██████████████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91148
wandb: best/eval_avg_mil_loss 0.29963
wandb:  best/eval_ensemble_f1 0.91148
wandb:            eval/avg_f1 0.89046
wandb:      eval/avg_mil_loss 0.33158
wandb:       eval/ensemble_f1 0.89046
wandb:           train/avg_f1 0.74767
wandb:      train/ensemble_f1 0.74767
wandb:         train/mil_loss 0.97771
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run absurd-sweep-26 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/92ttvk4t
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095511-92ttvk4t/logs
wandb: ERROR Run 92ttvk4t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: fu2ndrax with config:
wandb: 	actor_learning_rate: 3.807382116395525e-06
wandb: 	attention_dropout_p: 0.26214818620141983
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 152
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.9732489516228262
wandb: 	temperature: 7.863696308005664
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_095751-fu2ndrax
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-27
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fu2ndrax
wandb: uploading history steps 137-152, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇▇██
wandb: best/eval_avg_mil_loss █▃▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▆▇▇██
wandb:            eval/avg_f1 ▄▄▅▅▅▆▆▇▃▅▆▇▁▁▄▅██▆▆█▅▇▅▄▅▄▆▅▄▇█▅▇▅▁▇▆▅▅
wandb:      eval/avg_mil_loss ▂▅▅▄▄▅▃▁▁▅▆▄▁▇▁▁▂▆▁▆▅▆▁▆▄▃▄▃▂▃█▁▂▆▄▄▄▁▃▃
wandb:       eval/ensemble_f1 ▇▆▅▅▄▄▇▅▃▇▄▂▄▁▇▅▆▇██▆▆▆▆█▆█▆▆▂█▃█▇█▄▇▇██
wandb:           train/avg_f1 ▇▂▄▄▁▃▅▄▄▅▃▂▂▂▆▄▃▇█▇▃▄▆▇▅▄▆▄▅▆▅▇▂▆▇▂▆█▆▄
wandb:      train/ensemble_f1 ▇▄▃▅▇▅▆▅▂▆▅▂▃█▅▇▃█▃▁▇▇▆▂▇▆▃▆█▄▆▇▄▄▇▄▇▇▁▇
wandb:         train/mil_loss ▄▅▆▅▅▅█▆▄▂▂▃█▆█▅▄▅▆▃▄▅▅▄▂▃▅▅▆▄▃▆▄▆▃▄▇▇▆▁
wandb:      train/policy_loss █████████████████▁████▁█▆███████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▁▄███████████████▇███████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.88319
wandb: best/eval_avg_mil_loss 0.34184
wandb:  best/eval_ensemble_f1 0.88319
wandb:            eval/avg_f1 0.86131
wandb:      eval/avg_mil_loss 0.76871
wandb:       eval/ensemble_f1 0.86131
wandb:           train/avg_f1 0.77393
wandb:      train/ensemble_f1 0.77393
wandb:         train/mil_loss 0.89076
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run devoted-sweep-27 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/fu2ndrax
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_095751-fu2ndrax/logs
wandb: ERROR Run fu2ndrax errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 0foc5r86 with config:
wandb: 	actor_learning_rate: 8.915415213062558e-06
wandb: 	attention_dropout_p: 0.0730310987481913
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 132
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.662167154854286
wandb: 	temperature: 8.934936714030703
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100011-0foc5r86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-28
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0foc5r86
wandb: uploading history steps 124-133, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃██
wandb: best/eval_avg_mil_loss ▇█▇▁▂
wandb:  best/eval_ensemble_f1 ▁▂▃██
wandb:            eval/avg_f1 ▅▂▂█▂▆▅▄▄▇▃▅▆▅▆▇▅█▅▆▁▄▅▄▃▇▄▁▃▆▄▆▆▅▄▇▅▅▅▅
wandb:      eval/avg_mil_loss ▃▃▆▁▅▃▃▅▂▄▃▆▁▅▄▅▅▇▂▆▂▃▃█▆▅▃▁▁▄▆▅▅▁▅▆▅▅▃▂
wandb:       eval/ensemble_f1 ▅▆▅▆▄▅▆▅▅▆▅▄▄▁▆▅▅▄▃▃▅▇█▇▂▃▆▇▄▅█▇▅▆▅▅▆▅▆▃
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▁▁▇▄▄▄█▅▅▅▄▅▃▄▄▄▄▄▆▆▄▆▂▆▄▆▃▄▄█▇▄▄▆▃▆▃▃
wandb:      train/ensemble_f1 ▄▄▂▇▅▁▇▇▄▅▄▅▅▄▅▇▃▅█▇█▆█▄▇▅▂█▄▇▄▆▆▆▆▅▆▆▁▃
wandb:         train/mil_loss ▃█▆▃▅▁▃▂▃▅▄▃▁▆▆▁▅█▄▅▇▃▄▂▄▃▄▂▂▃▃█▃▂▆▂▄▅▄▄
wandb:      train/policy_loss ██▄█▄▄▁▄▁██▁▄▄▁▄▁▁█▄▁▁▁▄▄█▁▄██▁▁██▄█▁█▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████████▁█████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.30824
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.84296
wandb:      eval/avg_mil_loss 0.52569
wandb:       eval/ensemble_f1 0.84296
wandb:            test/avg_f1 0.69317
wandb:      test/avg_mil_loss 1.25316
wandb:       test/ensemble_f1 0.69317
wandb:           train/avg_f1 0.74129
wandb:      train/ensemble_f1 0.74129
wandb:         train/mil_loss 0.79157
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run peach-sweep-28 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/0foc5r86
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100011-0foc5r86/logs
wandb: Agent Starting Run: ya3t05rk with config:
wandb: 	actor_learning_rate: 1.0595850878935249e-06
wandb: 	attention_dropout_p: 0.4999530863901343
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 171
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.7793793509352421
wandb: 	temperature: 2.7842751263856105
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100240-ya3t05rk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-29
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ya3t05rk
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading history steps 131-132, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇▇█
wandb: best/eval_avg_mil_loss █▁▁▂▁
wandb:  best/eval_ensemble_f1 ▁▇▇▇█
wandb:            eval/avg_f1 █▃██▇█▇█▇█▇█▁██▁█▂▁█▇█▃██▂██▁█▇▂▂▄███▆██
wandb:      eval/avg_mil_loss ▄▄▅▅▄▁▁▁▁▁▁▄▁▁█▁▁▁▁▁▂▁▄▄▁▅▁▄▁▁▃▃▁▁▄▁█▄▁▄
wandb:       eval/ensemble_f1 ▇█▂▇▂██▇████████▅█▁▃▇█▇█▂▆▂▇▇█▇▂▇▄▂▃▃▂█▂
wandb:           train/avg_f1 ▅▂▃▅▂▇▆▃▇▁▂▅█▆▅▃▅▄▇█▇▆▅▇▃▅▅▅▃▇▅▃█▂▇▁▄▄▃█
wandb:      train/ensemble_f1 ▆▄▅▇▅▇▇▄▇█▅▇▇█▆▆█▅▇▅▅▇▆▇█▆▇▄▇▄▇▆▃▇▁▅▅▆▅█
wandb:         train/mil_loss ▂▅▃▁▅▂▅▁▃▃▅▂▃▆█▃▁▂▄▃▂▄▄▆▄▄▅▃▄▄▃▃▅▆▂▅█▅▄▂
wandb:      train/policy_loss ▄▄▄▄██▁▄▄▄▄▁▄▄▄▄▄▄▄▁▄▄▄▄▄▄▄▄█▄▄▄▁▄▄█▄▄▄▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.26247
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.87779
wandb:      eval/avg_mil_loss 1.61995
wandb:       eval/ensemble_f1 0.87779
wandb:           train/avg_f1 0.82099
wandb:      train/ensemble_f1 0.82099
wandb:         train/mil_loss 0.76401
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run helpful-sweep-29 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ya3t05rk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100240-ya3t05rk/logs
wandb: ERROR Run ya3t05rk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: ERROR Error while calling W&B API: Post "http://anaconda2.default.svc.cluster.local/search": read tcp 10.53.65.6:53154->10.55.247.53:80: read: connection reset by peer (<Response [500]>)
wandb: Job received.
wandb: Agent Starting Run: rfezlh46 with config:
wandb: 	actor_learning_rate: 7.448897603011291e-05
wandb: 	attention_dropout_p: 0.32510724629189003
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2985923617972627
wandb: 	temperature: 9.403746102373605
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100501-rfezlh46
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-30
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rfezlh46
wandb: uploading wandb-summary.json
wandb: uploading history steps 38-53, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▂▃▃█
wandb: best/eval_avg_mil_loss ▂█▄▂▁▂
wandb:  best/eval_ensemble_f1 ▁▂▂▃▃█
wandb:            eval/avg_f1 █▄█▂█▁█▇█▇▂▇█▇▇█▄▃█▂█▁█▇▇█▂█▇▃█▂▇▂▇█▂▃▄▇
wandb:      eval/avg_mil_loss ▁▃▁▄▁▁▅▃▃▅▅▅▃▅▃▃▃▅▇▅█▁▄▅▄▄▁▄▃▃▄▃▇▄▅▂▁▅▄▅
wandb:       eval/ensemble_f1 ▄█▂█▁█▇▇█▂█▇▇▄█▃█▂▇▁▇▇▇█▁▇▃▃█▂▂▇▁█▂▄▃▂▂█
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▃▃▆▄▄▇▅▂▆▇▆▃▅▆▁▆▇▄▄▄▅█▃▆▅▃▆▅▆▄▅▄▆▄▃▄▆▅▆
wandb:      train/ensemble_f1 ▄▂▃▆▃▃▇▅▁▆▅▂▄▆▄▄▇▃▃▄▅█▃▆▄▅▅▅▆▆▅▃▆▅▄▂▄▆▄▆
wandb:         train/mil_loss ▄▂▄▃▃▃▄▆▄█▆▅▃▅▂▁▃▂▆▄▃▄▄▂▃▄▄▃▃▃▄▄▄▁▄▁▄▆▃▄
wandb:      train/policy_loss ▁██▆███████████████████████████████████▄
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁██▆███████████████████████████████████▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91069
wandb: best/eval_avg_mil_loss 0.29639
wandb:  best/eval_ensemble_f1 0.91069
wandb:            eval/avg_f1 0.88575
wandb:      eval/avg_mil_loss 1.70264
wandb:       eval/ensemble_f1 0.88575
wandb:            test/avg_f1 0.91253
wandb:      test/avg_mil_loss 0.24929
wandb:       test/ensemble_f1 0.91253
wandb:           train/avg_f1 0.83561
wandb:      train/ensemble_f1 0.83561
wandb:         train/mil_loss 0.98566
wandb:      train/policy_loss -0.21201
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.21201
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-30 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/rfezlh46
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100501-rfezlh46/logs
wandb: Agent Starting Run: mrlt6jtl with config:
wandb: 	actor_learning_rate: 0.00013060894072685226
wandb: 	attention_dropout_p: 0.41900361105134104
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 52
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3447417018451988
wandb: 	temperature: 1.7288312126997052
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100548-mrlt6jtl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-31
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mrlt6jtl
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▇█
wandb: best/eval_avg_mil_loss █▄▂▁
wandb:  best/eval_ensemble_f1 ▁▅▇█
wandb:            eval/avg_f1 ██▄██▅█▇▆▅█▇███▇▄▅███▁▇▆▃█▅███▆█▅▄▆▄████
wandb:      eval/avg_mil_loss ▁▁▆▂▁▅▃▁▂▃▆▁▂▁▁▁▆█▅▁▁▇▁▇▃▁▁▄▁▁█▂▁▅▃▄▁▁▁▁
wandb:       eval/ensemble_f1 █▄▇▇█▅█▇▆▅█▇▇▇███▇▄▅█▇█▁▇▆▃█▇██▄▆█▄▇▄███
wandb:           train/avg_f1 █▆▁▃█▅▅▄▄▅▄▅▁▅▇▇▇▅▅▅▁▅▇▅▇▇▅▃▃▄▄▅▃▄█▆▅▂▆▅
wandb:      train/ensemble_f1 █▆▁▃█▅▄▅▅▄▅▇█▇▇▅▅█▅▇▆▅▇▅▇▇▅▃▃▄▅▃▄█▆▆▅▂▆▅
wandb:         train/mil_loss ▂▆▃▂█▅▄▆▆▃▆▂▂▁▃▄▅▄▄▄▃▄▃▃▄▁▅▃▅▂▇▅▃▄▂▇▄▃▆▇
wandb:      train/policy_loss ▂▃▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▄▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▃▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92177
wandb: best/eval_avg_mil_loss 0.22018
wandb:  best/eval_ensemble_f1 0.92177
wandb:            eval/avg_f1 0.91758
wandb:      eval/avg_mil_loss 0.2638
wandb:       eval/ensemble_f1 0.91758
wandb:           train/avg_f1 0.82061
wandb:      train/ensemble_f1 0.82061
wandb:         train/mil_loss 0.91865
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run azure-sweep-31 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/mrlt6jtl
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100548-mrlt6jtl/logs
wandb: ERROR Run mrlt6jtl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: x9wscoco with config:
wandb: 	actor_learning_rate: 4.7445565870788856e-05
wandb: 	attention_dropout_p: 0.29136497919624466
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 113
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6348066445967524
wandb: 	temperature: 7.701365397441169
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100647-x9wscoco
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-32
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x9wscoco
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▅█
wandb: best/eval_avg_mil_loss █▅▄▁▆
wandb:  best/eval_ensemble_f1 ▁▂▃▅█
wandb:            eval/avg_f1 ▆▅▇▇▃▆▅▄▇▄▇▇▅▅▇▇▆▇▇▇▆▆▅▇▇▆▇█▅▇▃▁▆▇▆▇▆▇▄▅
wandb:      eval/avg_mil_loss ▁▂▂▁▂▁▄▁▁▁▂▄▂▁▄▂▁▅▁▁▂▂▂▁▁▁▁▁▄▁▁▂▁▄█▂▄▁▁▂
wandb:       eval/ensemble_f1 ▆▆▆▆▃▇▆▄▅▆▂▆▅▇▇▃▆▃▇▆▅▆▆▆▆▅▅▆▆█▆▆▆▆▇▄▁▆▁▃
wandb:           train/avg_f1 ▄▆▇▄▃▃▄▄▇▆▇▂█▇▂▆▆▅▆█▇▄▄█▆▆▄▅▅▆▇▇█▆██▆▄▆▁
wandb:      train/ensemble_f1 ▄▅▅▄▆▄▃▅▄▄▆▇▄▆▂▆▅▆▆▅▅▇▃▅▆▅█▇▅▆▆▆▄█▄▆▆█▆▁
wandb:         train/mil_loss ▄▁▁▇▂▄▂▅▂▄▃▂▅▂▄▂▅▁▄▁▃█▂▁▃▂▂▁▂▂▅▂▃▃▄▂▃▄▃▃
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅█▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9334
wandb: best/eval_avg_mil_loss 0.28038
wandb:  best/eval_ensemble_f1 0.9334
wandb:            eval/avg_f1 0.84651
wandb:      eval/avg_mil_loss 0.57225
wandb:       eval/ensemble_f1 0.84651
wandb:           train/avg_f1 0.8314
wandb:      train/ensemble_f1 0.8314
wandb:         train/mil_loss 0.37286
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run morning-sweep-32 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/x9wscoco
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100647-x9wscoco/logs
wandb: ERROR Run x9wscoco errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: v3b2qznr with config:
wandb: 	actor_learning_rate: 1.136096610560762e-06
wandb: 	attention_dropout_p: 0.309034836871534
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 199
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.23641854096569528
wandb: 	temperature: 9.796430350538031
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_100900-v3b2qznr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-33
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v3b2qznr
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇████
wandb: best/eval_avg_mil_loss █▄▁▁▁▁
wandb:  best/eval_ensemble_f1 ▁▇████
wandb:            eval/avg_f1 ▂▅▅▃▅▆▁▇▅▆▇██▅▅▇█▆█▁▄█▄▃▅▂▃▅▂▅▆▆█▄▆▇▅▆█▅
wandb:      eval/avg_mil_loss █▁▃▁▄▄▁▃▅▄▃▆▁▃▅▇▆▁▂▅▄▂▁▂▆▆▆▄▅▄▄▃▃▄▄▅▄▆▃▁
wandb:       eval/ensemble_f1 ▇▅▄▆▅▇▅▆▄▇▇▆▅▅▅▅▆▁▆▄▅█▄▅▄▆█▇▃▅▅▅█▄▇▃▇█▇▇
wandb:           train/avg_f1 ▅▄▆▁▁▃▃▆▇▂▅▅▄██▆▄▂▅▅▄▇▃▂▄▇▆▅▅▇▄▆▃▄▆▃▅▅▄▇
wandb:      train/ensemble_f1 █▆▃▇▄▅▆▇▇█▇▄▁▇▄▄▇▇▃▅▄▅█▇▅▄▆▅▇▆▆▆▆▃▄▇▆▇▅▇
wandb:         train/mil_loss ▄█▄▄▂▅▂▂▄▁▁▂▃▃▄▆▁▄▆▆▂▃▁▃▃▂▇▅▃▄▇▂▂▄▆▂▃▂▂▄
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▁▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████▁██▃██████████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9105
wandb: best/eval_avg_mil_loss 0.37715
wandb:  best/eval_ensemble_f1 0.9105
wandb:            eval/avg_f1 0.77651
wandb:      eval/avg_mil_loss 0.90894
wandb:       eval/ensemble_f1 0.77651
wandb:           train/avg_f1 0.79656
wandb:      train/ensemble_f1 0.79656
wandb:         train/mil_loss 0.83596
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run misunderstood-sweep-33 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/v3b2qznr
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_100900-v3b2qznr/logs
wandb: ERROR Run v3b2qznr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 03zc7z19 with config:
wandb: 	actor_learning_rate: 4.530445757213499e-05
wandb: 	attention_dropout_p: 0.30840319073930034
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 107
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.04854126734388997
wandb: 	temperature: 3.9323132988103513
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101206-03zc7z19
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-34
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/03zc7z19
wandb: uploading history steps 95-108, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄▅▆▆▆▇█
wandb: best/eval_avg_mil_loss █▆▄▃▃▂▂▁
wandb:  best/eval_ensemble_f1 ▁▄▅▆▆▆▇█
wandb:            eval/avg_f1 ▄▂▃▄▁▁▄▁▃▅▄▆▃▄▅▁▂▆▃▂▂▅▄▇▄█▄▄▄▄▂▄▂▁▇▄▅▄▆▅
wandb:      eval/avg_mil_loss █▆▅▄▅▃▇▅▇▆▃▆█▅▅█▄▅▇▅▃▃█▂▃▅▂▃▂▅█▁▂▅▃▄█▆▃▂
wandb:       eval/ensemble_f1 ▂▅▅▇▃▅▄▄▃▆▂▁▆▅▇▂▅▄▅▇▇▃▂▆▄█▂▇▅▃▄▄▂▃▂▄▄███
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▂▇▆▆▄▅▃▃▄▅▃▆▄▂▃▆▇▃▃▆▆▇█▃▅▅▃▇▄▇▆▄▄▆▄▃▅▆▅
wandb:      train/ensemble_f1 ▆▇▇▆▆█▂▂▁▃▆▃▆██▅▆▆▆▆▄▃▅▂▇▄██▄▆▄▄▆▄▅▃▇▅▁▇
wandb:         train/mil_loss ▃█▇▆▃▅▄▅▅▅█▃▇▄▇▄▂▃▇▅▃▃▃▃▆▃▅▄▄▂▂▂▁▆▆▃▅▅▅█
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████████████████▆▁█████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.79914
wandb: best/eval_avg_mil_loss 1.33932
wandb:  best/eval_ensemble_f1 0.79914
wandb:            eval/avg_f1 0.7257
wandb:      eval/avg_mil_loss 1.57509
wandb:       eval/ensemble_f1 0.7257
wandb:            test/avg_f1 0.66977
wandb:      test/avg_mil_loss 1.60117
wandb:       test/ensemble_f1 0.66977
wandb:           train/avg_f1 0.63603
wandb:      train/ensemble_f1 0.63603
wandb:         train/mil_loss 1.90535
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run electric-sweep-34 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/03zc7z19
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101206-03zc7z19/logs
wandb: Agent Starting Run: 715w72xy with config:
wandb: 	actor_learning_rate: 3.99994404755441e-06
wandb: 	attention_dropout_p: 0.2802917714864346
wandb: 	attention_size: 32
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.4422523799958602
wandb: 	temperature: 2.948299834952862
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101411-715w72xy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-35
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/715w72xy
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss ▆▂▁█
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 ▇█▁▆▇██▇▇████▇███▇▅▆▆▇██▅██████▇██▆███▇█
wandb:      eval/avg_mil_loss ▂▂▁▇▃▁▁▁▁▁▁▃▂▁▃▁▂▁▂▁▁▆▂▁▁▂▁▆▁▅▁▁▂▄▂▂█▁▂▁
wandb:       eval/ensemble_f1 ▇█▁▆▂██▆▇█▇███▆▇█████▇█▁█▇▇▇▇█▇▇▇██▅▇█▇█
wandb:           train/avg_f1 ▅▅▇▄▄▇▆▄▆▇▇▁▄▇▆▅▅▆▇▄▃▆▅▃█▇▆▆▅▇█▃▇█▂▇▃▆▄▄
wandb:      train/ensemble_f1 ▆▃▇▇▇▃▄▅▇▁▁██▇▁▆▇▆▃▆▄▃▆▇▄▆▂▇▄▄▄▇▇▄█▇▃▅▅▇
wandb:         train/mil_loss ▃▅▂▂▂▂▆▂▂▃▂▂▁▁▃▂▃▃▂▂▂▅▁▂▁▂█▆▂▂▂▃▂▂▂▁▂▆▃▃
wandb:      train/policy_loss ▄▄▄▄▄▄▄█▁▄▄▄▄▄▁▄██▄▄█▄▄▄▄▁▄▁█▄█▄▄▁█▄▄▁█▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄█▄▁▄▄▁▁▄▄▁▄▁▁▁▄▁█▄█▄▁▄▄▄▁█▄▄▄█▄█▄▄▄█▄█
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91758
wandb: best/eval_avg_mil_loss 0.37406
wandb:  best/eval_ensemble_f1 0.91758
wandb:            eval/avg_f1 0.91009
wandb:      eval/avg_mil_loss 0.29175
wandb:       eval/ensemble_f1 0.91009
wandb:           train/avg_f1 0.89094
wandb:      train/ensemble_f1 0.89094
wandb:         train/mil_loss 0.38325
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pious-sweep-35 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/715w72xy
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101411-715w72xy/logs
wandb: ERROR Run 715w72xy errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: f4zzs5mm with config:
wandb: 	actor_learning_rate: 2.235129365795481e-05
wandb: 	attention_dropout_p: 0.4500569794895986
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 106
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.828959215920253
wandb: 	temperature: 7.660654759929711
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101609-f4zzs5mm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-36
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f4zzs5mm
wandb: uploading wandb-summary.json
wandb: uploading history steps 92-106, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss ▅█▁
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 ▇█▂█▁▇██▇▃█▃▄███▂███▃▇█▇█▂█▇▇█████▇▇██▂█
wandb:      eval/avg_mil_loss ▁▁▂▆▁▁▁█▁▁▁▁▁▂█▁▁▁▁▆▆▆▁▁▁▁█▇▄▁▁██▆▂▁▁▅▁▁
wandb:       eval/ensemble_f1 █▇▇██▂▇██▃███▄███▂███▁▇█▃▇██▂▅██▁▇▂▇████
wandb:           train/avg_f1 ▂█▆▇█▂▆█▅▂▅▅▃▆▆▇▅▃▄▆▅▃▆▇▆▁▆▇█▆▅▁▇▅▄▇▇▃▅▆
wandb:      train/ensemble_f1 █▇▂▆▆▅▇▆▂▇▆▇▄▃▅▅▆▃▄█▆▇▁▆▇▄▄▇▁▆█▆▆▅▆█▃▇▇▇
wandb:         train/mil_loss ▄▃▇▄▄▃▃▃▁▄▄▁▇▃▃▄▂▁▃▃▅▁▇▄▂▅▁█▆▁▁▁▃▆▆▄▂▄▃▃
wandb:      train/policy_loss ▅▅▅▅▅█▁▁▅▅▁▅▅▅█▁▅▅▁▅▅▅█▁▅█▅▅▅▁█▅▁▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▁▄▄▄▄▄▄▄▄▁▄▄▄▁▄▄▄█▄▄▄▄▁▄█▁▄▄▄▄▄▄▄▄▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91758
wandb: best/eval_avg_mil_loss 0.26097
wandb:  best/eval_ensemble_f1 0.91758
wandb:            eval/avg_f1 0.90599
wandb:      eval/avg_mil_loss 0.3307
wandb:       eval/ensemble_f1 0.90599
wandb:           train/avg_f1 0.88443
wandb:      train/ensemble_f1 0.88443
wandb:         train/mil_loss 0.95228
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run northern-sweep-36 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/f4zzs5mm
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101609-f4zzs5mm/logs
wandb: ERROR Run f4zzs5mm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: vxyzowkn with config:
wandb: 	actor_learning_rate: 7.070974417718699e-06
wandb: 	attention_dropout_p: 0.432688363675597
wandb: 	attention_size: 16
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5019304916993966
wandb: 	temperature: 7.747278474238532
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_101742-vxyzowkn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-37
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vxyzowkn
wandb: uploading history steps 182-200, summary; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▇▇▇▇███
wandb: best/eval_avg_mil_loss ▆█▃▂▄▄▁▁▄
wandb:  best/eval_ensemble_f1 ▁▆▇▇▇▇███
wandb:            eval/avg_f1 █▇█▇▆▃▆▄███▄██▇▇▇▁██████▆▁▇█▃▆▁▇▇██▇████
wandb:      eval/avg_mil_loss ▂▅▁▂▄▂▇▂▇▁▂▆▂▃▅▂▅▁▂▁▂▁▂▁▂█▁▂▁▇▁▁▁▄▄▇▄▂▁▁
wandb:       eval/ensemble_f1 ▇▃▇█▅▆█▇▇▇██▁▁█▆▇█▇▁█▇███▇▇▇███▅▃▇█▆███▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▇▇▄▃▆▅█▆▆▅▂▄▆▅▃▅▅▂▇█▆▄▇▆▃▇▅▆▃▅▆▇▃▆▃▂▆▆▁
wandb:      train/ensemble_f1 ▃▇▅▇▆▇▅▆▄▇▆▇▆▃▆▄▆▆▄▇▇▇▅▅▃▆▃▄▆▅▄▅▄▅▆█▁▅▅▅
wandb:         train/mil_loss ▃▅▃▃▁▁▂▂▃▃▅▂▃▂▄▅▄▃▂▄▇▅▃▃▃▄▂▅▅▃▆▂▄▄▃▁▃▂█▃
wandb:      train/policy_loss ███████▁████████▁███████████▁████████▁██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅▅▅▅▅█▅█▅▅▅▅▅▅▁▅▅█▅▅▅▅██▃▅▅▅▁▅▅▅▅▅▅▅▅█▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.92542
wandb: best/eval_avg_mil_loss 0.29646
wandb:  best/eval_ensemble_f1 0.92542
wandb:            eval/avg_f1 0.78609
wandb:      eval/avg_mil_loss 0.45299
wandb:       eval/ensemble_f1 0.78609
wandb:            test/avg_f1 0.68674
wandb:      test/avg_mil_loss 1.47346
wandb:       test/ensemble_f1 0.68674
wandb:           train/avg_f1 0.78299
wandb:      train/ensemble_f1 0.78299
wandb:         train/mil_loss 0.43919
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run snowy-sweep-37 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/vxyzowkn
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_101742-vxyzowkn/logs
wandb: Agent Starting Run: 1tcc3fsc with config:
wandb: 	actor_learning_rate: 1.033491747945693e-06
wandb: 	attention_dropout_p: 0.43124816917601183
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 200
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.31708257846036003
wandb: 	temperature: 6.253959621927948
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102033-1tcc3fsc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-38
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1tcc3fsc
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▅██
wandb: best/eval_avg_mil_loss █▃▇▁▃
wandb:  best/eval_ensemble_f1 ▁▁▅██
wandb:            eval/avg_f1 █▃▂██▇█▂▇▃▂▃▃▂▄██▇▆▄███▁▂██▄▇█▄▂▁▂█▇▇█▃▇
wandb:      eval/avg_mil_loss ▄▃▁▄▃▄▅▃▄▁▁▂▂▅▃▁▅▁▃▃▅▁▄█▁▁▃▁▄▇▃▅▂▅▃▄▄▂▄▁
wandb:       eval/ensemble_f1 █▂█▁▃██▂█▃▂▇█▄▂▁▁██▃▇▇▄▃▄██▂███▇▇█▇▇▇▆▃█
wandb:           train/avg_f1 █▄▄▄▂▇▆▄▇▃▄▆▆▆▆▆▆▃▆▅▅▇▂▂▄▁▂▄▆█▃▂▄▆▄▅▂▄▆▄
wandb:      train/ensemble_f1 ▃▄██▄▆▇▄▆▃▄▆▆▆▃▇▃▃▆▅▅▅▅▆▅▂▅▁▁▄▃▆▄▅▄▅▆▆▆▅
wandb:         train/mil_loss ▄▄▅▄▃▁▆▅▅▃▅▆▇▆▅▇▁▅▅▆▃▅▃▄▄▃▅▄█▅▄▄▅▁▅▂▃▅▄▂
wandb:      train/policy_loss ███████████████████████▄████████████▁███
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ██████████████████████▁███████████▁█████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.32163
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.86131
wandb:      eval/avg_mil_loss 0.46047
wandb:       eval/ensemble_f1 0.86131
wandb:           train/avg_f1 0.77811
wandb:      train/ensemble_f1 0.77811
wandb:         train/mil_loss 0.48224
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run rural-sweep-38 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/1tcc3fsc
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102033-1tcc3fsc/logs
wandb: ERROR Run 1tcc3fsc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: wbnyf8k9 with config:
wandb: 	actor_learning_rate: 0.00030859192796312495
wandb: 	attention_dropout_p: 0.0004803304856913493
wandb: 	attention_size: 16
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 77
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.5358195160439596
wandb: 	temperature: 9.398759120274786
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102236-wbnyf8k9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-39
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wbnyf8k9
wandb: uploading history steps 67-77, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▃█
wandb: best/eval_avg_mil_loss ▇█▁
wandb:  best/eval_ensemble_f1 ▁▃█
wandb:            eval/avg_f1 ▆▄▆█▂▆▄▅▅▇▂▅▆▁▆▄▅▄▅▃▆▂▁▅▅▅▄▆▁▆▄▇▅▇███▆▄▄
wandb:      eval/avg_mil_loss ▃▂▄▅▄▃▅▃▄▄▄▃▅▄█▄▄█▅▂▄▅▄▅▂▁▄▃█▅▆▇▃▃▃▁▃▆▁▆
wandb:       eval/ensemble_f1 ▆▅▇▂▆▆▅▅█▅▅▆▄▄▆▄▆▆▃█▇▂▅▅▃▄▁▆▆▄█▅▇██▅▄█▆▄
wandb:           train/avg_f1 ▆▃▄▁▆▆█▄▃▄▅▄▂▅▅▄▄▆▃▄▅▂▅▅▃▅▇▄▆▆▇▅▇▅▆▅▂▅▇▆
wandb:      train/ensemble_f1 ▃▄▁▆▅▄▃▆▅▇█▅▂▆▆▄▇▅▅▄▄▂▅▃▂▆▄▂▃▆▁█▄▇▇▃▂▅█▃
wandb:         train/mil_loss ▅▄▅▅▄▅▆▄▃▅▆▆▅▄▆▆▃▄▅▄▄▆▂▅▅▄▅▄▃█▄▂▇▄▄▄▆▅▁▄
wandb:      train/policy_loss ██████████▇█████████▁████████████████▆██
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████▁██████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91414
wandb: best/eval_avg_mil_loss 0.28171
wandb:  best/eval_ensemble_f1 0.91414
wandb:            eval/avg_f1 0.72001
wandb:      eval/avg_mil_loss 1.52762
wandb:       eval/ensemble_f1 0.72001
wandb:           train/avg_f1 0.79769
wandb:      train/ensemble_f1 0.79769
wandb:         train/mil_loss 0.86893
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run wandering-sweep-39 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wbnyf8k9
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102236-wbnyf8k9/logs
wandb: ERROR Run wbnyf8k9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 582, in main_sweep
wandb: ERROR     policy_network = train(
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/attention_pham.py", line 517, in train
wandb: ERROR     policy_network.load_state_dict(torch.load(early_stopping.model_address))
wandb: ERROR   File "/gpfs/work5/0/prjs1491/Attention-based-RL-MIL/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
wandb: ERROR     raise RuntimeError(
wandb: ERROR RuntimeError: Error(s) in loading state_dict for AttentionPolicyNetwork_pham:
wandb: ERROR 	Missing key(s) in state_dict: "selector.transformations.0.0.weight", "selector.transformations.0.0.bias", "selector.transformations.0.2.weight", "selector.transformations.0.2.bias", "selector.transformations.1.0.weight", "selector.transformations.1.0.bias", "selector.transformations.1.2.weight", "selector.transformations.1.2.bias", "selector.transformations.2.0.weight", "selector.transformations.2.0.bias", "selector.transformations.2.2.weight", "selector.transformations.2.2.bias", "selector.heads.0.weight", "selector.heads.0.bias", "selector.heads.1.weight", "selector.heads.1.bias", "selector.heads.2.weight", "selector.heads.2.bias", "selector.heads.3.weight", "selector.heads.3.bias". 
wandb: ERROR 	Unexpected key(s) in state_dict: "attn_layer.weight", "attn_layer.bias". 
wandb: ERROR 
wandb: Agent Starting Run: 6yj8ovdx with config:
wandb: 	actor_learning_rate: 0.0006208028085899434
wandb: 	attention_dropout_p: 0.28330284692376195
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 112
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2490365503198273
wandb: 	temperature: 0.8716648545617511
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102409-6yj8ovdx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-40
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6yj8ovdx
wandb: uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇███
wandb: best/eval_avg_mil_loss ▃█▃▁▆
wandb:  best/eval_ensemble_f1 ▁▇███
wandb:            eval/avg_f1 ▅▇▇██▁▃▂█▄▂█▇▇▇▅▃▂█▃▄▇▃█▁█▆█▃▇█▃▇███▇▂▇▇
wandb:      eval/avg_mil_loss ▁▁█▄▄▁▃▁▅▄▂▁▅▁▄▂▁▁▁▂▄▂▁▁▁▄▁▁▁▇▂▁▁▃▃▄▄▁▃▅
wandb:       eval/ensemble_f1 ▅▇▇█▇█▅▃▁▃██▅▇▇▅▇▂█▅█▄▃▇█▇▇█▃▆▇▇██▂▇▂▇▄▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▄▃▅▃▇▇▆▁▄▆▅▄▃▆▅▅▄▄▃▇▅▇▇▆▅▆▅▆▆█▄▅▇▅▇▆▆▆
wandb:      train/ensemble_f1 ▂▃▃▁▂▆▄▇▄▆▂▄▅▆▃▂▁▅▅▄▇▃▅▇▇▄▅▄▇█▆▄▅▆▃▆▄▅▅▃
wandb:         train/mil_loss ▄▁▃▂▅▆▃▇▆▄▆█▃▄▃▃▆▃▂▆▂▄▃▃▇▂▂▂▆▄▂▃▄▂▂▂▁▃▄▄
wandb:      train/policy_loss ████████████████▁███████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █████████████████▁███████████████▁██████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91009
wandb: best/eval_avg_mil_loss 0.38301
wandb:  best/eval_ensemble_f1 0.91009
wandb:            eval/avg_f1 0.8392
wandb:      eval/avg_mil_loss 0.77681
wandb:       eval/ensemble_f1 0.8392
wandb:            test/avg_f1 0.83139
wandb:      test/avg_mil_loss 0.47082
wandb:       test/ensemble_f1 0.83139
wandb:           train/avg_f1 0.80398
wandb:      train/ensemble_f1 0.80398
wandb:         train/mil_loss 0.87775
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run curious-sweep-40 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6yj8ovdx
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102409-6yj8ovdx/logs
wandb: Agent Starting Run: qehpkxah with config:
wandb: 	actor_learning_rate: 3.7549978265600065e-06
wandb: 	attention_dropout_p: 0.08213410503815488
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 124
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.2615767933125235
wandb: 	temperature: 4.60970241714177
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102547-qehpkxah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-41
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qehpkxah
wandb: uploading history steps 121-125, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▃▃▆▇█
wandb: best/eval_avg_mil_loss █▄▅▃▄▂▁
wandb:  best/eval_ensemble_f1 ▁▂▃▃▆▇█
wandb:            eval/avg_f1 ▇▇▄▆▅▃▇▇▄▇▅▅▇▅▂▆▇▆▆▆▇▅█▄▇▆▆▆▆▆▇▇▇▆▁▇▆▅▄▁
wandb:      eval/avg_mil_loss ▂▃█▁▅▄▆▂▁▄▂▂▆▂▂▂▂▄▁▂▁▁▁▂▂▄▃▁▂▂▁▆▂▄▁▄▁▁▁▄
wandb:       eval/ensemble_f1 ▅█▄▆▇▃▂▇▇█▇▆▆▇▆▆▇▆█▅▆▇▆▄▇▇▅█▅▆▆▅▅▁▇█▆▇▆▅
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▃▅▂▄▄▄▁▅▂█▆▆▃▆▆▂▃▅▅▄▄▄▆▇▂▂▆▆▄▅█▃▅▂▅▆▆▆▆▃
wandb:      train/ensemble_f1 ▄▆▃▅▃▃▃▄▅▃▆▆▅▄▇▂▇▃▃▄▁▄▃▅▅▄▇▂▆▅▃█▇█▄▆▆▄▆▄
wandb:         train/mil_loss ▅▄▅▇▆▄▅▃▂▆▂▄▅▅▄▃▇▇▅▃▄▃▃▃▇▆▄▃▇▆▃▆▁▃▇▄▁█▃▂
wandb:      train/policy_loss ▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▃▅▄▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▄█▁▄▄▁▁▁▄▁▁▄█▁▄▁██▁▁▄█▁▁▄█▁▄▁▄▄▄▁▄▁███▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91497
wandb: best/eval_avg_mil_loss 0.25838
wandb:  best/eval_ensemble_f1 0.91497
wandb:            eval/avg_f1 0.81018
wandb:      eval/avg_mil_loss 0.63645
wandb:       eval/ensemble_f1 0.81018
wandb:            test/avg_f1 0.83272
wandb:      test/avg_mil_loss 0.51427
wandb:       test/ensemble_f1 0.83272
wandb:           train/avg_f1 0.84322
wandb:      train/ensemble_f1 0.84322
wandb:         train/mil_loss 0.31378
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run olive-sweep-41 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qehpkxah
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102547-qehpkxah/logs
wandb: Agent Starting Run: pgwmhwwf with config:
wandb: 	actor_learning_rate: 0.00011361533731642604
wandb: 	attention_dropout_p: 0.38767662019673393
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 126
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8208958393364526
wandb: 	temperature: 6.378091182938381
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102811-pgwmhwwf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-42
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pgwmhwwf
wandb: uploading wandb-summary.json
wandb: uploading history steps 90-103, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁█
wandb: best/eval_avg_mil_loss █▁
wandb:  best/eval_ensemble_f1 ▁█
wandb:            eval/avg_f1 ██▃▂▄█▇██▃███████▁███▂▇▇█▃▄▂█▇▂██▂██▃███
wandb:      eval/avg_mil_loss ▁▁▆▂▄▁▁▄▁▁▁▁▁▁▁▁▁█▁▁▁▅▄▅▁▄▁▅▁▅▁▁▁▁▄▅▁▅▁▁
wandb:       eval/ensemble_f1 ███▂▃████▁████▃▁█▄████▂██▁▇█▁█▁█████▂███
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▁▆▆▃▃▃▄▃▆█▄▃██▆▆▄█▆▄▇▃▄▃▇▂▅▄▆▇▄▃▄▇▁▇▃▄▁▃
wandb:      train/ensemble_f1 ▅▄▆▅▄▂▆▇▅▄▇▅▆▇▃▄▄▆▆▆▂▃▅▄▄▃▄▃▆▅▇▆▆▁█▃▇▁▂▃
wandb:         train/mil_loss ▁▅▄▂█▃▂▄▂▅▄▅▂▃▅▂▄▄▁▂▄▁▄▂▃▄▄▂▅▁▃▅▂▁▃▄▃▄▂▂
wandb:      train/policy_loss ▅▅█▅█▅▅█▅▁▅▁█▅▁▅▅▅▅▁▅▅▅▅▅▁▅▅▁▅▅▅█▅█▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▅▅█▅▅▁▅▅▅▅██▅▁▅▅▅▅▅▅▅▅▅█▅▅▁▅▅▅██▅▅█▅▅▅▅▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9145
wandb: best/eval_avg_mil_loss 0.2857
wandb:  best/eval_ensemble_f1 0.9145
wandb:            eval/avg_f1 0.88936
wandb:      eval/avg_mil_loss 0.38856
wandb:       eval/ensemble_f1 0.88936
wandb:            test/avg_f1 0.90066
wandb:      test/avg_mil_loss 0.24405
wandb:       test/ensemble_f1 0.90066
wandb:           train/avg_f1 0.78804
wandb:      train/ensemble_f1 0.78804
wandb:         train/mil_loss 0.49216
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run azure-sweep-42 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/pgwmhwwf
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102811-pgwmhwwf/logs
wandb: Agent Starting Run: ucyqfgpi with config:
wandb: 	actor_learning_rate: 2.259112612448718e-05
wandb: 	attention_dropout_p: 0.1695637734840344
wandb: 	attention_size: 64
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 129
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.17347085215997515
wandb: 	temperature: 0.7872084217682607
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_102945-ucyqfgpi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-43
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ucyqfgpi
wandb: uploading history steps 121-130, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▇▇█
wandb: best/eval_avg_mil_loss █▄▄▁
wandb:  best/eval_ensemble_f1 ▁▇▇█
wandb:            eval/avg_f1 ██▆▇▇▇█▆▇█▂▇▆▆▅▆██▂▇▅▆▇▅▆▇▆▃▆▆▅▅▅▆█▅█▆▁▆
wandb:      eval/avg_mil_loss ▇▆▅█▆█▅▇▆▅▅▃▄▅▄▆▅▃▅▃▃▅▅▄▄▄▂▆▄▄█▄▆▃▃▃▅▁▄▄
wandb:       eval/ensemble_f1 ▅█▆█▆█▇▇▇▇▇▇▇▂▇█▆█▇▆▅▇▅▇▇▆▆█▆▅▄▇▇▇▅▆█▇▁▆
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▅▆▅▂▁▄▆▅▄▆▅▄▄▄▃▃▄▃▃▆▃▄▄▅▅█▅▄▄▆▃▃▇▄▄█▄▆▃
wandb:      train/ensemble_f1 ▅▄▃▆▅▃▃▅▄▅▆▄▅▆▅▁▆▅▅▅▄▆▃▄▆▅▅▅▁▄▄▅▅▆▅▅▅▅█▂
wandb:         train/mil_loss █▆▃▂▅▃█▄▄▆▄▆▄▁▁▁▄▂▃▃▂▃▄▂▄▃▄▄▄▂▅▃▃▅▃▃▂▅▁▂
wandb:      train/policy_loss ▅▁▅█▅▅█▅█▁█▅▅████▁▅██▁▅▅▅▁▁█▅▅▅▁█▅▁█▁▅█▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91778
wandb: best/eval_avg_mil_loss 0.24038
wandb:  best/eval_ensemble_f1 0.91778
wandb:            eval/avg_f1 0.9105
wandb:      eval/avg_mil_loss 0.23804
wandb:       eval/ensemble_f1 0.9105
wandb:            test/avg_f1 0.92999
wandb:      test/avg_mil_loss 0.16677
wandb:       test/ensemble_f1 0.92999
wandb:           train/avg_f1 0.8907
wandb:      train/ensemble_f1 0.8907
wandb:         train/mil_loss 0.29849
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run super-sweep-43 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/ucyqfgpi
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_102945-ucyqfgpi/logs
wandb: Agent Starting Run: wt4zdyzk with config:
wandb: 	actor_learning_rate: 2.076153959846433e-05
wandb: 	attention_dropout_p: 0.32040277317712185
wandb: 	attention_size: 64
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 160
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.6478552940762307
wandb: 	temperature: 6.603092398229919
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103215-wt4zdyzk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-44
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wt4zdyzk
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆█
wandb: best/eval_avg_mil_loss █▇▆▁
wandb:  best/eval_ensemble_f1 ▁▅▆█
wandb:            eval/avg_f1 █▆▅███▅▇▇▁█████▇▂▅▆▃▁▃▆▆███▁▇▇█▇█▆█▄▆▇▅█
wandb:      eval/avg_mil_loss ▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▄▃█▁▁▁▅▁▁▁▁▃▄▁▁▁▁▁▁▁▅▅▁▁
wandb:       eval/ensemble_f1 █▇▅█▆▇█▆▆▅▇█▇▇▅█▆▇█▆▇█▁▅████▇█▇▅▅▆▁█▄▅▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▄█▆▄▃▇▆▅▂▆▄▆▅▆▄▂▅▂▅▄▄▁▂▆▇▇▆▄▃▃▆▆▆▄▄▃▃▆▂
wandb:      train/ensemble_f1 ▅▅▅▆▆▆▅▄▇▅▆▂▇▂▇▇▂▆▇▁▅▆▆▇▄▄▂▆█▃▆▆▂▅▃▄▄▅▅▅
wandb:         train/mil_loss ▄▁▆█▂▄▆▅▂▃▂▅▄▅▄▄▃▁▅▄▄▄▃▄▄▇▄▆▃▆█▂▂▃▆▃▃▁▁▂
wandb:      train/policy_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91467
wandb: best/eval_avg_mil_loss 0.2755
wandb:  best/eval_ensemble_f1 0.91467
wandb:            eval/avg_f1 0.90623
wandb:      eval/avg_mil_loss 0.31229
wandb:       eval/ensemble_f1 0.90623
wandb:            test/avg_f1 0.76637
wandb:      test/avg_mil_loss 0.53058
wandb:       test/ensemble_f1 0.76637
wandb:           train/avg_f1 0.86299
wandb:      train/ensemble_f1 0.86299
wandb:         train/mil_loss 0.38927
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sleek-sweep-44 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/wt4zdyzk
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103215-wt4zdyzk/logs
wandb: Agent Starting Run: n7u4u86s with config:
wandb: 	actor_learning_rate: 0.0001109121520402896
wandb: 	attention_dropout_p: 0.1756132181188629
wandb: 	attention_size: 16
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 154
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.02756771749448328
wandb: 	temperature: 2.146075720357784
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103436-n7u4u86s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-45
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n7u4u86s
wandb: uploading history steps 101-113, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▆▆█
wandb: best/eval_avg_mil_loss █▁▄▃
wandb:  best/eval_ensemble_f1 ▁▆▆█
wandb:            eval/avg_f1 ▇▁█▇▇▇▇█▄▇▅█▇██▆▇▅▇▃▅███▃▁█▂▇▂▇▇██▅▇▇███
wandb:      eval/avg_mil_loss ▂▂▃▇▂▂▂█▂▁▁▁▃▁▂▂▁▁▁▂▁▃▂▁▆▃▁▂▂▂▂▁▂▁▁▂▂▂▁▁
wandb:       eval/ensemble_f1 ▅▁▇▄█▇▇▇▇▇▇█▇█████▇▅▃▄▇██▇▇██▇▇▇▇█████▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▇▆▄▄▇▅▃▄▅▆▃▆▃▅▇▆▅▅▇▆▇▇▆▆▁▁▅▇▄▃▆▇█▄▅▅▃▅▄
wandb:      train/ensemble_f1 ▇▅▃▇▃▃█▇█▆▃▅▅▇▄▄█▃▇▅▆▆▅▆▇▅▇▅▄▁▆▅▆▆▃▅▅▅▅▆
wandb:         train/mil_loss ▂▄▄▅▂▄▃▃▅▅▅▄▂▄▆▂▄▃▃▄▁▁▃█▁▅▆▂▁█▂▃▂▃▁▁▃▃▆▂
wandb:      train/policy_loss ▅▅▅▅▅▁▁▁▅▅▅▅█▅▅▅▁▅▅▅▅▅▁▅▅▅█▅█▅▅▅██▅▅█▁▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▁▁▅▅▅▁▁▅▁▅▅█▅▅▅▅▅▅▁▅▅▁█▅▅▁▅▅█▅▅█▅▁▅▅█▁▅▅
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91758
wandb: best/eval_avg_mil_loss 0.31876
wandb:  best/eval_ensemble_f1 0.91758
wandb:            eval/avg_f1 0.91373
wandb:      eval/avg_mil_loss 0.31141
wandb:       eval/ensemble_f1 0.91373
wandb:            test/avg_f1 0.91375
wandb:      test/avg_mil_loss 0.20094
wandb:       test/ensemble_f1 0.91375
wandb:           train/avg_f1 0.85705
wandb:      train/ensemble_f1 0.85705
wandb:         train/mil_loss 0.29956
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run vocal-sweep-45 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/n7u4u86s
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103436-n7u4u86s/logs
wandb: Agent Starting Run: db9p3obd with config:
wandb: 	actor_learning_rate: 6.980222390300796e-06
wandb: 	attention_dropout_p: 0.4488454451028573
wandb: 	attention_size: 128
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 148
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.8533224232639253
wandb: 	temperature: 7.029805398067582
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103624-db9p3obd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-46
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/db9p3obd
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▇█
wandb: best/eval_avg_mil_loss ▁█▁▁
wandb:  best/eval_ensemble_f1 ▁▂▇█
wandb:            eval/avg_f1 ▇█▁▂█▇▆▁▄▄▅█▇█▂▃██▃▆▆▇███▄██▁▇▅▂▇██▇▂▃██
wandb:      eval/avg_mil_loss ▅▁▁▅▁▃▁▁▅▁▂▁▃█▅▅▃▃▄▂▃▃▁▁▁▄▄▄▄▁▁▄▂▁▁▂▄▃▃▅
wandb:       eval/ensemble_f1 █▁▂▇██▁▇▇▅▇▁██▂▇▂▇██▅▄▄█▇█▇▇▅▁▂▆██▇▁▇▃█▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▆█▄▄▆▅▃▆▁▅█▆▇▅▆█▅▄▇▆▇▄▆▇▄▇█▅▅▄▅▆▄▆▇▅▆▆▅
wandb:      train/ensemble_f1 ▆▇▇▄▇▆▅▁▅▅▆▆██▆▆█▅▇▇▅▇▇▇▄▇▅▇▆▅▂▇▅▆▆▄▅▅█▄
wandb:         train/mil_loss ▄▄▄▆▂▇▅▆▄▂▄▆▂▂▄▃▄▅▂▂▃▃▃█▃▃█▁▃▃▇▂▃▆▂▁▂▁▃▅
wandb:      train/policy_loss ▅██▅▁█▅▁█▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▁▅█▁▁▅▁█▅▅▅▅▅▅▅
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▁█▄▄▄▄▄▄▄▄▄▄▄▁██▄█▄▄█▄▄▄▄▄▁▄▁▄▄▄▄▄▁▄█▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9216
wandb: best/eval_avg_mil_loss 0.25173
wandb:  best/eval_ensemble_f1 0.9216
wandb:            eval/avg_f1 0.69623
wandb:      eval/avg_mil_loss 0.94122
wandb:       eval/ensemble_f1 0.69623
wandb:            test/avg_f1 0.84076
wandb:      test/avg_mil_loss 0.74324
wandb:       test/ensemble_f1 0.84076
wandb:           train/avg_f1 0.77701
wandb:      train/ensemble_f1 0.77701
wandb:         train/mil_loss 0.88859
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run faithful-sweep-46 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/db9p3obd
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103624-db9p3obd/logs
wandb: Agent Starting Run: 6xezg0u4 with config:
wandb: 	actor_learning_rate: 0.0001898112177850988
wandb: 	attention_dropout_p: 0.10886442800668387
wandb: 	attention_size: 128
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 190
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.3430278759072731
wandb: 	temperature: 0.7276997666780338
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_103833-6xezg0u4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-47
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6xezg0u4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▅▆▇██
wandb: best/eval_avg_mil_loss ▅█▂▃▂▁
wandb:  best/eval_ensemble_f1 ▁▅▆▇██
wandb:            eval/avg_f1 ▆▇▁█▇▅▆▇▅▇▇▅▄▅▅▇▆▇▅█▇▃▆▅██▇▅▁▄▆▅▅▅▅▄▆▆▄▃
wandb:      eval/avg_mil_loss ▂▁▄▃▁▁▄▅▅▄▂█▇▂▃▄▆▁▂▁▇█▄▅▆▁▆▂▆▁▄▇▇▅▄▃▁▇▃▂
wandb:       eval/ensemble_f1 ▇█▃▅▆▇▇▄▄▇█▆▇▆▇▅▅▄▇█▆██▇▅▇▂▆▁▄▇▇▂█▃▄▆▄▄▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▄▃▁▃▄▄▃▄▅▅▆▅▅▂▅▆▅▅▄▆▅▄▃▅▅▅▄▇▅▄█▃▂▃▄▄▅▄▃
wandb:      train/ensemble_f1 ▄▆▇▄▆▃▇▆▃▅▅▆▇▆▇▆▆▆▇▁▄▆▆▅▅▃▄▆▇▅█▅█▃▇▃▇▆▄▇
wandb:         train/mil_loss ▄▃▂▇██▆▅▆▂▂▃▅▆▆▃▆▄▇▅▁▆▅▇▇▃▄▆▄▄▆▆▆▇▄▂▁▇▃▅
wandb:      train/policy_loss ███████████▂█▁██████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ████████████▁███████████████████████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91414
wandb: best/eval_avg_mil_loss 0.24059
wandb:  best/eval_ensemble_f1 0.91414
wandb:            eval/avg_f1 0.88992
wandb:      eval/avg_mil_loss 0.3445
wandb:       eval/ensemble_f1 0.88992
wandb:            test/avg_f1 0.79867
wandb:      test/avg_mil_loss 0.87831
wandb:       test/ensemble_f1 0.79867
wandb:           train/avg_f1 0.81119
wandb:      train/ensemble_f1 0.81119
wandb:         train/mil_loss 0.57603
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run sweet-sweep-47 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/6xezg0u4
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_103833-6xezg0u4/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: naxvs5ee with config:
wandb: 	actor_learning_rate: 4.06420046141033e-06
wandb: 	attention_dropout_p: 0.2262776387108461
wandb: 	attention_size: 32
wandb: 	batch_size: 128
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 144
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.12966451983246874
wandb: 	temperature: 5.228748566385927
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104049-naxvs5ee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-48
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/naxvs5ee
wandb: uploading history steps 134-145, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▂▄▇▇▇██
wandb: best/eval_avg_mil_loss ██▃▂▂▁▁▁
wandb:  best/eval_ensemble_f1 ▁▂▄▇▇▇██
wandb:            eval/avg_f1 ▃▄▇▆██▆▇▇▅█▄▇▅█▆▅▆▇▃▄▄▆▅▇▅█▁▇▄▆█▄▅▅█▇▄▄▁
wandb:      eval/avg_mil_loss ▄▂▅▂▄▇▃▁▅▁█▁▄▇▆▁▅▅▁▄▄▃▆▁▄▄▃▅▄▆▅▇▂▃▆▁▃▂▃▁
wandb:       eval/ensemble_f1 ▃▃▂▄▂▁██▆██▅▇█▂▃▁▂▅▆▆▇▄▅▅█▆▇▄▁█▇█▅█▇▄▅▇▄
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▄▅▃▄▅▄▃▃▅▅▆▅▁▄▄▅▆▇▅▅▄▆█▄▅▃▇▅▃▇▆▄▇▅▅▃▆▅▅▇
wandb:      train/ensemble_f1 █▃▂▅▅▆▃▂▂▅▁▃▅▅▂▃▆█▅▁▃▅▅▅▃▇█▆▆▅▁▅▂▇▃▃▅█▇▃
wandb:         train/mil_loss ▄▅▆▅▃▁▄▄▁▆▆▇█▆▇▅▆▂▅▅▆▅▃▂▆▅▄▅▅▆▅▇█▃▄▅▃▄▄▁
wandb:      train/policy_loss █▄████████▁███████████████▃█████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss █▆████████▁██████████████████▆██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.9216
wandb: best/eval_avg_mil_loss 0.2772
wandb:  best/eval_ensemble_f1 0.9216
wandb:            eval/avg_f1 0.48369
wandb:      eval/avg_mil_loss 2.57349
wandb:       eval/ensemble_f1 0.48369
wandb:            test/avg_f1 0.9141
wandb:      test/avg_mil_loss 0.20458
wandb:       test/ensemble_f1 0.9141
wandb:           train/avg_f1 0.71885
wandb:      train/ensemble_f1 0.71885
wandb:         train/mil_loss 0.88137
wandb:      train/policy_loss 0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run pious-sweep-48 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/naxvs5ee
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104049-naxvs5ee/logs
wandb: Agent Starting Run: dv2ms1f2 with config:
wandb: 	actor_learning_rate: 4.378907464089375e-05
wandb: 	attention_dropout_p: 0.10226360897753112
wandb: 	attention_size: 64
wandb: 	batch_size: 256
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 198
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.20600354421128075
wandb: 	temperature: 0.3350067126409795
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104305-dv2ms1f2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-49
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dv2ms1f2
wandb: uploading wandb-summary.json
wandb: uploading history steps 162-174, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▁▂▇▇▇██
wandb: best/eval_avg_mil_loss ▆█▅▃▃▃▁▁
wandb:  best/eval_ensemble_f1 ▁▁▂▇▇▇██
wandb:            eval/avg_f1 █▆█▅▆▅▆▆▅█▆█▆▆██▁▇▅▆▆▇▆█▆▆▁▁▁▆▇▇█▃▇▇▇█▇▇
wandb:      eval/avg_mil_loss ▃▄▁▂▂█▂▁▂▂▂▂▂▂▇▁▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▅▁▁▂▂▁
wandb:       eval/ensemble_f1 ▆▆▅▆▃▆██▆█▆▆▆▁█▅█▆▆█▆▆▇▁█▆▆▃█▇██▆▁▇▇█▇▇▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▇▅▅▄▆▄▇▂▃▃▅▄▄▅▇▂▆▆▁▅▆▅██▆▇▂▆▄▅▇▇▇▂▇█▅▅▇▃
wandb:      train/ensemble_f1 ▆▅▆▅▄▆▅▄█▄▅▇▅▇▅▆▁█▆▆▇▆▇▇▄▇▇▇▆▆▇█▆▅█▇▄▂▇█
wandb:         train/mil_loss ▁▇▇▁▄▂▅▇▅▅▆▁▁▄▁▄▁▅▃▂▆▁▂▁▁▂▂▂▁▂▂▃▂▁▃▄▅█▆▄
wandb:      train/policy_loss ███████▁████████████████████████████████
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ▄▄▄▄▄▄▄█▄▄█▄▄▄▄▄▄▄█▄▄▁▄▄▄▄▄▄▄▁▄▁▄▁▄▄▁▄▁▄
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91432
wandb: best/eval_avg_mil_loss 0.30718
wandb:  best/eval_ensemble_f1 0.91432
wandb:            eval/avg_f1 0.86861
wandb:      eval/avg_mil_loss 0.4446
wandb:       eval/ensemble_f1 0.86861
wandb:            test/avg_f1 0.9141
wandb:      test/avg_mil_loss 0.22377
wandb:       test/ensemble_f1 0.9141
wandb:           train/avg_f1 0.85017
wandb:      train/ensemble_f1 0.85017
wandb:         train/mil_loss 0.40587
wandb:      train/policy_loss -0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss -0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run deft-sweep-49 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/dv2ms1f2
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104305-dv2ms1f2/logs
wandb: Agent Starting Run: qydbowxw with config:
wandb: 	actor_learning_rate: 2.2923234612065392e-05
wandb: 	attention_dropout_p: 0.387620016998815
wandb: 	attention_size: 32
wandb: 	batch_size: 64
wandb: 	critic_learning_rate: 0
wandb: 	dropout_p: 0.5
wandb: 	early_stopping_patience: 100
wandb: 	epochs: 108
wandb: 	hdim: 8
wandb: 	learning_rate: 1e-06
wandb: 	reg_coef: 0.069447903117411
wandb: 	temperature: 5.0798805185721605
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/work5/0/prjs1491/Attention-based-RL-MIL/wandb/run-20250602_104535-qydbowxw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-50
wandb: ⭐️ View project at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: 🧹 View sweep at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/sweeps/vqaof6bn
wandb: 🚀 View run at https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qydbowxw
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:       best/eval_avg_f1 ▁▄█
wandb: best/eval_avg_mil_loss █▃▁
wandb:  best/eval_ensemble_f1 ▁▄█
wandb:            eval/avg_f1 █▃▇▂▇▆▂▅▇▅▅▇▅▆▇▇▅▇▄▆▄▅▃▅▁█▆▇▅▆█▆▃▇▅▅▂▆█▂
wandb:      eval/avg_mil_loss ▁▁▂▂▃▂▂▁▂▃▁▁▂▁▁▁▁█▃▁▂▃▁▁▂▄▄▁▃▁▁▁▂▃▁▃▁▃▄▁
wandb:       eval/ensemble_f1 ▆▇▅▂▃▅▆▅▇▅▆▆▇▄▅▇▄▇▄▅▄▆▂▅███▅▁▄█▆▅▇▅█▅█▆▇
wandb:            test/avg_f1 ▁
wandb:      test/avg_mil_loss ▁
wandb:       test/ensemble_f1 ▁
wandb:           train/avg_f1 ▅▂▄▃▅▃▆▆▅▆▅▄▄▄▃▄▂▅▅▄▅▄▅▅▆█▄▄▅▄▄▅▅▅▅▆▅▄▃▁
wandb:      train/ensemble_f1 ▆▂▆▅▆▃▅▅▄▃▆▆▅▃▅▃▄▅▅▄▅▅▄█▅▅▄▄▄▄▆▆▇▅▅▅▅▃▃▁
wandb:         train/mil_loss ▅▅▄▂▃▃▇▁▄▄▂▄▅▂█▃▅▂▇▃▁▅▁▂▃▄▅▃▅▃▅▁▃▁▇▃▂▂▃▄
wandb:      train/policy_loss ▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/reg_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train/total_loss ███▁█████████████████████████▁██████████
wandb:       train/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:       best/eval_avg_f1 0.91778
wandb: best/eval_avg_mil_loss 0.25547
wandb:  best/eval_ensemble_f1 0.91778
wandb:            eval/avg_f1 0.73752
wandb:      eval/avg_mil_loss 1.64114
wandb:       eval/ensemble_f1 0.73752
wandb:            test/avg_f1 0.92191
wandb:      test/avg_mil_loss 0.20698
wandb:       test/ensemble_f1 0.92191
wandb:           train/avg_f1 0.77671
wandb:      train/ensemble_f1 0.77671
wandb:         train/mil_loss 0.56591
wandb:      train/policy_loss 0.0
wandb:         train/reg_loss 0
wandb:       train/total_loss 0.0
wandb:       train/value_loss 0
wandb: 
wandb: 🚀 View run ethereal-sweep-50 at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis/runs/qydbowxw
wandb: ⭐️ View project at: https://wandb.ai/ninabraakman-university-of-amsterdam/MasterThesis
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250602_104535-qydbowxw/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
